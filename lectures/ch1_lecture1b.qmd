---
title: "Intro to linear systems"
lightbox: true
jupyter: python3
execute:
  cache: false
lecture_day: 1
publish: true
readings: "Ch. 1.1-1.3"
colab-link: '<a target="_blank" href="https://colab.research.google.com/github/KBurbank/stat24320/blob/main/lectures/ch1_lecture1b.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>'
---

# Linear systems


## What are linear systems?  

-   Set of one or more linear equations involving the same variables.
-   Each equation can be be put in form $a_1 x_1 + a_2 x_2 + \dots + a_n x_n+ b = 0$
  
. . .

$$
\begin{cases}
a_{11} x_1 + a_{12} x_2 +\dots + a_{1n} x_n = b_1 \\
a_{21} x_1 + a_{22} x_2  + \dots + a_{2n} x_n = b_2 \\ 
\vdots\\
a_{m1} x_1 + a_{m2} x_2 + \dots + a_{mn} x_n = b_m,
\end{cases}
$$

## Example: railroad cars

A chemical manufacturer wants to lease a fleet of 24 railroad tank cars with a combined carrying capacity of 520,000 gallons.

-   Tank cars with three different carrying capacities are available:
    -   8,000 gallons
    -   16,000 gallons
    -   24,000 gallons.
-   How many of each type of tank car should be leased?

::: notes
Can write this as $x \times 8+y \times 16 + z \times 24=520$ and also $x+y+z=24$. We won't have a single solution... Two equations, three unknowns.
:::


## Example: traffic flow. 

::: columns
::: column
![](https://cdn.mathpix.com/cropped/2024_02_28_3151daba57b91c8468a9g-37.jpg?height=403&width=504&top_left_y=2250&top_left_x=469)

-   Numbers = \# of vehicles/hr that enter and leave on that street.
-   $x_{1}, x_{2}, x_{3}$, and $x_{4}$: flow of traffic between the four intersections.
:::

::: column
-   Number of vehicles entering each intersection should always equal the number leaving. E.g.:
    -   1,500 vehicles enter the intersection of 5th Street and Washington Avenue each hour
    -   $x_{1}+x_{4}$ vehicles leave this intersection
    -   $\rightarrow$ $x_{1}+x_{4}=1,500$
-   Find the traffic flow at each of the other three intersections.
-   What is the \# of vehicles that travel from Washington Avenue to Lincoln Avenue on 5th Street?
:::
:::

::: notes
$x_1+x_2=1200$, and $x_2+x_3=1000$, and $x_3+x_4=1300$.
:::

## Example: US population

-   The U.S. population was approximately 75 million in 1900, 150 million in 1950, and 275 million in 2000.
-   Find a quadratic equation whose graph passes through the points $(0,75)$, $(50,150)$, $(100,275)$

. . .

Call the years $(0, 50, 100)$ $t_i$. Call the populations $(75,150,275)$ $y_i$.

Then we have

$$
\begin{cases}
c_1 t_1 + c_2 t_1^2 = y_1 \\
c_1 t_2 + c_2 t_2^2 = y_2 \\
c_1 t_3 + c_2 t_3^2 = y_3
\end{cases}
$$ 

. . .

Make the substitutions $c_i \rightarrow x_i$ and $t_i \rightarrow a_{1i}$ and $t^2_i \rightarrow a_{2i}$, then this becomes

$$
\begin{cases}
x_1 a_{11} + x_2 a_{21} =a_{11} x_1  + a_{21} x_2  = y_1 \\
x_1 a_{12} + x_2 a_{22} =a_{12} x_1  +a_{22} x_2 = y_2 \\
x_1 a_{13} + x_2 a_{23} = a_{13} x_1  + a_{23} x_2  = y_3
\end{cases}
$$ 

We see that we now have a standard system of linear equations.

# Solving linear systems

## Goals for solving algorithms

An algorithm should be:

-   feasible
-   accurate
    -   stable
-   efficient
    -   reusable computations

## Example: very simple linear system

$$
\begin{gather*}
2 x-y=1 \\
4x+4y=20 . \tag{1.5}
\end{gather*}
$$

::: notes
We can solve this by hand: first we might add four times the first row to the second, so that we get (4+8)x + 0y = 24, so that we realize x = $2$, and then we have 4-y=1 so y = 3.
:::

. . .

Make an *augmented matrix* to represent the problem:

$$
\begin{aligned}
& x \quad y=r . h . s . \\
& {\left[\begin{array}{rrr}
2 & -1 & 1 \\
4 & 4 & 20
\end{array}\right]}
\end{aligned}
$$

. . .

Manipulate the augmented matrix to solve the problem...

## Elementary Matrix Operations

When we do these on augmented matrices, the solutions are unchanged...

1.  $E_{i j}$ : **Swap**: Switch the $i$th and $j$ th rows of the matrix.
2.  $E_{i}(c)$ : **Scale**: Multiply the $i$th row by the nonzero constant $c$.
3.  $E_{i j}(d)$ : **Add**: Add $d$ times the $j$th row to the $i$th row.

## Reduced Row Echelon Form

Every matrix can be reduced by a sequence of elementary row operations to one and only one reduced row echelon form:

-   Nonzero rows of $R$ precede the zero rows.
-   Column numbers of the leading entries of the nonzero rows, say rows $1,2, \ldots, r$, form an increasing sequence of numbers $c_{1}<c_{2}<\cdots<c_{r}$.
-   Each leading entry is a 1.
-   Each leading entry has only zeros above it.

## Use elementary ops to get reduced row echelon form

### Gauss-Jordan elimination

1.  Swap rows to get non-zero number in row 1, column
2.  Get a **1** in the row 1, column 1.
3.  Make all other entries in column 1 **0**.
4.  Swap rows to get non-zero number in row 2, column 2. Make this entry **1**. Make all other entries in column 2 **0**.
5.  Repeat step 4 for row 3, column 3. Continue moving along the main diagonal until you reach the last row, or until the number is zero.

. . .

Vocabulary: process of getting a 1 in a location, and then making all other entries zeros in that column, is **pivoting**.

The number that is made a 1 is called the pivot element, and the row that contains the pivot element is called the **pivot row**.

## G-J Elimination for our toy system

$$
\left[\begin{array}{rrr}
4 & 4 & 20 \\
2 & -1 & 1
\end{array}\right] \overrightarrow{E_{1}(\frac{1}{4}}\left[\begin{array}{lll}
1 & 1 & 5 \\
2 & -1 & 1
\end{array}\right] \overrightarrow{E_{12}(-2)}\left[\begin{array}{lll}
1 & 1 & 5 \\
0 & -3 & -9
\end{array}\right]
$$


and then...

. . .


$$
\left[\begin{array}{rrr}
1 & 1 & 5 \\
0 & -3 & -9
\end{array}\right] \overrightarrow{E_{2}(-1 / 3)}\left[\begin{array}{lll}
1 & 1 & 5 \\
0 & 1 & 3
\end{array}\right] \overrightarrow{E_{12}(-1)}\left[\begin{array}{lll}
1 & 0 & 2 \\
0 & 1 & 3
\end{array}\right] .
$$

. . .

Writing this back as a linear system, we have:

$$
\begin{gather*}
2 x-y=1 \\
4 x+4 y=20 
\end{gather*}  \Longrightarrow {\begin{aligned}
& 1 \cdot x+0 \cdot y=2 \\
& 0 \cdot x+1 \cdot y=3
\end{aligned}}  \Longrightarrow {\begin{aligned}x &= 2 \\ y &= 3 \end{aligned}}
$$

## Now you try: birds in a tree

There are 2 trees in a garden (tree "A" and "B") and in both trees are some birds.

The birds of tree A say to the birds of tree B that if one of you comes to our tree, then our population will be double yours.

Then the birds of tree B tell the birds of tree A that if one of you comes here, then our population will be equal to yours.

How many birds in each tree?

(Solve by making an augmented matrix and doing G-J elimination.)

::: notes
The answer is 7 birds in tree A and 5 birds in tree B.
:::


::: aside
https://www.mathsisfun.com/puzzles/birds-in-trees.html
:::


## Example: Mining

::: aside
Meyer Ch 1.5
:::

A mine produces *silica*, *iron*, and *gold*

Needs **Money** (in \$\$), **operating time** (in hours), and **labor** (in person-hours).

-   1 pound of silica needs: \$.0055, . 0011 hours of operating time, and .0093 hours of labor.

-   1 pound of iron needs: \$.095, . 01 operating hours, and .025 labor hours.

-   1 pound of gold needs: \$ 960, 112 operating hours, and 560 labor hours.

## 

Suppose that during 600 hours of operation, exactly \$ 5000 and 3000 labor-hours are used.

How much silica ($x$), iron ($y$), and gold ($z$) were produced?

. . .

Set up the linear system whose solution will yield the values for $x, y$, and $z$.

. . .

$$
\begin{aligned}
.0055 x + .095 y + 960 z &=  5000 \quad \text{(dollars)}\\
.0011 x + .01 y + 112 z &= 600 \quad \text{(operating hours)}\\
.0093 x + .025 y + 560 z &= 3000\quad \text{(labor hours)}
\end{aligned}
$$

. . .

Make the *augmented matrix*:

$$
\left[\begin{array}{@{}ccc|c@{}}
.0055 & .095 & 960 & 5000 \\
 .0011 & .01  & 112 & 600 \\
 .0093 & .025 & 560 & 3000
\end{array}\right]
$$

::: notes
Draw the line on the augmented matrix.
:::

## 

We can solve this using Gauss-Jordan elimination.

```{python}
from sympy.matrices import Matrix, eye, zeros, ones, diag, GramSchmidt
from sympy.core.numbers import Number
from sympy import print_latex, latex,N
def round_expr(expr, num_digits):
    return expr.xreplace({n : round(n, num_digits) for n in expr.atoms(Number)})
```

```{python}
# Do the Gauss-Jordan elimination
M=Matrix([[.0055,.095,960,5000],[.0011,.01,112,600],[.0093,.025,560,3000]])
round_level = 2;
M2=N(M.elementary_row_op('n->n+km', row=1,row2=0, k=-(M[1,0]/M[0,0])),round_level)
M3=N(M2.elementary_row_op('n->n+km', row=2,row2=0, k=-(M2[2,0]/M2[0,0])),round_level)
M4=N(M3.elementary_row_op('n->n+km', row=2,row2=1, k=-(M3[2,1]/M3[1,1])),round_level)
```

```{python}
from IPython.display import Markdown
def pp(x):
  print("$"+latex(x)+"$")
def ppd(x): 
  print("$$"+latex(x)+"$$")
def mm(x):
  Markdown(latex(x))
```

## Getting into row echelon form, rounding after 3 digits

$$`{python} Markdown(latex(M))` \overrightarrow{E_{12}(\frac{-1}{5})} `{python} Markdown(latex(M2))`$$

$$\overrightarrow{E_{13}(\frac{93}{55})} `{python} Markdown(latex(M3))`$$

$$\overrightarrow{E_{23}(\frac{-14}{.9})} `{python} Markdown(latex(M4))`$$

## 

```{python}
soln=M4[0:3,0:3].solve(rhs=M4[:,3])
```

This has solutions $`{python} Markdown(latex(soln))`$.

. . .

How do these compare to the exact solutions? These are

```{python}
M[0:3,0:3].solve(rhs=M[:,3])
```

## Doing it again, rounding after 15 digits

```{python}
# Do the Gauss-Jordan elimination
M=Matrix([[.0055,.095,960,5000],[.0011,.01,112,600],[.0093,.025,560,3000]])
round_level = 15;
ml = latex(M)
M2=N(M.elementary_row_op('n->n+km', row=1,row2=0, k=-(M[1,0]/M[0,0])),round_level)
M3=N(M2.elementary_row_op('n->n+km', row=2,row2=0, k=-(M2[2,0]/M2[0,0])),round_level)
M4=N(M3.elementary_row_op('n->n+km', row=2,row2=1, k=-(M3[2,1]/M3[1,1])),round_level)
```

$`{python} Markdown(latex(M))` \rightarrow `{python} Markdown(latex(M2))`$

$\rightarrow `{python} Markdown(latex(M3))` \rightarrow `{python} Markdown(latex(M4))`$

```{python}
soln2=M4[0:3,0:3].solve(rhs=M4[:,3])
```

This has solutions $`{python} Markdown(latex(soln2))`$.

Reminder, the exact solution was:
```{python}
M[0:3,0:3].solve(rhs=M[:,3])
```

# Comparing the G-J algorithm with our goals

## Reminder of goals

An algorithm should be:

::: nonincremental
-   feasible
-   accurate
    -   stable
-   efficient
    -   reusable computations
:::


# Roundoff errors

## Roundoff errors with G-J elimination

Try this on your calculator. What do you get?

$$
\left(\left(\frac{2}{3}+100\right)-100\right)-\frac{2}{3}\stackrel{?}{=}0
$$

. . .

Now try this...

$$
\left(\left(\frac{2}{3}+100\right)-100\right)-\frac{2}{3}\stackrel{?}{=}0
$$

::: notes
On my calculator, I get 3.33e-32. That's a small number, but not zero!

What is happening is that 2/3 = 0.666666...., and your calculator represents this as a very long string but eventually puts in a 7.

Then when it adds the 100, it has to drop a few of those digits in its representation. They are then gone! When we subtract the 100, we don't get them back again.
:::

##

-   Let $\epsilon$ be a number so small that our calculator yields $1+\epsilon=1$.

-   With this calculator, $1+1 / \epsilon=(\epsilon+1) / \epsilon=1 / \epsilon$

-   Want to solve the linear system

-   $$
      \begin{align*}
      \epsilon x_{1}+x_{2} & =1\\
      x_{1}-x_{2} & =0 .
      \end{align*}
    $$




## Roundoff errors with G-J elimination {.scrollable}

With our calculator,

::: notes
In the second step, we are replacing $-1+\frac{1}{\epsilon}$ with just $\frac{1}{\epsilon}$. It's a case where we are adding together numbers of very different scale.
:::

$$
    \left[\begin{array}{rrr}
    \epsilon & 1 & 1 \\
    1 & -1 & 0
    \end{array}\right] \overrightarrow{E_{21}\left(-\frac{1}{\epsilon}\right)}\left[\begin{array}{ccc}
    \epsilon & 1 & 1 \\
    0 & \frac{1}{\epsilon} & -\frac{1}{\epsilon}
    \end{array}\right] \overrightarrow{E_{2}(\epsilon)}\left[\begin{array}{ccc}
    \epsilon & 1 & 1 \\
    0 & 1 & 1
    \end{array}\right] \\ \overrightarrow{E_{12}(-1)}\left[\begin{array}{ccc}
    \epsilon & 0 & 0 \\
    0 & 1 & 1
    \end{array}\right]  \overrightarrow{E_{1}\left(\frac{1}{\epsilon}\right)}\left[\begin{array}{lll}
    1 & 0 & 0 \\
    0 & 1 & 1
    \end{array}\right] .
$$

. . .

Calculated solution: $x_{1}=0, x_{2}=1$

Correct answer should be

$$ x_{1}=x_{2}=\frac{1}{1+\epsilon}=0.999999099999990 \ldots$$

## Sensitivity to small changes

Problem arose because we took a computational step where we added two numbers of very different scale, essentially losing the smaller number.

Led to a big changes in output!

There is no general cure for these difficulties...

Want to be *aware* of them, know when we are doing computations that might be susceptible.

## Partial pivoting

We can improve performance of G-J by introducing a new step into the algorithm:

1.  Find the entry in the left column [with the largest absolute value]{style="color:red;"}. This entry is called the pivot. Perform row interchange (if necessary), so that *the pivot* is in the first row.

2.  Use a row operation to get a 1 as the entry in the first row and first column.

3.  Use row operations to make all other entries as zeros in column one.

4.  Interchange rows if necessary to obtain a nonzero number [with the largest absolute value]{style="color:red;"} in the second row, second column. Use a row operation to make this entry 1. Use row operations to make all other entries as zeros in column two.

5.  Repeat step 4 for row 3, column 3. Continue moving along the main diagonal until you reach the last row, or until the number is zero.

::: notes
Full pivoting is where we also move the columns around to get the largest element to the front.

" A square matrix 𝐴 has an 𝐿𝑈 factorization (without pivoting) if, and only if, no zero is encountered in the pivot position when computing an 𝐿𝑈 factorization of 𝐴. However, when does computations using floating point numbers a pivot that is nearly zero can lead to dramatic rounding errors. The simple workaround is to always permute the rows of the matrix such that the largest nonzero entry in a column is chosen as the pivot entry. This ensures that a nearly zero is never chosen. Complete pivoting goes even further by using row and column permutations to select the largest entry in the entire matrix as the pivot entry." from [here](https://math.stackexchange.com/questions/1535376/complete-pivoting-vs-partial-pivoting-in-gauss-elimination)

It's very rare to find yourself in a situation where you need complete pivoting, and sometimes it can be quite computationally expensive.
:::

## Using partial pivoting in our example

::: notes
work this one out by hand
:::

# Ill-conditioned linear systems

## Ill-conditioned linear systems

::: notes
Sometimes it's not the procedure that introduces the susceptibility to small changes, but the actual problem itself. Then, small differences in the inputs lead to big differences in the *exact solutions*.
:::

::: r-fit-text
A system of linear equations is said to be **ill-conditioned** when some small perturbation in the system (in the $b$s) can produce relatively large changes in the exact solution (in the $x$'s). Otherwise, the system is said to be **well-conditioned**.
:::

## Ill-conditioned linear systems

::: aside
Meyer Ch 1.6
:::

::: columns
::: {.column width="49%"}
Consider 
$$
\begin{aligned}
& .835 x+.667 y=.168, \\
& .333 x+.266 y=.067,
\end{aligned}
$$

Exact solution:

$$
x=1 \quad \text { and } \quad y=-1 .
$$
:::

::: {.column width="49%"}
::: fragment
But if we change just one digit...

$$
\begin{aligned}
& .835 x+.667 y=.168, \\
& .333 x+.266 y=.06\color{red}{6},
\end{aligned}
$$

Now exact solution:

$$
\hat{x}=-666 \quad \text { and } \quad \hat{y}=834
$$
:::
:::
:::

::: notes
What's going on here? One thing to notice is that the lines corresponding to solutions for each equation have very similar slopes: `r .667/.835` vs `r .266/.33`.
:::

## 

What if $b_1$ and $b_2$ are the results of an experiment, need to be read off a dial? Suppose:

-   dial can be read to tolerance of $\pm .001$,
-   values for $b_{1}$ and $b_{2}$ are read as .168 and .067, respectively. Then the exact solution is

::: fragment

$$
\begin{equation*}
(x, y)=(1,-1) 
\end{equation*}
$$

:::

-   But due to uncertainty, we have

::: fragment

$$
\begin{equation*}
.167 \leq b_{1} \leq .169 \quad \text { and } \quad .066 \leq b_{2} \leq .068
\end{equation*}
$$

:::

## 

::: {r-fit-text}
| $b_1$ | $b_2$ | $x$  | $y$   |
|-------|-------|------|-------|
| .168  | .067  | 1    | -1    |
| .167  | .068  | 934  | -1169 |
| .169  | .066  | -932 | 1169  |

: Possible readings {.striped .hover}
:::

## Geometrical interpretation

If two straight lines are almost parallel and if one of the lines is moved only slightly, then the point of intersection is drastically altered.

![](https://cdn.mathpix.com/cropped/2024_02_23_364f690b3cef9227e7c9g-038.jpg?height=457&width=922&top_left_y=280&top_left_x=477)

The point of intersection is the solution of the associated $2 \times 2$ linear system, so this is also drastically altered.

## 

-   Often in real life, coefficients are empirically obtained
-   Will be off from "true" values by small amounts
-   For ill-conditioned systems, this means that solutions can be very far off from true solutions
-   We'll cover techniques for quantifying conditioning, later in quarter
-   For now, can just try making small changes to some coefficients. Big changes in result? Ill-conditioned system!

# Example: Polynomial interpolation

## Polynomial interpolation

::: aside
https://colab.research.google.com/github/quantecon/lecture-julia.notebooks/blob/main/tools_and_techniques/iterative_methods_sparsity.ipynb#scrollTo=9aa1791b
:::

Suppose we'd like to find a polynomial that can interpolate a given function.

Take points 𝑥0,…𝑥𝑁 and values 𝑦0,…𝑦𝑁

Simple way: finding the coefficients \$ c_1, \ldots c_n \$ where

$$
P(x) = \sum_{i=0}^N c_i x^i
$$

. . .

This is a system of equations:

$$
\begin{array}
    cy_0 = c_0 + c_1 x_0 + \ldots c_N x_0^N\\
    \,\ldots\\
    \,y_N = c_0 + c_1 x_N + \ldots c_N x_N^N
\end{array}
$$

## Polynomial interpolation

Or, stacking, $c = \begin{bmatrix} c_0 & \ldots & c_N\end{bmatrix}$, $y = \begin{bmatrix} y_0 & \ldots & y_N\end{bmatrix}$ , and

$$
A = \begin{bmatrix} 1 & x_0 & x_0^2 & \ldots &x_0^N\\
                    \vdots & \vdots & \vdots & \vdots & \vdots \\
                    1 & x_N & x_N^2 & \ldots & x_N^N
    \end{bmatrix}
$$ 

Let's try solving this for a simple function, $y=exp(x)$.

## Polynomial interpolation

$$
x*y
$$


```{python}
import numpy as np
import matplotlib.pyplot as plt

# We are introducing a small error into b. Does it have a big impact on the x's we find?
def approx(n_points,jiggle=0):
  x = np.linspace(1,12,num=n_points)
  #xshuff = x+np.random.normal(size=n_points,scale=jiggle)
  y = np.exp(x)+np.exp(x/4)*np.random.normal(size=n_points,scale=jiggle)
  # Get matrix of exponents of x values => A
  n = len(x)
  A = np.zeros([n,n])
  for i in range(n):
    A[::,i] = np.power(x.T,i)
  b = y

  # Solve Ax=b linear eq. system to get 
  s = np.linalg.solve(A, b)
  # where x denotes coeffs of polynomial in reverse order
  # Flip polynomial coeffs
  s = np.flip(s,axis=0)
  # Print polynomial coeffs
  return s, x, y, A
s, x, y, A = approx(4)
# Evaluate polynomial at X axis and plot result
```

We'll start with picking 4 interpolating points: $\vec x = `{python} Markdown(latex(x.T))`$. Then we have our matrix A:

```{python}
np.set_printoptions(suppress=True)

```

$$A = \begin{bmatrix} 1 & x_0 & x_0^2 & \ldots &x_0^N\\
                    \vdots & \vdots & \vdots & \vdots & \vdots \\
                    1 & x_N & x_N^2 & \ldots & x_N^N
    \end{bmatrix} = `{python} Markdown(latex(Matrix(A)))` $$

Our $y$ values are $\vec y = `{python} Markdown(latex(y))`$. We could solve this using Gauss-Jordan elimination, or here the solving algorithm built into Numpy.

```{python}
def make_plots(s, x, y, A):
  x_axis = np.linspace(np.min(x), np.max(x), num=5000)
  y_axis = np.polyval(s, x_axis)
  y_pred = np.polyval(s,x)
  plt.clf();
  plt.plot(x_axis, y_axis);
  plt.title("n = " + str(len(x)));
  plt.plot(x,y,'ro');
  plt.plot(x_axis,np.exp(x_axis));
  plt.show();
  
def error_norm(s,x,y,A):
    y_pred = np.polyval(s,x)
    return(np.linalg.norm(y_pred-np.exp(x),np.inf))
```

## Results for n=4 and n=10 points

::: columns
::: column
```{python}
_ = plt.figure(figsize=(6,6));
make_plots(*approx(4,jiggle=1000));
```
:::

::: column
::: fragment
```{python}
_ = plt.figure(figsize=(6,6));
make_plots(*approx(15,jiggle=1000));
```
:::
:::
:::

## Max error for different values of n

```{python}
my_error = []
for i in range(1,21):
  my_error.append(error_norm(*approx(i)))
```

```{python}
plt.clf()
plt.plot(my_error)
plt.xlabel("Number of interpolating points")
plt.ylabel("Maximum absolute error")
plt.show()
```

## The problem: linear dependency

```{python}
import matplotlib.pyplot as plt
import numpy as np

x = np.arange(0, 1.1, 0.1)
plt.plot(x, x, marker='o', linestyle='-', color='black', label='n=1')

colors = plt.cm.rainbow(np.linspace(0, 1, 6))

for i in range(1, 7):
  plt.plot(x, x**i, marker='o', linestyle='-', color=colors[i-1], label=f'n={i+1}')

plt.title("x^n for n=1 to 7")
plt.legend()
plt.show()
```

::: notes
As n gets higher, $x^n$ gets closer to $x^{n+1}$ (at least in the range $(0,1)$). This means that a solution of $a_n x^n + a_{n+1} x^{n+1}$ will be very similar to e.g. $a_{n+1} x^n + a_{n} x^{n+1}$
:::