{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ch2 Lecture 4\n",
        "\n",
        "# *MCMC*\n",
        "\n",
        "## *Idea of MCMC*\n",
        "\n",
        "# *Restricted Boltzmann Machine*\n",
        "\n",
        "## *Intro to the idea of a Restricted Boltzmann Machine*\n",
        "\n",
        "## *Math of the RBM*\n",
        "\n",
        "From\n",
        "https://ml-lectures.org/docs/unsupervised_learning/ml_unsupervised-1.html\n",
        "\n",
        "![](https://cdn.mathpix.com/cropped/2024_04_03_f85fd2de2d1ba12d877ag-1.jpg?height=327&width=612&top_left_y=1105&top_left_x=434)\n",
        "\n",
        ". . .\n",
        "\n",
        "States are determined by an **energy function**\n",
        "$E(\\mathbf{v}, \\mathbf{h})$. $$\n",
        "E(\\mathbf{v}, \\mathbf{h})=-\\sum_{i} a_{i} v_{i}-\\sum_{j} b_{j} h_{j}-\\sum_{i j} v_{i} W_{i j} h_{j} \n",
        "$$\n",
        "\n",
        ". . .\n",
        "\n",
        "Then the probability distribution is given by the Boltzmann\n",
        "distribution:\n",
        "\n",
        "$P_{\\mathrm{rbm}}(\\mathbf{v}, \\mathbf{h})=\\frac{1}{Z} e^{-E(\\mathbf{v}, \\mathbf{h})}$\n",
        "where $Z=\\sum_{\\mathbf{v}, \\mathbf{h}} e^{-E(\\mathbf{v}, \\mathbf{h})}$\n",
        "\n",
        "## \n",
        "\n",
        "The probability of a visible vector $\\mathbf{v}$ is given by\n",
        "marginalizing over the hidden variables:\n",
        "\n",
        "$$\n",
        "\\begin{equation*}\n",
        "P_{\\mathrm{rbm}}(\\mathbf{v})=\\sum_{\\mathbf{h}} P_{\\mathrm{rbm}}(\\mathbf{v}, \\mathbf{h})=\\frac{1}{Z} \\sum_{h} e^{-E(\\mathbf{v}, \\mathbf{h})}\n",
        "\\end{equation*}\n",
        "$$\n",
        "\n",
        "Conveniently, this gives each visible unit an **independent**\n",
        "probability of activation:\n",
        "\n",
        "$$\n",
        "P_{\\mathrm{rbm}}\\left(v_{i}=1 | \\mathbf{h}\\right)=\\sigma\\left(a_{i}+\\sum_{j} W_{i j} h_{j}\\right), \\quad i=1, \\ldots, n_{\\mathrm{v}}\n",
        "$$\n",
        "\n",
        "## \n",
        "\n",
        "The same is true for hidden units, given the visible units:\n",
        "\n",
        "$$\n",
        "\\begin{equation*}\n",
        "P_{\\mathrm{rbm}}\\left(h_{j}=1 | \\mathbf{v}\\right)=\\sigma\\left(b_{j}+\\sum_{i} v_{i} W_{i j}\\right) \\quad j=1, \\ldots, n_{\\mathrm{h}}\n",
        "\\end{equation*}\n",
        "$$\n",
        "\n",
        "## *Training*\n",
        "\n",
        "Consider a set of binary input data $\\mathbf{x}_{k}, k=1, \\ldots, M$,\n",
        "drawn from a probability distribution $P_{\\text {data }}(\\mathbf{x})$.\n",
        "\n",
        "Goal: tune the parameters $\\{\\mathbf{a}, \\mathbf{b}, W\\}$ such that\n",
        "after training\n",
        "$P_{\\mathrm{rbm}}(\\mathbf{x}) \\approx P_{\\mathrm{data}}(\\mathbf{x})$.\n",
        "\n",
        ". . .\n",
        "\n",
        "To do this, we need to be able to estimate $P_{\\mathrm{rbm}}$!\n",
        "\n",
        "Unfortunately, this is often intractable, because it requires\n",
        "calculating the partition function $Z$.\n",
        "\n",
        "## *Details of the training*\n",
        "\n",
        "We want to maximize the log-likelihood of the data under the model: $$\n",
        "L(\\mathbf{a}, \\mathbf{b}, W)=-\\sum_{k=1}^{M} \\log P_{\\mathrm{rbm}}\\left(\\mathbf{x}_{k}\\right)\n",
        "$$\n",
        "\n",
        ". . .\n",
        "\n",
        "Take derivatives of this with respect to the parameters, and use\n",
        "gradient descent:\n",
        "\n",
        "$$\n",
        "\\begin{equation*}\n",
        "\\frac{\\partial L(\\mathbf{a}, \\mathbf{b}, W)}{\\partial W_{i j}}=-\\sum_{k=1}^{M} \\frac{\\partial \\log P_{\\mathrm{rbm}}\\left(\\mathbf{x}_{k}\\right)}{\\partial W_{i j}}\n",
        "\\end{equation*}\n",
        "$$\n",
        "\n",
        "## \n",
        "\n",
        "The derivative has two terms: $$\n",
        "\\begin{equation*}\n",
        "\\frac{\\partial \\log P_{\\mathrm{rbm}}(\\mathbf{x})}{\\partial W_{i j}}=x_{i} P_{\\mathrm{rbm}}\\left(h_{j}=1 |\\mathbf{x}\\right)-\\sum_{\\mathbf{v}} v_{i} P_{\\mathrm{rbm}}\\left(h_{j}=1 | \\mathbf{v}\\right) P_{\\mathrm{rbm}}(\\mathbf{v}) \n",
        "\\end{equation*}\n",
        "$$\n",
        "\n",
        ". . .\n",
        "\n",
        "Use this to update the weights:\n",
        "\n",
        "$$\n",
        "W_{i j} \\rightarrow W_{i j}-\\eta \\frac{\\partial L(a, b, W)}{\\partial W_{i j}}\n",
        "$$\n",
        "\n",
        "Problem: the second term in the derivative is intractable! It has\n",
        "$2^{n_{\\mathrm{v}}}$ terms:\n",
        "\n",
        "$$ \n",
        "\\sum_{\\mathbf{v}} v_{i} P_{\\mathrm{rbm}}\\left(h_{j}=1 | \\mathbf{v}\\right) P_{\\mathrm{rbm}}(\\mathbf{v}) \n",
        "$$\n",
        "\n",
        "Instead, we will use **Gibbs sampling** to estimate\n",
        "$P_{\\mathrm{rbm}}(\\mathbf{v})$.\n",
        "\n",
        "## *Gibbs sampling to the rescue*\n",
        "\n",
        "Input: Any visible vector $\\mathbf{v}(0)$\n",
        "\n",
        "Output: Visible vector $\\mathbf{v}(r)$\n",
        "\n",
        "for: $n=1 \\backslash$ dots $r$\n",
        "\n",
        "$\\operatorname{sample} \\mathbf{h}(n)$ from\n",
        "$P_{\\mathrm{rbm}}(\\mathbf{h} \\mathbf{v}=\\mathbf{v}(n-1))$\n",
        "\n",
        "sample $\\mathbf{v}(n)$ from\n",
        "$P_{\\mathrm{rbm}}(\\mathbf{v} \\mathbf{h}=\\mathbf{h}(n))$ end\n",
        "\n",
        "## *Using an RBM*\n",
        "\n",
        "![](https://cdn.mathpix.com/cropped/2024_04_03_f85fd2de2d1ba12d877ag-5.jpg?height=645&width=1285&top_left_y=122&top_left_x=146)\n",
        "\n",
        "# *LU Factorization*\n",
        "\n",
        "## \n",
        "\n",
        "Suppose we want to solve a nonsingular linear system $A x=b$ repeatedly,\n",
        "with different choices of $b$.\n",
        "\n",
        ". . .\n",
        "\n",
        "![](https://cdn.mathpix.com/cropped/2024_02_16_adbe27593182d62240abg-017.jpg?height=411&width=612&top_left_y=508&top_left_x=453)\n",
        "\n",
        "$$\n",
        "\\begin{equation*}\n",
        "-y_{i-1}+2 y_{i}-y_{i+1}=\\frac{h^{2}}{K} f\\left(x_{i}\\right), i=1,2, \\ldots, n \n",
        "\\end{equation*}\n",
        "$$\n",
        "\n",
        ". . .\n",
        "\n",
        "Perhaps you want to experiment with different functions for the heat\n",
        "source term.\n",
        "\n",
        ". . .\n",
        "\n",
        "What do we do? Each time, we create the augmented matrix\n",
        "$\\widetilde{A}=[A \\mid b]$, then get it into reduced row echelon form.\n",
        "\n",
        ". . .\n",
        "\n",
        "Each time change $b$, we have to redo all the work of Gaussian or\n",
        "Gauss-Jordan elimination !\n",
        "\n",
        ". . .\n",
        "\n",
        "Especially frustrating because the main part of our work is the same:\n",
        "putting the part of $\\widetilde{A}$ corresponding to the coefficient\n",
        "matrix $A$ into reduced row echelon form.\n",
        "\n",
        "## *LU Factorization: Saving that work*\n",
        "\n",
        "Goal: Find a way to record our work on $A$, so that solving a new system\n",
        "involves very little additional work.\n",
        "\n",
        "LU Factorization: Let $A$ be an $n \\times n$ matrix. An LU factorization\n",
        "of $A$ is a pair of $n \\times n$ matrices $L, U$ such that\n",
        "\n",
        "1.  $L$ is lower triangular.\n",
        "2.  $U$ is upper triangular.\n",
        "3.  $A=L U$.\n",
        "\n",
        ". . .\n",
        "\n",
        "Why is this so wonderful? Triangular systems $A \\mathbf{x}=\\mathbf{b}$\n",
        "are easy to solve.\n",
        "\n",
        "Remember: If $A$ is upper triangular, we can solve for the last\n",
        "variable, then the next-to-last variable, etc.\n",
        "\n",
        "## *Solving an upper triangular system*\n",
        "\n",
        "Let’s say we have the following system:\n",
        "\n",
        "$A x = b$ where A is the upper-triangular matrix\n",
        "$A = \\begin{bmatrix} 2 & 1 & 0 \\\\ 0 & 1 & -1 \\\\ 0 & 0 & -1 \\end{bmatrix}$,\n",
        "and we want to solve for\n",
        "$b = \\begin{bmatrix} 1 \\\\ 1 \\\\ -2 \\end{bmatrix}$.\n",
        "\n",
        ". . .\n",
        "\n",
        "We form the augmented matrix\n",
        "$\\widetilde{A} = [A | b] = \\begin{bmatrix} 2 & 1 & 0 & | & 1 \\\\ 0 & 1 & -1 & | & 1 \\\\ 0 & 0 & -1 & | & -2 \\end{bmatrix}$.\n",
        "\n",
        ". . .\n",
        "\n",
        "Back substitution:\n",
        "\n",
        "1.  Last equation: $-x_3 = -2$, so $x_3 = 2$.\n",
        "2.  Substitute this value into the second equation, $x_2 - x_3 = 1$, so\n",
        "    $x_2 = 3$.\n",
        "3.  Finally, we substitute $x_2$ and $x_3$ into the first equation,\n",
        "    $2x_1 + x_2 = 1$, so $x_1 = -1$.\n",
        "\n",
        "## *Solving a lower triangular system*\n",
        "\n",
        "If $A$ is lower triangular, we can solve for the first variable, then\n",
        "the second variable, etc.\n",
        "\n",
        "Let’s say we have the following system:\n",
        "\n",
        "$A y = b$ where A is the lower-triangular matrix\n",
        "$A = \\begin{bmatrix} 1 & 0 & 0 \\\\ -1 & 1 & 0 \\\\ 1 & 2 & 1 \\end{bmatrix}$,\n",
        "and we want to solve for\n",
        "$b = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix}$.\n",
        "\n",
        ". . .\n",
        "\n",
        "We form the augmented matrix\n",
        "$\\widetilde{A} = [A | b] = \\begin{bmatrix} 1 & 0 & 0 & | & 1 \\\\ -1 & 1 & 0 & | & 0 \\\\ 1 & 2 & 1 & | & 1 \\end{bmatrix}$.\n",
        "\n",
        ". . .\n",
        "\n",
        "Forward substitution:\n",
        "\n",
        "1.  First equation: $y_1 = 1$.\n",
        "2.  Substitute this value into the second equation, $-y_1 + y_2 = 0$, so\n",
        "    $y_2 = 1$.\n",
        "3.  Finally, we substitute $y_1$ and $y_2$ into the third equation,\n",
        "    $y_1 + 2y_2 + y_3 = 1$, so $y_3 = -2$.\n",
        "\n",
        ". . .\n",
        "\n",
        "This was just as easy as solving the upper triangular system!\n",
        "\n",
        "## *Solving $A x = b$ with LU factorization*\n",
        "\n",
        "Now suppose we want to solve $A x = b$ and we know that $A = L U$. The\n",
        "original system becomes $L U x = b$.\n",
        "\n",
        "Introduce an intermediate variable $y = U x$. Our system is now\n",
        "$L y = b$. Now perform these steps:\n",
        "\n",
        "1.  **Forward solve**: Solve lower triangular system $L y = b$ for the\n",
        "    variable $y$.\n",
        "2.  **Back solve**: Solve upper triangular system $U x = y$ for the\n",
        "    variable $x$.\n",
        "3.  This does it!\n",
        "\n",
        ". . .\n",
        "\n",
        "Once we have the matrices $L, U$, the right-hand sides only come when\n",
        "solving the two triangular systems. Easy!\n",
        "\n",
        "## *Example*\n",
        "\n",
        "You are given that\n",
        "\n",
        "$$\n",
        "A=\\left[\\begin{array}{rrr}\n",
        "2 & 1 & 0 \\\\\n",
        "-2 & 0 & -1 \\\\\n",
        "2 & 3 & -3\n",
        "\\end{array}\\right]=\\left[\\begin{array}{rrr}\n",
        "1 & 0 & 0 \\\\\n",
        "-1 & 1 & 0 \\\\\n",
        "1 & 2 & 1\n",
        "\\end{array}\\right]\\left[\\begin{array}{rrr}\n",
        "2 & 1 & 0 \\\\\n",
        "0 & 1 & -1 \\\\\n",
        "0 & 0 & -1\n",
        "\\end{array}\\right] .\n",
        "$$\n",
        "\n",
        "Solve this system for\n",
        "$\\mathbf{b} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix}$.\n",
        "\n",
        ". . .\n",
        "\n",
        "Forward solve: $$\n",
        "\\left[\\begin{array}{rrr}\n",
        "1 & 0 & 0 \\\\\n",
        "-1 & 1 & 0 \\\\\n",
        "1 & 2 & 1\n",
        "\\end{array}\\right]\\left[\\begin{array}{l}\n",
        "y_{1} \\\\\n",
        "y_{2} \\\\\n",
        "y_{3}\n",
        "\\end{array}\\right]=\\left[\\begin{array}{l}\n",
        "1 \\\\\n",
        "0 \\\\\n",
        "1\n",
        "\\end{array}\\right]\n",
        "$$\n",
        "\n",
        "$y_{1}=1$, then $y_{2}=0+1 y_{1}=1$, then $y_{3}=1-1 y_{1}-2 y_{2}=-2$.\n",
        "\n",
        "Back solve:\n",
        "\n",
        "$$\n",
        "\\left[\\begin{array}{rrr}\n",
        "2 & 1 & 0 \\\\\n",
        "0 & 1 & -1 \\\\\n",
        "0 & 0 & -1\n",
        "\\end{array}\\right]\\left[\\begin{array}{l}\n",
        "x_{1} \\\\\n",
        "x_{2} \\\\\n",
        "x_{3}\n",
        "\\end{array}\\right]=\\left[\\begin{array}{r}\n",
        "1 \\\\\n",
        "1 \\\\\n",
        "-2\n",
        "\\end{array}\\right]\n",
        "$$\n",
        "\n",
        "$x_{3}=-2 /(-1)=2$, then $x_{2}=1+x_{3}=3$, then\n",
        "$x_{1}=\\left(1-1 x_{2}\\right) / 2=-1$.\n",
        "\n",
        "## *When we can do LU factorization*\n",
        "\n",
        "-   Not all square matrices have LU factorizations! This one doesn’t:\n",
        "    $\\left[\\begin{array}{ll}0 & 1 \\\\ 1 & 0\\end{array}\\right]$\n",
        "-   If Gaussian elimination can be performed on the matrix $A$ **without\n",
        "    row exchanges**, then the factorization exists\n",
        "    -   (it’s really a by-product of Gaussian elimination.)\n",
        "-   If row exchanges are needed, there is still a factorization that\n",
        "    will work, but it’s a bit more complicated.\n",
        "\n",
        "## *Intuition behind LU factorization*\n",
        "\n",
        "## *Example*\n",
        "\n",
        "Here we do Gaussian elimination on the matrix\n",
        "$A = \\begin{bmatrix} 2 & 1 & 0 \\\\ -2 & 0 & -1 \\\\ 2 & 3 & -3 \\end{bmatrix}$:\n",
        "\n",
        "$\\left[\\begin{smallmatrix}2 & 1 & 0\\\\-2 & 0 & -1\\\\2 & 3 & -3\\end{smallmatrix}\\right]$\n",
        "$\\xrightarrow[E_{31}(-1)]{E_{21}(1)}$\n",
        "$\\left[\\begin{smallmatrix}2 & 1 & 0\\\\0 & 1 & -1\\\\0 & 2 & -3\\end{smallmatrix}\\right]$\n",
        "$\\xrightarrow[E_{32}(-2)]{\\longrightarrow}$\n",
        "$\\left[\\begin{smallmatrix}2 & 1 & 0\\\\0 & 1 & -1\\\\0 & 0 & -1\\end{smallmatrix}\\right]$\n",
        "\n",
        "Let’s put those elementary row operations into matrix form. There were\n",
        "three of them:\n",
        "\n",
        "1.  $E_{21}(1)$ :\n",
        "    $\\begin{bmatrix} 1 & 0 & 0 \\\\ 1 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$\n",
        "2.  $E_{31}(-1)$:\n",
        "    $\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ -1 & 0 & 1 \\end{bmatrix}$\n",
        "3.  $E_{32}(-2)$:\n",
        "    $\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & -2 & 1 \\end{bmatrix}$\n",
        "\n",
        "The **inverses** of these matrices are\n",
        "\n",
        "1.  $\\begin{bmatrix} 1 & 0 & 0 \\\\ -1 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$,\n",
        "    $\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 1 \\end{bmatrix}$,\n",
        "    and\n",
        "    $\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 2 & 1 \\end{bmatrix}$.\n",
        "\n",
        "## \n",
        "\n",
        "The product of all these matrices is\n",
        "\n",
        "$$\n",
        "\\left[\\begin{matrix}1 & 0 & 0\\\\0 & 1 & 0\\\\-1 & 0 & 1\\end{matrix}\\right]\\left[\\begin{matrix}1 & 0 & 0\\\\1 & 1 & 0\\\\0 & 0 & 1\\end{matrix}\\right]\\left[\\begin{matrix}1 & 0 & 0\\\\0 & 1 & 0\\\\0 & -2 & 1\\end{matrix}\\right]=\\left[\\begin{matrix}1 & 0 & 0\\\\1 & 1 & 0\\\\-1 & -2 & 1\\end{matrix}\\right]\n",
        "$$\n",
        "\n",
        "This is a lower triangular matrix, and it is the inverse of the matrix\n",
        "we used to do Gaussian elimination.\n",
        "\n",
        "We can also see that the entries below the diagonal are the negatives of\n",
        "the multipliers we used in the elimination steps.\n",
        "\n",
        "## *Steps to LU factorization*\n",
        "\n",
        "Let $\\left[a_{i j}^{(k)}\\right]$ be the matrix obtained from $A$ after\n",
        "using the $k$ th pivot to clear out entries below it.\n",
        "\n",
        ". . .\n",
        "\n",
        "(The original matrix is $A=\\left[a_{i j}^{(0)}\\right]$)\n",
        "\n",
        ". . .\n",
        "\n",
        "All the row operations we will use include ratios\n",
        "$\\left(-a_{i j} / a_{j j}\\right)$.\n",
        "\n",
        ". . .\n",
        "\n",
        "The row-adding elementary operations are of the form\n",
        "\n",
        "$E_{i j}\\left(-a_{i j}^{(k)} / a_{j j}^{(k)}\\right)$\n",
        "\n",
        ". . .\n",
        "\n",
        "We can give these ratios a name: **multipliers**.\n",
        "\n",
        "$m_{i j}=-a_{i j}^{(k)} / a_{j j}^{(k)}$, where $i>j$\n",
        "\n",
        ". . .\n",
        "\n",
        "If Gaussian elimination is used without row exchanges on the nonsingular\n",
        "matrix $A$, resulting in the upper triangular matrix $U$, and if $L$ is\n",
        "the unit lower triangular matrix whose entries below the diagonal are\n",
        "the negatives of the multipliers $m_{i j}$, then $A=L U$.\n",
        "\n",
        "## *Storing the multipliers as we go*\n",
        "\n",
        "For efficiency, we can just “store” the multipliers in the lower\n",
        "triangular part of the matrix on the left as we go along, since that\n",
        "will be zero anyways.\n",
        "\n",
        "$$\n",
        "\\left[\\begin{array}{rrr}\n",
        "(2) & 1 & 0 \\\\\n",
        "-2 & 0 & -1 \\\\\n",
        "2 & 3 & -3\n",
        "\\end{array}\\right] \\xrightarrow[E_{31}(-1)]{E_{21}(1)}\\left[\\begin{array}{rrr}\n",
        "2 & 1 & 0 \\\\\n",
        "-1 & (1) & -1 \\\\\n",
        "1 & 2 & -3\n",
        "\\end{array}\\right] \\xrightarrow[E_{32}(-2)]{\\longrightarrow}\\left[\\begin{array}{rrr}\n",
        "2 & 1 & 0 \\\\\n",
        "-1 & 1 & -1 \\\\\n",
        "1 & 2 & -1\n",
        "\\end{array}\\right] .\n",
        "$$\n",
        "\n",
        ". . .\n",
        "\n",
        "Now we read off the results from the final matrix:\n",
        "\n",
        "$$\n",
        "L=\\left[\\begin{array}{rrr}\n",
        "1 & 0 & 0 \\\\\n",
        "1 & 1 & 0 \\\\\n",
        "-1 & 2 & 1\n",
        "\\end{array}\\right] \\text { and } U=\\left[\\begin{array}{rrr}\n",
        "2 & 1 & 0 \\\\\n",
        "0 & 1 & -1 \\\\\n",
        "0 & 0 & -1\n",
        "\\end{array}\\right]\n",
        "$$\n",
        "\n",
        "## *Superaugmented matrix*\n",
        "\n",
        "Could we just keep track by using the superaugmented matrix, like we did\n",
        "last lecture? What would that look like?\n",
        "\n",
        "**pause**\n",
        "\n",
        "$\\left[\\begin{smallmatrix}2 & 1 & 0 & 1 & 0 & 0\\\\-2 & 0 & -1 & 0 & 1 & 0\\\\2 & 3 & -3 & 0 & 0 & 1\\end{smallmatrix}\\right]$\n",
        "$\\xrightarrow[E_{31}(-1)]{E_{21}(1)}$\n",
        "$\\left[\\begin{smallmatrix}2 & 1 & 0 & 1 & 0 & 0\\\\0 & 1 & -1 & 1 & 1 & 0\\\\0 & 2 & -3 & -1 & 0 & 1\\end{smallmatrix}\\right]$\n",
        "$\\xrightarrow[E_{32}(-2)]{\\longrightarrow}$\n",
        "$\\left[\\begin{smallmatrix}2 & 1 & 0 & 1 & 0 & 0\\\\0 & 1 & -1 & 1 & 1 & 0\\\\0 & 0 & -1 & -3 & -2 & 1\\end{smallmatrix}\\right]$\n",
        "\n",
        ". . .\n",
        "\n",
        "Our superaugmented matrix does become an upper triangular matrix on the\n",
        "left and a lower triangular matrix on the right.\n",
        "\n",
        "Unfortunately, the lower triangular matrix on the right is\n",
        "$\\tilde{L}^{-1}$, not $\\tilde{L}$.\n",
        "\n",
        "So we can’t just read off $L$ and $U$ from the superaugmented matrix.\n",
        "\n",
        "## *PLU factorization*\n",
        "\n",
        "What if we need row exchanges?\n",
        "\n",
        "-   We could start off by doing all the row-exchanging elementary\n",
        "    operations that we need, and store the product of these\n",
        "    row-exchanging matrices as a matrix $P$.\n",
        "\n",
        "-   This product is called a **permutation matrix**\n",
        "\n",
        "-   Applying the correct permuatation matrix $P$ to $A$, we get a matrix\n",
        "    for which Gaussian elimination will succeed without further row\n",
        "    exchanges.\n",
        "\n",
        ". . .\n",
        "\n",
        "Now we have a theorem that applies to all nonsingular matrices:\n",
        "\n",
        "If $A$ is a nonsingular matrix, then there exists a permutation matrix\n",
        "$P$, upper triangular matrix $U$, and unit lower triangular matrix $L$\n",
        "such that $P A=L U$.\n",
        "\n",
        ". . .\n",
        "\n",
        "So, if you’ve got a nonsingular matrix $A$, you can always find a\n",
        "permutation matrix $P$, an upper triangular matrix $U$, and a unit lower\n",
        "triangular matrix $L$ that satisfy $P A=L U$. Pretty neat, huh?"
      ],
      "id": "6da6c6b1-cc8a-448e-b761-f30b6680af0f"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  }
}