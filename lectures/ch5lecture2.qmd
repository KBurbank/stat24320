---
title: Ch5 Lecture 2
lecture_day: 12
readings: "5.2,5.3"
publish: true
execute: 
  cache: true
lightbox: true
filters:
    - pyodide
kernel: jupyter
format: 
    course-presentation-revealjs:
      css: theorem.css
      code-fold: show
---

# Eigensystems of Symmetric Matrices

## Positive Definite Matrices

::: definition
A matrix $A$ is called **positive definite** if $x^{T} A x>0$ for all nonzero vectors $x$.
:::

. . .

::: theorem
A symmetric matrix $K=K^{T}$ is positive definite if and only if all of its eigenvalues are strictly positive.
:::

. . .

Proof: If $\mathbf{x}=\mathbf{v} \neq \mathbf{0}$ is an eigenvector with (necessarily real) eigenvalue $\lambda$, then

$$
\begin{equation*}
0<\mathbf{v}^{T} K \mathbf{v}=\mathbf{v}^{T}(\lambda \mathbf{v})=\lambda \mathbf{v}^{T} \mathbf{v}=\lambda\|\mathbf{v}\|^{2} 
\end{equation*}
$$

So $\lambda>0$

. . .

Conversely, suppose $K$ has all positive eigenvalues.

 Let $\mathbf{u}_{1}, \ldots, \mathbf{u}_{n}$ be the orthonormal eigenvector basis with $K \mathbf{u}_{j}=\lambda_{j} \mathbf{u}_{j}$ with $\lambda_{j}>0$.

$$
\mathbf{x}=c_{1} \mathbf{u}_{1}+\cdots+c_{n} \mathbf{u}_{n}, \quad \text { we obtain } \quad K \mathbf{x}=c_{1} \lambda_{1} \mathbf{u}_{1}+\cdots+c_{n} \lambda_{n} \mathbf{u}_{n}
$$

. . .

Therefore, 

$$
\mathbf{x}^{T} K \mathbf{x}=\left(c_{1} \mathbf{u}_{1}^{T}+\cdots+c_{n} \mathbf{u}_{n}^{T}\right)\left(c_{1} \lambda_{1} \mathbf{u}_{1}+\cdots+c_{n} \lambda_{n} \mathbf{u}_{n}\right)=\lambda_{1} c_{1}^{2}+\cdots+\lambda_{n} c_{n}^{2}>0
$$



##

::: theorem

Let $A=A^{T}$ be a real symmetric $n \times n$ matrix. Then

(a) All the eigenvalues of $A$ are real.

. . .

(b) Eigenvectors corresponding to distinct eigenvalues are orthogonal.

. . .

(c) There is an orthonormal basis of $\mathbb{R}^{n}$ consisting of $n$ eigenvectors of $A$. In particular, all real symmetric matrices are non-defective and real diagonalizable.

:::

## Example

$$
A=\left(\begin{array}{ll}3 & 1 \\ 1 & 3\end{array}\right)
$$

. . .

We compute the determinant in the characteristic equation

$$
\operatorname{det}(A-\lambda \mathrm{I})=\operatorname{det}\left(\begin{array}{cc}
3-\lambda & 1 \\
1 & 3-\lambda
\end{array}\right)=(3-\lambda)^{2}-1=\lambda^{2}-6 \lambda+8
$$

. . .

$$
\lambda^{2}-6 \lambda+8=(\lambda-4)(\lambda-2)=0
$$

. . .

Eigenvectors: 

For the first eigenvalue, the eigenvector equation is

$$
(A-4 \mathrm{I}) \mathbf{v}=\left(\begin{array}{rr}
-1 & 1 \\
1 & -1
\end{array}\right)\binom{x}{y}=\binom{0}{0}, \quad \text { or } \quad \begin{array}{r}
-x+y=0 \\
x-y=0
\end{array}
$$

. . .

General solution:
$$
x=y=a, \quad \text { so } \quad \mathbf{v}=\binom{a}{a}=a\binom{1}{1}
$$

. . .
$$
\lambda_{1}=4, \quad \mathbf{v}_{1}=\binom{1}{1}, \quad \lambda_{2}=2, \quad \mathbf{v}_{2}=\binom{-1}{1}
$$

. . .

The eigenvectors are orthogonal: $\mathbf{v}_{1} \cdot \mathbf{v}_{2}=0$

## Proof of part (a)

Let $A=A^{T}$ be a real symmetric $n \times n$ matrix. Then

(a) All the eigenvalues of $A$ are real.

. . .

Suppose $\lambda$ is a complex eigenvalue with complex eigenvector $\mathbf{v} \in \mathbb{C}^{n}$. 


$$
(A \mathbf{v}) \cdot \mathbf{v}=(\lambda \mathbf{v}) \cdot \mathbf{v}=\lambda\|\mathbf{v}\|^{2}
$$

. . .

Now, if $A$ is real and symmetric,

$$
\begin{equation*}
(A \mathbf{v}) \cdot \mathbf{w}=\left(\mathbf{v}^T A^T \right)\mathbf{w} = \mathbf{v} \cdot(A \mathbf{w}) \quad \text { for all } \quad \mathbf{v}, \mathbf{w} \in \mathbb{C}^{n} 
\end{equation*}
$$


Therefore

$$
(A \mathbf{v}) \cdot \mathbf{v}=\mathbf{v} \cdot(A \mathbf{v})=\mathbf{v} \cdot(\lambda \mathbf{v})=\mathbf{v}^{T} \overline{\lambda \mathbf{v}}=\bar{\lambda}\|\mathbf{v}\|^{2}
$$

(We didn't talk about this, but for complex vectors, we have $\mathbf{v} \cdot \mathbf{w}=\mathbf{w} \cdot \mathbf{v}^{*}$)

. . .

Therefore, we have $\lambda\|\mathbf{v}\|^{2}=\bar{\lambda}\|\mathbf{v}\|^{2}$, which implies $\lambda=\bar{\lambda}$, so $\lambda$ is real.

## Proof of part (b)

(b) Eigenvectors corresponding to distinct eigenvalues are orthogonal.


Suppose $A \mathbf{v}=\lambda \mathbf{v}, A \mathbf{w}=\mu \mathbf{w}$, where $\lambda \neq \mu$ are distinct real eigenvalues. 


$\lambda \mathbf{v} \cdot \mathbf{w}=(A \mathbf{v}) \cdot \mathbf{w}=\mathbf{v} \cdot(A \mathbf{w})=\mathbf{v} \cdot(\mu \mathbf{w})=\mu \mathbf{v} \cdot \mathbf{w}, \quad$ and hence $\quad(\lambda-\mu) \mathbf{v} \cdot \mathbf{w}=0$.

. . .

Since $\lambda \neq \mu$, this implies that $\mathbf{v} \cdot \mathbf{w}=0$, so the eigenvectors $\mathbf{v}, \mathbf{w}$ are orthogonal.

## Proof of part (c)

(c) There is an orthonormal basis of $\mathbb{R}^{n}$ consisting of $n$ eigenvectors of $A$. 

. . .

If the eigenvalues are distinct, then the eigenvectors are orthogonal by part (b).

. . .

If the eigenvalues are repeated, then we can use the Gram-Schmidt process to orthogonalize the eigenvectors.

## Diagonalizability of Symmetric Matrices: the Spectral Theorem

- Every real, symmetric matrix admits an eigenvector basis, and hence is diagonalizable.
- Moreover, since we can choose eigenvectors that form an orthonormal basis, the diagonalizing matrix takes a particularly simple form.
- Recall that an $n \times n$ matrix $Q$ is orthogonal if and only if its columns form an orthonormal basis of $\mathbb{R}^{n}$.

. . .

Writing our diagonalization from the previous lecture, specifically for the case of a real symmetric matrix $A$:

::: theorem
If $A=A^{T}$ is a real symmetric $n \times n$ matrix, then there exists an orthogonal matrix $Q$ and a real diagonal matrix $\Lambda$ such that

$$
\begin{equation*}
A=Q \Lambda Q^{-1}=Q \Lambda Q^{T} 
\end{equation*}
$$

The eigenvalues of $A$ appear on the diagonal of $\Lambda$, while the columns of $Q$ are the corresponding orthonormal eigenvectors.
:::

## Quadratic Forms

::: definition
A **quadratic form** is a homogeneous polynomial of degree 2 in $n$ variables $x_{1}, \ldots, x_{n}$. For example, in $x, y, z$: $Q(x, y, z)=a x^{2}+b y^{2}+c z^{2}+2 d x y+2 e y z+2 f z x$.
:::

. . .

Every quadratic form can be written in matrix form as $Q(\mathbf{x})=\mathbf{x}^{T} A \mathbf{x}$.

. . .

Example: 

$$
Q(x, y, z)=x^{2}+2 y^{2}+z^{2}+2 x y+y z+3 x z .
$$

. . .

$$
\begin{aligned}
x(x+2 y+3 z)+y(2 y+z)+z^{2} & =\left[\begin{array}{lll}
x & y & z
\end{array}\right]\left[\begin{array}{c}
x+2 y+3 z \\
2 y+z \\
z
\end{array}\right] \\
& =\left[\begin{array}{lll}
x & y & z
\end{array}\right]\left[\begin{array}{lll}
1 & 2 & 3 \\
0 & 2 & 1 \\
0 & 0 & 1
\end{array}\right]\left[\begin{array}{c}
x \\
y \\
z
\end{array}\right]=\mathbf{x}^{T} A \mathbf{x},
\end{aligned}
$$

Now, if we have a quadratic form $Q(\mathbf{x})=\mathbf{x}^{T} A \mathbf{x}$, we always write this in terms of an equivalent symmetric matrix $B$ as $Q(\mathbf{x})=\mathbf{x}^{T} B \mathbf{x}$ where $B=\frac{1}{2}(A+A^{T})$.

(See Exercise 2.4.34 in your texbook.)

. . .

So in this case, we can write $Q(\mathbf{x})=\mathbf{x}^{T} B \mathbf{x}$ where 

$$
B=\frac{1}{2}\left[\begin{array}{lll}
2 & 2 & 3 \\
2 & 4 & 1 \\
3 & 1 & 2
\end{array}\right]
$$

Check:

```{python}
#| echo: true
import sympy as sp
B = sp.Matrix([[2,2,3],[2,4,1],[3,1,2]])/2
x = sp.Matrix(sp.symbols(['x','y','z']))
(x.T*B*x).expand()
```

Yes, this is the same as the original quadratic form.

## Diagonalizing Quadratic Forms

Because symmetric matrices are diagonalizable, we can always find a basis in which the quadratic form takes a particularly simple form.

We can write $Q(\mathbf{x})=\mathbf{x}^{T} B \mathbf{x}=x^{T} P D P^{T} x$ where $P$ is the matrix of eigenvectors of $B$ and $D$ is the diagonal matrix of eigenvalues of $B$. 

. . .

Then if we define new variables $\mathbf{y}=P^{T} \mathbf{x}$, we have $Q(\mathbf{x})=\mathbf{y}^{T} D \mathbf{y}$

. . .

which just becomes a sum of squares.

##

Example: 

Suppose we have the quadratic form $3x^2+2xy+3y^2$. We can write this in matrix form as $Q(\mathbf{x})=\mathbf{x}^{T} B \mathbf{x}$ where

$$
B=\frac{1}{2}\left[\begin{array}{ll}
3 & 1 \\
1 & 3
\end{array}\right]
$$

. . .

We diagonalize $B$:

$$
\left(\begin{array}{ll}
3 & 1 \\
1 & 3
\end{array}\right)=A=P D P^{T}=\left(\begin{array}{cc}
\frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \\
\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}
\end{array}\right)\left(\begin{array}{ll}
4 & 0 \\
0 & 2
\end{array}\right)\left(\begin{array}{cc}
\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \\
-\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}
\end{array}\right)
$$

Now, if we define $\mathbf{y}=P^{T} \mathbf{x}$, we have $Q(\mathbf{x})=\mathbf{y}^{T} D \mathbf{y}$, or

$$
q(\mathbf{x})=3 x_{1}^{2}+2 x_{1} x_{2}+3 x_{2}^{2}=4 y_{1}^{2}+2 y_{2}^{2}
$$

##

The numbers aren't always clean, thougbh!




```{python}
#| echo: true
Q,Lambda = B.diagonalize()
Q
```

```{python}
Lambda
```

Yuck!

