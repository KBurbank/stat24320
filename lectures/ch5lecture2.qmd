---
title: Ch5 Lecture 2
lecture_day: 12
readings: "5.2,5.3"
publish: true
execute: 
  cache: true
lightbox: true
filters:
    - pyodide
kernel: jupyter
format: 
    course-presentation-revealjs:
      css: theorem.css
      code-fold: show
---

# Eigensystems of Symmetric Matrices

## Positive Definite Matrices

::: definition
A matrix $A$ is called **positive definite** if $x^{T} A x>0$ for all nonzero vectors $x$.
:::

. . .

::: theorem
A symmetric matrix $K=K^{T}$ is positive definite if and only if all of its eigenvalues are strictly positive.
:::

. . .

Proof: If $\mathbf{x}=\mathbf{v} \neq \mathbf{0}$ is an eigenvector with (necessarily real) eigenvalue $\lambda$, then

$$
\begin{equation*}
0<\mathbf{v}^{T} K \mathbf{v}=\mathbf{v}^{T}(\lambda \mathbf{v})=\lambda \mathbf{v}^{T} \mathbf{v}=\lambda\|\mathbf{v}\|^{2} 
\end{equation*}
$$

So $\lambda>0$

. . .

Conversely, suppose $K$ has all positive eigenvalues.

 Let $\mathbf{u}_{1}, \ldots, \mathbf{u}_{n}$ be the orthonormal eigenvector basis with $K \mathbf{u}_{j}=\lambda_{j} \mathbf{u}_{j}$ with $\lambda_{j}>0$.

$$
\mathbf{x}=c_{1} \mathbf{u}_{1}+\cdots+c_{n} \mathbf{u}_{n}, \quad \text { we obtain } \quad K \mathbf{x}=c_{1} \lambda_{1} \mathbf{u}_{1}+\cdots+c_{n} \lambda_{n} \mathbf{u}_{n}
$$

. . .

Therefore, 

$$
\mathbf{x}^{T} K \mathbf{x}=\left(c_{1} \mathbf{u}_{1}^{T}+\cdots+c_{n} \mathbf{u}_{n}^{T}\right)\left(c_{1} \lambda_{1} \mathbf{u}_{1}+\cdots+c_{n} \lambda_{n} \mathbf{u}_{n}\right)=\lambda_{1} c_{1}^{2}+\cdots+\lambda_{n} c_{n}^{2}>0
$$



##

::: theorem

Let $A=A^{T}$ be a real symmetric $n \times n$ matrix. Then

(a) All the eigenvalues of $A$ are real.

. . .

(b) Eigenvectors corresponding to distinct eigenvalues are orthogonal.

. . .

(c) There is an orthonormal basis of $\mathbb{R}^{n}$ consisting of $n$ eigenvectors of $A$. In particular, all real symmetric matrices are non-defective and real diagonalizable.

:::

## Example

$$
A=\left(\begin{array}{ll}3 & 1 \\ 1 & 3\end{array}\right)
$$

. . .

We compute the determinant in the characteristic equation

$$
\operatorname{det}(A-\lambda \mathrm{I})=\operatorname{det}\left(\begin{array}{cc}
3-\lambda & 1 \\
1 & 3-\lambda
\end{array}\right)=(3-\lambda)^{2}-1=\lambda^{2}-6 \lambda+8
$$

. . .

$$
\lambda^{2}-6 \lambda+8=(\lambda-4)(\lambda-2)=0
$$

. . .

Eigenvectors: 

For the first eigenvalue, the eigenvector equation is

$$
(A-4 \mathrm{I}) \mathbf{v}=\left(\begin{array}{rr}
-1 & 1 \\
1 & -1
\end{array}\right)\binom{x}{y}=\binom{0}{0}, \quad \text { or } \quad \begin{array}{r}
-x+y=0 \\
x-y=0
\end{array}
$$

##

General solution:
$$
x=y=a, \quad \text { so } \quad \mathbf{v}=\binom{a}{a}=a\binom{1}{1}
$$

. . .
$$
\lambda_{1}=4, \quad \mathbf{v}_{1}=\binom{1}{1}, \quad \lambda_{2}=2, \quad \mathbf{v}_{2}=\binom{-1}{1}
$$

. . .

The eigenvectors are orthogonal: $\mathbf{v}_{1} \cdot \mathbf{v}_{2}=0$

## Proof of part (a)

Let $A=A^{T}$ be a real symmetric $n \times n$ matrix. Then

(a) All the eigenvalues of $A$ are real.

. . .

Suppose $\lambda$ is a complex eigenvalue with complex eigenvector $\mathbf{v} \in \mathbb{C}^{n}$. 


$$
(A \mathbf{v}) \cdot \mathbf{v}=(\lambda \mathbf{v}) \cdot \mathbf{v}=\lambda\|\mathbf{v}\|^{2}
$$

. . .

Now, if $A$ is real and symmetric,

$$
\begin{equation*}
(A \mathbf{v}) \cdot \mathbf{w}=\left(\mathbf{v}^T A^T \right)\mathbf{w} = \mathbf{v} \cdot(A \mathbf{w}) \quad \text { for all } \quad \mathbf{v}, \mathbf{w} \in \mathbb{C}^{n} 
\end{equation*}
$$


Therefore

$$
(A \mathbf{v}) \cdot \mathbf{v}=\mathbf{v} \cdot(A \mathbf{v})=\mathbf{v} \cdot(\lambda \mathbf{v})=\mathbf{v}^{T} \overline{\lambda \mathbf{v}}=\bar{\lambda}\|\mathbf{v}\|^{2}
$$

::: notes
(We didn't talk about this, but for complex vectors, we have $\mathbf{v} \cdot \mathbf{w}=\mathbf{w} \cdot \mathbf{v}^{*}$)
:::
. . .

$\Rightarrow$,  $\lambda\|\mathbf{v}\|^{2}=\bar{\lambda}\|\mathbf{v}\|^{2}$ $\Rightarrow$ $\lambda=\bar{\lambda}$, so $\lambda$ is real.

## Proof of part (b)

Part b: Eigenvectors corresponding to distinct eigenvalues are orthogonal.


Suppose $A \mathbf{v}=\lambda \mathbf{v}, A \mathbf{w}=\mu \mathbf{w}$, where $\lambda \neq \mu$ are distinct real eigenvalues. 

. . .

$\lambda \mathbf{v} \cdot \mathbf{w}=(A \mathbf{v}) \cdot \mathbf{w}=\mathbf{v} \cdot(A \mathbf{w})=\mathbf{v} \cdot(\mu \mathbf{w})=\mu \mathbf{v} \cdot \mathbf{w}, \quad$ and hence $\quad(\lambda-\mu) \mathbf{v} \cdot \mathbf{w}=0$.

. . .

Since $\lambda \neq \mu$, this implies that $\mathbf{v} \cdot \mathbf{w}=0$, so the eigenvectors $\mathbf{v}, \mathbf{w}$ are orthogonal.

## Proof of part (c)

Part c: There is an orthonormal basis of $\mathbb{R}^{n}$ consisting of $n$ eigenvectors of $A$. 

. . .

If the eigenvalues are distinct, then the eigenvectors are orthogonal by part (b).

. . .

If the eigenvalues are repeated, then we can use the Gram-Schmidt process to orthogonalize the eigenvectors.

## Diagonalizability of Symmetric Matrices: the Spectral Theorem

- Every real, symmetric matrix admits an eigenvector basis, and hence is diagonalizable.
- Moreover, since we can choose eigenvectors that form an orthonormal basis, the diagonalizing matrix takes a particularly simple form.
- Recall that an $n \times n$ matrix $Q$ is orthogonal if and only if its columns form an orthonormal basis of $\mathbb{R}^{n}$.

. . .

Writing our diagonalization from the previous lecture, specifically for the case of a real symmetric matrix $A$:

::: theorem
If $A=A^{T}$ is a real symmetric $n \times n$ matrix, then there exists an orthogonal matrix $Q$ and a real diagonal matrix $\Lambda$ such that

$$
\begin{equation*}
A=Q \Lambda Q^{-1}=Q \Lambda Q^{T} 
\end{equation*}
$$

The eigenvalues of $A$ appear on the diagonal of $\Lambda$, while the columns of $Q$ are the corresponding orthonormal eigenvectors.
:::

## One example of a useful symmetric matrix: Quadratic Forms

::: definition
A **quadratic form** is a homogeneous polynomial of degree 2 in $n$ variables $x_{1}, \ldots, x_{n}$. For example, in $x, y, z$: $Q(x, y, z)=a x^{2}+b y^{2}+c z^{2}+2 d x y+2 e y z+2 f z x$.
:::

. . .

Every quadratic form can be written in matrix form as $Q(\mathbf{x})=\mathbf{x}^{T} A \mathbf{x}$.

. . .

Example: 

$$
Q(x, y, z)=x^{2}+2 y^{2}+z^{2}+2 x y+y z+3 x z .
$$

. . .

$$
\begin{aligned}
x(x+2 y+3 z)+y(2 y+z)+z^{2} & =\left[\begin{array}{lll}
x & y & z
\end{array}\right]\left[\begin{array}{c}
x+2 y+3 z \\
2 y+z \\
z
\end{array}\right] \\
& =\left[\begin{array}{lll}
x & y & z
\end{array}\right]\left[\begin{array}{lll}
1 & 2 & 3 \\
0 & 2 & 1 \\
0 & 0 & 1
\end{array}\right]\left[\begin{array}{c}
x \\
y \\
z
\end{array}\right]=\mathbf{x}^{T} A \mathbf{x},
\end{aligned}
$$

Now, if we have a quadratic form $Q(\mathbf{x})=\mathbf{x}^{T} A \mathbf{x}$, we always write this in terms of an equivalent symmetric matrix $B$ as $Q(\mathbf{x})=\mathbf{x}^{T} B \mathbf{x}$ where $B=\frac{1}{2}(A+A^{T})$.

(See Exercise 2.4.34 in your texbook.)

##

So in this case, we can write $Q(\mathbf{x})=\mathbf{x}^{T} B \mathbf{x}$ where 

$$
B=\frac{1}{2}\left[\begin{array}{lll}
2 & 2 & 3 \\
2 & 4 & 1 \\
3 & 1 & 2
\end{array}\right]
$$

Check:

```{python}
#| echo: true
import sympy as sp
B = sp.Matrix([[2,2,3],[2,4,1],[3,1,2]])/2
x = sp.Matrix(sp.symbols(['x','y','z']))
(x.T*B*x).expand()
```

Yes, this is the same as the original quadratic form.

## Diagonalizing Quadratic Forms

Symmetric matrices are diagonalizable $\Rightarrow$ we can always find a basis in which the quadratic form takes a particularly simple form. Just diagonalize:

. . .

$Q(\mathbf{x})=\mathbf{x}^{T} B \mathbf{x}=x^{T} P D P^{T} x$ where $P$ is the matrix of eigenvectors of $B$ and $D$ is the diagonal matrix of eigenvalues of $B$. 

. . .

Then if we define new variables $\mathbf{y}=P^{T} \mathbf{x}$, we have $Q(\mathbf{x})=\mathbf{y}^{T} D \mathbf{y}$

. . .

which just becomes a sum of squares:

$$
q(\mathbf{x})=\lambda_{1} y_{1}^{2}+\cdots+\lambda_{n} y_{n}^{2}
$$


##

Example: 

Suppose we have the quadratic form $3x^2+2xy+3y^2$. We can write this in matrix form as $Q(\mathbf{x})=\mathbf{x}^{T} B \mathbf{x}$ where $\mathbf{x}=\binom{x_1}{x_2}$ and

$$
B=\frac{1}{2}\left[\begin{array}{ll}
3 & 1 \\
1 & 3
\end{array}\right]
$$

. . .

We diagonalize $B$:

$$
\left(\begin{array}{ll}
3 & 1 \\
1 & 3
\end{array}\right)=A=P D P^{T}=\left(\begin{array}{cc}
\frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \\
\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}
\end{array}\right)\left(\begin{array}{ll}
4 & 0 \\
0 & 2
\end{array}\right)\left(\begin{array}{cc}
\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \\
-\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}
\end{array}\right)
$$

Now, if we define $\mathbf{y}=P^{T} \mathbf{x}=\frac{1}{\sqrt{2}}\binom{x_1+x_2}{-x_1+x_2}$, we have $Q(\mathbf{x})=\mathbf{y}^{T} D \mathbf{y}$, or

$$
q(\mathbf{x})=3 x_{1}^{2}+2 x_{1} x_{2}+3 x_{2}^{2}=4 y_{1}^{2}+2 y_{2}^{2}
$$

##

The numbers aren't always clean, though!




```{python}
#| echo: true
Q,Lambda = B.diagonalize()
Q
```

```{python}
Lambda
```

Yuck!



##

We can visualize the previous example as a *rotation* of the axes (a change of basis) to a new coordinate system where the quadratic form is just a sum of squares.

```{python}
#| echo: true
#| name: rotation
import numpy as np
import matplotlib.pyplot as plt
x = np.linspace(-2,2,100)
y = np.linspace(-2,2,100)
X,Y = np.meshgrid(x,y)
Z = 3*X**2+2*X*Y+3*Y**2
plt.contour(X,Y,Z,levels=[1,2,3,4,5,6,7,8,9,10])
plt.xlabel('x1')
plt.ylabel('x2')
plt.axis('equal')
# plot the vector P.T times (1,0) and (0,1)
P = np.array([[1/np.sqrt(2),1/np.sqrt(2)],[-1/np.sqrt(2),1/np.sqrt(2)]])
v1 = P.T @ np.array([1,0])
v2 = P.T @ np.array([0,1])
plt.quiver(0,0,v1[0],v1[1],angles='xy',scale_units='xy',scale=1,color='r')
plt.quiver(0,0,v2[0],v2[1],angles='xy',scale_units='xy',scale=1,color='r')
# label the two quivers ("x1=1, x2=0" and "x1=0, x2=1")
plt.text(v1[0],v1[1],'y(x1=1,x2=0)',fontsize=12)
plt.text(v2[0],v2[1],'y(x1=0,x2=1)',fontsize=12)
plt.show()
```

##

Now make the same plot but in y coordinates:

```{python}
#| echo: true
#| name: rotation2
#| depends: rotation
x = np.linspace(-2,2,100)
y = np.linspace(-2,2,100)
X,Y = np.meshgrid(x,y)
Z = 4*X**2+2*Y**2
plt.contour(X,Y,Z,levels=[1,2,3,4,5,6,7,8,9,10])
plt.xlabel('y1')
plt.ylabel('y2')
plt.axis('equal')
v1 = P.T @ np.array([1,0])
v2 = P.T @ np.array([0,1])
plt.quiver(0,0,1,0,angles='xy',scale_units='xy',scale=1,color='r')
plt.quiver(0,0,0,1,angles='xy',scale_units='xy',scale=1,color='r')
# label the two quivers ("x1=1, x2=0" and "x1=0, x2=1")
plt.show()
```

##

In general, we can think of the diagonalization as a *rotation* of the axes followed by a *scaling* of the axes.

We often visualize this by plotting the effects of the transformations on the unit circle.


![](https://cdn.mathpix.com/cropped/2024_04_29_4f6d38c079cada204833g-36.jpg?height=369&width=854&top_left_y=257&top_left_x=630)


## Singular Values

We've talked a lot about eigenvalues and eigenvectors, but these only make any sense for square matrices. What can we do for a general $m \times n$ matrix $A$?

. . .

It turns out we can learn a lot from the matrix $A^{T} A$ (or $A A^{T}$), which is always square and symmetric.

. . .

::: definition
The **singular values** $\sigma_{1}, \ldots, \sigma_{r}$ of an $m \times n$ matrix $A$ are the positive square roots, $\sigma_{i}=\sqrt{\lambda_{i}}>0$, of the nonzero eigenvalues of the associated "Gram matrix" $K=A^{T} A$. 

. . .

The corresponding eigenvectors of $K$ are known as the **singular vectors** of $A$.
:::

. . .

All of the eigenvalues of $K$ are real and nonnegative -- but some may be zero.

. . .

If $K=A^{T} A$ has repeated eigenvalues, the singular values of $A$ are repeated with the same multiplicities.

. . .

The number $r$ of singular values is equal to the rank of the matrices $A$ and $K$.

## Example

Let $A=\left(\begin{array}{ll}3 & 5 \\ 4 & 0\end{array}\right)$. 

$$
K=A^{T} A=\left(\begin{array}{ll}
3 & 4 \\
5 & 0
\end{array}\right)\left(\begin{array}{ll}
3 & 5 \\
4 & 0
\end{array}\right)=\left(\begin{array}{ll}
25 & 15 \\
15 & 25
\end{array}\right)
$$

. . .

This has eigenvalues $\lambda_{1}=40, \lambda_{2}=10$, and corresponding eigenvectors $\mathbf{v}_{1}=\binom{1}{1}, \mathbf{v}_{2}=\binom{1}{-1}$. 

. . .

Therefore, the singular values of $A$ are $\sigma_{1}=\sqrt{40}=2 \sqrt{10}, \sigma_{2}=\sqrt{10}$.

*pause*

::: notes
Are the singular values here equal to the eigenvalues of $A$? No!

The eigenvalues of A  are $\lambda_{1}=\frac{1}{2}(3+\sqrt{89}) \simeq 6.2170$ and $\lambda_{2}=\frac{1}{2}(3-\sqrt{89}) \simeq-3.2170$, nor are the singular vectors eigenvectors of $A$.
:::

## Singular Values of a Symmetric Matrix

If $A$ is a symmetric matrix, then $K = A^{T} A= A^2$.

So if $\mathbf{v}$ is an eigenvector of $A$ with eigenvalue $\lambda$, then $A \mathbf{v}=\lambda \mathbf{v}$, and $K \mathbf{v}=A^{T} A \mathbf{v}=A \lambda \mathbf{v}=\lambda^{2} \mathbf{v}$.

. . .

Therefore the eigenvalues of $K$ are the squares of the eigenvalues of $A$.

. . .

Thus the singular values of $A$, which are the square roots of the non-zero eigenvalues of $K$, are the absolute values of the eigenvalues of $A$.

