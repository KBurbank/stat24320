[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stat 24320",
    "section": "",
    "text": "Title\n            \n                \n                    Lecture Day\n                \n                \n                \n                    Readings\n                \n                \n                    Colab Link\n        \n        \n                    \n                        \n                            Intro to linear systems\n                        \n                    \n\n                    \n                        \n                            1\n                        \n                        \n                        \n                            Ch. 1.1-1.3\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            More Systems of Linear Equations\n                        \n                    \n\n                    \n                        \n                            2\n                        \n                        \n                        \n                            Ch. 1.4-1.5\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch2 Lecture 1\n                        \n                    \n\n                    \n                        \n                            3\n                        \n                        \n                        \n                            2.1-2.3\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch2 Lecture 2\n                        \n                    \n\n                    \n                        \n                            4\n                        \n                        \n                        \n                            2.3-2.4\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch2 Lecture 3\n                        \n                    \n\n                    \n                        \n                            5\n                        \n                        \n                        \n                            2.3-2.8\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch2 Lecture 4\n                        \n                    \n\n                    \n                        \n                            6\n                        \n                        \n                        \n                            2.8\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch2 Lecture 5\n                        \n                    \n\n                    \n                        \n                            7\n                        \n                        \n                        \n                            2.4,2.5,2.6\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch3 Lecture 1\n                        \n                    \n\n                    \n                        \n                            8\n                        \n                        \n                        \n                            3.3, 3.4, 3.6\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch4 Lecture 1\n                        \n                    \n\n                    \n                        \n                            9\n                        \n                        \n                        \n                            4.1,4.2\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch4 Lecture 2\n                        \n                    \n\n                    \n                        \n                            10\n                        \n                        \n                        \n                            4.2,4.3,4.4\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch5 Lecture 1\n                        \n                    \n\n                    \n                        \n                            11\n                        \n                        \n                        \n                            5.1,5.2\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch5 Lecture 2\n                        \n                    \n\n                    \n                        \n                            11\n                        \n                        \n                        \n                            5.1,5.2,5.3\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch 5 Lecture 3\n                        \n                    \n\n                    \n                        \n                            12\n                        \n                        \n                        \n                            \n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch5 Lecture 2\n                        \n                    \n\n                    \n                        \n                            12\n                        \n                        \n                        \n                            5.6\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch5 Lecture 4\n                        \n                    \n\n                    \n                        \n                            14\n                        \n                        \n                        \n                            \n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch5 Lecture 5\n                        \n                    \n\n                    \n                        \n                            15\n                        \n                        \n                        \n                            5.1,5.2,5.3\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch6 Fourier\n                        \n                    \n\n                    \n                        \n                            18\n                        \n                        \n                        \n                            5.1,5.2,5.3\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Review\n                        \n                    \n\n                    \n                        \n                            18\n                        \n                        \n                        \n                            Everything\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            The Fourier Series\n                        \n                    \n\n                    \n                        \n                            \n                        \n                        \n                        \n                            \n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n    \n\nNo matching items"
  },
  {
    "objectID": "index.html#lecture-notes",
    "href": "index.html#lecture-notes",
    "title": "Stat 24320",
    "section": "",
    "text": "Title\n            \n                \n                    Lecture Day\n                \n                \n                \n                    Readings\n                \n                \n                    Colab Link\n        \n        \n                    \n                        \n                            Intro to linear systems\n                        \n                    \n\n                    \n                        \n                            1\n                        \n                        \n                        \n                            Ch. 1.1-1.3\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            More Systems of Linear Equations\n                        \n                    \n\n                    \n                        \n                            2\n                        \n                        \n                        \n                            Ch. 1.4-1.5\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch2 Lecture 1\n                        \n                    \n\n                    \n                        \n                            3\n                        \n                        \n                        \n                            2.1-2.3\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch2 Lecture 2\n                        \n                    \n\n                    \n                        \n                            4\n                        \n                        \n                        \n                            2.3-2.4\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch2 Lecture 3\n                        \n                    \n\n                    \n                        \n                            5\n                        \n                        \n                        \n                            2.3-2.8\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch2 Lecture 4\n                        \n                    \n\n                    \n                        \n                            6\n                        \n                        \n                        \n                            2.8\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch2 Lecture 5\n                        \n                    \n\n                    \n                        \n                            7\n                        \n                        \n                        \n                            2.4,2.5,2.6\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch3 Lecture 1\n                        \n                    \n\n                    \n                        \n                            8\n                        \n                        \n                        \n                            3.3, 3.4, 3.6\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch4 Lecture 1\n                        \n                    \n\n                    \n                        \n                            9\n                        \n                        \n                        \n                            4.1,4.2\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch4 Lecture 2\n                        \n                    \n\n                    \n                        \n                            10\n                        \n                        \n                        \n                            4.2,4.3,4.4\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch5 Lecture 1\n                        \n                    \n\n                    \n                        \n                            11\n                        \n                        \n                        \n                            5.1,5.2\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch5 Lecture 2\n                        \n                    \n\n                    \n                        \n                            11\n                        \n                        \n                        \n                            5.1,5.2,5.3\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch 5 Lecture 3\n                        \n                    \n\n                    \n                        \n                            12\n                        \n                        \n                        \n                            \n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch5 Lecture 2\n                        \n                    \n\n                    \n                        \n                            12\n                        \n                        \n                        \n                            5.6\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch5 Lecture 4\n                        \n                    \n\n                    \n                        \n                            14\n                        \n                        \n                        \n                            \n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch5 Lecture 5\n                        \n                    \n\n                    \n                        \n                            15\n                        \n                        \n                        \n                            5.1,5.2,5.3\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch6 Fourier\n                        \n                    \n\n                    \n                        \n                            18\n                        \n                        \n                        \n                            5.1,5.2,5.3\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Review\n                        \n                    \n\n                    \n                        \n                            18\n                        \n                        \n                        \n                            Everything\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            The Fourier Series\n                        \n                    \n\n                    \n                        \n                            \n                        \n                        \n                        \n                            \n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n    \n\nNo matching items"
  },
  {
    "objectID": "index.html#notebooks",
    "href": "index.html#notebooks",
    "title": "Stat 24320",
    "section": "Notebooks",
    "text": "Notebooks\n\n\n    \n            \n                \n        \n            Title\n            \n                \n                    Lecture Day\n                \n                \n                    Colab Link\n        \n        \n                    \n                        \n                            PageRank Tutorial\n                        \n                    \n\n                    \n                        \n                            1\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Turing Patterns\n                        \n                    \n\n                    \n                        \n                            4\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            MCMC Pagerank\n                        \n                    \n\n                    \n                        \n                            6\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n    \n\nNo matching items"
  },
  {
    "objectID": "index.html#homeworks",
    "href": "index.html#homeworks",
    "title": "Stat 24320",
    "section": "Homeworks",
    "text": "Homeworks\n\n\n            \n                \n        \n            Title\n            \n                \n                    Due Date\n                \n                \n                \n                    Textbook Chapters\n                \n                \n                    Solutions\n        \n        \n                    \n                        \n                            Homework 1\n                        \n                    \n\n                    \n                        \n                            Wednesday, Week 2\n                        \n                        \n                        \n                            1\n                        \n                        \n                            \n                           \n                            \n                                \n                                        Homework 1 Solutions\n                                    \n                            \n\n                \n\n                \n                \n                    \n                        \n                            Homework 2\n                        \n                    \n\n                    \n                        \n                            Wednesday, Week 3\n                        \n                        \n                        \n                            2\n                        \n                        \n                            \n                           \n                            \n                                \n                                        Homework 2 Solutions\n                                    \n                            \n\n                \n\n                \n                \n                    \n                        \n                            Homework 3\n                        \n                    \n\n                    \n                        \n                            Wednesday, Week 4\n                        \n                        \n                        \n                            2.4, 2.5, 2.8\n                        \n                        \n                            \n                           \n                            \n                                \n                                        Homework 3 Solutions\n                                    \n                            \n\n                \n\n                \n                \n                    \n                        \n                            Homework 4\n                        \n                    \n\n                    \n                        \n                            Friday, Week 6\n                        \n                        \n                        \n                            3.4, 3.6, 4.2\n                        \n                        \n                            \n                           \n                            \n                                \n                                        Homework 4 Solutions\n                                    \n                            \n\n                \n\n                \n                \n                    \n                        \n                            Homework 5\n                        \n                    \n\n                    \n                        \n                            Wednesday, Week 7\n                        \n                        \n                        \n                            5.1,5.2,5.3\n                        \n                        \n                            \n                           \n                            \n                                \n                                        Homework 5 Solutions\n                                    \n                            \n\n                \n\n                \n                \n                    \n                        \n                            Homework 6\n                        \n                    \n\n                    \n                        \n                            Friday, Week 9\n                        \n                        \n                        \n                            5.3, 5.4, 5.6, 6.6\n                        \n                        \n                            \n                           \n                            \n\n                \n\n                \n                \n                    \n                        \n                            Projects 1\n                        \n                    \n\n                    \n                        \n                            Friday, Week 4\n                        \n                        \n                        \n                            1, 2\n                        \n                        \n                            \n                           \n                            \n\n                \n\n                \n                \n                    \n                        \n                            Projects 2\n                        \n                    \n\n                    \n                        \n                            Friday, Week 7\n                        \n                        \n                        \n                            3, 4\n                        \n                        \n                            \n                           \n                            \n\n                \n\n                \n                \n                    \n                        \n                            Projects 3\n                        \n                    \n\n                    \n                        \n                            Friday, exam week\n                        \n                        \n                        \n                            5\n                        \n                        \n                            \n                           \n                            \n\n                \n\n                \n    \n\nNo matching items"
  },
  {
    "objectID": "HW/HW1.html",
    "href": "HW/HW1.html",
    "title": "Homework 1",
    "section": "",
    "text": "1 \nBy solving a \\(3 \\times 3\\) system, find the coefficients in the equation of the parabola \\(y=\\alpha+\\beta x+\\gamma x^{2}\\) that passes through the points \\((1,1),(2,2)\\), and \\((3,0)\\).\n\n\n2 \nSuppose that 100 insects are distributed in an enclosure consisting of four chambers with passageways between them as shown below.\n\nAt the end of one minute, the insects have redistributed themselves. Assume that a minute is not enough time for an insect to visit more than one chamber and that at the end of a minute \\(40 \\%\\) of the insects in each chamber have not left the chamber they occupied at the beginning of the minute. The insects that leave a chamber disperse uniformly among the chambers that are directly accessible from the one they initially occupied-e.g., from \\(\\# 3\\), half move to \\(\\# 2\\) and half move to \\(\\# 4\\).\n\nIf at the end of one minute there are \\(12,25,26\\), and 37 insects in chambers \\(\\# 1, \\# 2, \\# 3\\), and \\(\\# 4\\), respectively, determine what the initial distribution had to be.\nIf the initial distribution is \\(20,20,20,40\\), what is the distribution at the end of one minute?\n\n\n\n3 \nSuppose that \\([\\mathbf{A} \\mid \\mathbf{b}]\\) is the augmented matrix associated with a linear system. You know that performing row operations on \\([\\mathbf{A} \\mid \\mathbf{b}]\\) does not change the solution of the system. However, no mention of column operations was ever made because column operations can alter the solution.\n\nDescribe the effect on the solution of a linear system when columns \\(\\mathbf{A}_{* j}\\) and \\(\\mathbf{A}_{* k}\\) are interchanged.\nDescribe the effect when column \\(\\mathbf{A}_{* j}\\) is replaced by \\(\\alpha \\mathbf{A}_{* j}\\) for \\(\\alpha \\neq 0\\).\nDescribe the effect when \\(\\mathbf{A}_{* j}\\) is replaced by \\(\\mathbf{A}_{* j}+\\alpha \\mathbf{A}_{* k}\\). Hint: Experiment with a \\(2 \\times 2\\) or \\(3 \\times 3\\) system.\n\n\n\n4 \nUse the Gauss-Jordan method to solve the following three systems at the same time.\n\\[\n\\begin{array}{rr|l|l}\n2 x_{1}-x_{2} & =1 & 0 & 0 \\\\\n-x_{1}+2 x_{2}-x_{3}&=0 & 1 & 0 \\\\\n-x_{2}+x_{3}&=0 & 0 & 1\n\\end{array}\n\\]\n\n\n5 \nConsider the following system:\n\\[\n\\begin{aligned}\n10^{-3} x-y & =1, \\\\\nx+y & =0 .\n\\end{aligned}\n\\]\n\nUse 3-digit arithmetic with no pivoting to solve this system.\nFind a system that is exactly satisfied by your solution from part (a), and note how close this system is to the original system.\nNow use partial pivoting and 3-digit arithmetic to solve the original system.\nFind a system that is exactly satisfied by your solution from part (c), and note how close this system is to the original system.\nUse exact arithmetic to obtain the solution to the original system, and compare the exact solution with the results of parts (a) and (c).\nRound the exact solution to three significant digits, and compare the result with those of parts (a) and (c).\n\n\n\n6 \nConsider the following well-scaled matrix:\n\\[\n\\mathbf{W}_{n}=\\left(\\begin{array}{rrrrrrr}\n1 & 0 & 0 & \\cdots & 0 & 0 & 1 \\\\\n-1 & 1 & 0 & \\cdots & 0 & 0 & 1 \\\\\n-1 & -1 & 1 & \\ddots & 0 & 0 & 1 \\\\\n\\vdots & \\vdots & \\ddots & \\ddots & \\ddots & \\vdots & \\vdots \\\\\n-1 & -1 & -1 & \\ddots & 1 & 0 & 1 \\\\\n-1 & -1 & -1 & \\cdots & -1 & 1 & 1 \\\\\n-1 & -1 & -1 & \\cdots & -1 & -1 & 1\n\\end{array}\\right)\n\\]\n\nReduce \\(\\mathbf{W}_{n}\\) to an upper-triangular form using Gaussian elimination with partial pivoting, and determine the element of maximal magnitude that emerges during the elimination procedure.\n\n\n\n7 \nAnswer True/False and explain your answers: 1. If a linear system is inconsistent, then the rank of the augmented matrix exceeds the number of unknowns. 2. Any homogeneous linear system is consistent. 3. A system of 3 linear equations in 4 unknowns has infinitely many solutions. 4. Every matrix can be reduced to only one matrix in row echelon form. 5. Any homogeneous linear system with more equations than unknowns has a nontrivial solution."
  },
  {
    "objectID": "HW/HW1.nosol.html",
    "href": "HW/HW1.nosol.html",
    "title": "Homework 1",
    "section": "",
    "text": "1 \nBy solving a \\(3 \\times 3\\) system, find the coefficients in the equation of the parabola \\(y=\\alpha+\\beta x+\\gamma x^{2}\\) that passes through the points \\((1,1),(2,2)\\), and \\((3,0)\\).\n\n\n2 \nSuppose that 100 insects are distributed in an enclosure consisting of four chambers with passageways between them as shown below.\n\nAt the end of one minute, the insects have redistributed themselves. Assume that a minute is not enough time for an insect to visit more than one chamber and that at the end of a minute \\(40 \\%\\) of the insects in each chamber have not left the chamber they occupied at the beginning of the minute. The insects that leave a chamber disperse uniformly among the chambers that are directly accessible from the one they initially occupied-e.g., from \\(\\# 3\\), half move to \\(\\# 2\\) and half move to \\(\\# 4\\).\n\nIf at the end of one minute there are \\(12,25,26\\), and 37 insects in chambers \\(\\# 1, \\# 2, \\# 3\\), and \\(\\# 4\\), respectively, determine what the initial distribution had to be.\nIf the initial distribution is \\(20,20,20,40\\), what is the distribution at the end of one minute?\n\n\n\n3 \nSuppose that \\([\\mathbf{A} \\mid \\mathbf{b}]\\) is the augmented matrix associated with a linear system. You know that performing row operations on \\([\\mathbf{A} \\mid \\mathbf{b}]\\) does not change the solution of the system. However, no mention of column operations was ever made because column operations can alter the solution.\n\nDescribe the effect on the solution of a linear system when columns \\(\\mathbf{A}_{* j}\\) and \\(\\mathbf{A}_{* k}\\) are interchanged.\nDescribe the effect when column \\(\\mathbf{A}_{* j}\\) is replaced by \\(\\alpha \\mathbf{A}_{* j}\\) for \\(\\alpha \\neq 0\\).\nDescribe the effect when \\(\\mathbf{A}_{* j}\\) is replaced by \\(\\mathbf{A}_{* j}+\\alpha \\mathbf{A}_{* k}\\). Hint: Experiment with a \\(2 \\times 2\\) or \\(3 \\times 3\\) system.\n\n\n\n4 \nUse the Gauss-Jordan method to solve the following three systems at the same time.\n\\[\n\\begin{array}{rr|l|l}\n2 x_{1}-x_{2} & =1 & 0 & 0 \\\\\n-x_{1}+2 x_{2}-x_{3}&=0 & 1 & 0 \\\\\n-x_{2}+x_{3}&=0 & 0 & 1\n\\end{array}\n\\]\n\n\n5 \nConsider the following system:\n\\[\n\\begin{aligned}\n10^{-3} x-y & =1, \\\\\nx+y & =0 .\n\\end{aligned}\n\\]\n\nUse 3-digit arithmetic with no pivoting to solve this system.\nFind a system that is exactly satisfied by your solution from part (a), and note how close this system is to the original system.\nNow use partial pivoting and 3-digit arithmetic to solve the original system.\nFind a system that is exactly satisfied by your solution from part (c), and note how close this system is to the original system.\nUse exact arithmetic to obtain the solution to the original system, and compare the exact solution with the results of parts (a) and (c).\nRound the exact solution to three significant digits, and compare the result with those of parts (a) and (c).\n\n\n\n6 \nConsider the following well-scaled matrix:\n\\[\n\\mathbf{W}_{n}=\\left(\\begin{array}{rrrrrrr}\n1 & 0 & 0 & \\cdots & 0 & 0 & 1 \\\\\n-1 & 1 & 0 & \\cdots & 0 & 0 & 1 \\\\\n-1 & -1 & 1 & \\ddots & 0 & 0 & 1 \\\\\n\\vdots & \\vdots & \\ddots & \\ddots & \\ddots & \\vdots & \\vdots \\\\\n-1 & -1 & -1 & \\ddots & 1 & 0 & 1 \\\\\n-1 & -1 & -1 & \\cdots & -1 & 1 & 1 \\\\\n-1 & -1 & -1 & \\cdots & -1 & -1 & 1\n\\end{array}\\right)\n\\]\n\nReduce \\(\\mathbf{W}_{n}\\) to an upper-triangular form using Gaussian elimination with partial pivoting, and determine the element of maximal magnitude that emerges during the elimination procedure.\n\n\n\n7 \nAnswer True/False and explain your answers: 1. If a linear system is inconsistent, then the rank of the augmented matrix exceeds the number of unknowns. 2. Any homogeneous linear system is consistent. 3. A system of 3 linear equations in 4 unknowns has infinitely many solutions. 4. Every matrix can be reduced to only one matrix in row echelon form. 5. Any homogeneous linear system with more equations than unknowns has a nontrivial solution."
  },
  {
    "objectID": "HW/HW1.sol.html",
    "href": "HW/HW1.sol.html",
    "title": "Homework 1 Solutions",
    "section": "",
    "text": "1 \nBy solving a \\(3 \\times 3\\) system, find the coefficients in the equation of the parabola \\(y=\\alpha+\\beta x+\\gamma x^{2}\\) that passes through the points \\((1,1),(2,2)\\), and \\((3,0)\\).\n\n(Meyer 1.2.10) \\(\\alpha=-3, \\beta=\\frac{11}{2}\\), and \\(\\gamma=-\\frac{3}{2}\\)\n\n\n\n2 \nSuppose that 100 insects are distributed in an enclosure consisting of four chambers with passageways between them as shown below.\n\nAt the end of one minute, the insects have redistributed themselves. Assume that a minute is not enough time for an insect to visit more than one chamber and that at the end of a minute \\(40 \\%\\) of the insects in each chamber have not left the chamber they occupied at the beginning of the minute. The insects that leave a chamber disperse uniformly among the chambers that are directly accessible from the one they initially occupied-e.g., from \\(\\# 3\\), half move to \\(\\# 2\\) and half move to \\(\\# 4\\).\n\nIf at the end of one minute there are \\(12,25,26\\), and 37 insects in chambers \\(\\# 1, \\# 2, \\# 3\\), and \\(\\# 4\\), respectively, determine what the initial distribution had to be.\nIf the initial distribution is \\(20,20,20,40\\), what is the distribution at the end of one minute?\n\n\n(Meyer 1.2.11) (a) If \\(x_{i}=\\) the number initially in chamber \\(\\# i\\), then\n\\[\n\\begin{aligned}\n.4 x_{1}+0 x_{2}+0 x_{3}+.2 x_{4} & =12 \\\\\n0 x_{1}+.4 x_{2}+.3 x_{3}+.2 x_{4} & =25 \\\\\n0 x_{1}+.3 x_{2}+.4 x_{3}+.2 x_{4} & =26 \\\\\n.6 x_{1}+.3 x_{2}+.3 x_{3}+.4 x_{4} & =37\n\\end{aligned}\n\\]\nand the solution is \\(x_{1}=10, x_{2}=20, x_{3}=30\\), and \\(x_{4}=40\\).\n\n\\(16,22,22,40\\)\n\n\n\n\n3 \nSuppose that \\([\\mathbf{A} \\mid \\mathbf{b}]\\) is the augmented matrix associated with a linear system. You know that performing row operations on \\([\\mathbf{A} \\mid \\mathbf{b}]\\) does not change the solution of the system. However, no mention of column operations was ever made because column operations can alter the solution.\n\nDescribe the effect on the solution of a linear system when columns \\(\\mathbf{A}_{* j}\\) and \\(\\mathbf{A}_{* k}\\) are interchanged.\nDescribe the effect when column \\(\\mathbf{A}_{* j}\\) is replaced by \\(\\alpha \\mathbf{A}_{* j}\\) for \\(\\alpha \\neq 0\\).\nDescribe the effect when \\(\\mathbf{A}_{* j}\\) is replaced by \\(\\mathbf{A}_{* j}+\\alpha \\mathbf{A}_{* k}\\). Hint: Experiment with a \\(2 \\times 2\\) or \\(3 \\times 3\\) system.\n\n\n(Meyer 1.2.13) a. This has the effect of interchanging the order of the unknowns \\(-x_{j}\\) and \\(x_{k}\\) are p b. The solution to the new system is the same as the solution to the old system except that the solution for the \\(j^{\\text {th }}\\) unknown of the new system is \\(\\hat{x}_{j}=\\frac{1}{\\alpha} x_{j}\\). This has the effect of “changing the units” of the \\(j^{\\text {th }}\\) unknown. c. The solution to the new system is the same as the solution for the old system except that the solution for the \\(k^{t h}\\) unknown in the new system is \\(\\hat{x}_{k}=x_{k}-\\alpha x_{j}\\).\n\n\n\n4 \nUse the Gauss-Jordan method to solve the following three systems at the same time.\n\\[\n\\begin{array}{rr|l|l}\n2 x_{1}-x_{2} & =1 & 0 & 0 \\\\\n-x_{1}+2 x_{2}-x_{3}&=0 & 1 & 0 \\\\\n-x_{2}+x_{3}&=0 & 0 & 1\n\\end{array}\n\\]\n\n(Meyer 1.3.3) \\[\n\\left(\\begin{array}{lll}1 & 1 & 1 \\\\ 1 & 2 & 2 \\\\ 1 & 2 & 3\\end{array}\\right)\n\\]\n\n\n\n5 \nConsider the following system:\n\\[\n\\begin{aligned}\n10^{-3} x-y & =1, \\\\\nx+y & =0 .\n\\end{aligned}\n\\]\n\nUse 3-digit arithmetic with no pivoting to solve this system.\nFind a system that is exactly satisfied by your solution from part (a), and note how close this system is to the original system.\nNow use partial pivoting and 3-digit arithmetic to solve the original system.\nFind a system that is exactly satisfied by your solution from part (c), and note how close this system is to the original system.\nUse exact arithmetic to obtain the solution to the original system, and compare the exact solution with the results of parts (a) and (c).\nRound the exact solution to three significant digits, and compare the result with those of parts (a) and (c).\n\n\n(Meyer 1.5.1) (a)\n\\(\\left[\\begin{smallmatrix}0.001 & -1 & 1\\\\1 & 1 & 0\\end{smallmatrix}\\right]\\) \\(\\overrightarrow{E_{2,1}(1000)}\\) \\(\\left[\\begin{smallmatrix}0.001 & -1.0 & 1.0\\\\0 & 1.0 \\cdot 10^{3} & -1.0 \\cdot 10^{3}\\end{smallmatrix}\\right]\\) \\(\\overrightarrow{E_{1}(1000)}\\) \\(\\left[\\begin{smallmatrix}1.00 & -1.00 \\cdot 10^{3} & 1.00 \\cdot 10^{3}\\\\0 & 1.00 \\cdot 10^{3} & -1.00 \\cdot 10^{3}\\end{smallmatrix}\\right]\\)\nThe solution is \\((0,-1)\\).\n\nOne example of a system whose exact solution is \\(0,-1\\) would be\n\n\\[\n\\begin{aligned}\n2x-y & =1, \\\\\nx+y & =-1 .\n\\end{aligned}\n\\]\nThis is not close at all to the original system!\n\n\n\n\\(\\left[\\begin{smallmatrix}0.001 & -1 & 1\\\\1 & 1 & 0\\end{smallmatrix}\\right]\\) \\(\\overrightarrow{E_{2,1}}\\) \\(\\left[\\begin{smallmatrix}1.0 & 1.0 & 0\\\\0.001 & -1.0 & 1.0\\end{smallmatrix}\\right]\\) \\(\\overrightarrow{E_{2,1}(1/1000)}\\) \\(\\left[\\begin{smallmatrix}1.00 & 1.00 & 0\\\\-7.25 \\cdot 10^{-8} & -1.00 & 1.00\\end{smallmatrix}\\right]\\)\nSolution is \\((1,-1)\\).\n\nOne example of a system whose exact solution is \\(1,-1\\) would be \\[\n\\begin{aligned}\n0 x-y & =1, \\\\\nx+y & =0 .\n\\end{aligned}\n\\]\n\nThis is very close to the original system!\n\n\\(\\left(\\frac{1}{1.001}, \\frac{-1}{1.001}\\right)\\)\n\nThis is close to our solution in (c).\n\n\n\n6 \nConsider the following well-scaled matrix:\n\\[\n\\mathbf{W}_{n}=\\left(\\begin{array}{rrrrrrr}\n1 & 0 & 0 & \\cdots & 0 & 0 & 1 \\\\\n-1 & 1 & 0 & \\cdots & 0 & 0 & 1 \\\\\n-1 & -1 & 1 & \\ddots & 0 & 0 & 1 \\\\\n\\vdots & \\vdots & \\ddots & \\ddots & \\ddots & \\vdots & \\vdots \\\\\n-1 & -1 & -1 & \\ddots & 1 & 0 & 1 \\\\\n-1 & -1 & -1 & \\cdots & -1 & 1 & 1 \\\\\n-1 & -1 & -1 & \\cdots & -1 & -1 & 1\n\\end{array}\\right)\n\\]\n\nReduce \\(\\mathbf{W}_{n}\\) to an upper-triangular form using Gaussian elimination with partial pivoting, and determine the element of maximal magnitude that emerges during the elimination procedure.\n\n\n(Meyer 1.5.7) \\(2^{n-1}\\)\n\n\n\n7 \nAnswer True/False and explain your answers: 1. If a linear system is inconsistent, then the rank of the augmented matrix exceeds the number of unknowns. 2. Any homogeneous linear system is consistent. 3. A system of 3 linear equations in 4 unknowns has infinitely many solutions. 4. Every matrix can be reduced to only one matrix in row echelon form. 5. Any homogeneous linear system with more equations than unknowns has a nontrivial solution.\n\n(Shores 1.4.19)\n\nFalse. One very simple counterexample: \\(0 x=1\\).\n\nAnother: \\(\\left[\\begin{array}{ll}1 & 1 \\\\ 1 & 1\\end{array}\\right]\\left[\\begin{array}{l}x_{1} \\\\ x_{2}\\end{array}\\right]=\\left[\\begin{array}{l}1 \\\\ 2\\end{array}\\right]\\). The rank of the augmented matrix is 2, but the system is inconsistent.\nWe know that \\(b\\) must not be a linear combination of the columns of \\(A\\) for the system to be inconsistent; however, there is no requirement that the columns of \\(A\\) be linearly independent. Therefore the rank of the augmented matrix could be less than the number of unknowns.\n\nTrue, a homogeneous system must have the trivial solution.\nFalse, it could be inconsistent (and therefore have zero solutions).\nFalse. Here is a counterexample. The matrix \\(\\left[\\begin{array}{ll}2 & 2 \\\\ 0 & 1\\end{array}\\right]\\) can be reduced to either \\(\\left[\\begin{array}{ll}2 & 0 \\\\ 0 & 1\\end{array}\\right]\\) or \\(\\left[\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right]\\). (Only the latter is in reduced row echelon form, which must be unique.)\nFalse, a counterexample is \\(\\left[\\begin{array}{l}1 \\\\ 2\\end{array}\\right] x=\\left[\\begin{array}{l}0 \\\\ 0\\end{array}\\right]\\)."
  },
  {
    "objectID": "HW/HW2.sol.html",
    "href": "HW/HW2.sol.html",
    "title": "Homework 2 Solutions",
    "section": "",
    "text": "1 \nGiven that a linear system in the unknowns \\(x_{1}, x_{2}, x_{3}, x_{4}\\) has general solution \\(\\left(x_{2}+3 x_{4}+4, x_{2}, 2-x_{4}, x_{4}\\right)\\) for free variables \\(x_{2}, x_{4}\\), find a minimal reduced row echelon for this system.\n\nExercise 2.1.13\nWe know that there are at least two equations in the system (since we have two constraints in the general solution.) We can write these two constraints as:\n\\(R=\\begin{bmatrix}1 & -1 & 0 & -3 & 4 \\\\ 0 & 0 & 1 & 1 & 2\\end{bmatrix}\\)\nWe could also have written\n\\(R=\\begin{bmatrix}1 & -1 & 0 & -3 & 4 \\\\ 0 & 0 & 1 & 1 & 2 \\\\ 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0\\end{bmatrix}\\), which has the same solutions.\nJust for kicks, checking in Sympy:\n\nfrom sympy import print_latex\ndef my_print(x, *args, **kwargs):\n    print_latex(x, itex=False, mode='equation', *args, **kwargs)\n\n\nfrom sympy import Matrix\nR=Matrix([[1, -1, 0, -3], [0, 0, 1, 1]])\nrhs = Matrix([4,2])\nR.gauss_jordan_solve(rhs)\nmy_print(R.gauss_jordan_solve(rhs))\n\n\\begin{equation}\\left( \\left[\\begin{matrix}\\tau_{0} + 3 \\tau_{1} + 4\\\\\\tau_{0}\\\\2 - \\tau_{1}\\\\\\tau_{1}\\end{matrix}\\right], \\  \\left[\\begin{matrix}\\tau_{0}\\\\\\tau_{1}\\end{matrix}\\right]\\right)\\end{equation}\n\n\nfrom sympy import Matrix, print_latex\nR=Matrix([[1, -1, 0, -3], [0, 0, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]])\nrhs = Matrix([4,2,0,0])\nmy_print(R.gauss_jordan_solve(rhs))\n\\[\\begin{equation}\\left( \\left[\\begin{matrix}\\tau_{0} + 3 \\tau_{1} + 4\\\\\\tau_{0}\\\\2 - \\tau_{1}\\\\\\tau_{1}\\end{matrix}\\right], \\  \\left[\\begin{matrix}\\tau_{0}\\\\\\tau_{1}\\end{matrix}\\right]\\right)\\end{equation}\\]\n\n\n\n2 \nUse the technique of Example 2.10 in your textbook to balance the following chemical equation:\n\\[\n\\mathrm{C}_{8} \\mathrm{H}_{18}+\\mathrm{O}_{2} \\rightarrow \\mathrm{CO}_{2}+\\mathrm{H}_{2} \\mathrm{O} .\n\\]\n\nExercise 2.2.23\nWith vectors indicating amount of \\(\\mathrm{C}, \\mathrm{H}\\) and \\(\\mathrm{O}\\), variables the number of molecules of each compound occurring, system represented is\n\\[\nx_{1}\\left[\\begin{array}{r}\n8 \\\\\n18 \\\\\n0\n\\end{array}\\right]+x_{2}\\left[\\begin{array}{l}\n0 \\\\\n0 \\\\\n2\n\\end{array}\\right]=x_{3}\\left[\\begin{array}{l}\n1 \\\\\n0 \\\\\n2\n\\end{array}\\right]+x_{4}\\left[\\begin{array}{l}\n0 \\\\\n2 \\\\\n1\n\\end{array}\\right]\n\\]\nresulting in coefficient matrix\n\\[\n\\left[\\begin{array}{cccc}\n8 & 0 & -1 & 0 \\\\\n18 & 0 & 0 & -2 \\\\\n0 & 2 & -2 & -1\n\\end{array}\\right] \\rightarrow\\left[\\begin{array}{cccc}\n1 & 0 & 0 & -\\frac{1}{9} \\\\\n0 & 1 & 0 & -\\frac{25}{18} \\\\\n0 & 0 & 1 & -\\frac{8}{9}\n\\end{array}\\right]\n\\]\nThe smallest integer solution is \\(x_{1}=2, x_{2}=23, x_{3}=16, x_{4}=18\\).\n\n\n\\begin{equation}\\left[\\begin{matrix}2 & 25 & 16 & 18\\end{matrix}\\right]\\end{equation}\n\\begin{equation}\\left[\\begin{matrix}1 & 0 & 0 & - \\frac{1}{9}\\\\0 & 1 & 0 & - \\frac{25}{18}\\\\0 & 0 & 1 & - \\frac{8}{9}\\end{matrix}\\right]\\end{equation}\n\n\n\n\n\n3 \nExpress the following functions, if linear, as matrix operators. (If not linear, explain why.)\n\n\\(T\\left(\\left(x_{1}, x_{2}\\right)\\right)=\\left(x_{1}+x_{2}, 2 x_{1}, 4 x_{2}-x_{1}\\right)\\)\n\\(T\\left(\\left(x_{1}, x_{2}\\right)\\right)=\\left(x_{1}+x_{2}, 2 x_{1} x_{2}\\right)\\)\n\\(T\\left(\\left(x_{1}, x_{2}, x_{3}\\right)\\right)=\\left(2 x_{3},-x_{1}\\right)\\)\n\\(T\\left(\\left(x_{1}, x_{2}, x_{3}\\right)\\right)=\\left(x_{2}-x_{1}, x_{3}, x_{2}+x_{3}\\right)\\)\n\n\nExercise 2.3.3\nOperator is \\(T_{A}\\) with:\n\n\\(A=\\left[\\begin{array}{rr}1 & 1 \\\\ 2 & 0 \\\\ 4 & -1\\end{array}\\right]\\)\nnonlinear\n\\(\\left[\\begin{array}{rrr}0 & 0 & 2 \\\\ -1 & 0 & 0\\end{array}\\right]\\left(\\right.\\)\n\n\n\\(A=\\left[\\begin{array}{rrr}-1 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 1 & 1\\end{array}\\right]\\)\n\n\n\n\n4 \nA fixed-point of a linear operator \\(T_{A}\\) is a vector \\(\\mathbf{x}\\) such that \\(T_{A}(\\mathbf{x})=\\mathbf{x}\\). Find all fixed points, if any, of the linear operators in the previous exercise.\n\nExercise 2.3.9\nWe require our matrix to be square (i.e., have the same number of rows and columns) to have a fixed point. For the matrices in the previous exercise, only the matrix in part (d) is square. The fixed points are the solutions to the system of equations\n\\[\n\\left[\\begin{array}{rrr}-1 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 1 & 1\\end{array}\\right]\\left[\\begin{array}{l}x_{1} \\\\ x_{2} \\\\ x_{3}\\end{array}\\right]=\\left[\\begin{array}{l}x_{1} \\\\ x_{2} \\\\ x_{3}\\end{array}\\right]\n\\]\nFirst, we subtract the right-hand side from the left-hand side to get\n\\[\n\\left[\\begin{array}{rrr}-1 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 1 & 1\\end{array}\\right]\\left[\\begin{array}{l}x_{1} \\\\ x_{2} \\\\ x_{3}\\end{array}\\right]-\\left[\\begin{array}{l}x_{1} \\\\ x_{2} \\\\ x_{3}\\end{array}\\right]=\\left[\\begin{array}{l}0 \\\\ 0 \\\\ 0\\end{array}\\right]\n\\]\nThis simplifies to\n\\[\n\\left[\\begin{array}{rrr}-2 & 1 & 0 \\\\ 0 & -1 & 1 \\\\ 0 & 1 & 0\\end{array}\\right]\\left[\\begin{array}{l}x_{1} \\\\ x_{2} \\\\ x_{3}\\end{array}\\right]=\\left[\\begin{array}{l}0 \\\\ 0 \\\\ 0\\end{array}\\right]\n\\]\nPutting our matrix into reduced row echelon form, we have\n\\[\n\\left[\\begin{array}{rrr}-2 & 1 & 0 \\\\ 0 & -1 & 1 \\\\ 0 & 1 & 0\\end{array}\\right] \\rightarrow\\left[\\begin{array}{rrr}1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1\\end{array}\\right]\n\\]\nThis is the identity matrix, which means that the system of equations is nonsingular and has a unique solution. Moreover, the right-hand side of the equation is the zero vector, which means that the fixed point is the zero vector: \\(\\left(x_{1}, x_{2}, x_{3}\\right)=\\left(0,0,0\\right)\\).\n\nfrom sympy import eye\n#| echo: false\n#| output: none\n\nA = Matrix([[-1, 1, 0], [0, 0, 1], [0, 1, 1]])\n(A-eye(3))\n# append a zero vector to the right of the matrix\nR = (A-eye(3)).row_join(Matrix([0,0,0]))\nR.rref()\n\n(Matrix([\n [1, 0, 0, 0],\n [0, 1, 0, 0],\n [0, 0, 1, 0]]),\n (0, 1, 2))\n\n\n\n\n\n5 \nA linear operator on \\(\\mathbb{R}^{2}\\) is defined by first applying a scaling operator with scale factors of 2 in the \\(x\\)-direction and 4 in the \\(y\\)-direction, followed by a counterclockwise rotation about the origin of \\(\\pi / 6\\) radians. Express this operator and the operator that results from reversing the order of the scaling and rotation as matrix operators.\n\nExercise 2.3.5 The scaling operator is given by the matrix\n\\[\n\\left[\\begin{array}{ll}\n2 & 0 \\\\\n0 & 4\n\\end{array}\\right]\n\\]\nTo create a counterclockwise rotation by \\(\\theta\\) radians, we use the matrix\n\\[\n\\left[\\begin{array}{ll}\n\\cos \\theta & -\\sin \\theta \\\\\n\\sin \\theta & \\cos \\theta\n\\end{array}\\right]\n\\]\nHere, we want to rotate by \\(\\pi / 6\\) radians, so we have\n\\[\n\\left[\\begin{array}{ll}\n\\cos \\frac{\\pi}{6} & -\\sin \\frac{\\pi}{6} \\\\\n\\sin \\frac{\\pi}{6} & \\cos \\frac{\\pi}{6}\n\\end{array}\\right]=\\left[\\begin{array}{ll}\n\\frac{\\sqrt{3}}{2} & -\\frac{1}{2} \\\\\n\\frac{1}{2} & \\frac{\\sqrt{3}}{2}\n\\end{array}\\right]\n\\]\nThe operator for the scaling followed by the rotation is the product of these two matrices:\n\\[T_{A}, A=\n\\left[\\begin{array}{ll}\n\\frac{\\sqrt{3}}{2} & -\\frac{1}{2} \\\\\n\\frac{1}{2} & \\frac{\\sqrt{3}}{2}\n\\end{array}\\right]\\left[\\begin{array}{ll}\n2 & 0 \\\\\n0 & 4\n\\end{array}\\right]=\\left[\\begin{array}{ll}\n\\sqrt{3} & -2 \\\\\n1 & 2 \\sqrt{3}\n\\end{array}\\right]\n\\]\nThe operator for the rotation followed by the scaling is the product of the matrices in reverse order:\n\\[T_{B}, B=\n\\left[\\begin{array}{ll}\n2 & 0 \\\\\n0 & 4\n\\end{array}\\right]\\left[\\begin{array}{ll}\n\\frac{\\sqrt{3}}{2} & -\\frac{1}{2} \\\\\n\\frac{1}{2} & \\frac{\\sqrt{3}}{2}\n\\end{array}\\right]=\\left[\\begin{array}{ll}\n\\sqrt{3} & -1 \\\\\n2 & 2 \\sqrt{3}\n\\end{array}\\right]\n\\]\n\n\n\n6 \nFind a scaling operator \\(S\\) and shearing operator \\(H\\) such that the concatenation \\(S \\circ H\\) maps the points \\((1,0)\\) to \\((2,0)\\) and \\((0,1)\\) to \\((4,3)\\).\n\nExercise 2.3.7\nWe know that the shearing operator must add a multiple of the \\(x_2\\) component to the \\(x_1\\) component. For now, we can write this as\n\\[\n\\left[\\begin{array}{ll}\n1 & \\alpha \\\\\n0 & 1\n\\end{array}\\right]\n\\]\nThen we can apply the shearing operator to our two points to get\n\\[\n\\left[\\begin{array}{ll}\n1 & \\alpha \\\\\n0 & 1\n\\end{array}\\right]\\left[\\begin{array}{l}\n1 \\\\\n0\n\\end{array}\\right]=\\left[\\begin{array}{l}\n1 \\\\\n0\n\\end{array}\\right]\n\\]\nand\n\\[\n\\left[\\begin{array}{ll}\n1 & \\alpha \\\\\n0 & 1\n\\end{array}\\right]\\left[\\begin{array}{l}\n0 \\\\\n1\n\\end{array}\\right]=\\left[\\begin{array}{l}\n\\alpha \\\\\n1\n\\end{array}\\right]\n\\]\nWe know our scaling operator will be of the form\n\\[\n\\left[\\begin{array}{ll}\ns & 0 \\\\\n0 & t\n\\end{array}\\right]\n\\]\nApplying this to our two points, we get\n\\[\n\\left[\\begin{array}{ll}\ns & 0 \\\\\n0 & t\n\\end{array}\\right]\\left[\\begin{array}{l}\n1 \\\\\n0\n\\end{array}\\right]=\\left[\\begin{array}{l}\ns \\\\\n0\n\\end{array}\\right]\n\\]\nand\n\\[\n\\left[\\begin{array}{ll}\ns & 0 \\\\\n0 & t\n\\end{array}\\right]\\left[\\begin{array}{l}\n\\alpha \\\\\n1\n\\end{array}\\right]=\\left[\\begin{array}{l}\ns \\alpha \\\\\nt\n\\end{array}\\right]\n\\]\nWe can now set up the equations\n\\[\n\\begin{aligned}\ns &=2 , 0=0\\\\\n\\alpha s &=4,\nt =3\n\\end{aligned}\n\\]\nIt is clear that the solutions to these equations are \\(s=2, t=3, \\alpha=2\\). Thus, our scaling operator is\n\\[\nS=\\left[\\begin{array}{ll}2 & 0 \\\\ 0 & 3\\end{array}\\right]\n\\]\nand our shearing operator is\n\\[\nH=\\left[\\begin{array}{ll}1 & 2 \\\\ 0 & 1\\end{array}\\right]\n\\]\nChecking:\n\nS = Matrix([[2, 0], [0, 3]])\nH = Matrix([[1, 2], [0, 1]])\nS*H*Matrix([1,0]), S*H*Matrix([0,1])\n\n(Matrix([\n [2],\n [0]]),\n Matrix([\n [4],\n [3]]))\n\n\n\n\n\n7 \nGiven transition matrices for discrete dynamical systems\n\n\\(\\left[\\begin{array}{rrr}.1 & .3 & 0 \\\\ 0 & .4 & 1 \\\\ .9 & .3 & 0\\end{array}\\right] \\quad\\) (b) \\(\\left[\\begin{array}{lll}0 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 0\\end{array}\\right] \\quad\\) (c) \\(\\left[\\begin{array}{rrr}.5 & .3 & 0 \\\\ 0 & .4 & 0 \\\\ .5 & .3 & 1\\end{array}\\right] \\quad\\) (d) \\(\\left[\\begin{array}{rrr}0 & 0 & 0.9 \\\\ 0.5 & 0 & 0 \\\\ 0 & 0.5 & 0.1\\end{array}\\right]\\) and initial state vector \\(\\mathbf{x}^{(0)}=\\frac{1}{2}(1,1,0)\\), calculate the first and second state vector for each system and determine whether it is a Markov chain.\n\n\nExercise 2.3.11\nWe know going in that (d) is not a stochastic matrix, because the neither the first or second column sums to 1. The others are stochastic.\nFirst and second states are (a) \\((0.2,0.2,0.6)\\), \\((0.08,0.68,0.24)\\) (b) \\(\\frac{1}{2}(0,1,1), \\frac{1}{2}(1,1,0)\\) (c) \\((0.4,0.3,0.4),(0.26,0.08,0.66)\\) (d) \\((0,0.25,0.25),(0.225,0,0.15)\\)\nAs expected, the states sum to 1 for a-c, but not for d. Therefore a-c are Markov chains.\n\n\n\n8 \nFor each of the dynamical systems of the previous exercise, determine by calculation whether the system tends to a limiting steady-state vector. If so, what is it?\n\nExercise 2.3.12\nWe can do this in Python by multiplying the initial state vector by each matrix a large number of times and seeing if the result converges.\n\nfrom sympy import Matrix\nA1 = Matrix([[0.1, 0.3, 0], [0, 0.4, 1], [0.9, 0.3, 0]])\nA2 = Matrix([[0, 0, 1], [0, 1, 0], [1, 0, 0]])\nA3 = Matrix([[0.5, 0.3, 0], [0, 0.4, 0], [0.5, 0.3, 1]])\nA4 = Matrix([[0, 0, 0.9], [0.5, 0, 0], [0, 0.5, 0.1]])\nx = Matrix([1/2, 1/2, 0])\n\ndef limit(A, x0, n):\n    print(x0)\n    for i in range(n):\n        x0 = A*x0\n        # This was initially confusing -- printing only every 10 iterations, I missed the fact that the chain alternates between two states.\n        if i % 10 == 0 or i % 10 == 1:\n            print(x0)\n\nprint(\"A1\")\nlimit(A1, x, 100)\nprint(\"A2\")\nlimit(A2, x, 100)\nprint(\"A3\")\nlimit(A3, x, 100)\nprint(\"A4\")\nlimit(A4, x, 100)\n\nA1\nMatrix([[0.500000000000000], [0.500000000000000], [0]])\nMatrix([[0.200000000000000], [0.200000000000000], [0.600000000000000]])\nMatrix([[0.0800000000000000], [0.680000000000000], [0.240000000000000]])\nMatrix([[0.172271278520000], [0.517519069520000], [0.310209651960000]])\nMatrix([[0.172482848708000], [0.517217279768000], [0.310299871524000]])\nMatrix([[0.172413911708915], [0.517241315268424], [0.310344773022661]])\nMatrix([[0.172413785751419], [0.517241299130031], [0.310344915118550]])\nMatrix([[0.172413793080544], [0.517241379195438], [0.310344827724019]])\nMatrix([[0.172413793066686], [0.517241379402194], [0.310344827531121]])\nMatrix([[0.172413793103395], [0.517241379310495], [0.310344827586110]])\nMatrix([[0.172413793103488], [0.517241379310308], [0.310344827586204]])\nMatrix([[0.172413793103448], [0.517241379310345], [0.310344827586207]])\nMatrix([[0.172413793103448], [0.517241379310345], [0.310344827586207]])\nMatrix([[0.172413793103448], [0.517241379310345], [0.310344827586207]])\nMatrix([[0.172413793103448], [0.517241379310345], [0.310344827586207]])\nMatrix([[0.172413793103448], [0.517241379310345], [0.310344827586207]])\nMatrix([[0.172413793103448], [0.517241379310345], [0.310344827586207]])\nMatrix([[0.172413793103448], [0.517241379310345], [0.310344827586207]])\nMatrix([[0.172413793103448], [0.517241379310345], [0.310344827586207]])\nMatrix([[0.172413793103448], [0.517241379310345], [0.310344827586207]])\nMatrix([[0.172413793103448], [0.517241379310345], [0.310344827586207]])\nA2\nMatrix([[0.500000000000000], [0.500000000000000], [0]])\nMatrix([[0], [0.500000000000000], [0.500000000000000]])\nMatrix([[0.500000000000000], [0.500000000000000], [0]])\nMatrix([[0], [0.500000000000000], [0.500000000000000]])\nMatrix([[0.500000000000000], [0.500000000000000], [0]])\nMatrix([[0], [0.500000000000000], [0.500000000000000]])\nMatrix([[0.500000000000000], [0.500000000000000], [0]])\nMatrix([[0], [0.500000000000000], [0.500000000000000]])\nMatrix([[0.500000000000000], [0.500000000000000], [0]])\nMatrix([[0], [0.500000000000000], [0.500000000000000]])\nMatrix([[0.500000000000000], [0.500000000000000], [0]])\nMatrix([[0], [0.500000000000000], [0.500000000000000]])\nMatrix([[0.500000000000000], [0.500000000000000], [0]])\nMatrix([[0], [0.500000000000000], [0.500000000000000]])\nMatrix([[0.500000000000000], [0.500000000000000], [0]])\nMatrix([[0], [0.500000000000000], [0.500000000000000]])\nMatrix([[0.500000000000000], [0.500000000000000], [0]])\nMatrix([[0], [0.500000000000000], [0.500000000000000]])\nMatrix([[0.500000000000000], [0.500000000000000], [0]])\nMatrix([[0], [0.500000000000000], [0.500000000000000]])\nMatrix([[0.500000000000000], [0.500000000000000], [0]])\nA3\nMatrix([[0.500000000000000], [0.500000000000000], [0]])\nMatrix([[0.400000000000000], [0.200000000000000], [0.400000000000000]])\nMatrix([[0.260000000000000], [0.0800000000000000], [0.660000000000000]])\nMatrix([[0.000913647940000000], [2.09715200000000e-5], [0.999065380540000]])\nMatrix([[0.000463115426000000], [8.38860800000000e-6], [0.999528495966000]])\nMatrix([[9.47077246639594e-7], [2.19902325555200e-9], [0.999999050723730]])\nMatrix([[4.74198330296463e-7], [8.79609302220801e-10], [0.999999524922060]])\nMatrix([[9.30630821712715e-10], [2.30584300921370e-13], [0.999999999069139]])\nMatrix([[4.65384586146634e-10], [9.22337203685479e-14], [0.999999999534523]])\nMatrix([[9.09422166223752e-13], [2.41785163922926e-17], [0.999999999999090]])\nMatrix([[4.54718336666794e-13], [9.67140655691706e-18], [0.999999999999545]])\nMatrix([[8.88170813796524e-16], [2.53530120045647e-21], [0.999999999999999]])\nMatrix([[4.44086167488622e-16], [1.01412048018259e-21], [0.999999999999999]])\nMatrix([[8.67360940451606e-19], [2.65845599156984e-25], [1.00000000000000]])\nMatrix([[4.33680549979483e-19], [1.06338239662794e-25], [1.00000000000000]])\nMatrix([[8.47032863626506e-22], [2.78759314981634e-29], [1.00000000000000]])\nMatrix([[4.23516440176032e-22], [1.11503725992654e-29], [1.00000000000000]])\nMatrix([[8.27180603784018e-25], [2.92300327466182e-33], [1.00000000000000]])\nMatrix([[4.13590302768910e-25], [1.16920130986473e-33], [1.00000000000000]])\nMatrix([[8.07793566026819e-28], [3.06499108173179e-37], [1.00000000000000]])\nMatrix([[4.03896783105359e-28], [1.22599643269272e-37], [1.00000000000000]])\nA4\nMatrix([[0.500000000000000], [0.500000000000000], [0]])\nMatrix([[0], [0.250000000000000], [0.250000000000000]])\nMatrix([[0.225000000000000], [0], [0.150000000000000]])\nMatrix([[0.00302039010000000], [0.00120386925000000], [0.00264539827500000]])\nMatrix([[0.00238085844750000], [0.00151019505000000], [0.000866474452500000]])\nMatrix([[4.03791461577412e-5], [2.52845782290541e-5], [2.17203569987220e-5]])\nMatrix([[1.95483212988498e-5], [2.01895730788706e-5], [1.48143248143993e-5]])\nMatrix([[4.06315437745616e-7], [3.50928578393455e-7], [2.81839630258029e-7]])\nMatrix([[2.53655667232226e-7], [2.03157718872808e-7], [2.03648252222531e-7]])\nMatrix([[4.89866853485548e-9], [3.95031105574765e-9], [3.70325613478943e-9]])\nMatrix([[3.33293052131049e-9], [2.44933426742774e-9], [2.34548114135277e-9]])\nMatrix([[6.19440844858878e-11], [4.70794262956052e-11], [4.43926066707180e-11]])\nMatrix([[3.99533460036462e-11], [3.09720422429439e-11], [2.79789738148744e-11]])\nMatrix([[7.53264744484970e-13], [5.82090739342239e-13], [5.32977001565985e-13]])\nMatrix([[4.79679301409387e-13], [3.76632372242485e-13], [3.44343069827718e-13]])\nMatrix([[9.09975559486833e-15], [7.10204455266123e-15], [6.50648903042589e-15]])\nMatrix([[5.85584012738330e-15], [4.54987779743417e-15], [4.20167117937320e-15]])\nMatrix([[1.10752324227116e-16], [8.61303873249460e-17], [7.93329748585034e-17]])\nMatrix([[7.13996773726531e-17], [5.53761621135579e-17], [5.09984911483233e-17]])\nMatrix([[1.34906603357799e-18], [1.04742159016943e-18], [9.64385211129635e-19]])\nMatrix([[8.67946690016671e-19], [6.74533016788995e-19], [6.20149316197677e-19]])\n\n\nAnswers:\n\nTends to steady state \\((0.172414,0.517241,0.310345)\\).\nNo steady state (alternates).\nYes, steady state \\((0,0,1)\\).\nYes, steady state \\((0,0,0)\\).\n\n\n\n\n9 \nA population is modeled with two states, immature and mature, and the resulting structured population model transition matrix is \\(\\left[\\begin{array}{cc}\\frac{1}{2} & 1 \\\\ \\frac{1}{2} & 0\\end{array}\\right]\\).\n\nExplain what this matrix says about the two states.\nStarting with a population of \\((30,100)\\), does the population stabilize, increase or decrease over time? If it stabilizes, to what distribution?\n\n\n\nfrom sympy import Matrix\nA = Matrix([[1/2, 1], [1/2, 0]])\nx = Matrix([30, 100])\nfor i in range(100):\n    x = A*x\n    if i % 10 == 0:\n        print(x)\n\nMatrix([[115.000000000000], [15.0000000000000]])\nMatrix([[86.6943359375000], [43.3056640625000]])\nMatrix([[86.6666936874390], [43.3333063125610]])\nMatrix([[86.6666666930541], [43.3333333069459]])\nMatrix([[86.6666666666924], [43.3333333333076]])\nMatrix([[86.6666666666667], [43.3333333333333]])\nMatrix([[86.6666666666667], [43.3333333333333]])\nMatrix([[86.6666666666667], [43.3333333333333]])\nMatrix([[86.6666666666667], [43.3333333333333]])\nMatrix([[86.6666666666667], [43.3333333333333]])\n\n\nExercise 2.3.13\nSolution. (a) The first column says that \\(50 \\%\\) of the immature become mature and \\(50 \\%\\) of the immature remain immature in one time period. The second column says that none of the mature survive, but each mature individual produces one immature in one time period. (b) The total populations after \\(0,3,6,9,18\\) time periods is a constant 130 , and populations tend to approximately \\((86.667,43.333)\\).\n\n\n\n10 \nA digraph \\(G\\) has vertex set \\(V=\\{1,2,3,4,5\\}\\) and edge set \\(E=\\) \\(\\{(2,1),(1,5),(2,5),(5,4),(4,2),(4,3),(3,2)\\}\\). Sketch a picture of the graph \\(G\\) and find its adjacency matrix. Use this to find the power of each vertex of the graph and determine whether this graph is dominance-directed.\n\nExercise 2.3.15\nPowers of vertices \\(1-5\\) are \\(2,4,3,5,3\\), respectively. Graph is dominance directed (there are no bi-directional edges between any pairs of vertices), adjacency matrix is \\(\\left[\\begin{array}{ccccc}0 & 0 & 0 & 0 & 1 \\\\ 1 & 0 & 0 & 0 & 1 \\\\ 0 & 1 & 0 & 0 & 0 \\\\ 0 & 1 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 & 0\\end{array}\\right]\\). Picture:\n\n\n\n\n11 \nConsider the linear difference \\(y_{k+2}-y_{k+1}-y_{k}=0\\).\n\nExpress this difference in matrix form.\nFind the first ten terms of the solution to this difference given the initial conditions \\(y_{0}=0, y_{1}=1\\). (This is the well-known Fibonacci sequence.)\n\n\nExercise 2.3.19\n\n\\(\\left[\\begin{array}{l}y_{k+1} \\\\ y_{k+2}\\end{array}\\right]=\\left[\\begin{array}{ll}0 & 1 \\\\ 1 & 1\\end{array}\\right]\\left[\\begin{array}{c}y_{k} \\\\ y_{k+1}\\end{array}\\right]\\)\nThe first ten terms are \\(0,1,1,2,3,5,8,13,21,34\\).\n\n\n\n\n12 \nSuppose that in Example 2.27 you invest \\(\\$ 1,000\\) initially (the zeroth year) and no further amounts. Make a table of the value of your investment for years 0 to 12. Also include a column that calculates the annual interest rate that your investment is earning each year, based on the current and previous year’s values. What conclusions do you draw? You will need a technology tool for this exercise.\n\nProblem 2.3.24 Solution.\nAfter a sizable third year earning of \\(24 \\%\\), the annual rate appears to settle down to \\(14.41 \\%\\). Partial table:\n\n\n\nYear\nValue\nInterest Rate (%)\n\n\n\n\n0\n1000\n-\n\n\n1\n1000\n0\n\n\n3\n1240\n24\n\n\n6\n1859\n14.1\n\n\n9\n2785\n14.41\n\n\n12\n4171\n14.41"
  },
  {
    "objectID": "HW/HW2.html",
    "href": "HW/HW2.html",
    "title": "Homework 2",
    "section": "",
    "text": "1 \nGiven that a linear system in the unknowns \\(x_{1}, x_{2}, x_{3}, x_{4}\\) has general solution \\(\\left(x_{2}+3 x_{4}+4, x_{2}, 2-x_{4}, x_{4}\\right)\\) for free variables \\(x_{2}, x_{4}\\), find a minimal reduced row echelon for this system.\n\n\n2 \nUse the technique of Example 2.10 in your textbook to balance the following chemical equation:\n\\[\n\\mathrm{C}_{8} \\mathrm{H}_{18}+\\mathrm{O}_{2} \\rightarrow \\mathrm{CO}_{2}+\\mathrm{H}_{2} \\mathrm{O} .\n\\]\n\n\n3 \nExpress the following functions, if linear, as matrix operators. (If not linear, explain why.)\n\n\\(T\\left(\\left(x_{1}, x_{2}\\right)\\right)=\\left(x_{1}+x_{2}, 2 x_{1}, 4 x_{2}-x_{1}\\right)\\)\n\\(T\\left(\\left(x_{1}, x_{2}\\right)\\right)=\\left(x_{1}+x_{2}, 2 x_{1} x_{2}\\right)\\)\n\\(T\\left(\\left(x_{1}, x_{2}, x_{3}\\right)\\right)=\\left(2 x_{3},-x_{1}\\right)\\)\n\\(T\\left(\\left(x_{1}, x_{2}, x_{3}\\right)\\right)=\\left(x_{2}-x_{1}, x_{3}, x_{2}+x_{3}\\right)\\)\n\n\n\n4 \nA fixed-point of a linear operator \\(T_{A}\\) is a vector \\(\\mathbf{x}\\) such that \\(T_{A}(\\mathbf{x})=\\mathbf{x}\\). Find all fixed points, if any, of the linear operators in the previous exercise.\n\n\n5 \nA linear operator on \\(\\mathbb{R}^{2}\\) is defined by first applying a scaling operator with scale factors of 2 in the \\(x\\)-direction and 4 in the \\(y\\)-direction, followed by a counterclockwise rotation about the origin of \\(\\pi / 6\\) radians. Express this operator and the operator that results from reversing the order of the scaling and rotation as matrix operators.\n\n\n6 \nFind a scaling operator \\(S\\) and shearing operator \\(H\\) such that the concatenation \\(S \\circ H\\) maps the points \\((1,0)\\) to \\((2,0)\\) and \\((0,1)\\) to \\((4,3)\\).\n\n\n7 \nGiven transition matrices for discrete dynamical systems\n\n\\(\\left[\\begin{array}{rrr}.1 & .3 & 0 \\\\ 0 & .4 & 1 \\\\ .9 & .3 & 0\\end{array}\\right] \\quad\\) (b) \\(\\left[\\begin{array}{lll}0 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 0\\end{array}\\right] \\quad\\) (c) \\(\\left[\\begin{array}{rrr}.5 & .3 & 0 \\\\ 0 & .4 & 0 \\\\ .5 & .3 & 1\\end{array}\\right] \\quad\\) (d) \\(\\left[\\begin{array}{rrr}0 & 0 & 0.9 \\\\ 0.5 & 0 & 0 \\\\ 0 & 0.5 & 0.1\\end{array}\\right]\\) and initial state vector \\(\\mathbf{x}^{(0)}=\\frac{1}{2}(1,1,0)\\), calculate the first and second state vector for each system and determine whether it is a Markov chain.\n\n\n\n8 \nFor each of the dynamical systems of the previous exercise, determine by calculation whether the system tends to a limiting steady-state vector. If so, what is it?\n\n\n9 \nA population is modeled with two states, immature and mature, and the resulting structured population model transition matrix is \\(\\left[\\begin{array}{cc}\\frac{1}{2} & 1 \\\\ \\frac{1}{2} & 0\\end{array}\\right]\\).\n\nExplain what this matrix says about the two states.\nStarting with a population of \\((30,100)\\), does the population stabilize, increase or decrease over time? If it stabilizes, to what distribution?\n\n\n\n10 \nA digraph \\(G\\) has vertex set \\(V=\\{1,2,3,4,5\\}\\) and edge set \\(E=\\) \\(\\{(2,1),(1,5),(2,5),(5,4),(4,2),(4,3),(3,2)\\}\\). Sketch a picture of the graph \\(G\\) and find its adjacency matrix. Use this to find the power of each vertex of the graph and determine whether this graph is dominance-directed.\n\n\n11 \nConsider the linear difference \\(y_{k+2}-y_{k+1}-y_{k}=0\\).\n\nExpress this difference in matrix form.\nFind the first ten terms of the solution to this difference given the initial conditions \\(y_{0}=0, y_{1}=1\\). (This is the well-known Fibonacci sequence.)\n\n\n\n12 \nSuppose that in Example 2.27 you invest \\(\\$ 1,000\\) initially (the zeroth year) and no further amounts. Make a table of the value of your investment for years 0 to 12. Also include a column that calculates the annual interest rate that your investment is earning each year, based on the current and previous year’s values. What conclusions do you draw? You will need a technology tool for this exercise."
  },
  {
    "objectID": "HW/projects_1.html",
    "href": "HW/projects_1.html",
    "title": "Projects 1",
    "section": "",
    "text": "Instructions\n\nPick three projects from the list below.\nPlease format each project as a Jupyter notebook or a Quarto document (or some similar format which allows me to see/run code.)\nYou are welcome to work with a partner on some or all of the projects, but please make sure you are each contributing to each part of the project – don’t just divide up the work.\n\nIf you work with a partner, you may choose to submit one report for both of you, or you may submit separate reports. Either way, please make sure to indicate who your partner was. (Please indicate very clearly what you are doing – which reports if any are joint, and with whom, etc, so I don’t get confused!)\n\nThis is the first time I’m assigning something like this, so please feel to reach out with questions or suggestions for improvement.\nProjects will be due on Friday night, 11:59pm.\n\nChapter 1 projects\nGas in a Tube\n(Shores 62)\nProblem Description: You are given a long tube of still dry air in which there are 7 sampling/insertion points equally spaced \\(1 / 6\\) meters apart from each other. The position of each point is measured by setting the leftmost point at 0.0 meters and rightmost at 1.0 meters. Initially, a small amount of a certain gas is inserted in the central insertion point. Subsequently, measurements of the concentration of the gas at each sampling/insertion point are taken at later times in seconds. The results of these measurements, which you may assume are accurate to about 2-3 digits, are specified in Table 1.1. Based on this information, your task is to determine the best estimate you can find for the true value of the diffusion coefficient \\(D\\) of this gas in a motionless air medium. Use this estimate and a marching method to calculate values of the material density function on the interval \\([0,1]\\) at times \\(t=210\\) and \\(t=300\\) and at the given spatial nodes.\n\nConcentration data measurements of a gaseous material.\n\n\n\n\n\n\n\n\n\n\n\n\nSec \\(\\backslash\\) Meter\n0\n\\(1 / 6\\)\n\\(1 / 3\\)\n\\(1 / 2\\)\n\\(2 / 3\\)\n\\(5 / 6\\)\n1\n\n\n\n\n\\(t=240\\)\n0.0\n0.032\n1.23\n3.69\n1.23\n0.032\n0.0\n\n\n\\(t=270\\)\n0.0\n0.051\n1.21\n3.48\n1.21\n0.051\n0.0\n\n\n\nProcedure: You should use equation (1.10) or some variant to move backward and forward in time. These will result in linear systems, which ALAMA calculator or another technology tool can solve. One way to proceed is simply to use trial and error until you think you’ve hit on a reasonable value of \\(D\\), that is, the one that gives the best approximation to \\(t=180\\) from the \\(t=360\\) values. Do not expect perfect matches - the data is relatively sparse. Then march backwards in time once more to get the initial values at \\(t=0\\). Finally, march forward in time to compute and plot the resulting approximate density function.\nOutput: Discus your results and provide a graph of profiles of the material density function at times in the data table along with your computed profiles.\nComments: This project introduces you to a very interesting area of mathematics called “inverse theory.” The idea is, rather than proceeding from problem (the governing equations for concentration values) to solution (concentration profiles), you are given the “solution,” namely the measured solution values at various points, and are to determine from this information the “problem,” i.e., the diffusion coefficient needed to define the governing equations.\nChapter 2 Projects\nLU Factorization\nWe didn’t yet get to the discussion of this in class. If you’re interested in doing this project, you can look at the second part of the Chapter 2 Lecture 4 slides.\n(Shores p. 177) Write a program module that implements Theorem 2.14 using partial pivoting and implicit row exchanges. This means that space is allocated for the \\(n \\times n\\) matrix \\(A=[a[i, j]]\\) and an array of row indices, say indx \\([i]\\). Initially, indx should consist of the integers \\(1,2, \\ldots, n\\). Whenever two rows need to be exchanged, say the first and third, then the indices indx[1] and indx[3] are exchanged. References to array elements throughout the Gaussian elimination process should be indirect: Refer to the \\((1,4)\\) th entry of \\(A\\) as the element \\(a[\\operatorname{indx}[1], 4]\\). This method of reference has the same effect as physically exchanging rows, but without the work. It also has the appealing feature that we can design the algorithm as though no row exchanges have taken place provided we replace the direct reference \\(a[i, j]\\) by the indirect reference \\(a[\\operatorname{indx}[i], j]\\). The module should return the lower/upper matrix in the format of Example 2.70 as well as the permuted array indx \\([i]\\). Effectively, this index array tells the user what the permutation matrix \\(P\\) is.\nUse this module to implement an LU system solver module that uses the \\(\\mathrm{LU}\\) factorization to solve a general linear system. Also write a module that finds the inverse of an \\(n \\times n\\) matrix \\(A\\) by first using the LU factorization module, then making repeated use of the LU system solver to solve \\(A \\mathbf{x}^{(i)}=\\mathbf{e}_{i}\\), where \\(\\mathbf{e}_{i}\\) is the \\(i\\) th column of the identity. Then we will have\n\\[\nA^{-1}=\\left[\\mathbf{x}^{(1)}, \\mathbf{x}^{(2)}, \\ldots, \\mathbf{x}^{(n)}\\right]\n\\]\nBe sure to document and test your code and report on the results.\nMarkov Chains\n(Shores p. 177)\nRefer to Example 2.19 and Section 2.3 for background. Three automobile insurance firms compete for a fixed market of customers. Annual premiums are sold to these customers. Label the companies A, B, and C. You work for Company A, and your team of market analysts has done a survey that draws the following conclusions: In each of the past three years, the number of A customers switching to B is \\(20 \\%\\), and to \\(\\mathrm{C}\\) is \\(30 \\%\\). The number of \\(\\mathrm{B}\\) customers switching to \\(\\mathrm{A}\\) is \\(20 \\%\\), and to \\(\\mathrm{C}\\) is \\(20 \\%\\). The number of \\(\\mathrm{C}\\) customers switching to \\(\\mathrm{A}\\) is \\(30 \\%\\), and to \\(\\mathrm{B}\\) is \\(10 \\%\\). Those who do not switch continue to use their current company’s insurance for the next year. Model this market as a Markov chain. Display the transition matrix for the model. Illustrate the workings of the model by showing what it would predict as the market shares three years from now if currently \\(\\mathrm{A}, \\mathrm{B}\\), and \\(\\mathrm{C}\\) owned equal shares of the market.\nThe next part of your problem is as follows: Your team has tested two advertising campaigns in some smaller test markets and are confident that the first campaign will convince \\(20 \\%\\) of the B customers who would otherwise stay with B in a given year to switch to A. The second advertising campaign would convince \\(20 \\%\\) of the \\(\\mathrm{C}\\) customers who would otherwise stay with C in a given year to switch to A. Both campaigns have about equal costs and would not change other customers’ habits. Make a recommendation, based on your experiments with various possible initial state vectors for the market. Will these campaigns actually improve your company’s market share? If so, which one do you recommend? Write up your recommendation in the form of a report, with supporting evidence. It’s a good idea to hedge on your bets a little by pointing out limitations to your model and claims, so devote a few sentences to those points.\nIt would be a plus to carry the analysis further (your manager might appreciate that). For instance, you could turn the additional market share from, say B customers, into a variable and plot the long-term gain for your company against this variable. A manager could use this data to decide whether it was worthwhile to attempt gaining more customers from B.\nSports Ranking\n(Shores p. 180)\nRefer to Example 2.24 and Section 2.3 for background. As a sports analyst you are given the following data about a league of seven teams numbered 1-7, where the pair \\((j, k)\\) represents a game in which team \\(j\\) defeated team \\(k\\) :\n\\[\n\\begin{aligned}\nE= & \\{(1,2),(7,3),(2,4),(4,5),(3,2),(5,1),(6,1),(3,1),(7,2),(2,6), \\\\\n& (3,4),(7,4),(5,7),(6,4),(3,5),(5,6),(7,1),(5,2),(7,6),(1,4),(6,3)\\}\n\\end{aligned}\n\\]\nBased on these data you are to rank the teams. To this end, begin with the simplest method, ranking by win/loss record. Next, treat the data as defining a digraph. Begin this analysis by constructing the adjacency matrix of this digraph and drawing a picture of the digraph either by hand or using some software. Then rank the teams by using the following methods: First use the method of Example 2.26 to find a power ranking of each team. Then use the reverse PageRank idea of Example 2.47 to rank the the teams.\nNext, suppose you are given additional information, namely, the game margins (winning score minus losing score) for each game. Following is a list of these margins matching the order of matches in the definition of \\(E\\) :\n\\[\nM=\\{4,8,7,3,7,7,23,15,6,18,13,14,7,13,7,18,45,10,19,14,13\\}\n\\]\nIn order to utilize these data examine your picture of the digraph and label each edge with the margin that matches it in \\(M\\). You are now dealing with a weighted graph and one can construct a different sort of “adjacency matrix” by entering this margin in the \\((i, j)\\) th entry according as team \\(i\\) defeated team \\(j\\) by that margin. Use this approach to calculate “power ranking”.\nIsoRank\nRead the discussion of IsoRank on pages 172-176.\nThe notion of embedding one graph into another is a useful idea for some scientific studies. In this report you will test the basic idea of network embedding by using the variant IsoRank of the PageRank technique on three relatively simple examples. This project requires a technology tool for these calculations and the resulting output should be interpreted as in the discussion of the IsoRank technique following Example 2.73 of Section 2.8.\n\n\n\nFigure 2.13\n\n\n\n\n\nFigure 2.14\n\n\nBy an isomorphism of graphs we mean a one-to-one edge preserving map of vertices from one graph onto another. One can think of an isomorphism as simply a relabeling of the vertices of a graph. The first test is to provide an example of how well IsoRank can recognize isomorphisms. Consider the graph of Figure 2.13. Let \\(G_{1}\\) be the graph with vertices \\(1,2,3,4,5\\) in that order and \\(G_{2}\\) the same graph with vertices \\(A, B, C, D, E\\) in that order. Apply IsoRank to these two graphs and discuss the validity of your results.\nThe next embedding test is to remove the edge connecting vertices \\(B\\) and \\(C\\) in Figure 2.12 and use IsoRank with teleportation vector \\(\\mathbf{v}=\\mathbf{e} / 15\\) and teleportation parameter \\(\\alpha=0.85\\) to find the best matchings of the graph \\(G_{1}\\) with the resulting graph \\(G_{2}\\). List all possible mappings that are calculated.\nThe last embedding test is to use IsoRank with teleportation vector \\(\\mathbf{v}=\\) \\(\\mathrm{e} / 15\\) and teleportation parameter \\(\\alpha=0.85\\), along with correction vector \\(\\mathbf{u}=\\mathbf{e} / 5\\) for \\(G_{2}\\), to find the best matchings of the digraph \\(G_{1}\\) with the digraph \\(G_{2}\\) in Figure 2.14. List all possible mappings and discuss your calculations."
  },
  {
    "objectID": "HW/projects_1.sol.html",
    "href": "HW/projects_1.sol.html",
    "title": "Projects 1 Solutions",
    "section": "",
    "text": "Instructions\n\nPick three projects from the list below.\nPlease format each project as a Jupyter notebook or a Quarto document (or some similar format which allows me to see/run code.)\nYou are welcome to work with a partner on some or all of the projects, but please make sure you are each contributing to each part of the project – don’t just divide up the work.\n\nIf you work with a partner, you may choose to submit one report for both of you, or you may submit separate reports. Either way, please make sure to indicate who your partner was. (Please indicate very clearly what you are doing – which reports if any are joint, and with whom, etc, so I don’t get confused!)\n\nThis is the first time I’m assigning something like this, so please feel to reach out with questions or suggestions for improvement.\nProjects will be due on Friday night, 11:59pm.\n\nChapter 1 projects\nGas in a Tube\n(Shores 62)\nProblem Description: You are given a long tube of still dry air in which there are 7 sampling/insertion points equally spaced \\(1 / 6\\) meters apart from each other. The position of each point is measured by setting the leftmost point at 0.0 meters and rightmost at 1.0 meters. Initially, a small amount of a certain gas is inserted in the central insertion point. Subsequently, measurements of the concentration of the gas at each sampling/insertion point are taken at later times in seconds. The results of these measurements, which you may assume are accurate to about 2-3 digits, are specified in Table 1.1. Based on this information, your task is to determine the best estimate you can find for the true value of the diffusion coefficient \\(D\\) of this gas in a motionless air medium. Use this estimate and a marching method to calculate values of the material density function on the interval \\([0,1]\\) at times \\(t=210\\) and \\(t=300\\) and at the given spatial nodes.\nTable: Concentration data measurements of a gaseous material.\nProcedure: You should use equation (1.10) or some variant to move backward and forward in time. These will result in linear systems, which ALAMA calculator or another technology tool can solve. One way to proceed is simply to use trial and error until you think you’ve hit on a reasonable value of \\(D\\), that is, the one that gives the best approximation to \\(t=180\\) from the \\(t=360\\) values. Do not expect perfect matches - the data is relatively sparse. Then march backwards in time once more to get the initial values at \\(t=0\\). Finally, march forward in time to compute and plot the resulting approximate density function.\nOutput: Discus your results and provide a graph of profiles of the material density function at times in the data table along with your computed profiles.\nComments: This project introduces you to a very interesting area of mathematics called “inverse theory.” The idea is, rather than proceeding from problem (the governing equations for concentration values) to solution (concentration profiles), you are given the “solution,” namely the measured solution values at various points, and are to determine from this information the “problem,” i.e., the diffusion coefficient needed to define the governing equations.\nChapter 2 Projects\nLU Factorization\nWe didn’t yet get to the discussion of this in class. If you’re interested in doing this project, you can look at the second part of the Chapter 2 Lecture 4 slides.\n(Shores p. 177) Write a program module that implements Theorem 2.14 using partial pivoting and implicit row exchanges. This means that space is allocated for the \\(n \\times n\\) matrix \\(A=[a[i, j]]\\) and an array of row indices, say indx \\([i]\\). Initially, indx should consist of the integers \\(1,2, \\ldots, n\\). Whenever two rows need to be exchanged, say the first and third, then the indices indx[1] and indx[3] are exchanged. References to array elements throughout the Gaussian elimination process should be indirect: Refer to the \\((1,4)\\) th entry of \\(A\\) as the element \\(a[\\operatorname{indx}[1], 4]\\). This method of reference has the same effect as physically exchanging rows, but without the work. It also has the appealing feature that we can design the algorithm as though no row exchanges have taken place provided we replace the direct reference \\(a[i, j]\\) by the indirect reference \\(a[\\operatorname{indx}[i], j]\\). The module should return the lower/upper matrix in the format of Example 2.70 as well as the permuted array indx \\([i]\\). Effectively, this index array tells the user what the permutation matrix \\(P\\) is.\nUse this module to implement an LU system solver module that uses the \\(\\mathrm{LU}\\) factorization to solve a general linear system. Also write a module that finds the inverse of an \\(n \\times n\\) matrix \\(A\\) by first using the LU factorization module, then making repeated use of the LU system solver to solve \\(A \\mathbf{x}^{(i)}=\\mathbf{e}_{i}\\), where \\(\\mathbf{e}_{i}\\) is the \\(i\\) th column of the identity. Then we will have\n\\[\nA^{-1}=\\left[\\mathbf{x}^{(1)}, \\mathbf{x}^{(2)}, \\ldots, \\mathbf{x}^{(n)}\\right]\n\\]\nBe sure to document and test your code and report on the results.\nMarkov Chains\n(Shores p. 177)\nRefer to Example 2.19 and Section 2.3 for background. Three automobile insurance firms compete for a fixed market of customers. Annual premiums are sold to these customers. Label the companies A, B, and C. You work for Company A, and your team of market analysts has done a survey that draws the following conclusions: In each of the past three years, the number of A customers switching to B is \\(20 \\%\\), and to \\(\\mathrm{C}\\) is \\(30 \\%\\). The number of \\(\\mathrm{B}\\) customers switching to \\(\\mathrm{A}\\) is \\(20 \\%\\), and to \\(\\mathrm{C}\\) is \\(20 \\%\\). The number of \\(\\mathrm{C}\\) customers switching to \\(\\mathrm{A}\\) is \\(30 \\%\\), and to \\(\\mathrm{B}\\) is \\(10 \\%\\). Those who do not switch continue to use their current company’s insurance for the next year. Model this market as a Markov chain. Display the transition matrix for the model. Illustrate the workings of the model by showing what it would predict as the market shares three years from now if currently \\(\\mathrm{A}, \\mathrm{B}\\), and \\(\\mathrm{C}\\) owned equal shares of the market.\nThe next part of your problem is as follows: Your team has tested two advertising campaigns in some smaller test markets and are confident that the first campaign will convince \\(20 \\%\\) of the B customers who would otherwise stay with B in a given year to switch to A. The second advertising campaign would convince \\(20 \\%\\) of the \\(\\mathrm{C}\\) customers who would otherwise stay with C in a given year to switch to A. Both campaigns have about equal costs and would not change other customers’ habits. Make a recommendation, based on your experiments with various possible initial state vectors for the market. Will these campaigns actually improve your company’s market share? If so, which one do you recommend? Write up your recommendation in the form of a report, with supporting evidence. It’s a good idea to hedge on your bets a little by pointing out limitations to your model and claims, so devote a few sentences to those points.\nIt would be a plus to carry the analysis further (your manager might appreciate that). For instance, you could turn the additional market share from, say B customers, into a variable and plot the long-term gain for your company against this variable. A manager could use this data to decide whether it was worthwhile to attempt gaining more customers from B.\nSports Ranking\n(Shores p. 180)\nRefer to Example 2.24 and Section 2.3 for background. As a sports analyst you are given the following data about a league of seven teams numbered 1-7, where the pair \\((j, k)\\) represents a game in which team \\(j\\) defeated team \\(k\\) :\n\\[\n\\begin{aligned}\nE= & \\{(1,2),(7,3),(2,4),(4,5),(3,2),(5,1),(6,1),(3,1),(7,2),(2,6), \\\\\n& (3,4),(7,4),(5,7),(6,4),(3,5),(5,6),(7,1),(5,2),(7,6),(1,4),(6,3)\\}\n\\end{aligned}\n\\]\nBased on these data you are to rank the teams. To this end, begin with the simplest method, ranking by win/loss record. Next, treat the data as defining a digraph. Begin this analysis by constructing the adjacency matrix of this digraph and drawing a picture of the digraph either by hand or using some software. Then rank the teams by using the following methods: First use the method of Example 2.26 to find a power ranking of each team. Then use the reverse PageRank idea of Example 2.47 to rank the the teams.\nNext, suppose you are given additional information, namely, the game margins (winning score minus losing score) for each game. Following is a list of these margins matching the order of matches in the definition of \\(E\\) :\n\\[\nM=\\{4,8,7,3,7,7,23,15,6,18,13,14,7,13,7,18,45,10,19,14,13\\}\n\\]\nIn order to utilize these data examine your picture of the digraph and label each edge with the margin that matches it in \\(M\\). You are now dealing with a weighted graph and one can construct a different sort of “adjacency matrix” by entering this margin in the \\((i, j)\\) th entry according as team \\(i\\) defeated team \\(j\\) by that margin. Use this approach to calculate “power ranking”.\nIsoRank\nRead the discussion of IsoRank on pages 172-176.\nThe notion of embedding one graph into another is a useful idea for some scientific studies. In this report you will test the basic idea of network embedding by using the variant IsoRank of the PageRank technique on three relatively simple examples. This project requires a technology tool for these calculations and the resulting output should be interpreted as in the discussion of the IsoRank technique following Example 2.73 of Section 2.8.\n\n\n\nFigure 2.13\n\n\n\n\n\nFigure 2.14\n\n\nBy an isomorphism of graphs we mean a one-to-one edge preserving map of vertices from one graph onto another. One can think of an isomorphism as simply a relabeling of the vertices of a graph. The first test is to provide an example of how well IsoRank can recognize isomorphisms. Consider the graph of Figure 2.13. Let \\(G_{1}\\) be the graph with vertices \\(1,2,3,4,5\\) in that order and \\(G_{2}\\) the same graph with vertices \\(A, B, C, D, E\\) in that order. Apply IsoRank to these two graphs and discuss the validity of your results.\nThe next embedding test is to remove the edge connecting vertices \\(B\\) and \\(C\\) in Figure 2.12 and use IsoRank with teleportation vector \\(\\mathbf{v}=\\mathbf{e} / 15\\) and teleportation parameter \\(\\alpha=0.85\\) to find the best matchings of the graph \\(G_{1}\\) with the resulting graph \\(G_{2}\\). List all possible mappings that are calculated.\nThe last embedding test is to use IsoRank with teleportation vector \\(\\mathbf{v}=\\) \\(\\mathrm{e} / 15\\) and teleportation parameter \\(\\alpha=0.85\\), along with correction vector \\(\\mathbf{u}=\\mathbf{e} / 5\\) for \\(G_{2}\\), to find the best matchings of the digraph \\(G_{1}\\) with the digraph \\(G_{2}\\) in Figure 2.14. List all possible mappings and discuss your calculations."
  },
  {
    "objectID": "HW/HW3.html",
    "href": "HW/HW3.html",
    "title": "Homework 3",
    "section": "",
    "text": "1 \nShow that if \\(P\\) and \\(Q\\) are stochastic matrices of the same size, then \\(P Q\\) is also stochastic.\n\n\n2 \nThe digraph \\(H\\) that results from reversing all the arrows in a digraph \\(G\\) is called reverse digraph of \\(G\\). Show that if \\(A\\) is the adjacency matrix for \\(G\\) then \\(A^{T}\\) is the adjacency matrix for the reverse digraph \\(H\\).\n\n\n3 \nSolve the PageRank problem with \\(P\\) as in Example 2.46, teleportation vector \\(\\mathbf{v}=\\frac{1}{6} \\mathbf{e}\\) and teleportation parameter \\(\\alpha=0.8\\). (In this example, the correction vector was \\(\\frac{1}{3}(1,1,1,0,0,0)\\); that’s what you’ll use here.)\n\n\n4 \nModify the surfing matrix \\(P\\) of Example 2.46 by using the correction vector \\(\\frac{1}{5}(1,1,1,0,1,1)\\) and solve the resulting PageRank problem with teleportation vector \\(\\mathbf{v}=\\frac{1}{6} \\mathbf{e}\\) and teleportation parameter \\(\\alpha=0.8\\).\n\n2.5.22\nSolution vector is \\(\\mathbf{x}=(95,133,189,75,123,123) / 738\\) or \\(\\mathbf{x}=\\) \\((0.101,0.141,0.200,0.087,0.236,0.236)\\) exactly.\n\n\n\n5 \nShow that there is more than one stationary state for the Markov chain of Example 2.46.\n\n\n6 \nRepair the dangling node problem of the graph of Figure 2.7 by creating a correction vector that makes transition to all nodes equally likely. (Note that this means all nodes, includes transitioning back to the dangling node.)\nNext, find all stationary states for the resulting Markov chain.\n\n\n7 \nSolve the nonlinear system of equations of Example 2.48 by using nine iterations of the vector Newton formula (2.5), starting with the initial guess \\(\\mathbf{x}^{(0)}=(0,1)\\). Evaluate \\(F\\left(\\mathbf{x}^{(9)}\\right)\\).\n\n\n8 \nFind the minimum value of the function \\(F(x, y)=\\left(x^{2}+y+1\\right)^{2}+\\) \\(x^{4}+y^{4}\\) by using the Newton method to find critical points of the function \\(F(x, y)\\), i.e., points where \\(f(x, y)=F_{x}(x, y)=0\\) and \\(g(x, y)=F_{y}(x, y)=0\\).\n\n\n9 \nApply the following digital filter to the noisy data of Example 2.71 and graph the results. Does it appear to be a low pass filter?\n\\[\ny_{k}=\\frac{1}{2} x_{k}+\\frac{1}{2} x_{k-1}, \\quad k=1,2, \\ldots, 33\n\\]\n\n\n10 \nApply the following digital filter to the noisy data of Example 2.71 and graph the results. Does it appear to be a high pass filter?\n\\[\ny_{k}=\\frac{1}{2} x_{k}-\\frac{1}{2} x_{k-1}, \\quad k=1,2, \\ldots, 33\n\\]\n\n\n11 \n(I’ll talk about LU factorization in class on the Wednesday that this homework is due; you may want to hold off on the next few problems until then.)\nShow that \\(L=\\left[\\begin{array}{lll}1 & 0 & 0 \\\\ 1 & 1 & 0 \\\\ 2 & 1 & 1\\end{array}\\right]\\) and \\(U=\\left[\\begin{array}{rrr}2 & -1 & 1 \\\\ 0 & 4 & -3 \\\\ 0 & 0 & -1\\end{array}\\right]\\) is an LU factorization of \\(A=\\left[\\begin{array}{rrr}2 & -1 & 1 \\\\ 2 & 3 & -2 \\\\ 4 & 2 & -2\\end{array}\\right]\\).\n\n\n12 \nBy hand:\nFind an LU factorization of the matrix \\(A=\\left[\\begin{array}{rrr}2 & 1 & 0 \\\\ -4 & -1 & -1 \\\\ 2 & 3 & -3\\end{array}\\right]\\).\n\n\n13 \nBy hand:\nFind a PLU factorization of the matrix \\(A=\\left[\\begin{array}{rrr}2 & 1 & 3 \\\\ -4 & -2 & -1 \\\\ 2 & 3 & -3\\end{array}\\right]\\)."
  },
  {
    "objectID": "HW/HW3.sol.html",
    "href": "HW/HW3.sol.html",
    "title": "Homework 3 Solutions",
    "section": "",
    "text": "1 \nShow that if \\(P\\) and \\(Q\\) are stochastic matrices of the same size, then \\(P Q\\) is also stochastic.\n\n2.4.29 Block \\(Q\\) into columns \\(\\mathbf{q}_{i}\\), each of which is a probability distribution vector, and obtain\n\\[\nP Q=P\\left[\\mathbf{q}_{1}, \\mathbf{q}_{2}, \\ldots, \\mathbf{q}_{n}\\right]=\\left[P \\mathbf{q}_{1}, P \\mathbf{q}_{2}, \\ldots, P \\mathbf{q}_{n}\\right]\n\\]\nSince any product of a stochastic matrix and probability distribution vector is itself a probability distribution vector, the result follows.\n\n\n\n2 \nThe digraph \\(H\\) that results from reversing all the arrows in a digraph \\(G\\) is called reverse digraph of \\(G\\). Show that if \\(A\\) is the adjacency matrix for \\(G\\) then \\(A^{T}\\) is the adjacency matrix for the reverse digraph \\(H\\).\n\n2.4.36\nIn terms of the edge set \\(E=\\left\\{\\left(\\left(v_{1}, w_{1}\\right),\\left(v_{2}, w_{2}\\right), \\ldots,\\left(v_{m}, w_{m}\\right)\\right)\\right\\}\\) of the digraph \\(G\\), the edge set of its reverse digraph \\(H\\) is\n\\[\nF=\\left\\{\\left(\\left(w_{1}, v_{1}\\right),\\left(w_{2}, v_{2}\\right), \\ldots,\\left(w_{m}, v_{m}\\right)\\right)\\right\\}\n\\]\nwhich means that whenever the edge \\(\\left(v_{k}, w_{k}\\right)\\) contributes 1 to the \\((i, j)\\) th entry of the adjacency matrix \\(A\\) of \\(G\\), the edge \\(\\left(w_{k}, v_{k}\\right)\\) contributes 1 to the \\((j, i)\\) th entry of the adjacence matrix \\(B\\) of \\(H\\). Hence, \\(B=A^{T}\\).\n\n\n\n3 \nSolve the PageRank problem with \\(P\\) as in Example 2.46, teleportation vector \\(\\mathbf{v}=\\frac{1}{6} \\mathbf{e}\\) and teleportation parameter \\(\\alpha=0.8\\). (In this example, the correction vector was \\(\\frac{1}{3}(1,1,1,0,0,0)\\); that’s what you’ll use here.)\n\n2.5.21\nSolution vector is \\(\\mathbf{x}=(95,133,189,75,123,123) / 738\\) exactly, \\(\\mathbf{x}=\\) \\((0.129,0.180,0.256,0.102,0.167,0.167)\\) approximately.\n\nfrom sympy import Matrix, Rational,N, nsimplify\nP = Matrix([\n    [0, 0, 1/3, 1/3, 0, 0],\n    [1/2, 0, 1/3, 1/3, 0, 0],\n    [1/2, 1, 0, 1/3, 0, 0],\n    [0, 0, 1/3, 0, 0, 0],\n    [0, 0, 0, 0, 0, 1],\n    [0, 0, 0, 0, 1, 0]\n])\nP=nsimplify(P,rational=True)\nv = Matrix([Rational(1,6)] * 6)\nalpha = Rational(8,10)\n\nWe start with our surfing matrix \\[\nP=\\left[\\begin{array}{llllll}\n0 & 0 & \\frac{1}{3} & \\frac{1}{3} & 0 & 0 \\\\\n\\frac{1}{2} & 0 & \\frac{1}{3} & \\frac{1}{3} & 0 & 0 \\\\\n\\frac{1}{2} & 1 & 0 & \\frac{1}{3} & 0 & 0 \\\\\n0 & 0 & \\frac{1}{3} & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 & 1 & 0\n\\end{array}\\right]\n\\]\nWe want to solve this:\n\\[\n\\begin{equation*}\n\\left(\\alpha P+(1-\\alpha) \\mathbf{v e}^{T}\\right) \\mathbf{x} =\\mathbf{x}\n\\end{equation*}\n\\]\nOne approach: find eigenvalues of value 1.\n\nM_sympy = alpha * P + (1 - alpha) * v * Matrix([1]*P.shape[0]).T\neigenvalues_sympy = M_sympy.eigenvals()\neigenvectors_sympy = M_sympy.eigenvects()\n\n\n# Find the first eigenvector whose eigenvalue equals 1\nfor eigenvalue, multiplicity, eigenvectors in eigenvectors_sympy:\n    if eigenvalue == 1:\n        ev = eigenvectors[0]\n        break\n# Normalize the eigenvector\nev = ev / sum(ev)\nev.T\n\n\\(\\displaystyle \\left[\\begin{matrix}\\frac{95}{738} & \\frac{133}{738} & \\frac{21}{82} & \\frac{25}{246} & \\frac{1}{6} & \\frac{1}{6}\\end{matrix}\\right]\\)\n\n\nSecond approach: subtract x from both sides of the equation and solve it using sympy’s solve function.\nNow we want to solve this:\n\\[\n\\begin{equation*}\n\\left(\\alpha P+(1-\\alpha) \\mathbf{v e}^{T}\\right) \\mathbf{x}-\\mathbf{x} =0\n\\end{equation*}\n\\]\nInitially, sympy complains that the matrix is not invertible. This means that there will be infinitely many solutions. We can use gauss_jordan_solve to find the general form…\n(Also note: if we don’t define \\(\\alpha\\) using symbols, sympy will make everything numeric and only find the trivial solution.)\n\nfrom sympy import symbols, solve, Matrix, zeros, eye\n\nx = Matrix(symbols('x:6'))\n\n# Define the equation\neqn = (alpha*P + (1-alpha)*v*Matrix([1]*P.shape[0]).T)-eye(6)\nsoln=eqn.gauss_jordan_solve(zeros(6,1))[0]\ndisplay(soln.T)\n\n\\(\\displaystyle \\left[\\begin{matrix}\\frac{95 \\tau_{0}}{123} & \\frac{133 \\tau_{0}}{123} & \\frac{63 \\tau_{0}}{41} & \\frac{25 \\tau_{0}}{41} & \\tau_{0} & \\tau_{0}\\end{matrix}\\right]\\)\n\n\nNext, we would need to pick a value for \\(\\tau_0\\) to find a specific solution. We want the sum of the elements of \\(\\mathbf{x}\\) to be 1; conveniently, if we simply divide by the sum of the elements of the general solution, the \\(\\tau_0\\) will cancel out.\n\nfrom sympy import simplify, collect, together\nfrom IPython.display import Latex\nimport sympy as sp\nfrom sympy.printing.latex import LatexPrinter\nsp.init_printing()\n\ndisplay(Latex(\"$$\\\\frac{1}{738}\"+sp.latex(simplify(soln/sum(soln)*738)[:,0])+\"\\\\approx\"+sp.latex(N(soln/sum(soln),4))+\"$$\"))\n\n\\[\\frac{1}{738}\\left[\\begin{matrix}95\\\\133\\\\189\\\\75\\\\123\\\\123\\end{matrix}\\right]\\approx\\left[\\begin{matrix}0.1287\\\\0.1802\\\\0.2561\\\\0.1016\\\\0.1667\\\\0.1667\\end{matrix}\\right]\\]\n\n\n\n\n\n4 \nModify the surfing matrix \\(P\\) of Example 2.46 by using the correction vector \\(\\frac{1}{5}(1,1,1,0,1,1)\\) and solve the resulting PageRank problem with teleportation vector \\(\\mathbf{v}=\\frac{1}{6} \\mathbf{e}\\) and teleportation parameter \\(\\alpha=0.8\\).\n\n2.5.22\nSolution vector is \\(\\mathbf{x}=(95,133,189,75,123,123) / 738\\) or \\(\\mathbf{x}=\\) \\((0.101,0.141,0.200,0.087,0.236,0.236)\\) exactly.\n\n\n\n5 \nShow that there is more than one stationary state for the Markov chain of Example 2.46.\n\n2.5.29\nSolution. The reduced row echelon form for \\(I-P\\) is\n Two solutions whose coordinates sum to one are \\(\\mathbf{x}=\\left(0,0,0,0, \\frac{1}{2}, \\frac{1}{2}\\right)\\) and \\[\\mathbf{x}=\\frac{1}{22}\\begin{bmatrix}4\\\\6\\\\9\\\\3\\\\0\\\\0\\end{bmatrix}\\approx \\left[\\begin{matrix}0.182\\\\0.273\\\\0.409\\\\0.136\\\\0\\\\0\\end{matrix}\\right]'\n\\] .\n\n\n\n6 \nRepair the dangling node problem of the graph of Figure 2.7 by creating a correction vector that makes transition to all nodes equally likely. (Note that this means all nodes, includes transitioning back to the dangling node.)\nNext, find all stationary states for the resulting Markov chain.\n\n2.5.30\nThe matrix \\(P\\) will be as in Example 2.46 except that the fourth column will have the value \\(1 / 6\\) in each entry. The reduced row echelon form for \\(I-P\\) is\n\\[\n\\left[\\begin{array}{cccccc}\n1 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 1 & -1 \\\\\n0 & 0 & 0 & 0 & 0 & 0\n\\end{array}\\right] .\n\\]\nThe only solution whose coordinates sum to one is \\(\\mathbf{x}=\\left(0,0,0,0, \\frac{1}{2}, \\frac{1}{2}\\right)\\).\n\n\n\n7 \nSolve the nonlinear system of equations of Example 2.48 by using nine iterations of the vector Newton formula (2.5), starting with the initial guess \\(\\mathbf{x}^{(0)}=(0,1)\\). Evaluate \\(F\\left(\\mathbf{x}^{(9)}\\right)\\).\n\n2.5.25 \\(\\quad \\mathbf{x}=(x, y), \\mathbf{x}^{(9)} \\approx\\left[\\begin{array}{r}1.00001 \\\\ -0.99999\\end{array}\\right], \\mathbf{F}\\left(\\mathbf{x}^{(9)}\\right) \\approx 10^{-6}\\left[\\begin{array}{r}-1.3422 \\\\ 2.0226\\end{array}\\right], \\mathbf{F}(\\mathbf{x})=\\)\n\\(\\left[\\begin{array}{l}x^{2}+\\sin (\\pi x y)-1 \\\\ x+y^{2}+e^{x+y}-3\\end{array}\\right], J_{\\mathbf{F}}(\\mathbf{x})=\\left[\\begin{array}{cc}2 x+\\cos (\\pi x y), \\pi y \\cos (\\pi x y) \\pi x \\\\ 1+e^{x+y}, & 2 y+e^{x+y}\\end{array}\\right]\\)\n\n\n\n8 \nFind the minimum value of the function \\(F(x, y)=\\left(x^{2}+y+1\\right)^{2}+\\) \\(x^{4}+y^{4}\\) by using the Newton method to find critical points of the function \\(F(x, y)\\), i.e., points where \\(f(x, y)=F_{x}(x, y)=0\\) and \\(g(x, y)=F_{y}(x, y)=0\\).\n\n2.5.26\n and all initial choices converge to \\((-0.96401,-0.82131)\\), and \\(F(-0.96401,-0.82131)=\\) 2.54632 .\n\n\n\n9 \nApply the following digital filter to the noisy data of Example 2.71 and graph the results. Does it appear to be a low pass filter?\n\\[\ny_{k}=\\frac{1}{2} x_{k}+\\frac{1}{2} x_{k-1}, \\quad k=1,2, \\ldots, 33\n\\]\n\n2.8.9\nYes, but rather mediocrely. A graph is in Figure 1 and comparison of exact and filtered in Figure 2: \n\n\n\n\n10 \nApply the following digital filter to the noisy data of Example 2.71 and graph the results. Does it appear to be a high pass filter?\n\\[\ny_{k}=\\frac{1}{2} x_{k}-\\frac{1}{2} x_{k-1}, \\quad k=1,2, \\ldots, 33\n\\]\n\n2.8.10\nYes, definitely high pass. A graph is in Figure 3:\n\n\n\n\n11 \n(I’ll talk about LU factorization in class on the Wednesday that this homework is due; you may want to hold off on the next few problems until then.)\nShow that \\(L=\\left[\\begin{array}{lll}1 & 0 & 0 \\\\ 1 & 1 & 0 \\\\ 2 & 1 & 1\\end{array}\\right]\\) and \\(U=\\left[\\begin{array}{rrr}2 & -1 & 1 \\\\ 0 & 4 & -3 \\\\ 0 & 0 & -1\\end{array}\\right]\\) is an LU factorization of \\(A=\\left[\\begin{array}{rrr}2 & -1 & 1 \\\\ 2 & 3 & -2 \\\\ 4 & 2 & -2\\end{array}\\right]\\).\n\n2.8.1\n\\(L U=\\left[\\begin{array}{lll}1 & 0 & 0 \\\\ 1 & 1 & 0 \\\\ 2 & 1 & 1\\end{array}\\right]\\left[\\begin{array}{rrr}2 & -1 & 1 \\\\ 0 & 4 & -3 \\\\ 0 & 0 & -1\\end{array}\\right]=A=\\left[\\begin{array}{rrr}2 & -1 & 1 \\\\ 2 & 3 & -2 \\\\ 4 & 2 & -2\\end{array}\\right]\\).\n\n\n\n12 \nBy hand:\nFind an LU factorization of the matrix \\(A=\\left[\\begin{array}{rrr}2 & 1 & 0 \\\\ -4 & -1 & -1 \\\\ 2 & 3 & -3\\end{array}\\right]\\).\n\n2.8.5\n\\(L=\\left[\\begin{array}{rrr}1 & 0 & 0 \\\\ -2 & 1 & 0 \\\\ 1 & 2 & 1\\end{array}\\right]\\) and \\(U=\\left[\\begin{array}{rrr}2 & 1 & 0 \\\\ 0 & 1 & -1 \\\\ 0 & 0 & -1\\end{array}\\right]\\).\n\n\n\n13 \nBy hand:\nFind a PLU factorization of the matrix \\(A=\\left[\\begin{array}{rrr}2 & 1 & 3 \\\\ -4 & -2 & -1 \\\\ 2 & 3 & -3\\end{array}\\right]\\).\n\n2.8.6\n\\(P=\\left[\\begin{array}{lll}1 & 0 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 1 & 0\\end{array}\\right], L=\\left[\\begin{array}{rrr}1 & 0 & 0 \\\\ 1 & 1 & 0 \\\\ -2 & 0 & 1\\end{array}\\right]\\) and \\(U=\\left[\\begin{array}{rrr}2 & 1 & 3 \\\\ 0 & 2 & -6 \\\\ 0 & 0 & 5\\end{array}\\right]\\)."
  },
  {
    "objectID": "HW/errata.sol.html",
    "href": "HW/errata.sol.html",
    "title": "Exercise 2.2.23",
    "section": "",
    "text": "Exercise 2.2.23\nNumbers are wrong in the “resulting coefficient matrix”. Final answer is correct.\n\n\nPage 285\nIn equation (2), should be $ = w_1 + $, not $ = u_1 + $\n\n\nPage 245\n“Hence, \\(\\mathbf{w}_1\\) and … \\(\\mathbf{w}_1\\)” should be “Hence, \\(\\mathbf{w}_1\\) and … \\(\\mathbf{w}_2\\)”."
  },
  {
    "objectID": "HW/errata.html",
    "href": "HW/errata.html",
    "title": "Exercise 2.2.23",
    "section": "",
    "text": "Exercise 2.2.23\nNumbers are wrong in the “resulting coefficient matrix”. Final answer is correct.\n\n\nPage 285\nIn equation (2), should be $ = w_1 + $, not $ = u_1 + $\n\n\nPage 245\n“Hence, \\(\\mathbf{w}_1\\) and … \\(\\mathbf{w}_1\\)” should be “Hence, \\(\\mathbf{w}_1\\) and … \\(\\mathbf{w}_2\\)”."
  },
  {
    "objectID": "HW/projects_2.sol.html",
    "href": "HW/projects_2.sol.html",
    "title": "Projects 2 Solutions",
    "section": "",
    "text": "Instructions:\nPlease do all three projects. However, in the first project (Modeling with Directed Graphs), you can choose which three parts of the five you’d like to complete."
  },
  {
    "objectID": "HW/projects_2.sol.html#project-4",
    "href": "HW/projects_2.sol.html#project-4",
    "title": "Projects 2 Solutions",
    "section": "",
    "text": "Project Descriptions: These projects introduce more applications of digraphs as mathematical modeling tools. You are given that the digraph \\(G\\) has vertex set \\(V=\\{1,2,3,4,5,6\\}\\) and edge set\n\\[\nE=\\{(1,2),(2,3),(3,4),(4,2),(1,4),(3,1),(3,6),(6,3),(4,5),(5,6)\\}\n\\]\nAddress the following points regarding \\(G\\).\n\n\nDraw a picture of this digraph. You may leave space in your report and draw this by hand, or if you prefer, you may use the computer drawing applications available to you on your system.\n\n\n\nExhibit the incidence matrix \\(A\\) of this digraph and find a basis for \\(\\mathcal{N}(A)\\) using its reduced row echelon form. Some of the basis elements may be algebraic but not directed loops. Use this basis to find a basis of directed loops (e.g., non-directed basis element \\(\\mathbf{c}_{1}\\) might be replaced by directed \\(\\mathbf{c}_{1}+\\mathbf{c}_{2}\\) ).\n\n\nThink of the digraph as representing an electrical circuit where an edge represents some electrical object like a resistor or capacitor. Each node represents the circuit space between these objects. and we can attach a potential value to each node, say the potentials are \\(x_{1}, \\ldots, x_{6}\\). The potential difference across an edge is the potential value of head minus tail. Kirchhoff’s second law of electrical circuits says that the sum of potential differences around a circuit loop must be zero. Assume and use the fact (p. 422) that \\(A \\mathbf{x}=\\mathbf{b}\\) implies that for all \\(\\mathbf{y} \\in \\mathcal{N}\\left(A^{T}\\right), \\mathbf{y}^{T} \\mathbf{b}=0\\) to find conditions that a vector \\(\\mathbf{b}\\) must satisfy in order for it to be a vector of potential differences for some potential distribution on the vertices."
  },
  {
    "objectID": "HW/projects_2.sol.html#project-5",
    "href": "HW/projects_2.sol.html#project-5",
    "title": "Projects 2 Solutions",
    "section": "",
    "text": "Assume that across each edge of a circuit a current flows. Thus, we can assign to each edge a “weight,” namely the current flow along the edge. This is an example of a weighted digraph. However, not just any set of current weights will do, since Kirchhoff’s first law of circuits says that the total flow of current in and out of any node should be 0 . Use this law to find a matrix condition that must be satisfied by the currents and solve it to exhibit some current flows.\nThink of the digraph as representing a directed communications network. Here loops determine which nodes have bidirectional communication since any two nodes of a loop can only communicate with each other by way of a loop. By examining only a basis of directed loops how could you determine which nodes in the network can communicate with each other?"
  },
  {
    "objectID": "HW/projects_2.sol.html#project-6",
    "href": "HW/projects_2.sol.html#project-6",
    "title": "Projects 2 Solutions",
    "section": "",
    "text": "Think of vertices of the digraph as representing airports and edges representing flight connections between airports for Gamma Airlines. Suppose further that for each connection there is a maximum number of daily flights that will be allowed by the destination airport from an origin airport and that, in the order that the edges in \\(E\\) are listed above, these limits are\n\n\\[\nM=\\{4,3,8,7,2,6,7,10,5,8\\} .\n\\]\nNow suppose that Gamma wants to maximize the flow of flights into airport 1 and out of airport 6 . Count inflows into an airport as positive and outflows as negative. Assume that the net in/outflow of Gamma flights at each airport 1 to 5 is zero, while the net inflow of such flights into airport 1 matches the net outflow from 6 .\n\nDescribe the problem of maximizing this inflow to airport 1 as a linear programming problem and express it in a standard form (block matrices are helpful.) Note that the appropriate variables are all outflows from one airport to another, i.e., along edges, together with the net inflow into airport 1.\nSolve the problem of part (a). Also solve the reverse problem: Maximize inflow into airport 6 and matching outflow from 1. Explain and justify your answers.\n\n\nWith the same limits on allowable flights into airports as in item 5, suppose that Gamma Airlines wants to determine an allocation of planes that will maximize their profits, given the following constraints: (1) Airports 1 and 6 have repair facilities for their planes, so no limit is placed on the inflow or outflow of their planes other than the airport limits. (2) Flights through airports 2-5 of Gamma planes are pass through, i.e., inflow and outflow must match. (3) Gamma has 32 planes available for this network of airports. (4) The profits per flight in thousands are, in the order that the edges in \\(E\\) are listed above,\n\n\\[\nP=\\{5,6,7,9,10,8,9,5,6,10\\}\n\\] (a) Set this problem up as a linear programming problem in standard form. Clearly identify the variables and explain how the constraints follow.\n\nSolve this problem explicitly and specify the operations taken to do so. Example 3.56 is instructive for this problem, so be aware of it. Use a technology tool that allows you to use elementary operations (ALAMA calculator has this capability)."
  },
  {
    "objectID": "HW/projects_2.html",
    "href": "HW/projects_2.html",
    "title": "Projects 2",
    "section": "",
    "text": "Instructions:\nPlease do all three projects. However, in the first project (Modeling with Directed Graphs), you can choose which three parts of the five you’d like to complete."
  },
  {
    "objectID": "HW/projects_2.html#project-4",
    "href": "HW/projects_2.html#project-4",
    "title": "Projects 2",
    "section": "",
    "text": "Project Descriptions: These projects introduce more applications of digraphs as mathematical modeling tools. You are given that the digraph \\(G\\) has vertex set \\(V=\\{1,2,3,4,5,6\\}\\) and edge set\n\\[\nE=\\{(1,2),(2,3),(3,4),(4,2),(1,4),(3,1),(3,6),(6,3),(4,5),(5,6)\\}\n\\]\nAddress the following points regarding \\(G\\).\n\n\nDraw a picture of this digraph. You may leave space in your report and draw this by hand, or if you prefer, you may use the computer drawing applications available to you on your system.\n\n\n\nExhibit the incidence matrix \\(A\\) of this digraph and find a basis for \\(\\mathcal{N}(A)\\) using its reduced row echelon form. Some of the basis elements may be algebraic but not directed loops. Use this basis to find a basis of directed loops (e.g., non-directed basis element \\(\\mathbf{c}_{1}\\) might be replaced by directed \\(\\mathbf{c}_{1}+\\mathbf{c}_{2}\\) ).\n\n\nThink of the digraph as representing an electrical circuit where an edge represents some electrical object like a resistor or capacitor. Each node represents the circuit space between these objects. and we can attach a potential value to each node, say the potentials are \\(x_{1}, \\ldots, x_{6}\\). The potential difference across an edge is the potential value of head minus tail. Kirchhoff’s second law of electrical circuits says that the sum of potential differences around a circuit loop must be zero. Assume and use the fact (p. 422) that \\(A \\mathbf{x}=\\mathbf{b}\\) implies that for all \\(\\mathbf{y} \\in \\mathcal{N}\\left(A^{T}\\right), \\mathbf{y}^{T} \\mathbf{b}=0\\) to find conditions that a vector \\(\\mathbf{b}\\) must satisfy in order for it to be a vector of potential differences for some potential distribution on the vertices."
  },
  {
    "objectID": "HW/projects_2.html#project-5",
    "href": "HW/projects_2.html#project-5",
    "title": "Projects 2",
    "section": "",
    "text": "Assume that across each edge of a circuit a current flows. Thus, we can assign to each edge a “weight,” namely the current flow along the edge. This is an example of a weighted digraph. However, not just any set of current weights will do, since Kirchhoff’s first law of circuits says that the total flow of current in and out of any node should be 0 . Use this law to find a matrix condition that must be satisfied by the currents and solve it to exhibit some current flows.\nThink of the digraph as representing a directed communications network. Here loops determine which nodes have bidirectional communication since any two nodes of a loop can only communicate with each other by way of a loop. By examining only a basis of directed loops how could you determine which nodes in the network can communicate with each other?"
  },
  {
    "objectID": "HW/projects_2.html#project-6",
    "href": "HW/projects_2.html#project-6",
    "title": "Projects 2",
    "section": "",
    "text": "Think of vertices of the digraph as representing airports and edges representing flight connections between airports for Gamma Airlines. Suppose further that for each connection there is a maximum number of daily flights that will be allowed by the destination airport from an origin airport and that, in the order that the edges in \\(E\\) are listed above, these limits are\n\n\\[\nM=\\{4,3,8,7,2,6,7,10,5,8\\} .\n\\]\nNow suppose that Gamma wants to maximize the flow of flights into airport 1 and out of airport 6 . Count inflows into an airport as positive and outflows as negative. Assume that the net in/outflow of Gamma flights at each airport 1 to 5 is zero, while the net inflow of such flights into airport 1 matches the net outflow from 6 .\n\nDescribe the problem of maximizing this inflow to airport 1 as a linear programming problem and express it in a standard form (block matrices are helpful.) Note that the appropriate variables are all outflows from one airport to another, i.e., along edges, together with the net inflow into airport 1.\nSolve the problem of part (a). Also solve the reverse problem: Maximize inflow into airport 6 and matching outflow from 1. Explain and justify your answers.\n\n\nWith the same limits on allowable flights into airports as in item 5, suppose that Gamma Airlines wants to determine an allocation of planes that will maximize their profits, given the following constraints: (1) Airports 1 and 6 have repair facilities for their planes, so no limit is placed on the inflow or outflow of their planes other than the airport limits. (2) Flights through airports 2-5 of Gamma planes are pass through, i.e., inflow and outflow must match. (3) Gamma has 32 planes available for this network of airports. (4) The profits per flight in thousands are, in the order that the edges in \\(E\\) are listed above,\n\n\\[\nP=\\{5,6,7,9,10,8,9,5,6,10\\}\n\\] (a) Set this problem up as a linear programming problem in standard form. Clearly identify the variables and explain how the constraints follow.\n\nSolve this problem explicitly and specify the operations taken to do so. Example 3.56 is instructive for this problem, so be aware of it. Use a technology tool that allows you to use elementary operations (ALAMA calculator has this capability)."
  },
  {
    "objectID": "lectures/ch3lecture1b.html#linear-independence",
    "href": "lectures/ch3lecture1b.html#linear-independence",
    "title": "Ch3 Lecture 1",
    "section": "Linear Independence",
    "text": "Linear Independence\n\nThe vectors \\(\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\ldots, \\mathbf{v}_{n}\\) are linearly dependent if there exist scalars \\(c_{1}, c_{2}, \\ldots, c_{n}\\), not all zero, such that\n\\[\n\\begin{equation*}\nc_{1} \\mathbf{v}_{1}+c_{2} \\mathbf{v}_{2}+\\cdots+c_{n} \\mathbf{v}_{n}=\\mathbf{0}\n\\end{equation*}\n\\]\nOtherwise, the vectors are called linearly independent.\n\n\nThis doesn’t mean that one of the vectors is a scalar multiple of another one."
  },
  {
    "objectID": "lectures/ch3lecture1b.html#checking-for-linear-independence",
    "href": "lectures/ch3lecture1b.html#checking-for-linear-independence",
    "title": "Ch3 Lecture 1",
    "section": "Checking for Linear Independence",
    "text": "Checking for Linear Independence\nIs this set of vectors linearly independent?\n\\[\n\\left[\\begin{array}{l}1 \\\\ 1 \\\\ 0\\end{array}\\right],\\left[\\begin{array}{l}0 \\\\ 1 \\\\ 1\\end{array}\\right],\\left[\\begin{array}{r}1 \\\\ -1 \\\\ -2\\end{array}\\right]\n\\]\n\nGoal: find some set of scalars \\(c_{1}, c_{2}, c_{3}\\), not all zero, such that \\(c_{1} \\mathbf{v}_{1}+c_{2} \\mathbf{v}_{2}+c_{3} \\mathbf{v}_{3}=\\mathbf{0}\\).\n\n\nDefine: - matrix \\(A=\\left[\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\mathbf{v}_{3}\\right]\\) - vector \\(\\mathbf{c}=\\left(c_{1}, c_{2}, c_{3}\\right)\\).\n\n\nThen:\n\\[\nc_{1} \\mathbf{v}_{1}+c_{2} \\mathbf{v}_{2}+c_{3} \\mathbf{v}_{3}=\\left[\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\mathbf{v}_{3}\\right]\\left[\\begin{array}{l}\nc_{1} \\\\\nc_{2} \\\\\nc_{3}\n\\end{array}\\right]=A \\mathbf{c}\n\\]\nSo we just need to see if there is a nontrivial solution to \\(A \\mathbf{c}=\\mathbf{0}\\)."
  },
  {
    "objectID": "lectures/ch3lecture1b.html#section",
    "href": "lectures/ch3lecture1b.html#section",
    "title": "Ch3 Lecture 1",
    "section": "",
    "text": "\\[\n\\begin{equation*}\ny_{i, t+1}=y_{i, t}+d\\left(y_{i-1, t}-2 y_{i, t}+y_{i+1, t}\\right)\n\\end{equation*}\n\\]\n\\(y_{0,t+1} = 0\\) \\(y_{1,t+1}=y_{1,t}+d\\left(0-2 y_{1,t}+y_{2,t}\\right)\\) \\(y_{3,t+1}=y_{3,t}+d\\left(y_{2,t}-2 y_{3,t}+y_{4,t}\\right)\\) … \\(y_{6,t+1}=0\\)\n\nWe want to solve these for \\(y_{i,t}\\) given \\(y_{i,t+1}\\).\n\\[\n\\begin{equation*}\ny_{i, t+1}-d\\left(y_{i-1, t}-2 y_{i, t}+y_{i+1, t}\\right)=y_{i, t}\n\\end{equation*}\n\\]\n\\[\n\\approx y_{i, t+1}-d\\left(y_{i-1, t+1}-2 y_{i, t+1}+y_{i+1, t+1}\\right) ?\n\\]"
  },
  {
    "objectID": "lectures/ch3lecture1b.html#example-2",
    "href": "lectures/ch3lecture1b.html#example-2",
    "title": "Ch3 Lecture 1",
    "section": "Example 2",
    "text": "Example 2\nAre the following vectors linearly independent?\n\\(\\left[\\begin{array}{l}0 \\\\ 1 \\\\ 0\\end{array}\\right],\\left[\\begin{array}{l}1 \\\\ 1 \\\\ 0\\end{array}\\right],\\left[\\begin{array}{l}0 \\\\ 1 \\\\ 1\\end{array}\\right]\\)\n\nSet up A = \\(\\left[\\begin{array}{lll}0 & 1 & 0 \\\\ 1 & 1 & 1 \\\\ 0 & 0 & 1\\end{array}\\right]\\)\n\n\nDifferent approach: \\[\n\\operatorname{det}\\left[\\begin{array}{lll}\n0 & 1 & 0 \\\\\n1 & 1 & 1 \\\\\n0 & 0 & 1\n\\end{array}\\right]=-1 \\operatorname{det}\\left[\\begin{array}{ll}\n1 & 0 \\\\\n0 & 1\n\\end{array}\\right]=-1\n\\]\n\n\nDeterminant is non-zero, so \\(A\\) is invertible (non-singular)\n\n\nOnly solution to \\(A \\mathbf{c}=0\\) is \\(\\mathbf{c}=0\\).\n\n\nSo the vectors are linearly independent."
  },
  {
    "objectID": "lectures/ch3lecture1b.html#summary-checking-for-linear-independence",
    "href": "lectures/ch3lecture1b.html#summary-checking-for-linear-independence",
    "title": "Ch3 Lecture 1",
    "section": "Summary: Checking for Linear Independence",
    "text": "Summary: Checking for Linear Independence\n\nSet up matrix \\(A\\) with the vectors as columns.\nEither:\n\nFind the reduced row echelon form of \\(A\\).\nCalculate the determinant of \\(A\\).\nCheck if the only solution to \\(A \\mathbf{c}=0\\) is \\(\\mathbf{c}=0\\)."
  },
  {
    "objectID": "lectures/ch3lecture1b.html#basis-of-a-vector-space",
    "href": "lectures/ch3lecture1b.html#basis-of-a-vector-space",
    "title": "Ch3 Lecture 1",
    "section": "Basis of a Vector Space",
    "text": "Basis of a Vector Space\n\nA spanning set is a set of vectors that can be combined to create any vector in the vector space.\n\n\nA basis for the vector space \\(V\\) is a spanning set of vectors \\(\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\ldots, \\mathbf{v}_{n}\\) that is a linearly independent set."
  },
  {
    "objectID": "lectures/ch3lecture1b.html#standard-basis",
    "href": "lectures/ch3lecture1b.html#standard-basis",
    "title": "Ch3 Lecture 1",
    "section": "Standard Basis",
    "text": "Standard Basis\n\nThe standard basis for \\(\\mathbb{R}^{n}\\) is the set of vectors \\(\\mathbf{e}_{1}, \\mathbf{e}_{2}, \\ldots, \\mathbf{e}_{n}\\) given by the columns of the \\(n \\times n\\) identity matrix."
  },
  {
    "objectID": "lectures/ch3lecture1b.html#showing-that-the-standard-basis-is-a-basis-for-mathbbrn-or-mathbbcn",
    "href": "lectures/ch3lecture1b.html#showing-that-the-standard-basis-is-a-basis-for-mathbbrn-or-mathbbcn",
    "title": "Ch3 Lecture 1",
    "section": "Showing that the Standard Basis is a Basis for \\(\\mathbb{R}^{n}\\) (or \\(\\mathbb{C}^{n}\\))",
    "text": "Showing that the Standard Basis is a Basis for \\(\\mathbb{R}^{n}\\) (or \\(\\mathbb{C}^{n}\\))\n\nLet \\(\\mathbf{v}=\\left(c_{1}, c_{2}, \\ldots, c_{n}\\right)\\) be a vector from \\(V\\)\n\\(c_{1}, c_{2}, \\ldots, c_{n}\\) are scalars (real or complex)\n\n\n\\[\n\\begin{aligned}\n\\mathbf{v} & =\\left[\\begin{array}{c}\nc_{1} \\\\\nc_{2} \\\\\n\\vdots \\\\\nc_{n}\n\\end{array}\\right]=c_{1}\\left[\\begin{array}{c}\n1 \\\\\n0 \\\\\n\\vdots \\\\\n0\n\\end{array}\\right]+c_{2}\\left[\\begin{array}{c}\n0 \\\\\n1 \\\\\n\\vdots \\\\\n0\n\\end{array}\\right]+\\cdots+c_{n}\\left[\\begin{array}{c}\n0 \\\\\n\\vdots \\\\\n0 \\\\\n1\n\\end{array}\\right] \\\\\n& =c_{1} \\mathbf{e}_{1}+c_{2} \\mathbf{e}_{2}+\\cdots+c_{n} \\mathbf{e}_{n} .\n\\end{aligned}\n\\]\n\nTwo points here: 1. you can make any vector in V out of a linear combination of the e’s, so the e’s span V. 2. the e’s are linearly independent because if you have a linear combination of them that equals 0, then each coefficient must be 0. 3. Therefore, the e’s are a basis of V."
  },
  {
    "objectID": "lectures/ch3lecture1b.html#invertibility-basis-and-linear-independence",
    "href": "lectures/ch3lecture1b.html#invertibility-basis-and-linear-independence",
    "title": "Ch3 Lecture 1",
    "section": "Invertibility, Basis, and Linear Independence",
    "text": "Invertibility, Basis, and Linear Independence\nAn \\(n \\times n\\) real matrix \\(A\\) is invertible if and only if its columns are linearly independent\n\nin which case they form a basis of \\(\\mathbb{R}^{n}\\)"
  },
  {
    "objectID": "lectures/ch3lecture1b.html#column-and-row-spaces",
    "href": "lectures/ch3lecture1b.html#column-and-row-spaces",
    "title": "Ch3 Lecture 1",
    "section": "Column and Row Spaces",
    "text": "Column and Row Spaces\n\nThe column space of the \\(m \\times n\\) matrix \\(A\\) is the subspace \\(\\mathcal{C}(A)\\) of \\(\\mathbb{R}^{m}\\) spanned by the columns of \\(A\\).\n\n\n\nThe row space of the \\(m \\times n\\) matrix \\(A\\) is the subspace \\(\\mathcal{R}(A)\\) of \\(\\mathbb{R}^{n}\\) spanned by the transposes of the rows of \\(A\\)"
  },
  {
    "objectID": "lectures/ch3lecture1b.html#null-space",
    "href": "lectures/ch3lecture1b.html#null-space",
    "title": "Ch3 Lecture 1",
    "section": "Null Space",
    "text": "Null Space\n\nThe null space of the \\(m \\times n\\) matrix \\(A\\) is the subset \\(\\mathcal{N}(A)\\) of \\(\\mathbb{R}^{n}\\)\n\\[\n\\mathcal{N}(A)=\\left\\{\\mathbf{x} \\in \\mathbb{R}^{n} \\mid A \\mathbf{x}=\\mathbf{0}\\right\\}\n\\]\n\n\n\\(\\mathcal{N}(A)\\) is just the solution set to \\(A \\mathbf{x}=\\mathbf{0}\\)\n\n\nFor example, if \\(A\\) is invertible, \\(A \\mathbf{x}=\\mathbf{0}\\) has only the trivial solution \\(\\mathbf{x}=\\mathbf{0}\\)\n\n\nso \\(\\mathcal{N}(A)\\) is just \\(\\left\\{\\mathbf{0}\\right\\}\\).\n\n\n\\(A\\) is invertible exactly if \\(\\mathcal{N}(A)=\\{\\mathbf{0}\\}\\)"
  },
  {
    "objectID": "lectures/ch3lecture1b.html#kernel-of-an-operator",
    "href": "lectures/ch3lecture1b.html#kernel-of-an-operator",
    "title": "Ch3 Lecture 1",
    "section": "Kernel of an Operator",
    "text": "Kernel of an Operator\n\nThe kernel of the linear operator \\(T\\) : \\(V \\rightarrow W\\) is the subspace of \\(V\\) defined by\n\\[\n\\operatorname{ker}(T)=\\{\\mathbf{x} \\in V \\mid T(\\mathbf{x})=\\mathbf{0}\\}\n\\]\n\nFor matrix operators kernels are the same thing as null spaces."
  },
  {
    "objectID": "lectures/ch3lecture1b.html#example",
    "href": "lectures/ch3lecture1b.html#example",
    "title": "Ch3 Lecture 1",
    "section": "Example",
    "text": "Example\nFind the null space of the matrix\n\\[\nA=\\left[\\begin{array}{rrrr}\n1 & 1 & 1 & -1 \\\\\n0 & 1 & 2 & 1\n\\end{array}\\right]\n\\]\ncorresponding to the system of equations \\[\n\\begin{aligned}\nx_{1}+x_{2}+x_{3}-x_{4} & =0 \\\\\nx_{2}+2 x_{3}+x_{4} & =0\n\\end{aligned}\n\\]\n\n\nFind the RREF of \\(A\\):\n\n\n\n\n\n\n\nPivots are in the first and second columns, so it follows that \\(x_{3}\\) and \\(x_{4}\\) are free, \\(x_{1}\\) and \\(x_{2}\\) are bound\n\n\\[\n\\begin{aligned}\n& x_{1}=x_{3}+2 x_{4} \\\\\n& x_{2}=-2 x_{3}-x_{4} .\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lectures/ch3lecture1b.html#section-1",
    "href": "lectures/ch3lecture1b.html#section-1",
    "title": "Ch3 Lecture 1",
    "section": "",
    "text": "\\[\ny_{t+1}=(I+dM)y_{t} \\Rightarrow y_{t}=(I+dM)^{-1}y_{t+1}\n\\]\n\\[\ny_{t}=y_{t+1}- dMy_{t} \\overset{?}\\approx y_{t+1}- d My_{t+1}\n\\]\n\nConnection: we will use math to show that the two are approximately equal.\n\n\nRemember Taylor series?\nFor small \\(x\\), we can approximate \\(\\frac{1}{1+x}\\) as \\(1-x+x^2-x^3+\\ldots\\)\n\n\nIf we have a matrix \\(A\\) such that \\(X = I + A\\) is invertible, then \\(X^{-1} = I - A + A^2 - A^3 + \\ldots\\)\n\n\nSo to first order in d,\n\\[\ny_{t}=(I+dM)^{-1}y_{t+1} \\approx \\left(I-dM\\right) y_{t+1}=y_{t+1}-d M y_{t+1}\n\\]\n\n\nNow \\(y_{t+1}=y_{t} + dM y_{t}\\). Plugging this in,\n\\(y_{t}=(I+dM)^{-1}y_{t+1} \\approx y_{t+1}-d M \\left(y_{t}+d M y_{t}\\right)\\approx y_{t+1}-d M y_{t}\\)"
  },
  {
    "objectID": "lectures/ch3lecture1b.html#using-the-null-space",
    "href": "lectures/ch3lecture1b.html#using-the-null-space",
    "title": "Ch3 Lecture 1",
    "section": "Using the Null Space",
    "text": "Using the Null Space\nSuppose that a Markov chain has an stable transition matrix \\(A=\\left[\\begin{array}{ll}0.7 & 0.4 \\\\ 0.3 & 0.6\\end{array}\\right]\\), so that\n\\[\n\\mathbf{x}^{(k+1)}=A \\mathbf{x}^{(k)}\n\\]\n\nWhat is the steady-state vector \\(\\mathbf{x}\\)? (The limit as \\(k \\rightarrow \\infty\\) of \\(\\mathbf{x}^{(k)}\\))\n\n\nTake limit of both sides:\n\\[\n\\mathbf{x}=A \\mathbf{x}\n\\]\n\n\\[\n\\mathbf{0}=\\mathbf{x}-A \\mathbf{x}=I \\mathbf{x}-A \\mathbf{x}=(I-A) \\mathbf{x}\n\\]\nSo \\(\\mathbf{x}\\) is in the null space of \\(I-A\\).\n\n\n\n\\[\n\\mathbf{0}=\\mathbf{x}-A \\mathbf{x}=I \\mathbf{x}-A \\mathbf{x}=(I-A) \\mathbf{x}\n\\]\nSo \\(\\mathbf{x}\\) is in the null space of \\(I-A\\)."
  },
  {
    "objectID": "lectures/ch3lecture1b.html#section-2",
    "href": "lectures/ch3lecture1b.html#section-2",
    "title": "Ch3 Lecture 1",
    "section": "",
    "text": "Is this set of vectors linearly independent?\n\\[\n\\left[\\begin{array}{l}1 \\\\ 1 \\\\ 0\\end{array}\\right],\\left[\\begin{array}{l}0 \\\\ 1 \\\\ 1\\end{array}\\right],\\left[\\begin{array}{r}1 \\\\ -1 \\\\ -2\\end{array}\\right]\n\\]\nPut in matrix form and find the reduced row echelon form: . . .\n\\[\n\\left[\\begin{array}{rrr}\n1 & 0 & 1 \\\\\n1 & 1 & -1 \\\\\n0 & 1 & -2\n\\end{array}\\right] \\xrightarrow[E_{21}(-1)]{\\longrightarrow}\\left[\\begin{array}{rrr}\n1 & 0 & 1 \\\\\n0 & 1 & -2 \\\\\n0 & 1 & -2\n\\end{array}\\right] \\xrightarrow[E_{32}(-1)]{\\longrightarrow}\\left[\\begin{array}{rrr}\n1 & 0 & 1 \\\\\n0 & 1 & -2 \\\\\n0 & 0 & 0\n\\end{array}\\right],\n\\]\n\nSolutions to the homogeneous system are \\(\\mathbf{c}=\\left(c_{1}=-c_{3},c_{2}= 2 c_{3}, c_{3}= c_{3}\\right)=\\)\nWhole vector is \\(c_{3}(-1,2,1)\\). Take \\(c_{3}=1\\) and we have that \\[\n-1 \\mathbf{v}_{1}+2 \\mathbf{v}_{2}+1 \\mathbf{v}_{3}=0\n\\]\n\n\n\\[\n-1 \\mathbf{v}_{1}+2 \\mathbf{v}_{2}+1 \\mathbf{v}_{3}=0\n\\]\nSo the vectors are linearly dependent."
  },
  {
    "objectID": "lectures/ch3lecture1b.html#using-the-null-space-to-find-a-basis",
    "href": "lectures/ch3lecture1b.html#using-the-null-space-to-find-a-basis",
    "title": "Ch3 Lecture 1",
    "section": "Using the Null Space to find a basis",
    "text": "Using the Null Space to find a basis\nFind a basis for the column space of the matrix\n\\[\nA=\\left[\\begin{array}{rrrr}\n1 & 1 & 1 & -1 \\\\\n0 & 1 & 2 & 1\n\\end{array}\\right]\n\\]\n\nFrom before, we found the RREF and used this to find a general form of the solutions to \\(A \\mathbf{x}=\\mathbf{0}\\)\n\\(\\mathbf{x}=\\left(x_{1}, x_{2}, x_{3}, x_{4}\\right)=\\) \\(\\left(x_{3}+2 x_{4},-2 x_{3}-x_{4}, x_{3}, x_{4}\\right)\\)\n\n\nWrite \\(A=\\left[\\mathbf{a}_{1}, \\mathbf{a}_{2}, \\mathbf{a}_{3}, \\mathbf{a}_{4}\\right]\\)\n\n\n\\(\\mathbf{0}=x_{1} \\mathbf{a}_{1}+x_{2} \\mathbf{a}_{2}+x_{3} \\mathbf{a}_{3}+x_{4} \\mathbf{a}_{4}\\)\n\n\n\\(\\mathbf{0}=\\left(x_{3}+2 x_{4}\\right) \\mathbf{a}_{1}-\\left(2 x_{3}+x_{4}\\right) \\mathbf{a}_{2}+x_{3} \\mathbf{a}_{3}+x_{4} \\mathbf{a}_{4}\\)\n\n\nTake \\(x_{3}=1\\) and \\(x_{4}=0\\). Then \\(\\mathbf{0}=\\mathbf{a}_{1}-2 \\mathbf{a}_{2}+\\mathbf{a}_{3}\\)\n\n\nSo \\(\\mathbf{a}_{3}\\) is a linear combination of \\(\\mathbf{a}_{1}\\) and \\(\\mathbf{a}_{2}\\)\n\n\nSimilarly, take \\(x_{3}=0\\) and \\(x_{4}=1\\). Then \\(\\mathbf{0}=2 \\mathbf{a}_{1}-\\mathbf{a}_{2}+\\mathbf{a}_{4}\\) so \\(\\mathbf{a}_{4}\\) is a linear combination of \\(\\mathbf{a}_{1}\\) and \\(\\mathbf{a}_{2}\\)\n\n\nSo \\(\\mathbf{a}_{1}\\) and \\(\\mathbf{a}_{2}\\) are a basis for the column space of \\(A\\)\n\\(\\mathcal{C}(A)=\\operatorname{span}\\left\\{\\mathbf{a}_{1}, \\mathbf{a}_{2}\\right\\}=\\operatorname{span}\\left\\{\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right],\\left[\\begin{array}{l}1 \\\\ 1\\end{array}\\right]\\right\\}\\)"
  },
  {
    "objectID": "lectures/ch3lecture1b.html#consistency-and-column-space-back-to-linear-systems",
    "href": "lectures/ch3lecture1b.html#consistency-and-column-space-back-to-linear-systems",
    "title": "Ch3 Lecture 1",
    "section": "Consistency and Column Space: Back to Linear Systems",
    "text": "Consistency and Column Space: Back to Linear Systems\n\nThe linear system \\(A \\mathbf{x}=\\mathbf{b}\\) of \\(m\\) equations in \\(n\\) unknowns is consistent if and only if \\(\\mathbf{b} \\in \\mathcal{C}(A)\\).\n\n\n\\(A \\mathbf{x}\\) is a linear combination of the columns of \\(A\\) with the entries of \\(\\mathbf{x}\\) as scalar coefficients\nIf \\(A \\mathbf{x}=\\mathbf{b}\\) has a solution means some linear combination of columns of \\(A\\) adds up to \\(\\mathbf{b}\\)\n\\(\\mathbf{b} \\in \\mathcal{C}(A)\\)"
  },
  {
    "objectID": "lectures/ch3lecture1b.html#how-to-tell-if-a-vector-is-in-a-spanning-space",
    "href": "lectures/ch3lecture1b.html#how-to-tell-if-a-vector-is-in-a-spanning-space",
    "title": "Ch3 Lecture 1",
    "section": "How to tell if a vector is in a spanning space",
    "text": "How to tell if a vector is in a spanning space\nSuppose we have a space \\(V\\) spanned by \\(\\mathbf{v}_{1}=(1,1,3,3), \\mathbf{v}_{2}=(0,2,2,4)\\), and and \\(\\mathbf{v}_{3}=(1,0,2,1)\\)\n\nOne of these vectors is in \\(V\\). Which one?\n\\(\\mathbf{u}=(2,1,5,4)\\) and \\(\\mathbf{w}=(1,0,0,0)\\)\n\n\nDefine \\(A=\\left[\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\mathbf{v}_{3}\\right]\\)\nThen \\(\\mathbf{u} \\in \\mathcal{C}(A)\\) if and only if \\(A \\mathbf{x}=\\mathbf{u}\\) has a solution (is consistent)\n\n\nSolve both at once: \\[\n[A|\\mathbf{u}| \\mathbf{w}]=\\left[\\begin{array}{lllll}\n1 & 0 & 1 & 2 & 1 \\\\\n1 & 2 & 0 & 1 & 0 \\\\\n3 & 2 & 2 & 5 & 0 \\\\\n3 & 4 & 1 & 4 & 0\n\\end{array}\\right] \\text { with reduced row echelon form }\\left[\\begin{array}{rrrrr}\n1 & 0 & 1 & 2 & 0 \\\\\n0 & 1 & -\\frac{1}{2} & -\\frac{1}{2} & 0 \\\\\n0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 & 0\n\\end{array}\\right]\n\\]\n\nObserve that there is a pivot in the fifth column but not in the fourth column. This tells us that the system with augmented matrix \\([A \\mid \\mathbf{u}]\\) is consistent, but the system with augmented matrix \\([A \\mid \\mathbf{w}]\\) is not consistent. Therefore, \\(\\mathbf{u} \\in \\operatorname{span}\\left\\{\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\mathbf{v}_{3}\\right\\}\\), but \\(\\mathbf{w} \\notin \\operatorname{span}\\left\\{\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\mathbf{v}_{3}\\right\\}\\).\n\n\n\n\\(\\mathbf{u} \\in \\operatorname{span}\\left\\{\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\mathbf{v}_{3}\\right\\}\\),\nbut \\(\\mathbf{w} \\notin \\operatorname{span}\\left\\{\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\mathbf{v}_{3}\\right\\}\\)\n\n\n\\[\n\\mathbf{u}=\\left(2-c_{3}\\right) \\mathbf{v}_{1}+\\frac{1}{2}\\left(c_{3}-1\\right) \\mathbf{v}_{2}+c_{3} \\mathbf{v}_{3}\n\\]"
  },
  {
    "objectID": "lectures/ch3lecture1b.html#summary",
    "href": "lectures/ch3lecture1b.html#summary",
    "title": "Ch3 Lecture 1",
    "section": "Summary",
    "text": "Summary"
  },
  {
    "objectID": "lectures/ch3lecture1b.html#in-matrix-form",
    "href": "lectures/ch3lecture1b.html#in-matrix-form",
    "title": "Ch3 Lecture 1",
    "section": "In matrix form",
    "text": "In matrix form\nIn matrix form, this becomes \\(y_{t+1}=y_{t}+ My_{t}\\):\n\\[\nM=\\left[\\begin{array}{ccccccc}\n-2 & 1 & 0 & 0 & 0 & 0 & 0 \\\\\n1 & -2 & 1 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & -2 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & -2 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & -2 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 1 & -2 & 1 \\\\\n0 & 0 & 0 & 0 & 0 & 1 & -2\n\\end{array}\\right]\n\\]\n\n\\[\ny_{t}=y_{t+1}- d My_{t} \\overset{?}\\approx y_{t+1}- My_{t+1}\n\\]\nTurns out it works… but why?"
  },
  {
    "objectID": "lectures/ch3lecture1b.html#using-matrix-inversion",
    "href": "lectures/ch3lecture1b.html#using-matrix-inversion",
    "title": "Ch3 Lecture 1",
    "section": "Using matrix inversion",
    "text": "Using matrix inversion\n\n\\(y_{t+1}=y_{t}+ My_{t}\\)\nis the same as \\(y_{t+1}=Iy_{t}+ My_{t}\\)\nin other words, \\(y_{t+1}=(I+M)y_{t}\\)\n\n\nHow do we solve this for \\(y_{t}\\), given \\(y_{t+1}\\)?\n\n\n\\[\ny_{t+1}=(I+M)y_{t} \\Rightarrow y_{t}=(I+M)^{-1}y_{t+1}\n\\]\n\n\nTwo different approaches – one correct, the other one gave approximately the right answer. Why?"
  },
  {
    "objectID": "lectures/ch3lecture1b.html#the-point",
    "href": "lectures/ch3lecture1b.html#the-point",
    "title": "Ch3 Lecture 1",
    "section": "The point",
    "text": "The point\nThe problem was much easier to solve once we put it in matrix form."
  },
  {
    "objectID": "lectures/ch3lecture1b.html#section-3",
    "href": "lectures/ch3lecture1b.html#section-3",
    "title": "Ch3 Lecture 1",
    "section": "",
    "text": "\\[\n\\left[\\begin{array}{c}\nx_{1} \\\\\nx_{2} \\\\\nx_{3} \\\\\nx_{4}\n\\end{array}\\right]=\\left[\\begin{array}{c}\nx_{3}+2 x_{4} \\\\\n-2 x_{3}-x_{4} \\\\\nx_{3} \\\\\nx_{4}\n\\end{array}\\right]=x_{3}\\left[\\begin{array}{r}\n1 \\\\\n-2 \\\\\n1 \\\\\n0\n\\end{array}\\right]+x_{4}\\left[\\begin{array}{r}\n2 \\\\\n-1 \\\\\n0 \\\\\n1\n\\end{array}\\right] .\n\\]\n\nThe general solution to the homogeneous system is an arbitrary linear combination of these two vectors\n\n\n\\[\n\\mathcal{N}(A)=\\operatorname{span}\\left\\{\\left[\\begin{array}{r}\n1 \\\\\n-2 \\\\\n1 \\\\\n0\n\\end{array}\\right],\\left[\\begin{array}{r}\n2 \\\\\n-1 \\\\\n0 \\\\\n1\n\\end{array}\\right]\\right\\} \\subseteq \\mathbb{R}^{4}\n\\]\n\n\nThese are independent, so they form a basis for \\(\\mathcal{N}(A)\\)"
  },
  {
    "objectID": "lectures/ch3lecture1b.html#section-4",
    "href": "lectures/ch3lecture1b.html#section-4",
    "title": "Ch3 Lecture 1",
    "section": "",
    "text": "\\[\nI-A=\\left[\\begin{array}{ll}\n1 & 0 \\\\\n0 & 1\n\\end{array}\\right]-\\left[\\begin{array}{ll}\n0.7 & 0.4 \\\\\n0.3 & 0.6\n\\end{array}\\right]=\\left[\\begin{array}{rr}\n0.3 & -0.4 \\\\\n-0.3 & 0.4\n\\end{array}\\right] \\text {. }\n\\]\n\n\\[\n\\left[\\begin{array}{rr}\n0.3 & -0.4 \\\\\n-0.3 & 0.4\n\\end{array}\\right] \\xrightarrow{E_{21}(1)}\\left[\\begin{array}{rr}\n1 & -4 / 3 \\\\\n0 & 0\n\\end{array}\\right]\n\\]\n\nnull space of \\(I-A\\) is spanned by the single vector \\((4 / 3,1)\\). In particular, any multiple of this vector qualifies as a possible limiting vector.\nIf we want a limiting vector whose entries are nonnegative and sum to 1 (which is required for states in a Markov chain), then the only choice is the vector resulting from dividing \\((4 / 3,1)\\) by the sum of its coordinates to obtain\n\nSolution set is \\(x_{1}=4 x_{2} / 3\\)\n\n\n\\[\n(3 / 7)(4 / 3,1)=(4 / 7,3 / 7) \\approx(0.57143,0.42857)\n\\]"
  },
  {
    "objectID": "lectures/ch3lecture1b.html#form-of-general-solution-to-a-linear-system",
    "href": "lectures/ch3lecture1b.html#form-of-general-solution-to-a-linear-system",
    "title": "Ch3 Lecture 1",
    "section": "Form of general solution to a linear system",
    "text": "Form of general solution to a linear system\n\n\nSuppose the system \\(A \\mathbf{x}=\\mathbf{b}\\) has a particular solution \\(\\mathbf{x}_{*}\\).\nThen the general solution \\(\\mathbf{x}\\) to this system can be described by the equation\n\n\n\\[\n\\mathbf{x}=\\mathbf{x}_{*}+\\mathbf{z}\n\\]\nwhere \\(\\mathbf{z}\\) runs over all elements of \\(\\mathcal{N}(A)\\).\n\n\n\nSolution set is the set of all translates of elements in the null space of \\(A\\) by some fixed vector.\n\n\nWhen n is 2 or 3 this says that the solution set is either a single point, a line or a plane—not necessarily through the origin!"
  },
  {
    "objectID": "lectures/ch3lecture1b.html#example-1",
    "href": "lectures/ch3lecture1b.html#example-1",
    "title": "Ch3 Lecture 1",
    "section": "Example",
    "text": "Example\nDescribe the solution sets to the system \\[\n\\begin{array}{r}\nx+2 y=3 \\\\\nx+y+t=3 .\n\\end{array}\n\\]\n\n\n\n\n\\[\n\\begin{aligned}\n& x=3-2 t \\\\\n& y=t \\\\\n& z=t .\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lectures/ch3lecture1b.html#null-space-algorithm",
    "href": "lectures/ch3lecture1b.html#null-space-algorithm",
    "title": "Ch3 Lecture 1",
    "section": "Null Space algorithm",
    "text": "Null Space algorithm\nGiven an \\(m \\times n\\) matrix \\(A\\).\n\nCompute the reduced row echelon form \\(R\\) of \\(A\\).\nUse \\(R\\) to find the general solution to the homogeneous system \\(A \\mathbf{x}=0\\).\n\n\n\nWrite the general solution \\(\\mathbf{x}=\\left(x_{1}, x_{2}, \\ldots, x_{n}\\right)\\) to the homogeneous system in the form\n\n\\[\n\\mathbf{x}=x_{i_{1}} \\mathbf{w}_{1}+x_{i_{2}} \\mathbf{w}_{2}+\\cdots+x_{i_{n-r}} \\mathbf{w}_{n-r}\n\\]\nwhere \\(x_{i_{1}}, x_{i_{2}}, \\ldots, x_{i_{n-r}}\\) are the free variables.\n\n\n\nList the vectors \\(\\mathbf{w}_{1}, \\mathbf{w}_{2}, \\ldots, \\mathbf{w}_{n-r}\\). These form a basis of \\(\\mathcal{N}(A)\\)."
  },
  {
    "objectID": "lectures/ch3lecture1b.html#showing-that-the-standard-basis-is-a-basis-for-the-reals-or-complex-numbers",
    "href": "lectures/ch3lecture1b.html#showing-that-the-standard-basis-is-a-basis-for-the-reals-or-complex-numbers",
    "title": "Ch3 Lecture 1",
    "section": "Showing that the Standard Basis is a Basis for the reals or complex numbers",
    "text": "Showing that the Standard Basis is a Basis for the reals or complex numbers\n\nLet \\(\\mathbf{v}=\\left(c_{1}, c_{2}, \\ldots, c_{n}\\right)\\) be a vector from \\(V\\)\n\\(c_{1}, c_{2}, \\ldots, c_{n}\\) are scalars (real or complex)\n\n\n\\[\n\\begin{aligned}\n\\mathbf{v} & =\\left[\\begin{array}{c}\nc_{1} \\\\\nc_{2} \\\\\n\\vdots \\\\\nc_{n}\n\\end{array}\\right]=c_{1}\\left[\\begin{array}{c}\n1 \\\\\n0 \\\\\n\\vdots \\\\\n0\n\\end{array}\\right]+c_{2}\\left[\\begin{array}{c}\n0 \\\\\n1 \\\\\n\\vdots \\\\\n0\n\\end{array}\\right]+\\cdots+c_{n}\\left[\\begin{array}{c}\n0 \\\\\n\\vdots \\\\\n0 \\\\\n1\n\\end{array}\\right] \\\\\n& =c_{1} \\mathbf{e}_{1}+c_{2} \\mathbf{e}_{2}+\\cdots+c_{n} \\mathbf{e}_{n} .\n\\end{aligned}\n\\]\n\nTwo points here: 1. you can make any vector in V out of a linear combination of the e’s, so the e’s span V. 2. the e’s are linearly independent because if you have a linear combination of them that equals 0, then each coefficient must be 0. 3. Therefore, the e’s are a basis of V."
  },
  {
    "objectID": "lectures/ch3lecture1.html#section",
    "href": "lectures/ch3lecture1.html#section",
    "title": "Ch3 Lecture 1",
    "section": "",
    "text": "\\[\n\\begin{equation*}\ny_{i, t+1}=y_{i, t}+d\\left(y_{i-1, t}-2 y_{i, t}+y_{i+1, t}\\right)\n\\end{equation*}\n\\]\n\\(y_{0,t+1} = 0\\) \\(y_{1,t+1}=y_{1,t}+d\\left(0-2 y_{1,t}+y_{2,t}\\right)\\) \\(y_{3,t+1}=y_{3,t}+d\\left(y_{2,t}-2 y_{3,t}+y_{4,t}\\right)\\) … \\(y_{6,t+1}=0\\)\n\nWe want to solve these for \\(y_{i,t}\\) given \\(y_{i,t+1}\\).\n\\[\n\\begin{equation*}\ny_{i, t+1}-d\\left(y_{i-1, t}-2 y_{i, t}+y_{i+1, t}\\right)=y_{i, t}\n\\end{equation*}\n\\]\n\\[\n\\approx y_{i, t+1}-d\\left(y_{i-1, t+1}-2 y_{i, t+1}+y_{i+1, t+1}\\right) ?\n\\]"
  },
  {
    "objectID": "lectures/ch3lecture1.html#in-matrix-form",
    "href": "lectures/ch3lecture1.html#in-matrix-form",
    "title": "Ch3 Lecture 1",
    "section": "In matrix form",
    "text": "In matrix form\nIn matrix form, this becomes \\(y_{t+1}=y_{t}+ d My_{t}\\):\n\\[\nM=\\left[\\begin{array}{ccccccc}\n-2 & 1 & 0 & 0 & 0 & 0 & 0 \\\\\n1 & -2 & 1 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & -2 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & -2 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & -2 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 1 & -2 & 1 \\\\\n0 & 0 & 0 & 0 & 0 & 1 & -2\n\\end{array}\\right]\n\\]\n\n\\[\ny_{t}=y_{t+1}- d My_{t} \\overset{?}\\approx y_{t+1}- d My_{t+1}\n\\]\nTurns out it works… but why?"
  },
  {
    "objectID": "lectures/ch3lecture1.html#using-matrix-inversion",
    "href": "lectures/ch3lecture1.html#using-matrix-inversion",
    "title": "Ch3 Lecture 1",
    "section": "Using matrix inversion",
    "text": "Using matrix inversion\n\n\\(y_{t+1}=y_{t}+ dMy_{t}\\)\nis the same as \\(y_{t+1}=Iy_{t}+ dMy_{t}\\)\nin other words, \\(y_{t+1}=(I+dM)y_{t}\\)\n\n\nHow do we solve this for \\(y_{t}\\), given \\(y_{t+1}\\)?\n\n\n\\[\ny_{t+1}=(I+dM)y_{t} \\Rightarrow y_{t}=(I+dM)^{-1}y_{t+1}\n\\]\n\n\nTwo different approaches – one correct, the other one gave approximately the right answer. Why?"
  },
  {
    "objectID": "lectures/ch3lecture1.html#section-1",
    "href": "lectures/ch3lecture1.html#section-1",
    "title": "Ch3 Lecture 1",
    "section": "",
    "text": "\\[\ny_{t+1}=(I+dM)y_{t} \\Rightarrow y_{t}=(I+dM)^{-1}y_{t+1}\n\\]\n\\[\ny_{t}=y_{t+1}- dMy_{t} \\overset{?}\\approx y_{t+1}- d My_{t+1}\n\\]\n\nConnection: we will use math to show that the two are approximately equal.\n\n\nRemember Taylor series?\nFor small \\(x\\), we can approximate \\(\\frac{1}{1+x}\\) as \\(1-x+x^2-x^3+\\ldots\\)\n\n\nIf we have a matrix \\(A\\) such that \\(X = I + A\\) is invertible, then \\(X^{-1} = I - A + A^2 - A^3 + \\ldots\\)\n\n\nSo to first order in d,\n\\[\ny_{t}=(I+dM)^{-1}y_{t+1} \\approx \\left(I-dM\\right) y_{t+1}=y_{t+1}-d M y_{t+1}\n\\]\n\n\nNow \\(y_{t+1}=y_{t} + dM y_{t}\\). Plugging this in,\n\\(y_{t}=(I+dM)^{-1}y_{t+1} \\approx y_{t+1}-d M \\left(y_{t}+d M y_{t}\\right)\\approx y_{t+1}-d M y_{t}\\)"
  },
  {
    "objectID": "lectures/ch3lecture1.html#the-point",
    "href": "lectures/ch3lecture1.html#the-point",
    "title": "Ch3 Lecture 1",
    "section": "The point",
    "text": "The point\nThe problem was much easier to solve once we put it in matrix form."
  },
  {
    "objectID": "lectures/ch3lecture1.html#linear-independence",
    "href": "lectures/ch3lecture1.html#linear-independence",
    "title": "Ch3 Lecture 1",
    "section": "Linear Independence",
    "text": "Linear Independence\n\nThe vectors \\(\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\ldots, \\mathbf{v}_{n}\\) are linearly dependent if there exist scalars \\(c_{1}, c_{2}, \\ldots, c_{n}\\), not all zero, such that\n\\[\n\\begin{equation*}\nc_{1} \\mathbf{v}_{1}+c_{2} \\mathbf{v}_{2}+\\cdots+c_{n} \\mathbf{v}_{n}=\\mathbf{0}\n\\end{equation*}\n\\]\nOtherwise, the vectors are called linearly independent.\n\n\nThis doesn’t mean that one of the vectors is a scalar multiple of another one."
  },
  {
    "objectID": "lectures/ch3lecture1.html#checking-for-linear-independence",
    "href": "lectures/ch3lecture1.html#checking-for-linear-independence",
    "title": "Ch3 Lecture 1",
    "section": "Checking for Linear Independence",
    "text": "Checking for Linear Independence\nIs this set of vectors linearly independent?\n\\[\n\\left[\\begin{array}{l}1 \\\\ 1 \\\\ 0\\end{array}\\right],\\left[\\begin{array}{l}0 \\\\ 1 \\\\ 1\\end{array}\\right],\\left[\\begin{array}{r}1 \\\\ -1 \\\\ -2\\end{array}\\right]\n\\]\n\nGoal: find some set of scalars \\(c_{1}, c_{2}, c_{3}\\), not all zero, such that \\(c_{1} \\mathbf{v}_{1}+c_{2} \\mathbf{v}_{2}+c_{3} \\mathbf{v}_{3}=\\mathbf{0}\\).\n\n\nDefine: - matrix \\(A=\\left[\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\mathbf{v}_{3}\\right]\\) - vector \\(\\mathbf{c}=\\left(c_{1}, c_{2}, c_{3}\\right)\\).\n\n\nThen:\n\\[\nc_{1} \\mathbf{v}_{1}+c_{2} \\mathbf{v}_{2}+c_{3} \\mathbf{v}_{3}=\\left[\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\mathbf{v}_{3}\\right]\\left[\\begin{array}{l}\nc_{1} \\\\\nc_{2} \\\\\nc_{3}\n\\end{array}\\right]=A \\mathbf{c}\n\\]\nSo we just need to see if there is a nontrivial solution to \\(A \\mathbf{c}=\\mathbf{0}\\)."
  },
  {
    "objectID": "lectures/ch3lecture1.html#section-2",
    "href": "lectures/ch3lecture1.html#section-2",
    "title": "Ch3 Lecture 1",
    "section": "",
    "text": "Is this set of vectors linearly independent?\n\\[\n\\left[\\begin{array}{l}1 \\\\ 1 \\\\ 0\\end{array}\\right],\\left[\\begin{array}{l}0 \\\\ 1 \\\\ 1\\end{array}\\right],\\left[\\begin{array}{r}1 \\\\ -1 \\\\ -2\\end{array}\\right]\n\\]\nPut in matrix form and find the reduced row echelon form: . . .\n\\[\n\\left[\\begin{array}{rrr}\n1 & 0 & 1 \\\\\n1 & 1 & -1 \\\\\n0 & 1 & -2\n\\end{array}\\right] \\xrightarrow[E_{21}(-1)]{\\longrightarrow}\\left[\\begin{array}{rrr}\n1 & 0 & 1 \\\\\n0 & 1 & -2 \\\\\n0 & 1 & -2\n\\end{array}\\right] \\xrightarrow[E_{32}(-1)]{\\longrightarrow}\\left[\\begin{array}{rrr}\n1 & 0 & 1 \\\\\n0 & 1 & -2 \\\\\n0 & 0 & 0\n\\end{array}\\right],\n\\]\n\nSolutions to the homogeneous system are \\(\\mathbf{c}=\\left(c_{1}=-c_{3},c_{2}= 2 c_{3}, c_{3}= c_{3}\\right)=\\)\nWhole vector is \\(c_{3}(-1,2,1)\\). Take \\(c_{3}=1\\) and we have that \\[\n-1 \\mathbf{v}_{1}+2 \\mathbf{v}_{2}+1 \\mathbf{v}_{3}=0\n\\]\n\n\n\\[\n-1 \\mathbf{v}_{1}+2 \\mathbf{v}_{2}+1 \\mathbf{v}_{3}=0\n\\]\nSo the vectors are linearly dependent."
  },
  {
    "objectID": "lectures/ch3lecture1.html#example-2",
    "href": "lectures/ch3lecture1.html#example-2",
    "title": "Ch3 Lecture 1",
    "section": "Example 2",
    "text": "Example 2\nAre the following vectors linearly independent?\n\\(\\left[\\begin{array}{l}0 \\\\ 1 \\\\ 0\\end{array}\\right],\\left[\\begin{array}{l}1 \\\\ 1 \\\\ 0\\end{array}\\right],\\left[\\begin{array}{l}0 \\\\ 1 \\\\ 1\\end{array}\\right]\\)\n\nSet up A = \\(\\left[\\begin{array}{lll}0 & 1 & 0 \\\\ 1 & 1 & 1 \\\\ 0 & 0 & 1\\end{array}\\right]\\)\n\n\nDifferent approach: \\[\n\\operatorname{det}\\left[\\begin{array}{lll}\n0 & 1 & 0 \\\\\n1 & 1 & 1 \\\\\n0 & 0 & 1\n\\end{array}\\right]=-1 \\operatorname{det}\\left[\\begin{array}{ll}\n1 & 0 \\\\\n0 & 1\n\\end{array}\\right]=-1\n\\]\n\n\nDeterminant is non-zero, so \\(A\\) is invertible (non-singular)\n\n\nOnly solution to \\(A \\mathbf{c}=0\\) is \\(\\mathbf{c}=0\\).\n\n\nSo the vectors are linearly independent."
  },
  {
    "objectID": "lectures/ch3lecture1.html#summary",
    "href": "lectures/ch3lecture1.html#summary",
    "title": "Ch3 Lecture 1",
    "section": "Summary",
    "text": "Summary"
  },
  {
    "objectID": "lectures/ch3lecture1.html#basis-of-a-vector-space",
    "href": "lectures/ch3lecture1.html#basis-of-a-vector-space",
    "title": "Ch3 Lecture 1",
    "section": "Basis of a Vector Space",
    "text": "Basis of a Vector Space\n\nA spanning set is a set of vectors that can be combined to create any vector in the vector space.\n\n\nA basis for the vector space \\(V\\) is a spanning set of vectors \\(\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\ldots, \\mathbf{v}_{n}\\) that is a linearly independent set."
  },
  {
    "objectID": "lectures/ch3lecture1.html#standard-basis",
    "href": "lectures/ch3lecture1.html#standard-basis",
    "title": "Ch3 Lecture 1",
    "section": "Standard Basis",
    "text": "Standard Basis\n\nThe standard basis for \\(\\mathbb{R}^{n}\\) is the set of vectors \\(\\mathbf{e}_{1}, \\mathbf{e}_{2}, \\ldots, \\mathbf{e}_{n}\\) given by the columns of the \\(n \\times n\\) identity matrix."
  },
  {
    "objectID": "lectures/ch3lecture1.html#showing-that-the-standard-basis-is-a-basis-for-the-reals-or-complex-numbers",
    "href": "lectures/ch3lecture1.html#showing-that-the-standard-basis-is-a-basis-for-the-reals-or-complex-numbers",
    "title": "Ch3 Lecture 1",
    "section": "Showing that the Standard Basis is a Basis for the reals or complex numbers",
    "text": "Showing that the Standard Basis is a Basis for the reals or complex numbers\n\nLet \\(\\mathbf{v}=\\left(c_{1}, c_{2}, \\ldots, c_{n}\\right)\\) be a vector from \\(V\\)\n\\(c_{1}, c_{2}, \\ldots, c_{n}\\) are scalars (real or complex)\n\n\n\\[\n\\begin{aligned}\n\\mathbf{v} & =\\left[\\begin{array}{c}\nc_{1} \\\\\nc_{2} \\\\\n\\vdots \\\\\nc_{n}\n\\end{array}\\right]=c_{1}\\left[\\begin{array}{c}\n1 \\\\\n0 \\\\\n\\vdots \\\\\n0\n\\end{array}\\right]+c_{2}\\left[\\begin{array}{c}\n0 \\\\\n1 \\\\\n\\vdots \\\\\n0\n\\end{array}\\right]+\\cdots+c_{n}\\left[\\begin{array}{c}\n0 \\\\\n\\vdots \\\\\n0 \\\\\n1\n\\end{array}\\right] \\\\\n& =c_{1} \\mathbf{e}_{1}+c_{2} \\mathbf{e}_{2}+\\cdots+c_{n} \\mathbf{e}_{n} .\n\\end{aligned}\n\\]\n\nTwo points here: 1. you can make any vector in V out of a linear combination of the e’s, so the e’s span V. 2. the e’s are linearly independent because if you have a linear combination of them that equals 0, then each coefficient must be 0. 3. Therefore, the e’s are a basis of V."
  },
  {
    "objectID": "lectures/ch3lecture1.html#invertibility-basis-and-linear-independence",
    "href": "lectures/ch3lecture1.html#invertibility-basis-and-linear-independence",
    "title": "Ch3 Lecture 1",
    "section": "Invertibility, Basis, and Linear Independence",
    "text": "Invertibility, Basis, and Linear Independence\nAn \\(n \\times n\\) real matrix \\(A\\) is invertible if and only if its columns are linearly independent\n\nin which case they form a basis of \\(\\mathbb{R}^{n}\\)"
  },
  {
    "objectID": "lectures/ch3lecture1.html#column-and-row-spaces",
    "href": "lectures/ch3lecture1.html#column-and-row-spaces",
    "title": "Ch3 Lecture 1",
    "section": "Column and Row Spaces",
    "text": "Column and Row Spaces\n\nThe column space of the \\(m \\times n\\) matrix \\(A\\) is the subspace \\(\\mathcal{C}(A)\\) of \\(\\mathbb{R}^{m}\\) spanned by the columns of \\(A\\).\n\n\n\nThe row space of the \\(m \\times n\\) matrix \\(A\\) is the subspace \\(\\mathcal{R}(A)\\) of \\(\\mathbb{R}^{n}\\) spanned by the transposes of the rows of \\(A\\)"
  },
  {
    "objectID": "lectures/ch3lecture1.html#null-space",
    "href": "lectures/ch3lecture1.html#null-space",
    "title": "Ch3 Lecture 1",
    "section": "Null Space",
    "text": "Null Space\n\nThe null space of the \\(m \\times n\\) matrix \\(A\\) is the subset \\(\\mathcal{N}(A)\\) of \\(\\mathbb{R}^{n}\\)\n\\[\n\\mathcal{N}(A)=\\left\\{\\mathbf{x} \\in \\mathbb{R}^{n} \\mid A \\mathbf{x}=\\mathbf{0}\\right\\}\n\\]\n\n\n\\(\\mathcal{N}(A)\\) is just the solution set to \\(A \\mathbf{x}=\\mathbf{0}\\)\n\n\nFor example, if \\(A\\) is invertible, \\(A \\mathbf{x}=\\mathbf{0}\\) has only the trivial solution \\(\\mathbf{x}=\\mathbf{0}\\)\n\n\nso \\(\\mathcal{N}(A)\\) is just \\(\\left\\{\\mathbf{0}\\right\\}\\).\n\n\n\\(A\\) is invertible exactly if \\(\\mathcal{N}(A)=\\{\\mathbf{0}\\}\\)"
  },
  {
    "objectID": "lectures/ch3lecture1.html#kernel-of-an-operator",
    "href": "lectures/ch3lecture1.html#kernel-of-an-operator",
    "title": "Ch3 Lecture 1",
    "section": "Kernel of an Operator",
    "text": "Kernel of an Operator\n\nThe kernel of the linear operator \\(T\\) : \\(V \\rightarrow W\\) is the subspace of \\(V\\) defined by\n\\[\n\\operatorname{ker}(T)=\\{\\mathbf{x} \\in V \\mid T(\\mathbf{x})=\\mathbf{0}\\}\n\\]\n\nFor matrix operators kernels are the same thing as null spaces."
  },
  {
    "objectID": "lectures/ch3lecture1.html#example",
    "href": "lectures/ch3lecture1.html#example",
    "title": "Ch3 Lecture 1",
    "section": "Example",
    "text": "Example\nFind the null space of the matrix\n\\[\nA=\\left[\\begin{array}{rrrr}\n1 & 1 & 1 & -1 \\\\\n0 & 1 & 2 & 1\n\\end{array}\\right]\n\\]\ncorresponding to the system of equations \\[\n\\begin{aligned}\nx_{1}+x_{2}+x_{3}-x_{4} & =0 \\\\\nx_{2}+2 x_{3}+x_{4} & =0\n\\end{aligned}\n\\]\n\n\nFind the RREF of \\(A\\):\n\n\n\n\n\n\n\nPivots are in the first and second columns, so it follows that \\(x_{3}\\) and \\(x_{4}\\) are free, \\(x_{1}\\) and \\(x_{2}\\) are bound\n\n\\[\n\\begin{aligned}\n& x_{1}=x_{3}+2 x_{4} \\\\\n& x_{2}=-2 x_{3}-x_{4} .\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lectures/ch3lecture1.html#section-3",
    "href": "lectures/ch3lecture1.html#section-3",
    "title": "Ch3 Lecture 1",
    "section": "",
    "text": "\\[\n\\left[\\begin{array}{c}\nx_{1} \\\\\nx_{2} \\\\\nx_{3} \\\\\nx_{4}\n\\end{array}\\right]=\\left[\\begin{array}{c}\nx_{3}+2 x_{4} \\\\\n-2 x_{3}-x_{4} \\\\\nx_{3} \\\\\nx_{4}\n\\end{array}\\right]=x_{3}\\left[\\begin{array}{r}\n1 \\\\\n-2 \\\\\n1 \\\\\n0\n\\end{array}\\right]+x_{4}\\left[\\begin{array}{r}\n2 \\\\\n-1 \\\\\n0 \\\\\n1\n\\end{array}\\right] .\n\\]\n\nThe general solution to the homogeneous system is an arbitrary linear combination of these two vectors\n\n\n\\[\n\\mathcal{N}(A)=\\operatorname{span}\\left\\{\\left[\\begin{array}{r}\n1 \\\\\n-2 \\\\\n1 \\\\\n0\n\\end{array}\\right],\\left[\\begin{array}{r}\n2 \\\\\n-1 \\\\\n0 \\\\\n1\n\\end{array}\\right]\\right\\} \\subseteq \\mathbb{R}^{4}\n\\]\n\n\nThese are independent, so they form a basis for \\(\\mathcal{N}(A)\\)"
  },
  {
    "objectID": "lectures/ch3lecture1.html#using-the-null-space",
    "href": "lectures/ch3lecture1.html#using-the-null-space",
    "title": "Ch3 Lecture 1",
    "section": "Using the Null Space",
    "text": "Using the Null Space\nSuppose that a Markov chain has an stable transition matrix \\(A=\\left[\\begin{array}{ll}0.7 & 0.4 \\\\ 0.3 & 0.6\\end{array}\\right]\\), so that\n\\[\n\\mathbf{x}^{(k+1)}=A \\mathbf{x}^{(k)}\n\\]\n\nWhat is the steady-state vector \\(\\mathbf{x}\\)? (The limit as \\(k \\rightarrow \\infty\\) of \\(\\mathbf{x}^{(k)}\\))\n\n\nTake limit of both sides:\n\\[\n\\mathbf{x}=A \\mathbf{x}\n\\]\n\n\\[\n\\mathbf{0}=\\mathbf{x}-A \\mathbf{x}=I \\mathbf{x}-A \\mathbf{x}=(I-A) \\mathbf{x}\n\\]\nSo \\(\\mathbf{x}\\) is in the null space of \\(I-A\\).\n\n\n\n\\[\n\\mathbf{0}=\\mathbf{x}-A \\mathbf{x}=I \\mathbf{x}-A \\mathbf{x}=(I-A) \\mathbf{x}\n\\]\nSo \\(\\mathbf{x}\\) is in the null space of \\(I-A\\)."
  },
  {
    "objectID": "lectures/ch3lecture1.html#section-4",
    "href": "lectures/ch3lecture1.html#section-4",
    "title": "Ch3 Lecture 1",
    "section": "",
    "text": "\\[\nI-A=\\left[\\begin{array}{ll}\n1 & 0 \\\\\n0 & 1\n\\end{array}\\right]-\\left[\\begin{array}{ll}\n0.7 & 0.4 \\\\\n0.3 & 0.6\n\\end{array}\\right]=\\left[\\begin{array}{rr}\n0.3 & -0.4 \\\\\n-0.3 & 0.4\n\\end{array}\\right] \\text {. }\n\\]\n\n\\[\n\\left[\\begin{array}{rr}\n0.3 & -0.4 \\\\\n-0.3 & 0.4\n\\end{array}\\right] \\xrightarrow{E_{21}(1)}\\left[\\begin{array}{rr}\n1 & -4 / 3 \\\\\n0 & 0\n\\end{array}\\right]\n\\]\n\nnull space of \\(I-A\\) is spanned by the single vector \\((4 / 3,1)\\). In particular, any multiple of this vector qualifies as a possible limiting vector.\nIf we want a limiting vector whose entries are nonnegative and sum to 1 (which is required for states in a Markov chain), then the only choice is the vector resulting from dividing \\((4 / 3,1)\\) by the sum of its coordinates to obtain\n\nSolution set is \\(x_{1}=4 x_{2} / 3\\)\n\n\n\\[\n(3 / 7)(4 / 3,1)=(4 / 7,3 / 7) \\approx(0.57143,0.42857)\n\\]"
  },
  {
    "objectID": "lectures/ch3lecture1.html#using-the-null-space-to-find-a-basis",
    "href": "lectures/ch3lecture1.html#using-the-null-space-to-find-a-basis",
    "title": "Ch3 Lecture 1",
    "section": "Using the Null Space to find a basis",
    "text": "Using the Null Space to find a basis\nFind a basis for the column space of the matrix\n\\[\nA=\\left[\\begin{array}{rrrr}\n1 & 1 & 1 & -1 \\\\\n0 & 1 & 2 & 1\n\\end{array}\\right]\n\\]\n\nFrom before, we found the RREF and used this to find a general form of the solutions to \\(A \\mathbf{x}=\\mathbf{0}\\)\n\\(\\mathbf{x}=\\left(x_{1}, x_{2}, x_{3}, x_{4}\\right)=\\) \\(\\left(x_{3}+2 x_{4},-2 x_{3}-x_{4}, x_{3}, x_{4}\\right)\\)\n\n\nWrite \\(A=\\left[\\mathbf{a}_{1}, \\mathbf{a}_{2}, \\mathbf{a}_{3}, \\mathbf{a}_{4}\\right]\\)\n\n\n\\(\\mathbf{0}=x_{1} \\mathbf{a}_{1}+x_{2} \\mathbf{a}_{2}+x_{3} \\mathbf{a}_{3}+x_{4} \\mathbf{a}_{4}\\)\n\n\n\\(\\mathbf{0}=\\left(x_{3}+2 x_{4}\\right) \\mathbf{a}_{1}-\\left(2 x_{3}+x_{4}\\right) \\mathbf{a}_{2}+x_{3} \\mathbf{a}_{3}+x_{4} \\mathbf{a}_{4}\\)\n\n\nTake \\(x_{3}=1\\) and \\(x_{4}=0\\). Then \\(\\mathbf{0}=\\mathbf{a}_{1}-2 \\mathbf{a}_{2}+\\mathbf{a}_{3}\\)\n\n\nSo \\(\\mathbf{a}_{3}\\) is a linear combination of \\(\\mathbf{a}_{1}\\) and \\(\\mathbf{a}_{2}\\)\n\n\nSimilarly, take \\(x_{3}=0\\) and \\(x_{4}=1\\). Then \\(\\mathbf{0}=2 \\mathbf{a}_{1}-\\mathbf{a}_{2}+\\mathbf{a}_{4}\\) so \\(\\mathbf{a}_{4}\\) is a linear combination of \\(\\mathbf{a}_{1}\\) and \\(\\mathbf{a}_{2}\\)\n\n\nSo \\(\\mathbf{a}_{1}\\) and \\(\\mathbf{a}_{2}\\) are a basis for the column space of \\(A\\)\n\\(\\mathcal{C}(A)=\\operatorname{span}\\left\\{\\mathbf{a}_{1}, \\mathbf{a}_{2}\\right\\}=\\operatorname{span}\\left\\{\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right],\\left[\\begin{array}{l}1 \\\\ 1\\end{array}\\right]\\right\\}\\)"
  },
  {
    "objectID": "lectures/ch3lecture1.html#consistency-and-column-space-back-to-linear-systems",
    "href": "lectures/ch3lecture1.html#consistency-and-column-space-back-to-linear-systems",
    "title": "Ch3 Lecture 1",
    "section": "Consistency and Column Space: Back to Linear Systems",
    "text": "Consistency and Column Space: Back to Linear Systems\n\nThe linear system \\(A \\mathbf{x}=\\mathbf{b}\\) of \\(m\\) equations in \\(n\\) unknowns is consistent if and only if \\(\\mathbf{b} \\in \\mathcal{C}(A)\\).\n\n\n\\(A \\mathbf{x}\\) is a linear combination of the columns of \\(A\\) with the entries of \\(\\mathbf{x}\\) as scalar coefficients\nIf \\(A \\mathbf{x}=\\mathbf{b}\\) has a solution means some linear combination of columns of \\(A\\) adds up to \\(\\mathbf{b}\\)\n\\(\\mathbf{b} \\in \\mathcal{C}(A)\\)"
  },
  {
    "objectID": "lectures/ch3lecture1.html#how-to-tell-if-a-vector-is-in-a-spanning-space",
    "href": "lectures/ch3lecture1.html#how-to-tell-if-a-vector-is-in-a-spanning-space",
    "title": "Ch3 Lecture 1",
    "section": "How to tell if a vector is in a spanning space",
    "text": "How to tell if a vector is in a spanning space\nSuppose we have a space \\(V\\) spanned by \\(\\mathbf{v}_{1}=(1,1,3,3), \\mathbf{v}_{2}=(0,2,2,4)\\), and and \\(\\mathbf{v}_{3}=(1,0,2,1)\\)\n\nOne of these vectors is in \\(V\\). Which one?\n\\(\\mathbf{u}=(2,1,5,4)\\) and \\(\\mathbf{w}=(1,0,0,0)\\)\n\n\nDefine \\(A=\\left[\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\mathbf{v}_{3}\\right]\\)\nThen \\(\\mathbf{u} \\in \\mathcal{C}(A)\\) if and only if \\(A \\mathbf{x}=\\mathbf{u}\\) has a solution (is consistent)\n\n\nSolve both at once: \\[\n[A|\\mathbf{u}| \\mathbf{w}]=\\left[\\begin{array}{lllll}\n1 & 0 & 1 & 2 & 1 \\\\\n1 & 2 & 0 & 1 & 0 \\\\\n3 & 2 & 2 & 5 & 0 \\\\\n3 & 4 & 1 & 4 & 0\n\\end{array}\\right] \\text { with reduced row echelon form }\\left[\\begin{array}{rrrrr}\n1 & 0 & 1 & 2 & 0 \\\\\n0 & 1 & -\\frac{1}{2} & -\\frac{1}{2} & 0 \\\\\n0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 & 0\n\\end{array}\\right]\n\\]\n\nObserve that there is a pivot in the fifth column but not in the fourth column. This tells us that the system with augmented matrix \\([A \\mid \\mathbf{u}]\\) is consistent, but the system with augmented matrix \\([A \\mid \\mathbf{w}]\\) is not consistent. Therefore, \\(\\mathbf{u} \\in \\operatorname{span}\\left\\{\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\mathbf{v}_{3}\\right\\}\\), but \\(\\mathbf{w} \\notin \\operatorname{span}\\left\\{\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\mathbf{v}_{3}\\right\\}\\).\n\n\n\n\\(\\mathbf{u} \\in \\operatorname{span}\\left\\{\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\mathbf{v}_{3}\\right\\}\\),\nbut \\(\\mathbf{w} \\notin \\operatorname{span}\\left\\{\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\mathbf{v}_{3}\\right\\}\\)\n\n\n\\[\n\\mathbf{u}=\\left(2-c_{3}\\right) \\mathbf{v}_{1}+\\frac{1}{2}\\left(c_{3}-1\\right) \\mathbf{v}_{2}+c_{3} \\mathbf{v}_{3}\n\\]"
  },
  {
    "objectID": "lectures/ch3lecture1.html#form-of-general-solution-to-a-linear-system",
    "href": "lectures/ch3lecture1.html#form-of-general-solution-to-a-linear-system",
    "title": "Ch3 Lecture 1",
    "section": "Form of general solution to a linear system",
    "text": "Form of general solution to a linear system\n\n\nSuppose the system \\(A \\mathbf{x}=\\mathbf{b}\\) has a particular solution \\(\\mathbf{x}_{*}\\).\nThen the general solution \\(\\mathbf{x}\\) to this system can be described by the equation\n\n\n\\[\n\\mathbf{x}=\\mathbf{x}_{*}+\\mathbf{z}\n\\]\nwhere \\(\\mathbf{z}\\) runs over all elements of \\(\\mathcal{N}(A)\\).\n\n\n\nSolution set is the set of all translates of elements in the null space of \\(A\\) by some fixed vector.\n\n\nWhen n is 2 or 3 this says that the solution set is either a single point, a line or a plane—not necessarily through the origin!"
  },
  {
    "objectID": "lectures/ch3lecture1.html#example-1",
    "href": "lectures/ch3lecture1.html#example-1",
    "title": "Ch3 Lecture 1",
    "section": "Example",
    "text": "Example\nDescribe the solution sets to the system \\[\n\\begin{array}{r}\nx+2 y=3 \\\\\nx+y+t=3 .\n\\end{array}\n\\]\n\n\n\n\n\\[\n\\begin{aligned}\n& x=3-2 t \\\\\n& y=t \\\\\n& z=t .\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lectures/ch3lecture1.html#null-space-algorithm",
    "href": "lectures/ch3lecture1.html#null-space-algorithm",
    "title": "Ch3 Lecture 1",
    "section": "Null Space algorithm",
    "text": "Null Space algorithm\nGiven an \\(m \\times n\\) matrix \\(A\\).\n\nCompute the reduced row echelon form \\(R\\) of \\(A\\).\nUse \\(R\\) to find the general solution to the homogeneous system \\(A \\mathbf{x}=0\\).\n\n\n\nWrite the general solution \\(\\mathbf{x}=\\left(x_{1}, x_{2}, \\ldots, x_{n}\\right)\\) to the homogeneous system in the form\n\n\\[\n\\mathbf{x}=x_{i_{1}} \\mathbf{w}_{1}+x_{i_{2}} \\mathbf{w}_{2}+\\cdots+x_{i_{n-r}} \\mathbf{w}_{n-r}\n\\]\nwhere \\(x_{i_{1}}, x_{i_{2}}, \\ldots, x_{i_{n-r}}\\) are the free variables.\n\n\n\nList the vectors \\(\\mathbf{w}_{1}, \\mathbf{w}_{2}, \\ldots, \\mathbf{w}_{n-r}\\). These form a basis of \\(\\mathcal{N}(A)\\)."
  },
  {
    "objectID": "lectures/test.html#a-slide",
    "href": "lectures/test.html#a-slide",
    "title": "Ch5 Lecture 4",
    "section": "A slide",
    "text": "A slide\n\n\n['/Library/Frameworks/Python.framework/Versions/3.8/lib/python38.zip', '/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8', '/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload', '', '/Users/kendra/Library/Python/3.8/lib/python/site-packages', '/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages']"
  },
  {
    "objectID": "lectures/ch4lecture1.html#linear-systems-revisted",
    "href": "lectures/ch4lecture1.html#linear-systems-revisted",
    "title": "Stat 24320",
    "section": "Linear systems revisted",
    "text": "Linear systems revisted"
  },
  {
    "objectID": "lectures/ch2_lecture5.html#block-multiplication-1",
    "href": "lectures/ch2_lecture5.html#block-multiplication-1",
    "title": "Ch2 Lecture 5",
    "section": "Block Multiplication",
    "text": "Block Multiplication\n\nSuppose we need to multiply these matrices…\n\\[\n\\left[\\begin{array}{llll}\n1 & 2 & 0 & 0 \\\\\n3 & 4 & 0 & 0 \\\\\n0 & 0 & 1 & 0\n\\end{array}\\right]\\left[\\begin{array}{llll}\n0 & 0 & 2 & 1 \\\\\n0 & 0 & 1 & 1 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right] .\n\\]\npause\n\nIt’s a pain, right?\nDraw out the blocks on the left and right matrices.\n\\[\nA=\\left[\\begin{array}{ll}\n1 & 2 \\\\\n3 & 4\n\\end{array}\\right], \\quad B=\\left[\\begin{array}{ll}\n1 & 0\n\\end{array}\\right], \\quad C=\\left[\\begin{array}{ll}\n2 & 1 \\\\\n-1 & 1\n\\end{array}\\right], \\quad I_{2}=\\left[\\begin{array}{ll}\n1 & 0 \\\\\n0 & 1\n\\end{array}\\right]\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture5.html#column-vectors-as-blocks",
    "href": "lectures/ch2_lecture5.html#column-vectors-as-blocks",
    "title": "Ch2 Lecture 5",
    "section": "Column Vectors as Blocks",
    "text": "Column Vectors as Blocks\n\\[\nA \\mathbf{x}=\\left[\\mathbf{a}_{1}, \\mathbf{a}_{2}, \\mathbf{a}_{3}\\right]\\left[\\begin{array}{c}\nx_{1} \\\\\nx_{2} \\\\\nx_{3}\n\\end{array}\\right]=\\mathbf{a}_{1} x_{1}+\\mathbf{a}_{2} x_{2}+\\mathbf{a}_{3} x_{3}\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture5.html#transpose-and-conjugate-transpose-1",
    "href": "lectures/ch2_lecture5.html#transpose-and-conjugate-transpose-1",
    "title": "Ch2 Lecture 5",
    "section": "Transpose and Conjugate Transpose",
    "text": "Transpose and Conjugate Transpose\n\n\n\nLet \\(A=\\left[a_{i j}\\right]\\) be an \\(m \\times n\\) matrix with (possibly) complex entries.\nThe transpose of \\(A\\) is the \\(n \\times m\\) matrix \\(A^{T}\\) obtained by interchanging the rows and columns of \\(A\\)\nThe conjugate of \\(A\\) is the matrix \\(\\bar{A}=\\left[\\overline{a_{i j}}\\right]\\)\nFinally, the conjugate (Hermitian) transpose of \\(A\\) is the matrix \\(A^{*}=\\bar{A}^{T}\\)."
  },
  {
    "objectID": "lectures/ch2_lecture5.html#example-1",
    "href": "lectures/ch2_lecture5.html#example-1",
    "title": "Ch2 Lecture 5",
    "section": "Example 1",
    "text": "Example 1\nFind the transpose and conjugate transpose of:\n\\[\n\\left[\\begin{array}{lll}1 & 0 & 2 \\\\ 0 & 1 & 1\\end{array}\\right]\n\\]\n\n\\[\n\\left[\\begin{array}{lll}\n1 & 0 & 2 \\\\\n0 & 1 & 1\n\\end{array}\\right]^{*}=\\left[\\begin{array}{lll}\n1 & 0 & 2 \\\\\n0 & 1 & 1\n\\end{array}\\right]^{T}=\\left[\\begin{array}{ll}\n1 & 0 \\\\\n0 & 1 \\\\\n2 & 1\n\\end{array}\\right]\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture5.html#example-2",
    "href": "lectures/ch2_lecture5.html#example-2",
    "title": "Ch2 Lecture 5",
    "section": "Example 2",
    "text": "Example 2\nFind the transpose and conjugate transpose of:\n\\[\n\\left[\\begin{array}{rr}1 & 1+\\mathrm{i} \\\\ 0 & 2 \\mathrm{i}\\end{array}\\right]\n\\]\n\n\\[\n\\left[\\begin{array}{rr}\n1 & 1+\\mathrm{i} \\\\\n0 & 2 \\mathrm{i}\n\\end{array}\\right]^{*}=\\left[\\begin{array}{rr}\n1 & 0 \\\\\n1-\\mathrm{i} & -2 \\mathrm{i}\n\\end{array}\\right], \\quad\\left[\\begin{array}{rr}\n1 & 1+\\mathrm{i} \\\\\n0 & 2 \\mathrm{i}\n\\end{array}\\right]^{T}=\\left[\\begin{array}{rr}\n1 & 0 \\\\\n1+\\mathrm{i} & 2 \\mathrm{i}\n\\end{array}\\right]\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture5.html#laws-of-matrix-transpose",
    "href": "lectures/ch2_lecture5.html#laws-of-matrix-transpose",
    "title": "Ch2 Lecture 5",
    "section": "Laws of Matrix Transpose",
    "text": "Laws of Matrix Transpose\nLet \\(A\\) and \\(B\\) be matrices of the appropriate sizes so that the following operations make sense, and \\(c\\) a scalar.\n\n\\((A+B)^{T}=A^{T}+B^{T}\\)\n\\((A B)^{T}=B^{T} A^{T}\\)\n\\((c A)^{T}=c A^{T}\\)\n\\(\\left(A^{T}\\right)^{T}=A\\)"
  },
  {
    "objectID": "lectures/ch2_lecture5.html#symmetric-and-hermetian-matrices",
    "href": "lectures/ch2_lecture5.html#symmetric-and-hermetian-matrices",
    "title": "Ch2 Lecture 5",
    "section": "Symmetric and Hermetian Matrices",
    "text": "Symmetric and Hermetian Matrices\n\nThe matrix \\(A\\) is said to be:\n\nsymmetric if \\(A^{T}=A\\)\nHermitian if \\(A^{*}=A\\)\n\n\n\n\nIs this matrix symmetric? Hermetian?\n\\(\\left[\\begin{array}{rr}1 & 1+\\mathrm{i} \\\\ 1-\\mathrm{i} & 2\\end{array}\\right]\\)\n\nIt’s Hermetian, but not symmetric.\n\\[\n\\left[\\begin{array}{rr}\n1 & 1+\\mathrm{i} \\\\\n1-\\mathrm{i} & 2\n\\end{array}\\right]^{*}=\\left[\\begin{array}{rr}\n1 & \\overline{1+\\mathrm{i}} \\\\\n\\overline{1-\\mathrm{i}} & 2\n\\end{array}\\right]^{T}=\\left[\\begin{array}{rr}\n1 & 1+\\mathrm{i} \\\\\n1-\\mathrm{i} & 2\n\\end{array}\\right]\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture5.html#inner-product",
    "href": "lectures/ch2_lecture5.html#inner-product",
    "title": "Ch2 Lecture 5",
    "section": "Inner product",
    "text": "Inner product\n\n\nLet \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) be column vectors of the same size, say \\(n \\times 1\\).\nThen the inner product of \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) is the scalar quantity \\(\\mathbf{u}^{T} \\mathbf{v}\\)\n\n\n\n\nFind the inner product of \\[\n\\mathbf{u}=\\left[\\begin{array}{r}\n2 \\\\\n-1 \\\\\n1\n\\end{array}\\right] \\text { and } \\mathbf{v}=\\left[\\begin{array}{l}\n3 \\\\\n4 \\\\\n1\n\\end{array}\\right]\n\\]\n\n\\[\n\\mathbf{u}^{T} \\mathbf{v}=[2,-1,1]\\left[\\begin{array}{l}\n3 \\\\\n4 \\\\\n1\n\\end{array}\\right]=2 \\cdot 3+(-1) 4+1 \\cdot 1=3\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture5.html#outer-product",
    "href": "lectures/ch2_lecture5.html#outer-product",
    "title": "Ch2 Lecture 5",
    "section": "Outer product",
    "text": "Outer product\n\n\nThe outer product of \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) is the \\(n \\times n\\) matrix \\(\\mathbf{u v}^{T}\\).\n\n\n\n\nFind the outer product of\n\\[\n\\mathbf{u}=\\left[\\begin{array}{r}\n2 \\\\\n-1 \\\\\n1\n\\end{array}\\right] \\text { and } \\mathbf{v}=\\left[\\begin{array}{l}\n3 \\\\\n4 \\\\\n1\n\\end{array}\\right]\n\\]\n\n\\[\n\\mathbf{u v}^{T}=\\left[\\begin{array}{r}\n2 \\\\\n-1 \\\\\n1\n\\end{array}\\right][3,4,1]=\\left[\\begin{array}{rrr}\n2 \\cdot 3 & 2 \\cdot 4 & 2 \\cdot 1 \\\\\n-1 \\cdot 3 & -1 \\cdot 4 & -1 \\cdot 1 \\\\\n1 \\cdot 3 & 1 \\cdot 4 & 1 \\cdot 1\n\\end{array}\\right]=\\left[\\begin{array}{rrr}\n6 & 8 & 2 \\\\\n-3 & -4 & -1 \\\\\n3 & 4 & 1\n\\end{array}\\right]\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture5.html#section",
    "href": "lectures/ch2_lecture5.html#section",
    "title": "Ch2 Lecture 5",
    "section": "",
    "text": "The determinant of a square \\(n \\times n\\) matrix \\(A=\\left[a_{i j}\\right]\\), \\(\\operatorname{det} A\\), is defined recursively:\nIf \\(n=1\\) then \\(\\operatorname{det} A=a_{11}\\);\n\notherwise,\n\nsuppose we have determinents for all square matrices of size less than \\(n\\)\nDefine \\(M_{i j}(A)\\) as the determinant of the \\((n-1) \\times(n-1)\\) matrix obtained from \\(A\\) by deleting the \\(i\\) th row and \\(j\\) th column of \\(A\\)\n\n\n\nthen\n\\[\n\\begin{aligned}\n\\operatorname{det} A & =\\sum_{k=1}^{n} a_{k 1}(-1)^{k+1} M_{k 1}(A) \\\\\n& =a_{11} M_{11}(A)-a_{21} M_{21}(A)+\\cdots+(-1)^{n+1} a_{n 1} M_{n 1}(A)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture5.html#laws-of-determinants",
    "href": "lectures/ch2_lecture5.html#laws-of-determinants",
    "title": "Ch2 Lecture 5",
    "section": "Laws of Determinants",
    "text": "Laws of Determinants\nDeterminant of an upper-triangular matrix: \\[\n\\begin{aligned}\n\\operatorname{det} A & =\\left|\\begin{array}{cccc}\na_{11} & a_{12} & \\cdots & a_{1 n} \\\\\n0 & a_{22} & \\cdots & a_{2 n} \\\\\n\\vdots & \\vdots & & \\vdots \\\\\n0 & 0 & \\cdots & a_{n n}\n\\end{array}\\right|=a_{11}\\left|\\begin{array}{cccc}\na_{22} & a_{23} & \\cdots & a_{2 n} \\\\\n0 & a_{33} & \\cdots & a_{3 n} \\\\\n\\vdots & \\vdots & & \\vdots \\\\\n0 & 0 & \\cdots & a_{n n}\n\\end{array}\\right| \\\\\n& =\\cdots=a_{11} \\cdot a_{22} \\cdots a_{n n} .\n\\end{aligned}\n\\]\n\nD1: If \\(A\\) is an upper triangular matrix, then the determinant of \\(A\\) is the product of all the diagonal elements of \\(A\\)."
  },
  {
    "objectID": "lectures/ch2_lecture5.html#more-laws-of-determinants",
    "href": "lectures/ch2_lecture5.html#more-laws-of-determinants",
    "title": "Ch2 Lecture 5",
    "section": "More Laws of Determinants",
    "text": "More Laws of Determinants\n\nD2: If \\(B\\) is obtained from \\(A\\) by multiplying one row of \\(A\\) by the scalar \\(c\\), then \\(\\operatorname{det} B=c \\cdot \\operatorname{det} A\\).\nD3: If \\(B\\) is obtained from \\(A\\) by interchanging two rows of \\(A\\), then \\(\\operatorname{det} B=\\) \\(-\\operatorname{det} A\\).\nD4: If \\(B\\) is obtained from \\(A\\) by adding a multiple of one row of \\(A\\) to another row of \\(A\\), then \\(\\operatorname{det} B=\\operatorname{det} A\\).\n\nNote: this means that the determinant of a matrix is unchanged by elementary row operations."
  },
  {
    "objectID": "lectures/ch2_lecture5.html#determinant-laws-in-terms-of-elementary-matrices",
    "href": "lectures/ch2_lecture5.html#determinant-laws-in-terms-of-elementary-matrices",
    "title": "Ch2 Lecture 5",
    "section": "Determinant Laws in terms of Elementary Matrices",
    "text": "Determinant Laws in terms of Elementary Matrices\n\nD2: \\(\\operatorname{det}\\left(E_{i}(c) A\\right)=c \\cdot \\operatorname{det} A\\) (remember that for \\(E_{i}(c)\\) to be an elementary matrix, \\(c \\neq 0\\) ).\nD3: \\(\\operatorname{det}\\left(E_{i j} A\\right)=-\\operatorname{det} A\\).\nD4: \\(\\operatorname{det}\\left(E_{i j}(s) A\\right)=\\operatorname{det} A\\)."
  },
  {
    "objectID": "lectures/ch2_lecture5.html#determinant-of-row-echelon-form",
    "href": "lectures/ch2_lecture5.html#determinant-of-row-echelon-form",
    "title": "Ch2 Lecture 5",
    "section": "Determinant of Row Echelon Form",
    "text": "Determinant of Row Echelon Form\nLet R be the reduced row echelon form of A, obtained through multiplication by elementary matrices:\n\\[\nR=E_{1} E_{2} \\cdots E_{k} A .\n\\]\n\nDeterminant of both sides:\n\\[\n\\operatorname{det} R=\\operatorname{det}\\left(E_{1} E_{2} \\cdots E_{k} A\\right)= \\pm(\\text { nonzero constant }) \\cdot \\operatorname{det} A \\text {. }\n\\]\n\n\nTherefore, \\(\\operatorname{det} A=0\\) precisely when \\(\\operatorname{det} R=0\\).\n\n\n\n\\(R\\) is upper triangular, so \\(\\operatorname{det} R\\) is the product of the diagonal entries of \\(R\\).\nIf \\(\\operatorname{rank} A&lt;n\\), then there will be zeros in some of the diagonal entries, so \\(\\operatorname{det} R=0\\).\nIf \\(\\operatorname{rank} A=n\\), the diagonal entries are all 1, so \\(\\operatorname{det} R=1\\).\n\nA square matrix with rank \\(n\\) is invertible\n\n\n\n\nTherefore,\nD5: The matrix \\(A\\) is invertible if and only if \\(\\operatorname{det} A \\neq 0\\)."
  },
  {
    "objectID": "lectures/ch2_lecture5.html#two-more-determinant-laws",
    "href": "lectures/ch2_lecture5.html#two-more-determinant-laws",
    "title": "Ch2 Lecture 5",
    "section": "Two more Determinant Laws",
    "text": "Two more Determinant Laws\nD6: Given matrices \\(A, B\\) of the same size,\n\\[\n\\operatorname{det} A B=\\operatorname{det} A \\operatorname{det} B \\text {. }\n\\]\n\n(but beware, \\(\\operatorname{det} A+\\operatorname{det} B \\neq \\operatorname{det}(A+B)\\))\nD7: For all square matrices \\(A\\), \\(\\operatorname{det} A^{T}=\\operatorname{det} A\\)"
  },
  {
    "objectID": "lectures/ch2_lecture5.html#easier-way-to-calculate-determinant",
    "href": "lectures/ch2_lecture5.html#easier-way-to-calculate-determinant",
    "title": "Ch2 Lecture 5",
    "section": "Easier way to calculate determinant",
    "text": "Easier way to calculate determinant\nJust use the det function in Python! (That was copilot’s autocomplete…)\n\nUse elementary row operations to get the matrix into upper triangular form, then multiply the diagonal entries."
  },
  {
    "objectID": "lectures/ch2_lecture5.html#an-inverse-formula",
    "href": "lectures/ch2_lecture5.html#an-inverse-formula",
    "title": "Ch2 Lecture 5",
    "section": "An Inverse Formula",
    "text": "An Inverse Formula\n\nLet \\(A=\\left[a_{i j}\\right]\\) be an \\(n \\times n\\) matrix. We have already seen that we can expand the determinant of \\(A\\) down any column of \\(A\\) (see the discussion following Example 2.59). These expansions lead to cofactor formulas for each column number \\(j\\) :\n\\[\n\\operatorname{det} A=\\sum_{k=1}^{n} a_{k j} A_{k j}=\\sum_{k=1}^{n} A_{k j} a_{k j}\n\\]\nThis formula resembles a matrix multiplication formula. Consider the slightly altered sum\n\\[\n\\sum_{k=1}^{n} A_{k i} a_{k j}=A_{1 i} a_{1 j}+A_{2 i} a_{2 j}+\\cdots+A_{n i} a_{n j}\n\\]\nThis is exactly what we would get if we replaced the \\(i\\) th column of the matrix \\(A\\) by its \\(j\\) th column and then computed the determinant of the resulting matrix by expansion down the \\(i\\) th column.\nBut such a matrix has two equal columns and therefore has a zero determinant. So this sum must be 0 if \\(i \\neq j\\). We can combine these two sums by means of the Kronecker delta.\n\\[\n\\begin{equation*}\n\\sum_{k=1}^{n} A_{k i} a_{k j}=\\delta_{i j} \\operatorname{det} A \\tag{2.9}\n\\end{equation*}\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture5.html#adjoint-minor-cofactor-matrices",
    "href": "lectures/ch2_lecture5.html#adjoint-minor-cofactor-matrices",
    "title": "Ch2 Lecture 5",
    "section": "Adjoint, Minor, Cofactor Matrices",
    "text": "Adjoint, Minor, Cofactor Matrices\n\n\n\\(M_{i j}(A)\\), the \\((i,j)\\) th minor of \\(A\\), is the determinant of the \\((n-1) \\times (n-1)\\) matrix obtained by deleting the \\(i\\)th row and \\(j\\)th column of \\(A\\).\n\\(A_{i j}=(-1)^{i+j} M_{i j}(A)\\) is the \\((i,j)\\)t h cofactor of \\(A\\).\nMatrix of minors \\(M(A)\\) is the matrix whose \\((i,j)\\) th entry is the minor \\(M_{i j}(A)\\).\nCofactor matrix \\(A_{\\text {cof }}\\) is the matrix whose \\((i,j)\\) th entry is the cofactor \\(A_{i j}\\).\nAdjoint of \\(A\\) is the transpose of the cofactor matrix, \\(A^{*}=A_{\\text {cof }}^{T}\\)."
  },
  {
    "objectID": "lectures/ch2_lecture5.html#example",
    "href": "lectures/ch2_lecture5.html#example",
    "title": "Ch2 Lecture 5",
    "section": "Example",
    "text": "Example\n\\(A=\\left[\\begin{array}{rrr}1 & 2 & 0 \\\\ 0 & 0 & -1 \\\\ 0 & 2 & 1\\end{array}\\right]\\)\n\nMatrix of minors:\n\n\n\nCofactor matrix and adjoint:\nOverlay with the checkerboard \\(\\left[\\begin{array}{l}+-+ \\\\ -+- \\\\ +-+\\end{array}\\right]\\). Take the transpose, to get\n\\[\n\\operatorname{adj} A=\\left[\\begin{array}{rrr}\n2 & -2 & -2 \\\\\n0 & 1 & 1 \\\\\n0 & -2 & 0\n\\end{array}\\right]\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture5.html#back-to-the-inverse-formula",
    "href": "lectures/ch2_lecture5.html#back-to-the-inverse-formula",
    "title": "Ch2 Lecture 5",
    "section": "Back to the Inverse Formula",
    "text": "Back to the Inverse Formula\n\\[\n\\begin{equation*}\n\\sum_{k=1}^{n} A_{k i} a_{k j}=\\delta_{i j} \\operatorname{det} A\n\\end{equation*}\n\\]\n\nWe recognize the left-hand side as the dot product of the \\(i\\) th row of the adjoint of \\(A\\) with the \\(j\\) th column of \\(A\\).\nSo then we have \\(\\left(A^{*} A\\right)_{i j}=\\delta_{i j} \\operatorname{det} A\\).\nIn matrix form, this is \\(A^{*} A=\\operatorname{det} A I_{n}\\).\nWe can divide both sides by \\(\\operatorname{det} A\\) (if the matrix is invertible, \\(\\operatorname{det} A \\neq 0\\)) to get \\(\\frac{A^{*} A}{\\operatorname{det} A}=I_{n}\\).\nThis gives us a formula for the inverse of \\(A\\):\n\\[\nA^{-1}=\\frac{1}{\\operatorname{det} A} A^{*}\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture5.html#checking-with-our-example",
    "href": "lectures/ch2_lecture5.html#checking-with-our-example",
    "title": "Ch2 Lecture 5",
    "section": "Checking, with our example",
    "text": "Checking, with our example\nRemeber, we had \\(A=\\left[\\begin{array}{rrr}1 & 2 & 0 \\\\ 0 & 0 & -1 \\\\ 0 & 2 & 1\\end{array}\\right]\\)\n\nWe had computed the adjoint as \\(A^{*}=\\left[\\begin{array}{rrr}2 & -2 & -2 \\\\ 0 & 1 & 1 \\\\ 0 & -2 & 0\\end{array}\\right]\\)\n\n\nCheck:\n\\[\nA \\operatorname{adj} A=\\left[\\begin{array}{rrr}\n1 & 2 & 0 \\\\\n0 & 0 & -1 \\\\\n0 & 2 & 1\n\\end{array}\\right]\\left[\\begin{array}{rrr}\n2 & -2 & -2 \\\\\n0 & 1 & 1 \\\\\n0 & -2 & 0\n\\end{array}\\right]=\\left[\\begin{array}{lll}\n2 & 0 & 0 \\\\\n0 & 2 & 0 \\\\\n0 & 0 & 2\n\\end{array}\\right]=(\\operatorname{det} A) I_{3} .\n\\]\n\n\nSo, \\(A^{-1}=\\frac{1}{\\operatorname{det} A} A^{*}\\)."
  },
  {
    "objectID": "lectures/ch2_lecture5.html#cramers-rule",
    "href": "lectures/ch2_lecture5.html#cramers-rule",
    "title": "Ch2 Lecture 5",
    "section": "Cramer’s Rule",
    "text": "Cramer’s Rule\nExplicit formula for solving linear systems with a nonsingular coefficient matrix.\n\nSolve \\(A \\mathbf{x}=\\mathbf{b}\\)\n\n\nMultiply both sides by \\(A^{-1}\\): \\(\\mathbf{x}=A^{-1} \\mathbf{b}\\)\n\n\nUse the formula for \\(A^{-1}\\): \\(\\mathbf{x}=\\frac{1}{\\operatorname{det} A} A^{*} \\mathbf{b}\\)\n\n\nThe \\(i\\) th component of \\(\\mathbf{x}\\) is\n\\[\nx_{i}=\\frac{1}{\\operatorname{det} A} \\sum_{j=1}^{n} A_{j i} b_{j}\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture5.html#interpretation-of-cramers-rule",
    "href": "lectures/ch2_lecture5.html#interpretation-of-cramers-rule",
    "title": "Ch2 Lecture 5",
    "section": "Interpretation of Cramer’s Rule",
    "text": "Interpretation of Cramer’s Rule\n\\[\nx_{i}=\\frac{1}{\\operatorname{det} A} \\sum_{j=1}^{n} A_{j i} b_{j}\n\\]\n\nThe summation term is exactly what we would obtain if we started with the determinant of the matrix \\(B_{i}\\) obtained from \\(A\\) by replacing the \\(i\\) th column of \\(A\\) by \\(\\mathbf{b}\\) and then expanding the determinant down the \\(i\\) th column. Therefore, we have arrived at the following rule:\n\n** pause **\n\n\n\nLet \\(A\\) be an invertible \\(n \\times n\\) matrix and \\(\\mathbf{b}\\) an \\(n \\times 1\\) column vector.\nDenote by \\(B_{i}\\) the matrix obtained from \\(A\\) by replacing the \\(i\\) th column of \\(A\\) by \\(\\mathbf{b}\\).\nThen the linear system \\(A \\mathbf{x}=\\mathbf{b}\\) has unique solution \\(\\mathbf{x}=\\left(x_{1}, x_{2}, \\ldots, x_{n}\\right)\\), where\n\\[\nx_{i}=\\frac{\\operatorname{det} B_{i}}{\\operatorname{det} A}, \\quad i=1,2, \\ldots, n\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture5.html#summary-of-laws-of-determinants",
    "href": "lectures/ch2_lecture5.html#summary-of-laws-of-determinants",
    "title": "Ch2 Lecture 5",
    "section": "Summary of Laws of Determinants",
    "text": "Summary of Laws of Determinants\nLet \\(A, B\\) be \\(n \\times n\\) matrices.\n\nD1: If \\(A\\) is upper triangular, \\(\\operatorname{det} A\\) is the product of all the diagonal elements of \\(A\\).\nD2: \\(\\operatorname{det}\\left(E_{i}(c) A\\right)=c \\cdot \\operatorname{det} A\\).\nD3: \\(\\operatorname{det}\\left(E_{i j} A\\right)=-\\operatorname{det} A\\).\nD4: \\(\\operatorname{det}\\left(E_{i j}(s) A\\right)=\\operatorname{det} A\\).\nD5: The matrix \\(A\\) is invertible if and only if \\(\\operatorname{det} A \\neq 0\\).\nD6: \\(\\operatorname{det} A B=\\operatorname{det} A \\operatorname{det} B\\).\nD7: \\(\\operatorname{det} A^{T}=\\operatorname{det} A\\).\nD8: \\(A \\operatorname{adj} A=(\\operatorname{adj} A) A=(\\operatorname{det} A) I\\).\nD9: If \\(\\operatorname{det} A \\neq 0\\), then \\(A^{-1}=\\frac{1}{\\operatorname{det} A} \\operatorname{adj} A\\)."
  },
  {
    "objectID": "lectures/ch2_lecture5.html#quadratic-forms",
    "href": "lectures/ch2_lecture5.html#quadratic-forms",
    "title": "Ch2 Lecture 5",
    "section": "Quadratic Forms",
    "text": "Quadratic Forms\nA quadratic form is a homogeneous polynomial of degree 2 in \\(n\\) variables. For example,\n\n\\[\nQ(x, y, z)=x^{2}+2 y^{2}+z^{2}+2 x y+y z+3 x z .\n\\]\n\n\nWe can express this in matrix form!\n\\[\n\\begin{aligned}\nx(x+2 y+3 z)+y(2 y+z)+z^{2} & =\\left[\\begin{array}{lll}\nx & y & z\n\\end{array}\\right]\\left[\\begin{array}{c}\nx+2 y+3 z \\\\\n2 y+z \\\\\nz\n\\end{array}\\right]\n\\end{aligned}\n\\]\n\n\n\\[\n\\begin{aligned}\n=\\left[\\begin{array}{lll}\nx & y & z\n\\end{array}\\right]\\left[\\begin{array}{lll}\n1 & 2 & 3 \\\\\n0 & 2 & 1 \\\\\n0 & 0 & 1\n\\end{array}\\right]\\left[\\begin{array}{c}\nx \\\\\ny \\\\\nz\n\\end{array}\\right]=\\mathbf{x}^{T} A \\mathbf{x},\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture3.html#difference-equations-1",
    "href": "lectures/ch2_lecture3.html#difference-equations-1",
    "title": "Ch2 Lecture 3",
    "section": "Difference Equations",
    "text": "Difference Equations"
  },
  {
    "objectID": "lectures/ch2_lecture3.html#digital-filters",
    "href": "lectures/ch2_lecture3.html#digital-filters",
    "title": "Ch2 Lecture 3",
    "section": "Digital Filters",
    "text": "Digital Filters\n\nAnother use for difference equations:\n\\[\n\\begin{equation*}\ny_{k}=a_{0} x_{k}+a_{1} x_{k-1}+\\cdots+a_{m} x_{k-m}, \\quad k=m, m+1, m+2, \\ldots,\n\\end{equation*}\n\\]\n\nBefore, in the case of temperature in the metal rod, we were trying to find values of \\(x\\) that would satisfy the equation. Here, different context: imagine the \\(x\\)’s are known inputs, perhaps discrete samples of a continuous variable, and \\(y\\) are outputs that we want to calculate, such as a filter.\n\n\n\\(x\\) is a continuous variable – perhaps sound in time domain, or parts of an image in space.\n\\(y\\) is some filtered version of \\(x\\)."
  },
  {
    "objectID": "lectures/ch2_lecture3.html#example-cleaning-up-a-noisy-signal",
    "href": "lectures/ch2_lecture3.html#example-cleaning-up-a-noisy-signal",
    "title": "Ch2 Lecture 3",
    "section": "Example: Cleaning up a noisy signal",
    "text": "Example: Cleaning up a noisy signal\nTrue signal: \\(f(t)=\\cos (\\pi t),-1 \\leq t \\leq 1\\)\nSignal plus noise: \\(g(t)=\\cos (\\pi t)+\\) \\(\\frac{1}{5} \\sin (24 \\pi t)+\\frac{1}{4} \\cos (30 \\pi t)\\)\n\n\n\n\n\n\n\n\n\n\nSuppose the exact signal we want is the line in black, and we’d like to measure it. But all we have is the noisy set of measurements in red.\nNotice the low-frequency part of the measurement is the signal, and the high-frequency part is the noise.\n\nHow can we use this fact to build a good filter?"
  },
  {
    "objectID": "lectures/ch2_lecture3.html#filtered-data",
    "href": "lectures/ch2_lecture3.html#filtered-data",
    "title": "Ch2 Lecture 3",
    "section": "Filtered data",
    "text": "Filtered data\nIdea: we can sample nearby points and take a weighted average of them.\n\\[\ny_{k}=\\frac{1}{4} x_{k+1}+\\frac{1}{2} x_{k}+\\frac{1}{4} x_{k-1}, \\quad k=1,2, \\ldots, 63\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nThis captured the low-frequency part of the signal, and filtered out the high-frequency noise. That’s just what we wanted! This is called a low-pass filter."
  },
  {
    "objectID": "lectures/ch2_lecture3.html#section",
    "href": "lectures/ch2_lecture3.html#section",
    "title": "Ch2 Lecture 3",
    "section": "",
    "text": "We can zoom in on a few points to see the effect of the filter.\n\n\n\n\n\n\n\n\n\n\nWhat if we subtract the values at \\(x_{k+1}\\) and \\(x_{k-1}\\) instead of adding them?"
  },
  {
    "objectID": "lectures/ch2_lecture3.html#now-you-try",
    "href": "lectures/ch2_lecture3.html#now-you-try",
    "title": "Ch2 Lecture 3",
    "section": "Now you try",
    "text": "Now you try\nSee if you can make a figure which will just find the noisyness…\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/ch2_lecture3.html#a-different-filter",
    "href": "lectures/ch2_lecture3.html#a-different-filter",
    "title": "Ch2 Lecture 3",
    "section": "A different filter",
    "text": "A different filter\nWhat if we subtract the values at \\(x_{k+1}\\) and \\(x_{k-1}\\) instead of adding them?\n\\[\ny_{k}=-\\frac{1}{4} x_{k+1}+\\frac{1}{2} x_{k}-\\frac{1}{4} x_{k-1}, \\quad k=1,2, \\ldots, 63\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nThis is a high-pass filter. It captures the high-frequency noise, but filters out the low-frequency signal."
  },
  {
    "objectID": "lectures/ch2_lecture3.html#definition-of-inverse-matrix",
    "href": "lectures/ch2_lecture3.html#definition-of-inverse-matrix",
    "title": "Ch2 Lecture 3",
    "section": "Definition of inverse matrix",
    "text": "Definition of inverse matrix\nLet \\(A\\) be a square matrix.\nInverse for \\(A\\) is a square matrix \\(B\\) of the same size as \\(A\\)\n\nsuch that \\(A B=I=B A\\).\n\nIf such a \\(B\\) exists, then the matrix \\(A\\) is said to be invertible.\nAlso called “singular” (non-invertible), or “nonsingular” (invertible)"
  },
  {
    "objectID": "lectures/ch2_lecture3.html#conditions-for-invertibility",
    "href": "lectures/ch2_lecture3.html#conditions-for-invertibility",
    "title": "Ch2 Lecture 3",
    "section": "Conditions for invertibility",
    "text": "Conditions for invertibility\nConditions for Invertibility The following are equivalent conditions on the square \\(n \\times n\\) matrix \\(A\\) :\n\nThe matrix \\(A\\) is invertible.\nThere is a square matrix \\(B\\) such that \\(B A=I\\).\nThe linear system \\(A \\mathbf{x}=\\mathbf{b}\\) has a unique solution for every right-hand-side vector \\(\\mathbf{b}\\).\nThe linear system \\(A \\mathbf{x}=\\mathbf{b}\\) has a unique solution for some right-hand-side vector \\(\\mathbf{b}\\).\nThe linear system \\(A \\mathbf{x}=0\\) has only the trivial solution.\n\\(\\operatorname{rank} A=n\\).\nThe reduced row echelon form of \\(A\\) is \\(I_{n}\\).\nThe matrix \\(A\\) is a product of elementary matrices.\nThere is a square matrix \\(B\\) such that \\(A B=I\\).\n\n\nWe can prove this by starting with 1, which implies 2, etc, and then 9 implies 1…"
  },
  {
    "objectID": "lectures/ch2_lecture3.html#elementary-matrices",
    "href": "lectures/ch2_lecture3.html#elementary-matrices",
    "title": "Ch2 Lecture 3",
    "section": "Elementary matrices",
    "text": "Elementary matrices\n\nEach of the operations in row reduction can be represented by a matrix \\(E\\).\n\n\nRemember: - \\(E_{i j}\\) : The elementary operation of switching the ith and jth rows of the matrix. - \\(E_{i}(c)\\) : The elementary operation of multiplying the ith row by the nonzero constant \\(c\\). - \\(E_{i j}(d)\\) : The elementary operation of adding \\(d\\) times the jth row to the ith row.\n\n\nFind an elementary matrix of size \\(n\\) is by performing the corresponding elementary row operation on the identity matrix \\(I_{n}\\).\n\n\nExample: Find the elementary matrix for \\(E_{13}(-4)\\)\n\n\nAdd -4 times the 3rd row of \\(I_{3}\\) to its first row…\n\n\n\\[\nE_{13}(-4)=\\left[\\begin{array}{rrr}\n1 & 0 & -4 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{array}\\right]\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture3.html#section-1",
    "href": "lectures/ch2_lecture3.html#section-1",
    "title": "Ch2 Lecture 3",
    "section": "",
    "text": "Recall an example from the first week:\nSolve the simple system\n\\[\n\\begin{gather*}\n2 x-y=1 \\\\\n4 x+4 y=20 . \\tag{1.5}\n\\end{gather*}\n\\]\n\n\\[\n\\begin{aligned}\n& {\\left[\\begin{array}{rrr}\n2 & -1 & 1 \\\\\n4 & 4 & 20\n\\end{array}\\right] \\overrightarrow{E_{12}}\\left[\\begin{array}{rrr}\n4 & 4 & 20 \\\\\n2 & -1 & 1\n\\end{array}\\right] \\overrightarrow{E_{1}(1 / 4)}\\left[\\begin{array}{rrr}\n1 & 1 & 5 \\\\\n2 & -1 & 1\n\\end{array}\\right]} \\\\\n& \\overrightarrow{E_{21}(-2)}\\left[\\begin{array}{rrr}\n1 & 1 & 5 \\\\\n0 & -3 & -9\n\\end{array}\\right] \\overrightarrow{E_{2}(-1 / 3)}\\left[\\begin{array}{lll}\n1 & 1 & 5 \\\\\n0 & 1 & 3\n\\end{array}\\right] \\overrightarrow{E_{12}(-1)}\\left[\\begin{array}{lll}\n1 & 0 & 2 \\\\\n0 & 1 & 3\n\\end{array}\\right] .\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture3.html#section-2",
    "href": "lectures/ch2_lecture3.html#section-2",
    "title": "Ch2 Lecture 3",
    "section": "",
    "text": "Rewrite this using matrix multiplication: \\[\n\\left[\\begin{array}{lll}\n1 & 0 & 2 \\\\\n0 & 1 & 3\n\\end{array}\\right]=E_{12}(-1) E_{2}(-1 / 3) E_{21}(-2) E_{1}(1 / 4) E_{12}\\left[\\begin{array}{rrr}\n2 & -1 & 1 \\\\\n4 & 4 & 20\n\\end{array}\\right] \\text {. }\n\\]\n\nWe are rewriting the row operations as a product of matrices. Notice too that the reduced row echelon form is the identity matrix."
  },
  {
    "objectID": "lectures/ch2_lecture3.html#section-3",
    "href": "lectures/ch2_lecture3.html#section-3",
    "title": "Ch2 Lecture 3",
    "section": "",
    "text": "Remove the last columns in each matrix above. (They were the “augmented” part of the original problem.) All the operations still work:\n\\[\n\\left[\\begin{array}{ll}\n1 & 0 \\\\\n0 & 1\n\\end{array}\\right]=E_{12}(-1) E_{2}(-1 / 3) E_{21}(-2) E_{1}(1 / 4) E_{12}\\left[\\begin{array}{rr}\n2 & -1 \\\\\n4 & 4\n\\end{array}\\right] \\text {. }\n\\]\n\nAha! We now have a formula for the inverse of the original matrix:\n\\[\nA^{-1}=E_{12}(-1) E_{2}(-1 / 3) E_{21}(-2) E_{1}(1 / 4) E_{12}\n\\]\n\nHow can we generalize this? How can we keep track of what we are doing?"
  },
  {
    "objectID": "lectures/ch2_lecture3.html#superaugmented-matrix",
    "href": "lectures/ch2_lecture3.html#superaugmented-matrix",
    "title": "Ch2 Lecture 3",
    "section": "Superaugmented matrix",
    "text": "Superaugmented matrix\n\nForm the superaugmented matrix \\([A \\mid I]\\).\nIf we perform the elementary operation \\(E\\) on the superaugmented matrix, we get the matrix \\(E\\) in the augmented part:\n\n\\[\nE[A \\mid I]=[E A \\mid E I]=[E A \\mid E]\n\\]\n\nThis can help us keep track of our operations as we do row reduction\nThe augmented part is just the product of the elementary matrices that we have used so far.\nNow continue applying elementary row operations until the part of the matrix originally occupied by \\(A\\) is reduced to the reduced row echelon form of \\(A\\). . . . \\[\n[A \\mid I] \\overrightarrow{E_{1}, E_{2}, \\ldots, E_{k}}[I \\mid B]\n\\]\n\\(B=E_{k} E_{k-1} \\cdots E_{1}\\) is the product of the various elementary matrices we used."
  },
  {
    "objectID": "lectures/ch2_lecture3.html#inverse-algorithm",
    "href": "lectures/ch2_lecture3.html#inverse-algorithm",
    "title": "Ch2 Lecture 3",
    "section": "Inverse Algorithm",
    "text": "Inverse Algorithm\nGiven an \\(n \\times n\\) matrix \\(A\\), to compute \\(A^{-1}\\) :\n\nForm the superaugmented matrix \\(\\widetilde{A}=\\left[A \\mid I_{n}\\right]\\).\nReduce the first \\(n\\) columns of \\(\\tilde{A}\\) to reduced row echelon form by performing elementary operations on the matrix \\(\\widetilde{A}\\) resulting in the matrix \\([R \\mid B]\\).\nIf \\(R=I_{n}\\) then set \\(A^{-1}=B\\); otherwise, \\(A\\) is singular and \\(A^{-1}\\) does not exist."
  },
  {
    "objectID": "lectures/ch2_lecture3.html#x2-matrices",
    "href": "lectures/ch2_lecture3.html#x2-matrices",
    "title": "Ch2 Lecture 3",
    "section": "2x2 matrices",
    "text": "2x2 matrices\nSuppose we have the 2x2 matrix\n\\[\nA=\\left[\\begin{array}{ll}\na & b \\\\\nc & d\n\\end{array}\\right]\n\\]\n\nDo row reduction on the superaugmented matrix:\n\\[\n\\left[\\begin{array}{ll|ll}\na & b & 1 & 0 \\\\\nc & d & 0 & 1\n\\end{array}\\right]\n\\]\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/ch2_lecture3.html#section-4",
    "href": "lectures/ch2_lecture3.html#section-4",
    "title": "Ch2 Lecture 3",
    "section": "",
    "text": "\\[\nA^{-1}=\\frac{1}{D}\\left[\\begin{array}{rr}\nd & -b \\\\\n-c & a\n\\end{array}\\right]\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture3.html#back-to-pagerank",
    "href": "lectures/ch2_lecture3.html#back-to-pagerank",
    "title": "Ch2 Lecture 3",
    "section": "Back to PageRank",
    "text": "Back to PageRank\n\nDo page ranking as in the first week:\n\nfor page \\(j\\) let \\(n_{j}\\) be its total number of outgoing links on that page.\nThen the score for vertex \\(i\\) is the sum of the scores of all vertices \\(j\\) that link to \\(i\\), divided by the total number of outgoing links on page \\(j\\). . . . \\[\n\\begin{equation*}\nx_{i}=\\sum_{x_{j} \\in L_{i}} \\frac{x_{j}}{n_{j}} . \\tag{1.4}\n\\end{equation*}\n\\]\n\n\n\\[\n\\begin{aligned}\n& x_{1}=\\frac{x_{3}}{3} \\\\\n& x_{2}=\\frac{x_{1}}{2}+\\frac{x_{3}}{3} \\\\\n& x_{3}=\\frac{x_{1}}{2}+\\frac{x_{2}}{1} \\\\\n& x_{4}=\\frac{x_{3}}{3} \\\\\n& x_{5}=\\frac{x_{6}}{1} \\\\\n& x_{6}=\\frac{x_{5}}{1}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture3.html#pagerank-as-a-matrix-equation",
    "href": "lectures/ch2_lecture3.html#pagerank-as-a-matrix-equation",
    "title": "Ch2 Lecture 3",
    "section": "PageRank as a matrix equation",
    "text": "PageRank as a matrix equation\nDefine the matrix \\(Q\\) and vector \\(\\mathbf{x}\\) by\n\\[\nQ=\\left[\\begin{array}{llllll}\n0 & 0 & \\frac{1}{3} & 0 & 0 & 0 \\\\\n\\frac{1}{2} & 0 & \\frac{1}{3} & 0 & 0 & 0 \\\\\n\\frac{1}{2} & 1 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & \\frac{1}{3} & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 & 1 & 0\n\\end{array}\\right], \\mathbf{x}=\\left(x_{1}, x_{2}, x_{3}, x_{4}, x_{5}\\right)\n\\]\n\nThen the equation \\(x=Q x\\) is equivalent to the system of equations above.\n\n\n\\(\\mathbf{x}\\) is a stationary vector for the transition matrix \\(Q\\)."
  },
  {
    "objectID": "lectures/ch2_lecture3.html#connection-between-adjacency-matrix-and-transition-matrix",
    "href": "lectures/ch2_lecture3.html#connection-between-adjacency-matrix-and-transition-matrix",
    "title": "Ch2 Lecture 3",
    "section": "Connection between adjacency matrix and transition matrix",
    "text": "Connection between adjacency matrix and transition matrix\n\n\\(A\\): adjacency matrix of a graph or digraph\n\\(D\\): be a diagonal matrix whose \\(i\\) th entry is either:\n\nthe inverse of the sum of all entries in the \\(i\\) th row of \\(\\mathrm{A}\\), or\nzero if if this sum is zero.\n\nThen \\(Q=A^{T} D\\) is the transition matrix for the page ranking of this graph.\n\n. . . Check:\n\\[\nA=\\left[\\begin{array}{llllll}\n0 & 1 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & 0 \\\\\n1 & 1 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 & 1 & 0\n\\end{array}\\right]\n\\]\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/ch2_lecture3.html#pagerank-as-a-markov-chain",
    "href": "lectures/ch2_lecture3.html#pagerank-as-a-markov-chain",
    "title": "Ch2 Lecture 3",
    "section": "PageRank as a Markov chain",
    "text": "PageRank as a Markov chain\nThink of web surfing as a random process\n\neach page is a state\n\nprobabilities of moving from one state to another given by a stochastic transition matrix P.\n\nequal probability of moving to any of the outgoing links of a page\n\n\n\\[\nP\\stackrel{?}{=}\\left[\\begin{array}{llllll}\n0 & 0 & \\frac{1}{3} & 0 & 0 & 0 \\\\\n\\frac{1}{2} & 0 & \\frac{1}{3} & 0 & 0 & 0 \\\\\n\\frac{1}{2} & 1 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & \\frac{1}{3} & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 & 1 & 0\n\\end{array}\\right]\n\\]\nPause: Is \\(P\\) a stochastic matrix?\n\nIs P a stochastic matrix? What’s the problem? (One of the columns doesn’t sum to 1.)"
  },
  {
    "objectID": "lectures/ch2_lecture3.html#correction-vector",
    "href": "lectures/ch2_lecture3.html#correction-vector",
    "title": "Ch2 Lecture 3",
    "section": "Correction vector",
    "text": "Correction vector\nWe can introduce a correction vector, equivalent to adding links from the dangling node to every other node, or to all connecting nodes (via some path).\n\n\\[\nP=\\left[\\begin{array}{llllll}\n0 & 0 & \\frac{1}{3} & \\frac{1}{3} & 0 & 0 \\\\\n\\frac{1}{2} & 0 & \\frac{1}{3} & \\frac{1}{3} & 0 & 0 \\\\\n\\frac{1}{2} & 1 & 0 & \\frac{1}{3} & 0 & 0 \\\\\n0 & 0 & \\frac{1}{3} & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 & 1 & 0\n\\end{array}\\right]\n\\]\n\n\nThis is now a stochastic matrix “surfing matrix”\n\n\nFind the new transition matrix \\(Q\\)…\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/ch2_lecture3.html#teleportation",
    "href": "lectures/ch2_lecture3.html#teleportation",
    "title": "Ch2 Lecture 3",
    "section": "Teleportation",
    "text": "Teleportation\n\nThere may not be a single stationary vector for the surfing matrix \\(P\\).\n\nCan you think of two different vectors that can never move towards each other? That is, can you think of ways of getting “stuck”?\n\n\nSolution: Jump around. Every so often, instead of following a link, jump to a random page.\n\n\nHow can we modify the surfing matrix to include this?\nIntroduce a teleportation matrix \\(E\\), which is a transition matrix that allows the surfer to jump to any page with equal probability.\nPause: What must \\(E\\) be?\n\n\n\nLet \\(\\mathbf{v}\\) be the teleportation vector with entries 1/n -Let \\(\\mathbf{e}=\\) \\((1,1, \\ldots, 1)\\) be the vector of all ones.\nThen \\(\\mathbf{v e}^{T}\\) is a stochastic matrix whose columns are all equal to \\(\\mathbf{v}\\). This is \\(E\\)."
  },
  {
    "objectID": "lectures/ch2_lecture3.html#pagerank-matrix",
    "href": "lectures/ch2_lecture3.html#pagerank-matrix",
    "title": "Ch2 Lecture 3",
    "section": "PageRank Matrix",
    "text": "PageRank Matrix\nLet:\n\n\\(P\\) be a stochastic matrix,\n\\(\\mathbf{v}\\) a distribution vector of compatible size\n\\(\\alpha\\) a teleportation parameter with \\(0&lt;\\alpha&lt;1\\).\nThen \\(\\alpha P+(1-\\alpha) \\mathbf{v e}^{T}\\) and\n. . . our goal is to find stationary vectors \\[\n\\begin{equation*}\n\\left(\\alpha P+(1-\\alpha) \\mathbf{v e}^{T}\\right) \\mathbf{x}=\\mathbf{x} \\tag{2.4}\n\\end{equation*}\n\\]\n\n\nRearranging,\n\\[\n\\begin{equation*}\n(I-\\alpha P) \\mathbf{x}=(1-\\alpha) \\mathbf{v} \\tag{2.5}\n\\end{equation*}\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture3.html#now-solve-pagerank-for-our-example",
    "href": "lectures/ch2_lecture3.html#now-solve-pagerank-for-our-example",
    "title": "Ch2 Lecture 3",
    "section": "Now solve pagerank for our example",
    "text": "Now solve pagerank for our example\nUse M.solve(b)…\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/ch2_lecture1.html#diffusion",
    "href": "lectures/ch2_lecture1.html#diffusion",
    "title": "Ch2 Lecture 1",
    "section": "Diffusion",
    "text": "Diffusion\n\nLast week, we talked about the diffusion equation. Here is the exact form:\n\\[\\frac{\\partial a(x,t)}{\\partial t} = D_{a}\\frac{\\partial^{2} a(x,t)}{\\partial x^{2}}\\]\n\nWalk through why we have a first derivative for time, and a second derivative for space. It’s because if the concentration is increasing steadily in space at a given position, the same amount will enter (from the left) and leave (from the right). We only end up with a changing concentration if the rate of entry and exit are different.\n\n\nWe approximated this using the finite-difference method:\nTime derivative (which we had set to zero):\n\\[\n\\frac{\\partial a(x,t)}{\\partial t} \\approx \\frac{1}{dt}(a_{x,t+1} - a_{x,t})\n\\]\nSpacial part of the derivative (which is usually know as the Laplacian):\n\\[\n\\frac{\\partial^{2} a(x,t)}{\\partial x^{2}} \\approx \\frac{1}{dx^{2}}(a_{x+1,t} + a_{x-1,t} - 2a_{x,t})\n\\]\n\n\nThis gives us the finite-difference equation:\n\\[\na_{x,t+1} = a_{x,t} + dt\\left(  \\frac{D_{a}}{dx^{2}}(a_{x+1,t} + a_{x-1,t} - 2a_{x,t})  \\right)\n\\]\n\n\nAdapted from this blog post."
  },
  {
    "objectID": "lectures/ch2_lecture1.html#boundary-conditions",
    "href": "lectures/ch2_lecture1.html#boundary-conditions",
    "title": "Ch2 Lecture 1",
    "section": "Boundary Conditions",
    "text": "Boundary Conditions\n\nLast week, with our metal rod, we had boundary conditions where the temperatures at the ends of the rod were fixed at 0 degrees.\nWe needed to say something about the boundaries to solve the system of equations.\nAnother option is periodic boundary conditions.\nThese are like imagining the rod is a loop, so the temperature at the end of the rod is the same as the temperature at the beginning of the rod."
  },
  {
    "objectID": "lectures/ch2_lecture1.html#periodic-boundary-conditions-np.roll",
    "href": "lectures/ch2_lecture1.html#periodic-boundary-conditions-np.roll",
    "title": "Ch2 Lecture 1",
    "section": "Periodic Boundary Conditions: np.roll",
    "text": "Periodic Boundary Conditions: np.roll\n\nWant an easy way to compute \\(a_{x+1}\\) for all \\(x\\)’s\nSuppose we have n points, and we want to compute \\(a_{x+1}\\) for all \\(x\\)=n.\nProblem: \\(a_{x+1}\\) would be off the end of the array!\nJust replace \\(a_{n+1}\\) with \\(a_{1}\\)\nCan do this easily with np.roll. Lets us compute \\(a_{x+1}\\) for all \\(x\\)’s.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/ch2_lecture1.html#laplacian-with-periodic-boundary-conditions-in-python",
    "href": "lectures/ch2_lecture1.html#laplacian-with-periodic-boundary-conditions-in-python",
    "title": "Ch2 Lecture 1",
    "section": "Laplacian with Periodic Boundary Conditions in Python",
    "text": "Laplacian with Periodic Boundary Conditions in Python\n\nimport numpy as np\ndef laplacian1D(a, dx):\n    return (\n        - 2 * a\n        + np.roll(a,1,axis=0)\n        + np.roll(a,-1,axis=0)\n    ) / (dx ** 2)\n\ndef laplacian2D(a, dx):\n    return (\n        - 4 * a\n        + np.roll(a,1,axis=0)\n        + np.roll(a,-1,axis=0)\n        + np.roll(a,+1,axis=1)\n        + np.roll(a,-1,axis=1)\n    ) / (dx ** 2)\n\n\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "lectures/ch2_lecture1.html#solving-and-animating",
    "href": "lectures/ch2_lecture1.html#solving-and-animating",
    "title": "Ch2 Lecture 1",
    "section": "Solving and animating",
    "text": "Solving and animating\n\nclass OneDimensionalDiffusionEquation(BaseStateSystem):\n    def __init__(self, D):\n        self.D = D\n        self.width = 1000\n        self.dx = 10 / self.width\n        self.dt = 0.9 * (self.dx ** 2) / (2 * D)\n        self.steps = int(0.1 / self.dt)\n\n    def initialise(self):\n        self.t = 0\n        self.X = np.linspace(-5,5,self.width)\n        self.a = np.exp(-self.X**2)\n\n    def update(self):\n        for _ in range(self.steps):\n            self.t += self.dt\n            self._update()\n\n    def _update(self):\n        La = laplacian1D(self.a, self.dx)\n        delta_a = self.dt * (self.D * La)\n        self.a += delta_a\n\n    def draw(self, ax):\n        ax.clear()\n        ax.plot(self.X,self.a, color=\"r\")\n        ax.set_ylim(0,1)\n        ax.set_xlim(-5,5)\n        ax.set_title(\"t = {:.2f}\".format(self.t))\n\none_d_diffusion = OneDimensionalDiffusionEquation(D=1)\n\none_d_diffusion.plot_time_evolution(\"diffusion.gif\")\n\n\n\n\ndiffusion.gif"
  },
  {
    "objectID": "lectures/ch2_lecture1.html#reaction-terms",
    "href": "lectures/ch2_lecture1.html#reaction-terms",
    "title": "Ch2 Lecture 1",
    "section": "Reaction Terms",
    "text": "Reaction Terms\n\nWe will have a system with two chemical components, \\(a\\) and \\(b\\).\nSuppose \\(a\\) activates genes which produce pigmentation. \\(b\\) inhibits \\(a\\)\nWill start with some random small initial concentrations of \\(a\\) and \\(b\\).\nEach will diffuse according to the diffusion equation.\nThey will also react with each other, changing the concentration of each.\n\n\nFor the reaction equations, will use the FitzHugh–Nagumo equation\n\\(R_a(a, b) = a - a^{3} - b + \\alpha\\)\n\\(R_{b}(a, b) = \\beta (a - b)\\)\nWhere \\(\\alpha\\) and \\(\\beta\\) are constants."
  },
  {
    "objectID": "lectures/ch2_lecture1.html#reaction-equation-evolution-at-one-point-in-space",
    "href": "lectures/ch2_lecture1.html#reaction-equation-evolution-at-one-point-in-space",
    "title": "Ch2 Lecture 1",
    "section": "Reaction Equation evolution at one point in space",
    "text": "Reaction Equation evolution at one point in space\n\nclass ReactionEquation(BaseStateSystem):\n    def __init__(self, Ra, Rb):\n        self.Ra = Ra\n        self.Rb = Rb\n        self.dt = 0.01\n        self.steps = int(0.1 / self.dt)\n\n    def initialise(self):\n        self.t = 0\n        self.a = 0.1\n        self.b = 0.7\n        self.Ya = []\n        self.Yb = []\n        self.X = []\n\n    def update(self):\n        for _ in range(self.steps):\n            self.t += self.dt\n            self._update()\n\n    def _update(self):\n        delta_a = self.dt * self.Ra(self.a,self.b)\n        delta_b = self.dt * self.Rb(self.a,self.b)\n\n        self.a += delta_a\n        self.b += delta_b\n\n    def draw(self, ax):\n        ax.clear()\n\n        self.X.append(self.t)\n        self.Ya.append(self.a)\n        self.Yb.append(self.b)\n\n        ax.plot(self.X,self.Ya, color=\"r\", label=\"A\")\n        ax.plot(self.X,self.Yb, color=\"b\", label=\"B\")\n        ax.legend()\n\n        ax.set_ylim(0,1)\n        ax.set_xlim(0,5)\n        ax.set_xlabel(\"t\")\n        ax.set_ylabel(\"Concentrations\")\n\nalpha, beta =  0.2, 5\n\ndef Ra(a,b): return a - a ** 3 - b + alpha\ndef Rb(a,b): return (a - b) * beta\n\none_d_reaction = ReactionEquation(Ra, Rb)\none_d_reaction.plot_time_evolution(\"reaction.gif\", n_steps=50)"
  },
  {
    "objectID": "lectures/ch2_lecture1.html#section",
    "href": "lectures/ch2_lecture1.html#section",
    "title": "Ch2 Lecture 1",
    "section": "",
    "text": "reaction.gif"
  },
  {
    "objectID": "lectures/ch2_lecture1.html#full-model",
    "href": "lectures/ch2_lecture1.html#full-model",
    "title": "Ch2 Lecture 1",
    "section": "Full Model",
    "text": "Full Model\nWe now have two parts: - a diffusion term that “spreads” out concentration - a reaction part the equalises the two concentrations. . . .\nWhat happens when we put the two together? Do we get something stable?"
  },
  {
    "objectID": "lectures/ch2_lecture1.html#section-1",
    "href": "lectures/ch2_lecture1.html#section-1",
    "title": "Ch2 Lecture 1",
    "section": "",
    "text": "def random_initialiser(shape):\n    return(\n        np.random.normal(loc=0, scale=0.05, size=shape),\n        np.random.normal(loc=0, scale=0.05, size=shape)\n    )\n\nclass OneDimensionalRDEquations(BaseStateSystem):\n    def __init__(self, Da, Db, Ra, Rb,\n                 initialiser=random_initialiser,\n                 width=1000, dx=1,\n                 dt=0.1, steps=1):\n\n        self.Da = Da\n        self.Db = Db\n        self.Ra = Ra\n        self.Rb = Rb\n\n        self.initialiser = initialiser\n        self.width = width\n        self.dx = dx\n        self.dt = dt\n        self.steps = steps\n\n    def initialise(self):\n        self.t = 0\n        self.a, self.b = self.initialiser(self.width)\n\n    def update(self):\n        for _ in range(self.steps):\n            self.t += self.dt\n            self._update()\n\n    def _update(self):\n\n        # unpack so we don't have to keep writing \"self\"\n        a,b,Da,Db,Ra,Rb,dt,dx = (\n            self.a, self.b,\n            self.Da, self.Db,\n            self.Ra, self.Rb,\n            self.dt, self.dx\n        )\n\n        La = laplacian1D(a, dx)\n        Lb = laplacian1D(b, dx)\n\n        delta_a = dt * (Da * La + Ra(a,b))\n        delta_b = dt * (Db * Lb + Rb(a,b))\n\n        self.a += delta_a\n        self.b += delta_b\n\n    def draw(self, ax):\n        ax.clear()\n        ax.plot(self.a, color=\"r\", label=\"A\")\n        ax.plot(self.b, color=\"b\", label=\"B\")\n        ax.legend()\n        ax.set_ylim(-1,1)\n        ax.set_title(\"t = {:.2f}\".format(self.t))\n\nDa, Db, alpha, beta = 1, 100, -0.005, 10\n\ndef Ra(a,b): return a - a ** 3 - b + alpha\ndef Rb(a,b): return (a - b) * beta\n\nwidth = 100\ndx = 1\ndt = 0.001\n\nOneDimensionalRDEquations(\n    Da, Db, Ra, Rb,\n    width=width, dx=dx, dt=dt,\n    steps=100\n).plot_time_evolution(\"1dRD.gif\", n_steps=150)"
  },
  {
    "objectID": "lectures/ch2_lecture1.html#section-2",
    "href": "lectures/ch2_lecture1.html#section-2",
    "title": "Ch2 Lecture 1",
    "section": "",
    "text": "1dRD.gif"
  },
  {
    "objectID": "lectures/ch2_lecture1.html#in-two-dimensions",
    "href": "lectures/ch2_lecture1.html#in-two-dimensions",
    "title": "Ch2 Lecture 1",
    "section": "In two dimensions",
    "text": "In two dimensions\n\n\n\n2dRD.png"
  },
  {
    "objectID": "lectures/ch2_lecture1.html#matrix-addition-and-subtraction",
    "href": "lectures/ch2_lecture1.html#matrix-addition-and-subtraction",
    "title": "Ch2 Lecture 1",
    "section": "Matrix Addition and Subtraction",
    "text": "Matrix Addition and Subtraction\nReview matrix addition and subtraction on your own!"
  },
  {
    "objectID": "lectures/ch2_lecture1.html#matrix-multiplication",
    "href": "lectures/ch2_lecture1.html#matrix-multiplication",
    "title": "Ch2 Lecture 1",
    "section": "Matrix Multiplication",
    "text": "Matrix Multiplication\nFor motivation:\n\\[\n2 x-3 y+4 z=5 \\text {. }\n\\]\n\nCan write as a “product” of the coefficient matrix \\([2,-3,4]\\) and and the column matrix of unknowns \\(\\left[\\begin{array}{l}x \\\\ y \\\\ z\\end{array}\\right]\\).\n\n\nThus, product is:\n\\[\n[2,-3,4]\\left[\\begin{array}{l}\nx \\\\\ny \\\\\nz\n\\end{array}\\right]=[2 x-3 y+4 z]\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture1.html#definition-of-matrix-product",
    "href": "lectures/ch2_lecture1.html#definition-of-matrix-product",
    "title": "Ch2 Lecture 1",
    "section": "Definition of matrix product",
    "text": "Definition of matrix product\n\\(A=\\left[a_{i j}\\right]\\): \\(m \\times p\\) matrix\n\\(B=\\left[b_{i j}\\right]\\): \\(p \\times n\\) matrix.\nProduct of \\(A\\) and \\(B\\) – \\(A B\\)\n\nis \\(m \\times n\\) matrix whose\n\\((i, j)\\) th entry is the entry of the product of the \\(i\\) th row of \\(A\\) and the \\(j\\) th column of \\(B\\);\n\n\nMore specifically, the \\((i, j)\\) th entry of \\(A B\\) is\n\\[\na_{i 1} b_{1 j}+a_{i 2} b_{2 j}+\\cdots+a_{i p} b_{p j} .\n\\]\nReminder: Matrix Multiplication is not Commutative\n\n\\(A B \\neq B A\\) in general."
  },
  {
    "objectID": "lectures/ch2_lecture1.html#linear-systems-as-a-matrix-product",
    "href": "lectures/ch2_lecture1.html#linear-systems-as-a-matrix-product",
    "title": "Ch2 Lecture 1",
    "section": "Linear Systems as a Matrix Product",
    "text": "Linear Systems as a Matrix Product\nWe can express a linear system of equations as a matrix product:\n\\[\n\\begin{aligned}\nx_{1}+x_{2}+x_{3} & =4 \\\\\n2 x_{1}+2 x_{2}+5 x_{3} & =11 \\\\\n4 x_{1}+6 x_{2}+8 x_{3} & =24\n\\end{aligned}\n\\]\n\n\\[\n\\mathbf{x}=\\left[\\begin{array}{l}\nx_{1} \\\\\nx_{2} \\\\\nx_{3}\n\\end{array}\\right], \\quad \\mathbf{b}=\\left[\\begin{array}{r}\n4 \\\\\n11 \\\\\n24\n\\end{array}\\right], \\quad \\text { and } A=\\left[\\begin{array}{lll}\n1 & 1 & 1 \\\\\n2 & 2 & 5 \\\\\n4 & 6 & 8\n\\end{array}\\right]\n\\]\n\nOf course, \\(A\\) is just the coefficient matrix of the system and \\(b\\) is the righthand-side vector, which we have seen several times before. But now these take on a new significance. Notice that if we take the first row of \\(A\\) and multiply it by \\(\\mathbf{x}\\) we get the left-hand side of the first equation of our system. Likewise for the second and third rows. Therefore, we may write in the language of matrices that\n\n\n\n\\[\nA \\mathbf{x}=\\left[\\begin{array}{lll}\n1 & 1 & 1 \\\\\n2 & 2 & 5 \\\\\n4 & 6 & 8\n\\end{array}\\right]\\left[\\begin{array}{l}\nx_{1} \\\\\nx_{2} \\\\\nx_{3}\n\\end{array}\\right]=\\left[\\begin{array}{r}\n4 \\\\\n11 \\\\\n24\n\\end{array}\\right]=\\mathbf{b}\n\\]\n\nWe can multiply this out and get that \\(x_1\\) multiplies each of the elements in the first column… (show this)"
  },
  {
    "objectID": "lectures/ch2_lecture1.html#matrix-multiplication-as-a-linear-combination-of-column-vectors",
    "href": "lectures/ch2_lecture1.html#matrix-multiplication-as-a-linear-combination-of-column-vectors",
    "title": "Ch2 Lecture 1",
    "section": "Matrix Multiplication as a Linear Combination of Column Vectors",
    "text": "Matrix Multiplication as a Linear Combination of Column Vectors\nAnother way of writing this system:\n\\[\nx_{1}\\left[\\begin{array}{l}\n1 \\\\\n2 \\\\\n4\n\\end{array}\\right]+x_{2}\\left[\\begin{array}{l}\n1 \\\\\n2 \\\\\n6\n\\end{array}\\right]+x_{3}\\left[\\begin{array}{l}\n1 \\\\\n5 \\\\\n8\n\\end{array}\\right]=\\left[\\begin{array}{r}\n4 \\\\\n11 \\\\\n24\n\\end{array}\\right] .\n\\]\n\nName the columns of A as \\(\\mathbf{a_1}, \\mathbf{a_2}, \\mathbf{a_3}\\), then we can write the matrix as \\(A=\\left[\\mathbf{a}_{1}, \\mathbf{a}_{2}, \\mathbf{a}_{3}\\right]\\)\n\n\nLet \\(\\mathbf{x}=\\left(x_{1}, x_{2}, x_{3}\\right)\\). Then\n\\[\nA \\mathbf{x}=x_{1} \\mathbf{a}_{1}+x_{2} \\mathbf{a}_{2}+x_{3} \\mathbf{a}_{3} .\n\\]\nThis is a very important way of thinking about matrix multiplication\n\nWhat happens if we are trying to get something which can’t be made up of any combination of the columns of \\(A\\)?\nWhat happens if two or more of the columns of \\(A\\) are the same?\nWhat if they are multiples of one another?\nWhat if one column is a linear combination of several others?"
  },
  {
    "objectID": "lectures/ch2_lecture1.html#try-it-yourself",
    "href": "lectures/ch2_lecture1.html#try-it-yourself",
    "title": "Ch2 Lecture 1",
    "section": "Try it yourself",
    "text": "Try it yourself\nTry to find the solution for the following system, by trying different values of \\(x_i\\) to use in a sum of the columns of \\(A\\).\n\\[ A \\mathbf{x} = \\left[\\begin{array}{lll}   1 & 1 & 1 \\\\   2 & 2 & 5 \\\\   4 & 6 & 8 \\end{array}\\right]\\left[\\begin{array}{l} x_1 \\\\ x_2 \\\\ x_3 \\end{array}\\right]=x_{1} \\mathbf{a}_{1}+x_{2} \\mathbf{a}_{2}+x_{3} \\mathbf{a}_{3} =\\left[\\begin{array}{l} 6 \\\\ 21 \\\\ 38 \\end{array}\\right]\n\\]\n\nThe answer is 2,1,3\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nNow try changing the right-hand side to a different vector. Can you still find a solution? (You may need to use non-integer values for the \\(x\\)’s.)"
  },
  {
    "objectID": "lectures/ch2_lecture1.html#section-3",
    "href": "lectures/ch2_lecture1.html#section-3",
    "title": "Ch2 Lecture 1",
    "section": "",
    "text": "This is a slightly changed system.\n\\[ A \\mathbf{x} = \\left[\\begin{array}{lll}   1 & 1 & 2 \\\\   1 & 2 & 2 \\\\   4 & 6 & 8 \\end{array}\\right]\\left[\\begin{array}{l} x_1 \\\\ x_2 \\\\ x_3 \\end{array}\\right]=x_{1} \\mathbf{a}_{1}+x_{2} \\mathbf{a}_{2}+x_{3} \\mathbf{a}_{3} =\\left[\\begin{array}{l} 11 \\\\ 12 \\\\ 46 \\end{array}\\right]\n\\]\n\nThe answer is 2, 1, 4\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nAre you able to find more than one solution? Can you find a right-hand-side that doesn’t have a solution?"
  },
  {
    "objectID": "lectures/ch2_lecture1.html#example-benzoic-acid",
    "href": "lectures/ch2_lecture1.html#example-benzoic-acid",
    "title": "Ch2 Lecture 1",
    "section": "Example: Benzoic acid",
    "text": "Example: Benzoic acid\nBenzoic acid (chemical formula \\(\\mathrm{C}_{7} \\mathrm{H}_{6} \\mathrm{O}_{2}\\) ) oxidizes to carbon dioxide and water.\n\\[\n\\mathrm{C}_{7} \\mathrm{H}_{6} \\mathrm{O}_{2}+\\mathrm{O}_{2} \\rightarrow \\mathrm{CO}_{2}+\\mathrm{H}_{2} \\mathrm{O} .\n\\]\nBalance this equation. (Make the number of atoms of each element match on the two sides of the equation.)\n\nDefine \\((c, o, h)\\) as the number of atoms of carbon, oxygen, and hydrogen atoms in the equation.\n\n\nNext let \\(x_1\\), \\(x_2\\), \\(x_3\\), and \\(x_4\\) be the number of molecules of benzoic acid, oxygen, carbon dioxide, and water, respectively.\n\n\nThen we have the equation\n\\[\nx_{1}\\left[\\begin{array}{l}\n7 \\\\\n2 \\\\\n6\n\\end{array}\\right]+x_{2}\\left[\\begin{array}{l}\n0 \\\\\n2 \\\\\n0\n\\end{array}\\right]=x_{3}\\left[\\begin{array}{l}\n1 \\\\\n2 \\\\\n0\n\\end{array}\\right]+x_{4}\\left[\\begin{array}{l}\n0 \\\\\n1 \\\\\n2\n\\end{array}\\right] .\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture1.html#section-4",
    "href": "lectures/ch2_lecture1.html#section-4",
    "title": "Ch2 Lecture 1",
    "section": "",
    "text": "Rearrange:\n\\[\nx_{1}\\left[\\begin{array}{l}\n7 \\\\\n2 \\\\\n6\n\\end{array}\\right]+x_{2}\\left[\\begin{array}{l}\n0 \\\\\n2 \\\\\n0\n\\end{array}\\right]=x_{3}\\left[\\begin{array}{l}\n1 \\\\\n2 \\\\\n0\n\\end{array}\\right]+x_{4}\\left[\\begin{array}{l}\n0 \\\\\n1 \\\\\n2\n\\end{array}\\right] .\n\\]\nbecomes\n\\[\nA \\mathbf{x}=\\left[\\begin{array}{cccc}\n7 & 0 & -1 & 0 \\\\\n2 & 2 & -2 & -1 \\\\\n6 & 0 & 0 & -2\n\\end{array}\\right]\\left[\\begin{array}{l}\nx_{1} \\\\\nx_{2} \\\\\nx_{3} \\\\\nx_{4}\n\\end{array}\\right]=\\left[\\begin{array}{l}\n0 \\\\\n0 \\\\\n0\n\\end{array}\\right]\n\\]\n\nThis is just like a matrix like we were solving in class last time."
  },
  {
    "objectID": "lectures/ch2_lecture1.html#section-5",
    "href": "lectures/ch2_lecture1.html#section-5",
    "title": "Ch2 Lecture 1",
    "section": "",
    "text": "We solve with row reduction:\n\\[\n\\begin{aligned}\n& {\\left[\\begin{array}{cccc}\n7 & 0 & -1 & 0 \\\\\n2 & 2 & -2 & -1 \\\\\n6 & 0 & 0 & -2\n\\end{array}\\right] \\xrightarrow[E_{21}\\left(-\\frac{2}{7}\\right)]{E_{31}\\left(-\\frac{6}{7}\\right)}\\left[\\begin{array}{cccc}\n7 & 0 & -1 & 0 \\\\\n0 & 2 & -\\frac{12}{7} & -1 \\\\\n0 & 0 & \\frac{6}{7} & -2\n\\end{array}\\right] \\begin{array}{c}\nE_{1}\\left(\\frac{1}{7}\\right) \\\\\nE_{2}\\left(\\frac{1}{2}\\right) \\\\\nE_{3}\\left(\\frac{7}{6}\\right)\n\\end{array} \\left[\\begin{array}{cccc}\n1 & 0 & -\\frac{1}{7} & 0 \\\\\n0 & 1 & -\\frac{6}{7} & -\\frac{1}{2} \\\\\n0 & 0 & 1 & -\\frac{7}{3}\n\\end{array}\\right]} \\\\\n& \\begin{array}{l}\n\\overrightarrow{E_{23}\\left(\\frac{6}{7}\\right)} \\\\\nE_{13}\\left(\\frac{1}{7}\\right)\n\\end{array}\\left[\\begin{array}{llll}\n1 & 0 & 0 & -\\frac{1}{3} \\\\\n0 & 1 & 0 & -\\frac{5}{2} \\\\\n0 & 0 & 1 & -\\frac{7}{3}\n\\end{array}\\right]\n\\end{aligned}\n\\]\n\n\\(x_{4}\\) is free, others are bound. Now pick smallest \\(x_4\\) where others are all positive integers…\n\n\n\\[\n2 \\mathrm{C}_{7} \\mathrm{H}_{6} \\mathrm{O}_{2}+15 \\mathrm{O}_{2} \\rightarrow 14 \\mathrm{CO}_{2}+6 \\mathrm{H}_{2} \\mathrm{O}\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture1.html#scaling",
    "href": "lectures/ch2_lecture1.html#scaling",
    "title": "Ch2 Lecture 1",
    "section": "Scaling",
    "text": "Scaling\nGoal: Make a matrix that will take a vector of coordinates \\(\\mathbf{x}\\) and scale each coordinate \\(x_i\\) by a factor of \\(z_i\\).\n\\[ A x = \\left[\\begin{array}{ll} a1 & a2 \\\\ a3 & a4 \\end{array}\\right] \\left[\\begin{array}{l} x_1 \\\\ x_2 \\end{array}\\right] = \\left[\\begin{array}{l} z_1 \\times x_1 \\\\ z_2\\times  x_2 \\end{array}\\right] \\]\n\n\\[ A = \\left[\\begin{array}{ll} z_1 & 0 \\\\ 0 & z_2 \\end{array}\\right] \\]"
  },
  {
    "objectID": "lectures/ch2_lecture1.html#shearing",
    "href": "lectures/ch2_lecture1.html#shearing",
    "title": "Ch2 Lecture 1",
    "section": "Shearing",
    "text": "Shearing\nShearing: adding a constant shear factor times one coordinate to another coordinate of the point.\nGoal: make a matrix which will transform each coordinate \\(x_i\\) into \\(x_i + \\sum_{j \\ne i} s_{j} \\times x_j\\).\n\\[ A x = \\left[\\begin{array}{ll} a1 & a2 \\\\ a3 & a4 \\end{array}\\right] \\left[\\begin{array}{l} x_1 \\\\ x_2 \\end{array}\\right] = \\left[\\begin{array}{l} x_1 + s_2 x_2 \\\\ x_2 + s_1 x_1 \\end{array}\\right] \\]\n\n\\[ A = \\left[\\begin{array}{ll} 1 & s_2 \\\\ s_1 & 1 \\end{array}\\right] \\]"
  },
  {
    "objectID": "lectures/ch2_lecture1.html#example",
    "href": "lectures/ch2_lecture1.html#example",
    "title": "Ch2 Lecture 1",
    "section": "Example",
    "text": "Example\n\nLet the scaling operator \\(S\\) on points in two dimensions have scale factors of \\(\\frac{3}{2}\\) in the \\(x\\)-direction and \\(\\frac{1}{2}\\) in the \\(y\\)-direction.\nLet the shearing operator \\(H\\) on these points have a shear factor of \\(\\frac{1}{2}\\) by the \\(y\\)-coordinate on the \\(x\\)-coordinate.\nExpress these operators as matrix operators and graph their action on four unit squares situated diagonally from the origin."
  },
  {
    "objectID": "lectures/ch2_lecture1.html#solution",
    "href": "lectures/ch2_lecture1.html#solution",
    "title": "Ch2 Lecture 1",
    "section": "Solution",
    "text": "Solution\n\nScaling operator \\(S\\):\n\n\n\\[\nS((x, y))=\\left[\\begin{array}{c}\n\\frac{3}{2} x \\\\\n\\frac{1}{2} y\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n\\frac{3}{2} & 0 \\\\\n0 & \\frac{1}{2}\n\\end{array}\\right]\\left[\\begin{array}{l}\nx \\\\\ny\n\\end{array}\\right]=T_{A}((x, y))\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture1.html#verify",
    "href": "lectures/ch2_lecture1.html#verify",
    "title": "Ch2 Lecture 1",
    "section": "Verify:",
    "text": "Verify:\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/ch2_lecture1.html#section-6",
    "href": "lectures/ch2_lecture1.html#section-6",
    "title": "Ch2 Lecture 1",
    "section": "",
    "text": "Shearing operator \\(H\\):\n\n\\[\nH((x, y))=\\left[\\begin{array}{c}\nx+\\frac{1}{2} y \\\\\ny\n\\end{array}\\right]=\\left[\\begin{array}{ll}\n1 & \\frac{1}{2} \\\\\n0 & 1\n\\end{array}\\right]\\left[\\begin{array}{l}\nx \\\\\ny\n\\end{array}\\right]=T_{B}((x, y))\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture1.html#verify-1",
    "href": "lectures/ch2_lecture1.html#verify-1",
    "title": "Ch2 Lecture 1",
    "section": "Verify",
    "text": "Verify\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/ch2_lecture1.html#concatenation-of-operators-s-and-h",
    "href": "lectures/ch2_lecture1.html#concatenation-of-operators-s-and-h",
    "title": "Ch2 Lecture 1",
    "section": "Concatenation of operators \\(S\\) and \\(H\\)",
    "text": "Concatenation of operators \\(S\\) and \\(H\\)\n\nThe concatenation \\(S \\circ H\\) of the scaling operator \\(S\\) and shearing operator \\(H\\) is the action of scaling followed by shearing.\nFunction composition corresponds to matrix multiplication\n\n\n\\[\n\\begin{aligned}\nS \\circ H((x, y)) & =T_{A} \\circ T_{B}((x, y))=T_{A B}((x, y)) \\\\\n& =\\left[\\begin{array}{cc}\n\\frac{3}{2} & 0 \\\\\n0 & \\frac{1}{2}\n\\end{array}\\right]\\left[\\begin{array}{ll}\n1 & \\frac{1}{2} \\\\\n0 & 1\n\\end{array}\\right]\\left[\\begin{array}{l}\nx \\\\\ny\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n\\frac{3}{2} & \\frac{3}{4} \\\\\n0 & \\frac{1}{2}\n\\end{array}\\right]\\left[\\begin{array}{l}\nx \\\\\ny\n\\end{array}\\right]=T_{C}((x, y)),\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture1.html#verify-2",
    "href": "lectures/ch2_lecture1.html#verify-2",
    "title": "Ch2 Lecture 1",
    "section": "Verify",
    "text": "Verify\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/ch2_lecture1.html#rotation",
    "href": "lectures/ch2_lecture1.html#rotation",
    "title": "Ch2 Lecture 1",
    "section": "Rotation",
    "text": "Rotation\nGoal: rotate a point in two dimensions counterclockwise by an angle \\(\\phi\\). Suppose the point is initally at an angle \\(\\theta\\) from the \\(x\\)-axis."
  },
  {
    "objectID": "lectures/ch2_lecture1.html#section-7",
    "href": "lectures/ch2_lecture1.html#section-7",
    "title": "Ch2 Lecture 1",
    "section": "",
    "text": "\\[\n\\left[\\begin{array}{l}\nx \\\\\ny\n\\end{array}\\right]\n=\\left[\\begin{array}{l}\nr \\cos \\theta \\\\\nr \\sin \\theta\n\\end{array}\\right]\n\\]\nWe can use trigonometry to find the values of x and y after rotation.\n\\[\n\\left[\\begin{array}{l}\nx^{\\prime} \\\\\ny^{\\prime}\n\\end{array}\\right] =\\left[\\begin{array}{l}\nr \\cos (\\theta+\\phi) \\\\\nr \\sin (\\theta+\\phi)\n\\end{array}\\right]=\\left[\\begin{array}{l}\nr \\cos \\theta \\cos \\phi-r \\sin \\theta \\sin \\phi \\\\\nr \\sin \\theta \\cos \\phi+r \\cos \\theta \\sin \\phi\n\\end{array}\\right]\n\\]\n\nUsing the double-angle rule,\n\\[\n=\\left[\\begin{array}{rr}\n\\cos \\theta &-\\sin \\theta \\\\\n\\sin \\theta & \\cos \\theta\n\\end{array}\\right]\\left[\\begin{array}{l}\nr \\cos \\phi \\\\\nr \\sin \\phi\n\\end{array}\\right]=\\left[\\begin{array}{rr}\n\\cos \\theta&-\\sin \\theta \\\\\n\\sin \\theta & \\cos \\theta\n\\end{array}\\right]\\left[\\begin{array}{l}\nx \\\\\ny\n\\end{array}\\right]\n\\]\n\n\nSo we define the rotation matrix \\(R(\\theta)\\) by\n\\[\nR(\\theta)=\\left[\\begin{array}{rr}\n\\cos \\theta & -\\sin \\theta \\\\\n\\sin \\theta & \\cos \\theta\n\\end{array}\\right]\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture1.html#now-you-try",
    "href": "lectures/ch2_lecture1.html#now-you-try",
    "title": "Ch2 Lecture 1",
    "section": "Now you try",
    "text": "Now you try\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n \n\n\n \n\n\n\n\n \n\n\n\ndiffusion.gif\nreaction.gif\n1dRD.gif\n2dRD.png"
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#what-are-linear-systems",
    "href": "lectures/ch1_lecture1b.html#what-are-linear-systems",
    "title": "Intro to linear systems",
    "section": "What are linear systems?",
    "text": "What are linear systems?\n\nSet of one or more linear equations involving the same variables.\nEach equation can be be put in form \\(a_1 x_1 + a_2 x_2 + \\dots + a_n x_n+ b = 0\\)\n\n\n\\[\n\\begin{cases}\na_{11} x_1 + a_{12} x_2 +\\dots + a_{1n} x_n = b_1 \\\\\na_{21} x_1 + a_{22} x_2  + \\dots + a_{2n} x_n = b_2 \\\\\n\\vdots\\\\\na_{m1} x_1 + a_{m2} x_2 + \\dots + a_{mn} x_n = b_m,\n\\end{cases}\n\\]"
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#example-railroad-cars",
    "href": "lectures/ch1_lecture1b.html#example-railroad-cars",
    "title": "Intro to linear systems",
    "section": "Example: railroad cars",
    "text": "Example: railroad cars\nA chemical manufacturer wants to lease a fleet of 24 railroad tank cars with a combined carrying capacity of 520,000 gallons.\n\nTank cars with three different carrying capacities are available:\n\n8,000 gallons\n16,000 gallons\n24,000 gallons.\n\nHow many of each type of tank car should be leased?\n\n\nCan write this as \\(x \\times 8+y \\times 16 + z \\times 24=520\\) and also \\(x+y+z=24\\). We won’t have a single solution… Two equations, three unknowns."
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#example-traffic-flow.",
    "href": "lectures/ch1_lecture1b.html#example-traffic-flow.",
    "title": "Intro to linear systems",
    "section": "Example: traffic flow.",
    "text": "Example: traffic flow.\n\n\n\n\nNumbers = # of vehicles/hr that enter and leave on that street.\n\\(x_{1}, x_{2}, x_{3}\\), and \\(x_{4}\\): flow of traffic between the four intersections.\n\n\n\nNumber of vehicles entering each intersection should always equal the number leaving. E.g.:\n\n1,500 vehicles enter the intersection of 5th Street and Washington Avenue each hour\n\\(x_{1}+x_{4}\\) vehicles leave this intersection\n\\(\\rightarrow\\) \\(x_{1}+x_{4}=1,500\\)\n\nFind the traffic flow at each of the other three intersections.\nWhat is the # of vehicles that travel from Washington Avenue to Lincoln Avenue on 5th Street?\n\n\n\n\n\\(x_1+x_2=1200\\), and \\(x_2+x_3=1000\\), and \\(x_3+x_4=1300\\)."
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#example-us-population",
    "href": "lectures/ch1_lecture1b.html#example-us-population",
    "title": "Intro to linear systems",
    "section": "Example: US population",
    "text": "Example: US population\n\nThe U.S. population was approximately 75 million in 1900, 150 million in 1950, and 275 million in 2000.\nFind a quadratic equation whose graph passes through the points \\((0,75)\\), \\((50,150)\\), \\((100,275)\\)\n\n\nCall the years \\((0, 50, 100)\\) \\(t_i\\). Call the populations \\((75,150,275)\\) \\(y_i\\).\nThen we have\n\\[\n\\begin{cases}\nc_1 t_1 + c_2 t_1^2 = y_1 \\\\\nc_1 t_2 + c_2 t_2^2 = y_2 \\\\\nc_1 t_3 + c_2 t_3^2 = y_3\n\\end{cases}\n\\]\n\n\nMake the substitutions \\(c_i \\rightarrow x_i\\) and \\(t_i \\rightarrow a_{1i}\\) and \\(t^2_i \\rightarrow a_{2i}\\), then this becomes\n\\[\n\\begin{cases}\nx_1 a_{11} + x_2 a_{21} =a_{11} x_1  + a_{21} x_2  = y_1 \\\\\nx_1 a_{12} + x_2 a_{22} =a_{12} x_1  +a_{22} x_2 = y_2 \\\\\nx_1 a_{13} + x_2 a_{23} = a_{13} x_1  + a_{23} x_2  = y_3\n\\end{cases}\n\\]\nWe see that we now have a standard system of linear equations."
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#goals-for-solving-algorithms",
    "href": "lectures/ch1_lecture1b.html#goals-for-solving-algorithms",
    "title": "Intro to linear systems",
    "section": "Goals for solving algorithms",
    "text": "Goals for solving algorithms\nAn algorithm should be:\n\nfeasible\naccurate\n\nstable\n\nefficient\n\nreusable computations"
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#example-very-simple-linear-system",
    "href": "lectures/ch1_lecture1b.html#example-very-simple-linear-system",
    "title": "Intro to linear systems",
    "section": "Example: very simple linear system",
    "text": "Example: very simple linear system\n\\[\n\\begin{gather*}\n2 x-y=1 \\\\\n4x+4y=20 . \\tag{1.5}\n\\end{gather*}\n\\]\n\nWe can solve this by hand: first we might add four times the first row to the second, so that we get (4+8)x + 0y = 24, so that we realize x = \\(2\\), and then we have 4-y=1 so y = 3.\n\n\nMake an augmented matrix to represent the problem:\n\\[\n\\begin{aligned}\n& x \\quad y=r . h . s . \\\\\n& {\\left[\\begin{array}{rrr}\n2 & -1 & 1 \\\\\n4 & 4 & 20\n\\end{array}\\right]}\n\\end{aligned}\n\\]\n\n\nManipulate the augmented matrix to solve the problem…"
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#elementary-matrix-operations",
    "href": "lectures/ch1_lecture1b.html#elementary-matrix-operations",
    "title": "Intro to linear systems",
    "section": "Elementary Matrix Operations",
    "text": "Elementary Matrix Operations\nWhen we do these on augmented matrices, the solutions are unchanged…\n\n\\(E_{i j}\\) : Swap: Switch the \\(i\\)th and \\(j\\) th rows of the matrix.\n\\(E_{i}(c)\\) : Scale: Multiply the \\(i\\)th row by the nonzero constant \\(c\\).\n\\(E_{i j}(d)\\) : Add: Add \\(d\\) times the \\(j\\)th row to the \\(i\\)th row."
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#reduced-row-echelon-form",
    "href": "lectures/ch1_lecture1b.html#reduced-row-echelon-form",
    "title": "Intro to linear systems",
    "section": "Reduced Row Echelon Form",
    "text": "Reduced Row Echelon Form\nEvery matrix can be reduced by a sequence of elementary row operations to one and only one reduced row echelon form:\n\nNonzero rows of \\(R\\) precede the zero rows.\nColumn numbers of the leading entries of the nonzero rows, say rows \\(1,2, \\ldots, r\\), form an increasing sequence of numbers \\(c_{1}&lt;c_{2}&lt;\\cdots&lt;c_{r}\\).\nEach leading entry is a 1.\nEach leading entry has only zeros above it."
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#use-elementary-ops-to-get-reduced-row-echelon-form",
    "href": "lectures/ch1_lecture1b.html#use-elementary-ops-to-get-reduced-row-echelon-form",
    "title": "Intro to linear systems",
    "section": "Use elementary ops to get reduced row echelon form",
    "text": "Use elementary ops to get reduced row echelon form"
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#g-j-elimination-for-our-toy-system",
    "href": "lectures/ch1_lecture1b.html#g-j-elimination-for-our-toy-system",
    "title": "Intro to linear systems",
    "section": "G-J Elimination for our toy system",
    "text": "G-J Elimination for our toy system\n\\[\n\\left[\\begin{array}{rrr}\n4 & 4 & 20 \\\\\n2 & -1 & 1\n\\end{array}\\right] \\overrightarrow{E_{1}(\\frac{1}{4}}\\left[\\begin{array}{lll}\n1 & 1 & 5 \\\\\n2 & -1 & 1\n\\end{array}\\right] \\overrightarrow{E_{12}(-2)}\\left[\\begin{array}{lll}\n1 & 1 & 5 \\\\\n0 & -3 & -9\n\\end{array}\\right]\n\\]\nand then…\n\n\\[\n\\left[\\begin{array}{rrr}\n1 & 1 & 5 \\\\\n0 & -3 & -9\n\\end{array}\\right] \\overrightarrow{E_{2}(-1 / 3)}\\left[\\begin{array}{lll}\n1 & 1 & 5 \\\\\n0 & 1 & 3\n\\end{array}\\right] \\overrightarrow{E_{12}(-1)}\\left[\\begin{array}{lll}\n1 & 0 & 2 \\\\\n0 & 1 & 3\n\\end{array}\\right] .\n\\]\n\n\nWriting this back as a linear system, we have:\n\\[\n\\begin{gather*}\n2 x-y=1 \\\\\n4 x+4 y=20\n\\end{gather*}  \\Longrightarrow {\\begin{aligned}\n& 1 \\cdot x+0 \\cdot y=2 \\\\\n& 0 \\cdot x+1 \\cdot y=3\n\\end{aligned}}  \\Longrightarrow {\\begin{aligned}x &= 2 \\\\ y &= 3 \\end{aligned}}\n\\]"
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#now-you-try-birds-in-a-tree",
    "href": "lectures/ch1_lecture1b.html#now-you-try-birds-in-a-tree",
    "title": "Intro to linear systems",
    "section": "Now you try: birds in a tree",
    "text": "Now you try: birds in a tree\nThere are 2 trees in a garden (tree “A” and “B”) and in both trees are some birds.\nThe birds of tree A say to the birds of tree B that if one of you comes to our tree, then our population will be double yours.\nThen the birds of tree B tell the birds of tree A that if one of you comes here, then our population will be equal to yours.\nHow many birds in each tree?\n(Solve by making an augmented matrix and doing G-J elimination.)\n\nThe answer is 7 birds in tree A and 5 birds in tree B.\n\n\n\nhttps://www.mathsisfun.com/puzzles/birds-in-trees.html"
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#example-mining",
    "href": "lectures/ch1_lecture1b.html#example-mining",
    "title": "Intro to linear systems",
    "section": "Example: Mining",
    "text": "Example: Mining\n\nA mine produces silica, iron, and gold\nNeeds Money (in $$), operating time (in hours), and labor (in person-hours).\n\n1 pound of silica needs: $.0055, . 0011 hours of operating time, and .0093 hours of labor.\n1 pound of iron needs: $.095, . 01 operating hours, and .025 labor hours.\n1 pound of gold needs: $ 960, 112 operating hours, and 560 labor hours.\n\n\nMeyer Ch 1.5"
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#section",
    "href": "lectures/ch1_lecture1b.html#section",
    "title": "Intro to linear systems",
    "section": "",
    "text": "Suppose that during 600 hours of operation, exactly $ 5000 and 3000 labor-hours are used.\nHow much silica (\\(x\\)), iron (\\(y\\)), and gold (\\(z\\)) were produced?\n\nSet up the linear system whose solution will yield the values for \\(x, y\\), and \\(z\\).\n\n\n\\[\n\\begin{aligned}\n.0055 x + .095 y + 960 z &=  5000 \\quad \\text{(dollars)}\\\\\n.0011 x + .01 y + 112 z &= 600 \\quad \\text{(operating hours)}\\\\\n.0093 x + .025 y + 560 z &= 3000\\quad \\text{(labor hours)}\n\\end{aligned}\n\\]\n\n\nMake the augmented matrix:\n\\[\n\\left[\\begin{array}{@{}ccc|c@{}}\n.0055 & .095 & 960 & 5000 \\\\\n.0011 & .01  & 112 & 600 \\\\\n.0093 & .025 & 560 & 3000\n\\end{array}\\right]\n\\]\n\nDraw the line on the augmented matrix."
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#section-1",
    "href": "lectures/ch1_lecture1b.html#section-1",
    "title": "Intro to linear systems",
    "section": "",
    "text": "We can solve this using Gauss-Jordan elimination."
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#getting-into-row-echelon-form-rounding-after-3-digits",
    "href": "lectures/ch1_lecture1b.html#getting-into-row-echelon-form-rounding-after-3-digits",
    "title": "Intro to linear systems",
    "section": "Getting into row echelon form, rounding after 3 digits",
    "text": "Getting into row echelon form, rounding after 3 digits\n\\[\\left[\\begin{matrix}0.0055 & 0.095 & 960 & 5000\\\\0.0011 & 0.01 & 112 & 600\\\\0.0093 & 0.025 & 560 & 3000\\end{matrix}\\right] \\overrightarrow{E_{12}(\\frac{-1}{5})} \\left[\\begin{matrix}0.0055 & 0.095 & 9.6 \\cdot 10^{2} & 5.0 \\cdot 10^{3}\\\\0 & -0.009 & -80.0 & -4.0 \\cdot 10^{2}\\\\0.0093 & 0.025 & 5.6 \\cdot 10^{2} & 3.0 \\cdot 10^{3}\\end{matrix}\\right]\\]\n\\[\\overrightarrow{E_{13}(\\frac{93}{55})} \\left[\\begin{matrix}0.0055 & 0.095 & 9.6 \\cdot 10^{2} & 5.0 \\cdot 10^{3}\\\\0 & -0.009 & -80.0 & -4.0 \\cdot 10^{2}\\\\0 & -0.14 & -1.1 \\cdot 10^{3} & -5.4 \\cdot 10^{3}\\end{matrix}\\right]\\]\n\\[\\overrightarrow{E_{23}(\\frac{-14}{.9})} \\left[\\begin{matrix}0.0055 & 0.095 & 9.6 \\cdot 10^{2} & 5.0 \\cdot 10^{3}\\\\0 & -0.009 & -80.0 & -4.0 \\cdot 10^{2}\\\\0 & 0 & 1.4 \\cdot 10^{2} & 5.7 \\cdot 10^{2}\\end{matrix}\\right]\\]"
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#section-2",
    "href": "lectures/ch1_lecture1b.html#section-2",
    "title": "Intro to linear systems",
    "section": "",
    "text": "This has solutions \\(\\left[\\begin{matrix}5.7 \\cdot 10^{4}\\\\8.9 \\cdot 10^{3}\\\\4.0\\end{matrix}\\right]\\).\n\nHow do these compare to the exact solutions? These are\n\n\n\\(\\displaystyle \\left[\\begin{matrix}56753.6889897841\\\\8626.56072644719\\\\4.02951191827469\\end{matrix}\\right]\\)"
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#doing-it-again-rounding-after-15-digits",
    "href": "lectures/ch1_lecture1b.html#doing-it-again-rounding-after-15-digits",
    "title": "Intro to linear systems",
    "section": "Doing it again, rounding after 15 digits",
    "text": "Doing it again, rounding after 15 digits\n\\(\\left[\\begin{matrix}0.0055 & 0.095 & 960 & 5000\\\\0.0011 & 0.01 & 112 & 600\\\\0.0093 & 0.025 & 560 & 3000\\end{matrix}\\right] \\rightarrow \\left[\\begin{matrix}0.0055 & 0.095 & 960.0 & 5000.0\\\\0 & -0.009 & -80.0 & -400.0\\\\0.0093 & 0.025 & 560.0 & 3000.0\\end{matrix}\\right]\\)\n\\(\\rightarrow \\left[\\begin{matrix}0.0055 & 0.095 & 960.0 & 5000.0\\\\0 & -0.009 & -80.0 & -400.0\\\\0 & -0.135636363636364 & -1063.27272727273 & -5454.54545454545\\end{matrix}\\right] \\rightarrow \\left[\\begin{matrix}0.0055 & 0.095 & 960.0 & 5000.0\\\\0 & -0.009 & -80.0 & -400.0\\\\0 & 0 & 142.383838383838 & 573.737373737372\\end{matrix}\\right]\\)\nThis has solutions \\(\\left[\\begin{matrix}56753.6889897845\\\\8626.56072644727\\\\4.02951191827468\\end{matrix}\\right]\\).\nReminder, the exact solution was:\n\n\n\\(\\displaystyle \\left[\\begin{matrix}56753.6889897841\\\\8626.56072644719\\\\4.02951191827469\\end{matrix}\\right]\\)"
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#reminder-of-goals",
    "href": "lectures/ch1_lecture1b.html#reminder-of-goals",
    "title": "Intro to linear systems",
    "section": "Reminder of goals",
    "text": "Reminder of goals\nAn algorithm should be:\n\n\nfeasible\naccurate\n\nstable\n\nefficient\n\nreusable computations"
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#roundoff-errors-with-g-j-elimination",
    "href": "lectures/ch1_lecture1b.html#roundoff-errors-with-g-j-elimination",
    "title": "Intro to linear systems",
    "section": "Roundoff errors with G-J elimination",
    "text": "Roundoff errors with G-J elimination\nTry this on your calculator. What do you get?\n\\[\n\\left(\\left(\\frac{2}{3}+100\\right)-100\\right)-\\frac{2}{3}\\stackrel{?}{=}0\n\\]\n\nNow try this…\n\\[\n\\left(\\left(\\frac{2}{3}+100\\right)-100\\right)-\\frac{2}{3}\\stackrel{?}{=}0\n\\]\n\nOn my calculator, I get 3.33e-32. That’s a small number, but not zero!\nWhat is happening is that 2/3 = 0.666666…., and your calculator represents this as a very long string but eventually puts in a 7.\nThen when it adds the 100, it has to drop a few of those digits in its representation. They are then gone! When we subtract the 100, we don’t get them back again."
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#section-3",
    "href": "lectures/ch1_lecture1b.html#section-3",
    "title": "Intro to linear systems",
    "section": "",
    "text": "Let \\(\\epsilon\\) be a number so small that our calculator yields \\(1+\\epsilon=1\\).\nWith this calculator, \\(1+1 / \\epsilon=(\\epsilon+1) / \\epsilon=1 / \\epsilon\\)\nWant to solve the linear system\n\\[\n  \\begin{align*}\n  \\epsilon x_{1}+x_{2} & =1\\\\\n  x_{1}-x_{2} & =0 .\n  \\end{align*}\n\\]"
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#roundoff-errors-with-g-j-elimination-1",
    "href": "lectures/ch1_lecture1b.html#roundoff-errors-with-g-j-elimination-1",
    "title": "Intro to linear systems",
    "section": "Roundoff errors with G-J elimination",
    "text": "Roundoff errors with G-J elimination\nWith our calculator,\n\nIn the second step, we are replacing \\(-1+\\frac{1}{\\epsilon}\\) with just \\(\\frac{1}{\\epsilon}\\). It’s a case where we are adding together numbers of very different scale.\n\n\\[\n    \\left[\\begin{array}{rrr}\n    \\epsilon & 1 & 1 \\\\\n    1 & -1 & 0\n    \\end{array}\\right] \\overrightarrow{E_{21}\\left(-\\frac{1}{\\epsilon}\\right)}\\left[\\begin{array}{ccc}\n    \\epsilon & 1 & 1 \\\\\n    0 & \\frac{1}{\\epsilon} & -\\frac{1}{\\epsilon}\n    \\end{array}\\right] \\overrightarrow{E_{2}(\\epsilon)}\\left[\\begin{array}{ccc}\n    \\epsilon & 1 & 1 \\\\\n    0 & 1 & 1\n    \\end{array}\\right] \\\\ \\overrightarrow{E_{12}(-1)}\\left[\\begin{array}{ccc}\n    \\epsilon & 0 & 0 \\\\\n    0 & 1 & 1\n    \\end{array}\\right]  \\overrightarrow{E_{1}\\left(\\frac{1}{\\epsilon}\\right)}\\left[\\begin{array}{lll}\n    1 & 0 & 0 \\\\\n    0 & 1 & 1\n    \\end{array}\\right] .\n\\]\n\nCalculated solution: \\(x_{1}=0, x_{2}=1\\)\nCorrect answer should be\n\\[ x_{1}=x_{2}=\\frac{1}{1+\\epsilon}=0.999999099999990 \\ldots\\]"
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#sensitivity-to-small-changes",
    "href": "lectures/ch1_lecture1b.html#sensitivity-to-small-changes",
    "title": "Intro to linear systems",
    "section": "Sensitivity to small changes",
    "text": "Sensitivity to small changes\nProblem arose because we took a computational step where we added two numbers of very different scale, essentially losing the smaller number.\nLed to a big changes in output!\nThere is no general cure for these difficulties…\nWant to be aware of them, know when we are doing computations that might be susceptible."
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#partial-pivoting",
    "href": "lectures/ch1_lecture1b.html#partial-pivoting",
    "title": "Intro to linear systems",
    "section": "Partial pivoting",
    "text": "Partial pivoting\nWe can improve performance of G-J by introducing a new step into the algorithm:\n\nFind the entry in the left column with the largest absolute value. This entry is called the pivot. Perform row interchange (if necessary), so that the pivot is in the first row.\nUse a row operation to get a 1 as the entry in the first row and first column.\nUse row operations to make all other entries as zeros in column one.\nInterchange rows if necessary to obtain a nonzero number with the largest absolute value in the second row, second column. Use a row operation to make this entry 1. Use row operations to make all other entries as zeros in column two.\nRepeat step 4 for row 3, column 3. Continue moving along the main diagonal until you reach the last row, or until the number is zero.\n\n\nFull pivoting is where we also move the columns around to get the largest element to the front.\n” A square matrix 𝐴 has an 𝐿𝑈 factorization (without pivoting) if, and only if, no zero is encountered in the pivot position when computing an 𝐿𝑈 factorization of 𝐴. However, when does computations using floating point numbers a pivot that is nearly zero can lead to dramatic rounding errors. The simple workaround is to always permute the rows of the matrix such that the largest nonzero entry in a column is chosen as the pivot entry. This ensures that a nearly zero is never chosen. Complete pivoting goes even further by using row and column permutations to select the largest entry in the entire matrix as the pivot entry.” from here\nIt’s very rare to find yourself in a situation where you need complete pivoting, and sometimes it can be quite computationally expensive."
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#using-partial-pivoting-in-our-example",
    "href": "lectures/ch1_lecture1b.html#using-partial-pivoting-in-our-example",
    "title": "Intro to linear systems",
    "section": "Using partial pivoting in our example",
    "text": "Using partial pivoting in our example\n\nwork this one out by hand"
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#ill-conditioned-linear-systems-1",
    "href": "lectures/ch1_lecture1b.html#ill-conditioned-linear-systems-1",
    "title": "Intro to linear systems",
    "section": "Ill-conditioned linear systems",
    "text": "Ill-conditioned linear systems\n\nSometimes it’s not the procedure that introduces the susceptibility to small changes, but the actual problem itself. Then, small differences in the inputs lead to big differences in the exact solutions.\n\n\nA system of linear equations is said to be ill-conditioned when some small perturbation in the system (in the \\(b\\)s) can produce relatively large changes in the exact solution (in the \\(x\\)’s). Otherwise, the system is said to be well-conditioned."
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#ill-conditioned-linear-systems-2",
    "href": "lectures/ch1_lecture1b.html#ill-conditioned-linear-systems-2",
    "title": "Intro to linear systems",
    "section": "Ill-conditioned linear systems",
    "text": "Ill-conditioned linear systems\n\n\n\nConsider \\[\n\\begin{aligned}\n& .835 x+.667 y=.168, \\\\\n& .333 x+.266 y=.067,\n\\end{aligned}\n\\]\nExact solution:\n\\[\nx=1 \\quad \\text { and } \\quad y=-1 .\n\\]\n\n\nBut if we change just one digit…\n\\[\n\\begin{aligned}\n& .835 x+.667 y=.168, \\\\\n& .333 x+.266 y=.06\\color{red}{6},\n\\end{aligned}\n\\]\nNow exact solution:\n\\[\n\\hat{x}=-666 \\quad \\text { and } \\quad \\hat{y}=834\n\\]\n\n\n\n\nWhat’s going on here? One thing to notice is that the lines corresponding to solutions for each equation have very similar slopes: r .667/.835 vs r .266/.33.\n\n\nMeyer Ch 1.6"
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#section-4",
    "href": "lectures/ch1_lecture1b.html#section-4",
    "title": "Intro to linear systems",
    "section": "",
    "text": "What if \\(b_1\\) and \\(b_2\\) are the results of an experiment, need to be read off a dial? Suppose:\n\ndial can be read to tolerance of \\(\\pm .001\\),\nvalues for \\(b_{1}\\) and \\(b_{2}\\) are read as .168 and .067, respectively. Then the exact solution is\n\n\n\\[\n\\begin{equation*}\n(x, y)=(1,-1)\n\\end{equation*}\n\\]\n\n\nBut due to uncertainty, we have\n\n\n\\[\n\\begin{equation*}\n.167 \\leq b_{1} \\leq .169 \\quad \\text { and } \\quad .066 \\leq b_{2} \\leq .068\n\\end{equation*}\n\\]"
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#section-5",
    "href": "lectures/ch1_lecture1b.html#section-5",
    "title": "Intro to linear systems",
    "section": "",
    "text": "Possible readings\n\n\n\\(b_1\\)\n\\(b_2\\)\n\\(x\\)\n\\(y\\)\n\n\n\n\n.168\n.067\n1\n-1\n\n\n.167\n.068\n934\n-1169\n\n\n.169\n.066\n-932\n1169"
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#geometrical-interpretation",
    "href": "lectures/ch1_lecture1b.html#geometrical-interpretation",
    "title": "Intro to linear systems",
    "section": "Geometrical interpretation",
    "text": "Geometrical interpretation\nIf two straight lines are almost parallel and if one of the lines is moved only slightly, then the point of intersection is drastically altered.\n\nThe point of intersection is the solution of the associated \\(2 \\times 2\\) linear system, so this is also drastically altered."
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#section-6",
    "href": "lectures/ch1_lecture1b.html#section-6",
    "title": "Intro to linear systems",
    "section": "",
    "text": "Often in real life, coefficients are empirically obtained\nWill be off from “true” values by small amounts\nFor ill-conditioned systems, this means that solutions can be very far off from true solutions\nWe’ll cover techniques for quantifying conditioning, later in quarter\nFor now, can just try making small changes to some coefficients. Big changes in result? Ill-conditioned system!"
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#polynomial-interpolation",
    "href": "lectures/ch1_lecture1b.html#polynomial-interpolation",
    "title": "Intro to linear systems",
    "section": "Polynomial interpolation",
    "text": "Polynomial interpolation\n\nSuppose we’d like to find a polynomial that can interpolate a given function.\nTake points 𝑥0,…𝑥𝑁 and values 𝑦0,…𝑦𝑁\nSimple way: finding the coefficients $ c_1, c_n $ where\n\\[\nP(x) = \\sum_{i=0}^N c_i x^i\n\\]\n\nThis is a system of equations:\n\\[\n\\begin{array}\n    cy_0 = c_0 + c_1 x_0 + \\ldots c_N x_0^N\\\\\n    \\,\\ldots\\\\\n    \\,y_N = c_0 + c_1 x_N + \\ldots c_N x_N^N\n\\end{array}\n\\]\n\n\nhttps://colab.research.google.com/github/quantecon/lecture-julia.notebooks/blob/main/tools_and_techniques/iterative_methods_sparsity.ipynb#scrollTo=9aa1791b"
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#polynomial-interpolation-1",
    "href": "lectures/ch1_lecture1b.html#polynomial-interpolation-1",
    "title": "Intro to linear systems",
    "section": "Polynomial interpolation",
    "text": "Polynomial interpolation\nOr, stacking, \\(c = \\begin{bmatrix} c_0 & \\ldots & c_N\\end{bmatrix}\\), \\(y = \\begin{bmatrix} y_0 & \\ldots & y_N\\end{bmatrix}\\) , and\n\\[\nA = \\begin{bmatrix} 1 & x_0 & x_0^2 & \\ldots &x_0^N\\\\\n                    \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n                    1 & x_N & x_N^2 & \\ldots & x_N^N\n    \\end{bmatrix}\n\\]\nLet’s try solving this for a simple function, \\(y=exp(x)\\)."
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#polynomial-interpolation-2",
    "href": "lectures/ch1_lecture1b.html#polynomial-interpolation-2",
    "title": "Intro to linear systems",
    "section": "Polynomial interpolation",
    "text": "Polynomial interpolation\n\\[\nx*y\n\\]\nWe’ll start with picking 4 interpolating points: \\(\\vec x = \\mathtt{\\text{[ 1.          4.66666667  8.33333333 12.        ]}}\\). Then we have our matrix A:\n\\[A = \\begin{bmatrix} 1 & x_0 & x_0^2 & \\ldots &x_0^N\\\\\n                    \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n                    1 & x_N & x_N^2 & \\ldots & x_N^N\n    \\end{bmatrix} = \\left[\\begin{matrix}1.0 & 1.0 & 1.0 & 1.0\\\\1.0 & 4.66666666666667 & 21.7777777777778 & 101.62962962963\\\\1.0 & 8.33333333333333 & 69.4444444444444 & 578.703703703703\\\\1.0 & 12.0 & 144.0 & 1728.0\\end{matrix}\\right] \\]\nOur \\(y\\) values are \\(\\vec y = \\mathtt{\\text{[     2.71828183    106.3426754    4160.26200538 162754.791419  ]}}\\). We could solve this using Gauss-Jordan elimination, or here the solving algorithm built into Numpy."
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#results-for-n4-and-n10-points",
    "href": "lectures/ch1_lecture1b.html#results-for-n4-and-n10-points",
    "title": "Intro to linear systems",
    "section": "Results for n=4 and n=10 points",
    "text": "Results for n=4 and n=10 points"
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#max-error-for-different-values-of-n",
    "href": "lectures/ch1_lecture1b.html#max-error-for-different-values-of-n",
    "title": "Intro to linear systems",
    "section": "Max error for different values of n",
    "text": "Max error for different values of n"
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#the-problem-linear-dependency",
    "href": "lectures/ch1_lecture1b.html#the-problem-linear-dependency",
    "title": "Intro to linear systems",
    "section": "The problem: linear dependency",
    "text": "The problem: linear dependency\n\n\n\n\n\n\n\n\n\n\nAs n gets higher, \\(x^n\\) gets closer to \\(x^{n+1}\\) (at least in the range \\((0,1)\\)). This means that a solution of \\(a_n x^n + a_{n+1} x^{n+1}\\) will be very similar to e.g. \\(a_{n+1} x^n + a_{n} x^{n+1}\\)"
  },
  {
    "objectID": "lectures/ch1_lecture2.html#a-resource-for-review",
    "href": "lectures/ch1_lecture2.html#a-resource-for-review",
    "title": "More Systems of Linear Equations",
    "section": "A resource for review",
    "text": "A resource for review\nThe glossary from the end of Strand: glossary.pdf"
  },
  {
    "objectID": "lectures/ch1_lecture2.html#systems-with-non-unique-solutions",
    "href": "lectures/ch1_lecture2.html#systems-with-non-unique-solutions",
    "title": "More Systems of Linear Equations",
    "section": "Systems with non-unique solutions",
    "text": "Systems with non-unique solutions\nSolve for the variables \\(x, y\\), and \\(z\\) in the system\n\\[\n\\begin{aligned}\nz & =2 \\\\\nx+y+z & =2 \\\\\n2 x+2 y+4 z & =8\n\\end{aligned}\n\\]\n\nMake an augmented matrix: Example 1.20. Solve for the variables \\(x, y\\), and \\(z\\) in the system\n\\[\n\\left[\\begin{array}{llll}\n0 & 0 & 1 & 2 \\\\\n1 & 1 & 1 & 2 \\\\\n2 & 2 & 4 & 8\n\\end{array}\\right]\n\\]\n\nNote that the first column corresponds to x, the second to y, and the third to z.\n\n\n\n\\[\n\\left[\\begin{array}{llll}\n0 & 0 & 1 & 2 \\\\\n1 & 1 & 1 & 2 \\\\\n2 & 2 & 4 & 8\n\\end{array}\\right] \\stackrel{E_{12}}{\\longrightarrow}\\left[\\begin{array}{llll}\n(1 & 1 & 1 & 2 \\\\\n0 & 0 & 1 & 2 \\\\\n2 & 2 & 4 & 8\n\\end{array}\\right] \\xrightarrow[E_{31}(-2)]{\\longrightarrow}\\left[\\begin{array}{rlll}\n1 & 1 & 1 & 2 \\\\\n0 & 0 & 2 & 4 \\\\\n0 & 0 & 1 & 2\n\\end{array}\\right]\n\\]\n\nNow we are stuck! The first row we are going to use to solve for x. Neither the second or third row tell us anything about y…"
  },
  {
    "objectID": "lectures/ch1_lecture2.html#section",
    "href": "lectures/ch1_lecture2.html#section",
    "title": "More Systems of Linear Equations",
    "section": "",
    "text": "We keep on going… \\[\n\\begin{aligned}\n& {\\left[\\begin{array}{rrrr}\n(1) & 1 & 1 & 2 \\\\\n0 & 0 & 2 & 4 \\\\\n0 & 0 & 1 & 2\n\\end{array}\\right] \\xrightarrow[E_{2}(1 / 2)]{\\longrightarrow}\\left[\\begin{array}{rrrr}\n1 & 1 & 1 & 2 \\\\\n0 & 0 & 1 & 2 \\\\\n0 & 0 & 1 & 2\n\\end{array}\\right]} \\\\\n& \\overrightarrow{E_{32}(-1)}\\left[\\begin{array}{rrrr}\n(1) & 1 & 1 & 2 \\\\\n0 & 0 & 1 & 2 \\\\\n0 & 0 & 0 & 0\n\\end{array}\\right] \\xrightarrow[E_{12}(-1)]{\\longrightarrow}\\left[\\begin{array}{rrrr}\n(1) & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 2 \\\\\n0 & 0 & 0 & 0\n\\end{array}\\right] .\n\\end{aligned}\n\\]\nThere’s still no information on \\(y\\).\n\\[\n\\begin{aligned}\n& x=-y \\\\\n& z=2 \\\\\n& y \\text { is free. }\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lectures/ch1_lecture2.html#free-versus-bound-variables",
    "href": "lectures/ch1_lecture2.html#free-versus-bound-variables",
    "title": "More Systems of Linear Equations",
    "section": "Free versus bound variables",
    "text": "Free versus bound variables\nSuppose we have another augmented matrix, which after GJ elimination has become:\n\\[\n\\left[\\begin{array}{rrrrr}\nx & y & z & w & \\mathrm{rhs} \\\\\n1 & 2 & 0 & -1 & 2 \\\\\n0 & 0 & 1 & 3 & 0 \\\\\n0 & 0 & 0 & 0 & 0\n\\end{array}\\right] .\n\\]\n\nSolutions are: \\[\n\\begin{aligned}\nx+2 y-w & =2 \\\\\nz+3 w & =0 .\n\\end{aligned}\n\\]\nHow can we tell from looking at the matrix which variables are free vs bound?\nColumns which contain pivots correspond to bound variables. The others correspond to free variables.\n(Columns with pivots are called “basic” columns.)"
  },
  {
    "objectID": "lectures/ch1_lecture2.html#zero-solutions",
    "href": "lectures/ch1_lecture2.html#zero-solutions",
    "title": "More Systems of Linear Equations",
    "section": "Zero solutions",
    "text": "Zero solutions\nSolve this system… \\[\n\\begin{array}{r}\nx+y=1 \\\\\n2 x+y=2 \\\\\n3 x+2 y=5 .\n\\end{array}\n\\]\n\n\\[\n\\left[\\begin{array}{lll}\n1 & 1 & 1 \\\\\n2 & 1 & 2 \\\\\n3 & 2 & 5\n\\end{array}\\right] \\xrightarrow[E_{21}(-2)]{E_{31}(-3)}\\left[\\begin{array}{lrl}\n1 & 1 & 1 \\\\\n0 & -1 & 0 \\\\\n0 & -1 & 2\n\\end{array}\\right] \\xrightarrow[E_{32}(-1)]{\\longrightarrow}\\left[\\begin{array}{rrr}\n1 & 1 & 1 \\\\\n0 & -1 & 0 \\\\\n0 & 0 & 2\n\\end{array}\\right]\n\\]\n\nWe didn’t even finish the G-J elimination, but there’s clearly a problem. We can’t have 0 = 2. Maybe not a surprise, since we have three equations with only two unknowns!\n\n\n\nSystem is inconsistent."
  },
  {
    "objectID": "lectures/ch1_lecture2.html#consistency",
    "href": "lectures/ch1_lecture2.html#consistency",
    "title": "More Systems of Linear Equations",
    "section": "Consistency",
    "text": "Consistency\nFor an augmented matrix in RREF, if the only only nonzero entry in a row appears on the right-hand side,\n\\[\\text { Row } i \\longrightarrow\\left(\n\\begin{array}{cccccc|c}\n* & * & * & * & * & * & * \\\\\n0 & 0 & 0 & * & * & * & * \\\\\n0 & 0 & 0 & 0 & * & * & * \\\\\n0 & 0 & 0 & 0 & 0 & 0 & \\alpha \\\\\n\\bullet & \\bullet & \\bullet & \\bullet & \\bullet & \\bullet & \\bullet \\\\\n\\bullet & \\bullet & \\bullet & \\bullet & \\bullet & \\bullet & \\bullet\n\\end{array}\n\\right) \\longleftarrow \\alpha \\neq 0\n\\]\nthe \\(i^{\\text {th }}\\) equation of the associated system is\n\\[\n0 x_{1}+0 x_{2}+\\cdots+0 x_{n}=\\alpha .\n\\]\nIf \\(\\alpha\\ne 0\\), this has no solution!\nThis is an inconsistent system.\n\nNote that there is not inconsistency if there is a row of all zeros."
  },
  {
    "objectID": "lectures/ch1_lecture2.html#what-reduced-row-echelon-form-tells-us",
    "href": "lectures/ch1_lecture2.html#what-reduced-row-echelon-form-tells-us",
    "title": "More Systems of Linear Equations",
    "section": "What Reduced Row Echelon Form tells us",
    "text": "What Reduced Row Echelon Form tells us\nSuppose a matrix \\(A_{m \\times n}\\) has a reduced row echelon form \\(E_{m \\times n}\\): \\[ E_A=\\left(\\begin{array}{cccccccc}1 & * & 0 & 0 & * & * & 0 & * \\\\ 0 & 0 & 1 & 0 & * & * & 0 & * \\\\ 0 & 0 & 0 & 1 & * & * & 0 & * \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 1 & * \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\end{array}\\right)\\]\n\nWhat can we learn from the RREF of a matrix? What does it tell us? It can tell us which variables are free vs bound, and therefore how many variables are bound. (This is the rank of the matrix!)"
  },
  {
    "objectID": "lectures/ch1_lecture2.html#which-variables-are-free-and-bound",
    "href": "lectures/ch1_lecture2.html#which-variables-are-free-and-bound",
    "title": "More Systems of Linear Equations",
    "section": "Which variables are free and bound",
    "text": "Which variables are free and bound\nSuppose a matrix \\(A_{m \\times n}\\) has a reduced row echelon form \\(E_{m \\times n}\\): \\[ E_A=\\left(\\begin{array}{cccccccc}1 & * & 0 & 0 & * & * & 0 & * \\\\ 0 & 0 & 1 & 0 & * & * & 0 & * \\\\ 0 & 0 & 0 & 1 & * & * & 0 & * \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 1 & * \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\end{array}\\right)\\]\n“Basic columns” correspond to the columns in \\(A\\) which contain the pivotal positions.\nEach non-basic column can be formed as a sum of the basic columns to the left.\nBasic columns are the same in \\(E\\) and \\(A\\)!"
  },
  {
    "objectID": "lectures/ch1_lecture2.html#rank",
    "href": "lectures/ch1_lecture2.html#rank",
    "title": "More Systems of Linear Equations",
    "section": "Rank",
    "text": "Rank\nSuppose a matrix \\(A_{m \\times n}\\) has a reduced row echelon form \\(E_{m \\times n}\\): \\[ E_A=\\left(\\begin{array}{cccccccc}1 & * & 0 & 0 & * & * & 0 & * \\\\ 0 & 0 & 1 & 0 & * & * & 0 & * \\\\ 0 & 0 & 0 & 1 & * & * & 0 & * \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 1 & * \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\end{array}\\right)\\]\nrank(\\(A\\)) = # of pivots = # of nonzero rows in \\(E\\) = # bound variables in the system"
  },
  {
    "objectID": "lectures/ch1_lecture2.html#nullity",
    "href": "lectures/ch1_lecture2.html#nullity",
    "title": "More Systems of Linear Equations",
    "section": "Nullity",
    "text": "Nullity\nSuppose a matrix \\(A_{m \\times n}\\) has a reduced row echelon form \\(E_{m \\times n}\\): \\[ E_A=\\left(\\begin{array}{cccccccc}1 & * & 0 & 0 & * & * & 0 & * \\\\ 0 & 0 & 1 & 0 & * & * & 0 & * \\\\ 0 & 0 & 0 & 1 & * & * & 0 & * \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 1 & * \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\end{array}\\right)\\]\nnullity(\\(A\\)) = # of non-pivot columns = # free variables in the system"
  },
  {
    "objectID": "lectures/ch1_lecture2.html#consistency-1",
    "href": "lectures/ch1_lecture2.html#consistency-1",
    "title": "More Systems of Linear Equations",
    "section": "Consistency",
    "text": "Consistency\nEquivalent ways of saying \\(A\\) is consistent:\n\nIn row reducing \\([\\mathbf{A} \\mid \\mathbf{b}]\\), a row of the following form never appears:\n\n. . . \\[\n\\left(\\begin{array}{llll|l}\n0 & 0 & \\cdots & 0 & \\alpha \\tag{2.3.2}\n\\end{array}\\right) \\text {, where } \\alpha \\neq 0 \\text {. }\n\\]\n\n\\(\\operatorname{rank}[\\mathbf{A} \\mid \\mathbf{b}]=\\operatorname{rank}(\\mathbf{A})\\), in which case either:\n\n\\(\\operatorname{rank}[\\mathbf{A}=n\\), and system has a unique solution or\n\\(\\operatorname{rank}[\\mathbf{A}&lt;n\\), and system has infinite solutions\n\n\\(\\mathbf{b}\\) is a nonbasic column in \\([\\mathbf{A} \\mid \\mathbf{b}]\\).\n\\(\\mathbf{b}\\) is a combination of the basic columns in \\(\\mathbf{A}\\).\n\n\n\nMeyers p. 51"
  },
  {
    "objectID": "lectures/ch1_lecture2.html#homogeneous-systems",
    "href": "lectures/ch1_lecture2.html#homogeneous-systems",
    "title": "More Systems of Linear Equations",
    "section": "Homogeneous Systems",
    "text": "Homogeneous Systems\n\nThe general linear system with \\(m \\times n\\) coefficient matrix \\(A\\) and right-hand-side vector \\(\\mathbf{b}\\) is homogeneous if the entries of \\(\\mathbf{b}\\) are all zero.\nOtherwise, the system is inhomogeneous.\nHomogeneous systems are always consistent!\nCan solve by setting all variables to zero. This is the trivial solution\nIf rank of the matrix = n, there is only the trivial solution"
  },
  {
    "objectID": "lectures/ch1_lecture2.html#pagerank",
    "href": "lectures/ch1_lecture2.html#pagerank",
    "title": "More Systems of Linear Equations",
    "section": "Pagerank",
    "text": "Pagerank\nGoal: rank pages in terms of “importance”. Suppose we have four pages:\n\n\n\n\nHow can we rank these pages? Which one is most important?\n\n\n\n\nFirst try: count number of backlinks. But that can be artifically inflated (just link to a page many times), and it doesn’t care about importance of the linking page.\nSecond try: For page \\(i\\) let \\(x_{i}\\) be its score and \\(L_{i}\\) be the set of all indices of pages linking to it. Then \\[\n\\begin{equation*}\nx_{i}=\\sum_{x_{j} \\in L_{i}} x_{j}\n\\end{equation*}\n\\]\nBut it’s still easy to game this system, by setting up a page and making a ton of outgoing links to pages we want to promote.\nThird try: For page \\(j\\) let \\(n_{j}\\) be its total number of outgoing links on that page. Then score is\n\\[\n\\begin{equation*}\nx_{i}=\\sum_{x_{j} \\in L_{i}} \\frac{x_{j}}{n_{j}}\n\\end{equation*}\n\\]"
  },
  {
    "objectID": "lectures/ch1_lecture2.html#working-through-pagerank-try-3",
    "href": "lectures/ch1_lecture2.html#working-through-pagerank-try-3",
    "title": "More Systems of Linear Equations",
    "section": "Working through PageRank: try 3",
    "text": "Working through PageRank: try 3\n\n\\[\n\\begin{equation*}\nx_{i}=\\sum_{x_{j} \\in L_{i}} \\frac{x_{j}}{n_{j}}\n\\end{equation*}\n\\]\n\n\n\nWrite down the system of equations this gives us. Then make it in an augmented matrix.\n\n\n\n\n\\[\n\\begin{aligned}\n& x_{1}=\\frac{x_{2}}{1}+\\frac{x_{3}}{3} \\\\\n& x_{2}=\\frac{x_{1}}{2}+\\frac{x_{3}}{3} \\\\\n& x_{3}=\\frac{x_{1}}{2}+\\frac{x_{4}}{1} \\\\\n& x_{4}=\\frac{x_{3}}{3} .\n\\end{aligned}\n\\]\nAugmented matrix:\n\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & -1 & -1 & 0 & 0\\\\-1 & 1 & -1 & 0 & 0\\\\-1 & 0 & 1 & -1 & 0\\\\0 & 0 & -1 & 1 & 0\\end{matrix}\\right]\\)"
  },
  {
    "objectID": "lectures/ch1_lecture2.html#jupyterlab",
    "href": "lectures/ch1_lecture2.html#jupyterlab",
    "title": "More Systems of Linear Equations",
    "section": "JupyterLab",
    "text": "JupyterLab\nhttps://tinyurl.com/stat24320notebook1"
  },
  {
    "objectID": "lectures/ch1_lecture2.html#now-what-about-the-second-try",
    "href": "lectures/ch1_lecture2.html#now-what-about-the-second-try",
    "title": "More Systems of Linear Equations",
    "section": "Now what about the second try?",
    "text": "Now what about the second try?\nRemember this was our ‘second try’: \\[\n\\begin{equation*}\nx_{i}=\\sum_{x_{j} \\in L_{i}} x_{j}\n\\end{equation*}\n\\]\nThis was the system of equations. \\[\n\\begin{aligned}\n& x_{1}=x_{2}+x_{3} \\\\\n& x_{2}=x_{1}+x_{3} \\\\\n& x_{3}=x_{1}+x_{4} \\\\\n& x_{4}=x_{3} .\n\\end{aligned}\n\\]\nWhat happens if we try to solve this? Try it in Python.\n\nWhat happens is that we get only the trivial solution. This makes sense because it is a homogeneous system with full rank."
  },
  {
    "objectID": "lectures/ch1_lecture2.html#discretizing-continuous-functions",
    "href": "lectures/ch1_lecture2.html#discretizing-continuous-functions",
    "title": "More Systems of Linear Equations",
    "section": "Discretizing continuous functions",
    "text": "Discretizing continuous functions\n\nRod with fixed temperatures \\(y_{left}\\) and \\(y_{right}\\) at each end, internal heat source given as \\(f(x), 0 \\leq x \\leq 1\\)\nWhat’s the temperature at each point along the length?"
  },
  {
    "objectID": "lectures/ch1_lecture2.html#section-1",
    "href": "lectures/ch1_lecture2.html#section-1",
    "title": "More Systems of Linear Equations",
    "section": "",
    "text": "Discrete approximation to temperature function \\((n=5)\\)\n\n\n\n\n\nequally spaced points, called nodes, \\(x_{0}=0, x_{1}, x_{2}, \\ldots, x_{n+1}=1\\), distance \\(h\\) apart.\nheat on the \\(i\\) th segment \\(\\approx\\) temperature at its left node.\ntemperature is \\(y(x)\\), \\(y_{i}=y\\left(x_{i}\\right)\\).\n\n\n\n\nEquations to balance the flow of heat from each node to its neighbors:\n\\[\n\\begin{equation*}\n-y_{i-1}+2 y_{i}-y_{i+1}=\\frac{h^{2}}{K} f\\left(x_{i}\\right)\n\\end{equation*}\n\\]"
  },
  {
    "objectID": "lectures/ch1_lecture2.html#section-2",
    "href": "lectures/ch1_lecture2.html#section-2",
    "title": "More Systems of Linear Equations",
    "section": "",
    "text": "Build the matrix representing the system of equations:\n\nN=6\nM = sym.zeros(N)\nfor i in range(N):\n  if (i&gt;0):\n      M[i-1,i]=-1\n  M[i,i]=2\n  if (i&lt;N-1):\n      M[i+1,i]=-1\nM\n\n\\(\\displaystyle \\left[\\begin{matrix}2 & -1 & 0 & 0 & 0 & 0\\\\-1 & 2 & -1 & 0 & 0 & 0\\\\0 & -1 & 2 & -1 & 0 & 0\\\\0 & 0 & -1 & 2 & -1 & 0\\\\0 & 0 & 0 & -1 & 2 & -1\\\\0 & 0 & 0 & 0 & -1 & 2\\end{matrix}\\right]\\)"
  },
  {
    "objectID": "lectures/ch1_lecture2.html#section-3",
    "href": "lectures/ch1_lecture2.html#section-3",
    "title": "More Systems of Linear Equations",
    "section": "",
    "text": "\\[\n\\begin{equation*}\n-y_{i-1}+2 y_{i}-y_{i+1}=\\frac{h^{2}}{K} f\\left(x_{i}\\right)\n\\end{equation*}\n\\]\nDecide on a function for f. Let’s say \\(\\frac{h^{2}}{K} f = x_i^2\\).\nSay that x is running from 0 to 1. Then we can have\n\nimport numpy as np\nx=np.arange(0,1,1/N)\nf=x**2\nx=sym.Matrix(x)\nf=sym.Matrix(f)"
  },
  {
    "objectID": "lectures/ch1_lecture2.html#section-4",
    "href": "lectures/ch1_lecture2.html#section-4",
    "title": "More Systems of Linear Equations",
    "section": "",
    "text": "sol=M.gauss_jordan_solve(f)\nsol[0]\n\n\\(\\displaystyle \\left[\\begin{matrix}0.416666666666667\\\\0.833333333333333\\\\1.22222222222222\\\\1.5\\\\1.52777777777778\\\\1.11111111111111\\end{matrix}\\right]\\)"
  },
  {
    "objectID": "lectures/ch1_lecture2.html#now-you",
    "href": "lectures/ch1_lecture2.html#now-you",
    "title": "More Systems of Linear Equations",
    "section": "Now you",
    "text": "Now you\n\nImplement this yourself in Python\nMake a plot of the output (you’ll want to use PyPlot)\nIncrease N and make more plots\nThis was assuming the temperature at the ends was 0. How can we change this?"
  },
  {
    "objectID": "lectures/ch1_lecture2.html#turing-pattern-formation",
    "href": "lectures/ch1_lecture2.html#turing-pattern-formation",
    "title": "More Systems of Linear Equations",
    "section": "Turing pattern formation",
    "text": "Turing pattern formation\nhttps://colab.research.google.com/github/ijmbarr/turing-patterns/blob/master/turing-patterns.ipynb#scrollTo=GsEPBsz33E14\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n \n\n\n \n\n\n\n\n \n\n\n\nDiscrete approximation to temperature function \\((n=5)\\)"
  },
  {
    "objectID": "lectures/ch2_lecture2.html#definitions",
    "href": "lectures/ch2_lecture2.html#definitions",
    "title": "Ch2 Lecture 2",
    "section": "Definitions",
    "text": "Definitions\nDefinition: A discrete linear dynamical system is a sequence of vectors \\(\\mathbf{x}^{(k)}, k=0,1, \\ldots\\), called states, which is defined by an initial vector \\(\\mathbf{x}^{(0)}\\) and by the rule\n\\[\n\\mathbf{x}^{(k+1)}=A \\mathbf{x}^{(k)}+\\mathbf{b}_{k}, \\quad k=0,1, \\ldots\n\\]\n\n\n\\(A\\) is a fixed square matrix, called the transition matrix of the system\nvectors \\(\\mathbf{b}_{k}, k=0,1, \\ldots\\) are called the input vectors of the system.\nIf we don’t specify input vectors, assume that \\(\\mathbf{b}_{k}=\\mathbf{0}\\) for all \\(k\\). Then call the system a homogeneous dynamical system."
  },
  {
    "objectID": "lectures/ch2_lecture2.html#stability",
    "href": "lectures/ch2_lecture2.html#stability",
    "title": "Ch2 Lecture 2",
    "section": "Stability",
    "text": "Stability\nAn important question: is the system stable?\nDoes \\(\\mathbf{x}^{(k)}\\) tend towards a constant state \\(\\mathrm{x}\\)?\n\nIf system is homogeneous, then if a stable state is the initial state, it will equal all subsequent states.\nDefinition: A vector \\(\\mathbf{x}\\) satisfying \\(\\mathbf{x}=A \\mathbf{x}\\), for a square matrix \\(A\\), is called a stationary vector for \\(A\\).\n\n\nIf \\(A\\) is the transition matrix for a homogeneous discrete dynamical system, we also call such a vector a stationary state."
  },
  {
    "objectID": "lectures/ch2_lecture2.html#example",
    "href": "lectures/ch2_lecture2.html#example",
    "title": "Ch2 Lecture 2",
    "section": "Example",
    "text": "Example\n\nTwo toothpaste companies compete for customers in a fixed market\nEach customer uses either Brand A or Brand B.\nBuying habits. In each quarter:\n\n\\(30 \\%\\) of A users will switch to B, while the rest stay with A.\n\\(40 \\%\\) of B users will switch to A , while the the rest stay with B.\n\nThis is an example of a Markov chain model."
  },
  {
    "objectID": "lectures/ch2_lecture2.html#section",
    "href": "lectures/ch2_lecture2.html#section",
    "title": "Ch2 Lecture 2",
    "section": "",
    "text": "Let \\(a_1\\) be the fraction of customers using Brand A at the end of the first quarter, and \\(b_1\\) be the fraction using Brand B.\nThen we have the following system of equations: \\[\n\\begin{aligned}\na_{1} & =0.7 a_{0}+0.4 b_{0} \\\\\nb_{1} & =0.3 a_{0}+0.6 b_{0}\n\\end{aligned}\n\\]\n\nMore generally,\n\\[\n\\begin{aligned}\na_{k+1} & =0.7 a_{k}+0.4 b_{k} \\\\\nb_{k+1} & =0.3 a_{k}+0.6 b_{k} .\n\\end{aligned}\n\\]\n\n\nIn matrix form,\n\\[\n\\mathbf{x}^{(k)}=\\left[\\begin{array}{c}\na_{k} \\\\\nb_{k}\n\\end{array}\\right] \\text { and } A=\\left[\\begin{array}{ll}\n0.7 & 0.4 \\\\\n0.3 & 0.6\n\\end{array}\\right]\n\\]\nwith\n\\[\n\\mathbf{x}^{(k+1)}=A \\mathbf{x}^{(k)}\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture2.html#section-1",
    "href": "lectures/ch2_lecture2.html#section-1",
    "title": "Ch2 Lecture 2",
    "section": "",
    "text": "We can continue into future quarters by multiplying by \\(A\\) again: \\[\n\\mathbf{x}^{(2)}=A \\mathbf{x}^{(1)}=A \\cdot\\left(A \\mathbf{x}^{(0)}\\right)=A^{2} \\mathbf{x}^{(0)}\n\\]\n\nIn general,\n\\[\n\\mathbf{x}^{(k)}=A \\mathbf{x}^{(k-1)}=A^{2} \\mathbf{x}^{(k-2)}=\\cdots=A^{k} \\mathbf{x}^{(0)} .\n\\]\n\n\nThis is true of any homogeneous linear dynamical system!\nFor any positive integer \\(k\\) and discrete dynamical system with transition matrix \\(A\\) and initial state \\(\\mathbf{x}^{(0)}\\), the \\(k\\)-th state is given by\n\\[\n\\mathbf{x}^{(k)}=A^{k} \\mathbf{x}^{(0)}\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture2.html#distribution-vector-and-stochastic-matrix",
    "href": "lectures/ch2_lecture2.html#distribution-vector-and-stochastic-matrix",
    "title": "Ch2 Lecture 2",
    "section": "Distribution vector and stochastic matrix",
    "text": "Distribution vector and stochastic matrix\n\n\\(\\mathbf{x}^{(k)}\\) of the toothpaste example are column vectors with nonnegative coordinates that sum to 1.\nSuch vectors are called distribution vectors.\nAlso, each of the columns the matrix \\(A\\) is a distribution vector.\nA square matrix \\(A\\) whose columns are distribution vectors is called a stochastic matrix."
  },
  {
    "objectID": "lectures/ch2_lecture2.html#markov-chain-definition",
    "href": "lectures/ch2_lecture2.html#markov-chain-definition",
    "title": "Ch2 Lecture 2",
    "section": "Markov Chain definition",
    "text": "Markov Chain definition\nA Markov chain is a discrete dynamical system whose initial state \\(\\mathbf{x}^{(0)}\\) is a distribution vector and whose transition matrix \\(A\\) is stochastic, i.e., each column of \\(A\\) is a distribution vector.\n\nThink this through. - Suppose our current state is \\(\\mathbf{e}_{j}\\) - The system has selected \\(j\\) th event exclusively.\n- The next state is \\(\\mathbf{p}_{j}=P \\mathbf{e}_{j}\\), that is, the \\(j\\) th column of \\(P\\). - This implies that the entry \\(p_{i j}\\) is the probability that the \\(i\\) th event will occur, given that the \\(j\\) th event has just occurred. - Since events are mutually exclusive and some subsequent event must occur, the sum of these probabilities is 1 ."
  },
  {
    "objectID": "lectures/ch2_lecture2.html#checking-that-our-toothpase-example-is-a-markov-chain",
    "href": "lectures/ch2_lecture2.html#checking-that-our-toothpase-example-is-a-markov-chain",
    "title": "Ch2 Lecture 2",
    "section": "Checking that our toothpase example is a Markov chain",
    "text": "Checking that our toothpase example is a Markov chain\n\\[\n\\mathbf{x}^{(k)}=\\left[\\begin{array}{c}\na_{k} \\\\\nb_{k}\n\\end{array}\\right] \\text { and } A=\\left[\\begin{array}{ll}\n0.7 & 0.4 \\\\\n0.3 & 0.6\n\\end{array}\\right]\n\\]\n\n\nThe toothpaste example is a Markov chain:\n\nThe columns of \\(A\\) are distribution vectors.\nThe columns of \\(A\\) sum to 1.\nThe events in the example are mutually exclusive."
  },
  {
    "objectID": "lectures/ch2_lecture2.html#stochastic-matrix-inequality",
    "href": "lectures/ch2_lecture2.html#stochastic-matrix-inequality",
    "title": "Ch2 Lecture 2",
    "section": "Stochastic Matrix Inequality",
    "text": "Stochastic Matrix Inequality\n\nWe can define the 1-norm of a vector \\(\\mathbf{x}\\) as \\(\\|\\mathbf{x}\\|_{1}=\\sum_{i=1}^{n}\\left|x_{i}\\right|\\).\n\nIf all the elements of a vector are nonnegative, then \\(\\|\\mathbf{x}\\|_{1}\\) is the sum of the elements.\n\nCan show: For any stochastic matrix \\(P\\) and compatible vector \\(\\mathbf{x},\\|P \\mathbf{x}\\|_{1} \\leq\\|\\mathbf{x}\\|_{1}\\), with equality if the coordinates of \\(\\mathbf{x}\\) are all nonnegative.\n→ if a state in a Markov chain is a distribution vector (nonnegative entries and sums to 1), then the sum of the coordinates of the next state will also sum to one\n→ all subsequent states in a Markov chain are themselves Markov Chain State distribution vectors."
  },
  {
    "objectID": "lectures/ch2_lecture2.html#moving-into-the-future",
    "href": "lectures/ch2_lecture2.html#moving-into-the-future",
    "title": "Ch2 Lecture 2",
    "section": "Moving into the future",
    "text": "Moving into the future\nSuppose that initially Brand A has all the customers (i.e., Brand B is just entering the market). What are the market shares 2 quarters later? 20 quarters?\n\n\nInitial state vector is \\(\\mathbf{x}^{(0)}=(1,0)\\).\nNow do the arithmetic to find \\(\\mathbf{x}^{(2)}\\) : . . . \\[\n\\begin{aligned}\n{\\left[\\begin{array}{l}\na_{2} \\\\\nb_{2}\n\\end{array}\\right] } & =\\mathbf{x}^{(2)}=A^{2} \\mathbf{x}^{(0)}=\\left[\\begin{array}{ll}\n0.7 & 0.4 \\\\\n0.3 & 0.6\n\\end{array}\\right]\\left(\\left[\\begin{array}{ll}\n0.7 & 0.4 \\\\\n0.3 & 0.6\n\\end{array}\\right]\\left[\\begin{array}{l}\n1 \\\\\n0\n\\end{array}\\right]\\right) \\\\\n& =\\left[\\begin{array}{ll}\n0.7 & 0.4 \\\\\n0.3 & 0.6\n\\end{array}\\right]\\left[\\begin{array}{l}\n0.7 \\\\\n0.3\n\\end{array}\\right]=\\left[\\begin{array}{l}\n.61 \\\\\n.39\n\\end{array}\\right] .\n\\end{aligned}\n\\]\n\n\nBrand A will have \\(61 \\%\\) of the market and Brand B will have \\(39 \\%\\) of the market in the second quarter."
  },
  {
    "objectID": "lectures/ch2_lecture2.html#toothpaste-after-many-quarters",
    "href": "lectures/ch2_lecture2.html#toothpaste-after-many-quarters",
    "title": "Ch2 Lecture 2",
    "section": "Toothpaste after many quarters",
    "text": "Toothpaste after many quarters\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNow:\n\nfind the state after 2 quarters.\nfind the state after 20 quarters. Use matrix exponentiation: A**n\nGet specific numbers if toothpaste A has 100% of the market at the beginning. (Use sym.subs())\nNow what if toothpaste B has 100% at the beginning?\n\n\n\nOur calculation is reusable! Could go back after and then change the inital state.\nNotice that we also get the same result eventually, no matter where we started"
  },
  {
    "objectID": "lectures/ch2_lecture2.html#example-structured-population-model",
    "href": "lectures/ch2_lecture2.html#example-structured-population-model",
    "title": "Ch2 Lecture 2",
    "section": "Example: Structured Population Model",
    "text": "Example: Structured Population Model\n\n\n\ninsect life stages\n\n\n\nEvery week, 20% of the eggs die, and 60% move to the larva stage.\nAlso, 10% of larvae die, 60% become adults\n20% of adults die. Each adult produces 0.25 eggs.\nInitially we have 10k adults, 8k larvae, 6k eggs. How does the population evolve over time?\n\n\n\n\n\n\nSet up \\(\\mathbf{x}^{(k)}=\\left(a_{k}, b_{k}, c_{k}\\right)\\) \\(\\mathbf{x}^{(0)}=(10,8,6)\\) at week 0 Note that after first week, we have 20% of the initial eggs (20% died, 60% became larvae…) Transition matrix is: \\[\nA=\\left[\\begin{array}{ccc}\n0.2 & 0 & 0.25 \\\\\n0.6 & 0.3 & 0 \\\\\n0 & 0.6 & 0.8\n\\end{array}\\right]\n\\]\nWhat do we think will happen over time? How can we tell?"
  },
  {
    "objectID": "lectures/ch2_lecture2.html#section-2",
    "href": "lectures/ch2_lecture2.html#section-2",
    "title": "Ch2 Lecture 2",
    "section": "",
    "text": "class InsectEvolution(BaseStateSystem):\n    def __init__(self,transition_matrix=np.array([[.2,0,.25],[.6,.3,0],[0,.6,.8]]),max_y=10):\n        self.steps = 10;\n        self.t=0\n        self.transition_matrix = transition_matrix\n        self.max_y=max_y\n\n    def initialise(self):\n        self.x = np.array([6,8,10])\n        self.Ya = []\n        self.Yb = []\n        self.Yc = []\n        self.X = []\n\n    def update(self):\n        for _ in range(self.steps):\n            self.t += 1\n            self._update()\n\n    def _update(self):\n        self.x = self.transition_matrix.dot(self.x)\n\n    def draw(self, ax):\n        ax.clear()\n        self.X.append(self.t)\n        self.Ya.append(self.x[0])\n        self.Yb.append(self.x[1])\n        self.Yc.append([self.x[2]])\n        ax.plot(self.X,self.Ya, color=\"r\", label=\"Eggs\")\n        ax.plot(self.X,self.Yb, color=\"b\", label=\"Larvae\")\n        ax.plot(self.X,self.Yc, color=\"g\", label=\"Adults\")\n        ax.legend()\n\n        ax.set_ylim(0,self.max_y)\n        ax.set_xlim(0,200)\n        ax.set_title(\"t = {:.2f}\".format(self.t))"
  },
  {
    "objectID": "lectures/ch2_lecture2.html#section-3",
    "href": "lectures/ch2_lecture2.html#section-3",
    "title": "Ch2 Lecture 2",
    "section": "",
    "text": "t1=np.array([[.2,0,.25],[.6,.3,0],[0,.6,.8]])\ndying_insects = InsectEvolution(t1)\ndying_insects.plot_time_evolution(\"insects.gif\")\n\n\n\n\ndying insects\n\n\n\n\nt2=np.array([[.4,0,.45],[.5,.1,0],[0,.6,.8]])\nliving_insects = InsectEvolution(t2,200)\nliving_insects.plot_time_evolution(\"insects2.gif\")\n\n\n\n\nliving insects"
  },
  {
    "objectID": "lectures/ch2_lecture2.html#graph",
    "href": "lectures/ch2_lecture2.html#graph",
    "title": "Ch2 Lecture 2",
    "section": "Graph",
    "text": "Graph\n\nA graph is a set \\(V\\) of vertices (or nodes), together with a set or list \\(E\\) of unordered pairs with coordinates in \\(V\\), called edges.\n\nWhat are some things which can be represented with graphs?"
  },
  {
    "objectID": "lectures/ch2_lecture2.html#digraph",
    "href": "lectures/ch2_lecture2.html#digraph",
    "title": "Ch2 Lecture 2",
    "section": "Digraph",
    "text": "Digraph\n\nA directed graph (or “digraph”) has directed edges.\n\nWhat are some things which can be represented with digraphs?"
  },
  {
    "objectID": "lectures/ch2_lecture2.html#walks",
    "href": "lectures/ch2_lecture2.html#walks",
    "title": "Ch2 Lecture 2",
    "section": "Walks",
    "text": "Walks\n\nA walk is a sequence of edges \\(\\left\\{v_{0}, v_{1}\\right\\},\\left\\{v_{1}, v_{2}\\right\\}, \\ldots,\\left\\{v_{m-1}, v_{m}\\right\\}\\) that goes from vertex \\(v_{0}\\) to vertex \\(v_{m}\\).\nThe length of the walk is \\(m\\).\n\nA directed walk is a sequence of directed edges.\n\nWhat is an example of what a walk tells us? Work through from one of the examples of graph or digraph that came up.\nOne example: how many people are in your extended network. “Six degrees of separation”…"
  },
  {
    "objectID": "lectures/ch2_lecture2.html#winning-and-losing",
    "href": "lectures/ch2_lecture2.html#winning-and-losing",
    "title": "Ch2 Lecture 2",
    "section": "Winning and losing",
    "text": "Winning and losing\n\nSuppose this represents wins and losses by different teams. How can we rank the teams?\n\nIdea: a team which beats another team is good. Even better if the team they beat had beaten another team…\nIdea: count the number of walks of length 1 or 2 originating from each vertex.\nHave them do this…\nPower of 1 is 5, vertex 2 is 4, vertex 3 is 7, vertex 4 is 4, vertex 5 is 1, and the power of vertex 6 is 0."
  },
  {
    "objectID": "lectures/ch2_lecture2.html#power",
    "href": "lectures/ch2_lecture2.html#power",
    "title": "Ch2 Lecture 2",
    "section": "Power",
    "text": "Power\nVertex power: the number of walks of length 1 or 2 originating from a vertex.\n\nMakes most sense for graphs which don’t have any self-loops and at most one edge between nodes. These are called dominance directed graphs\n\n\nHow can we compute the number of walks for a given graph?"
  },
  {
    "objectID": "lectures/ch2_lecture2.html#adjacency-matrix",
    "href": "lectures/ch2_lecture2.html#adjacency-matrix",
    "title": "Ch2 Lecture 2",
    "section": "Adjacency matrix",
    "text": "Adjacency matrix\nAdjacency matrix: A square matrix whose \\((i, j)\\) th entry is the number of edges going from vertex \\(i\\) to vertex \\(j\\)\n\nEdges in non-directed graphs appear twice (at (i,j) and at (j,i))\nEdges in digraphs appear only once\n\n\nWhat is the adjacency matrix for this graph?\n\n\n\n\\[\nB=\\left[\\begin{array}{llllll}\n0 & 1 & 0 & 1 & 0 & 1 \\\\\n1 & 0 & 1 & 0 & 0 & 1 \\\\\n0 & 1 & 0 & 0 & 1 & 0 \\\\\n1 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 1 & 0 & 0 & 1 \\\\\n1 & 1 & 0 & 1 & 1 & 0\n\\end{array}\\right]\n\\]\n\nWe can reconstruct the graph entirely from this matrix! It must have all the information encapsulated in it."
  },
  {
    "objectID": "lectures/ch2_lecture2.html#section-4",
    "href": "lectures/ch2_lecture2.html#section-4",
    "title": "Ch2 Lecture 2",
    "section": "",
    "text": "What is the adjacency matrix for this graph?\n\n\n\\[\nA=\\left[\\begin{array}{llllll}\n0 & 1 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 1 & 0 & 1 \\\\\n0 & 1 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 & 0 & 0\n\\end{array}\\right]\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture2.html#computing-power-from-an-adjacencty-matrix",
    "href": "lectures/ch2_lecture2.html#computing-power-from-an-adjacencty-matrix",
    "title": "Ch2 Lecture 2",
    "section": "Computing power from an adjacencty matrix",
    "text": "Computing power from an adjacencty matrix\n\\[\nA=\\left[\\begin{array}{llllll}\n0 & 1 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 1 & 0 & 1 \\\\\n0 & 1 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 & 0 & 0\n\\end{array}\\right]\n\\]\nHow can we count the walks of length 1 emanating from vertex \\(i\\)?\n\nAnswer: Add up the elements of the \\(i\\) th row of \\(A\\)."
  },
  {
    "objectID": "lectures/ch2_lecture2.html#section-5",
    "href": "lectures/ch2_lecture2.html#section-5",
    "title": "Ch2 Lecture 2",
    "section": "",
    "text": "\\[\nA=\\left[\\begin{array}{llllll}\n0 & 1 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 1 & 0 & 1 \\\\\n0 & 1 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 & 0 & 0\n\\end{array}\\right]\n\\]\nWhat about walks of length 2?\n\nStart by finding number of walks of length 2 from vertex i to vertex j.\n\n\n\\[\na_{i 1} a_{1 j}+a_{i 2} a_{2 j}+\\cdots+a_{i n} a_{n j} .\n\\]\n\n\nThis is just the \\((i, j)\\) th entry of the matrix \\(A^{2}\\).\n\nWhen is there an edge from \\(i\\) to \\(k\\) and then from \\(k\\) to \\(j\\)?\nIf the adjacency matrix has 1’s in both \\(a_{i,k}\\) and \\(a_{k,j}\\).\nCan represent this by the product…\nSo number of paths is the sum over k’s…\n\\[\na_{i 1} a_{1 j}+a_{i 2} a_{2 j}+\\cdots+a_{i n} a_{n j} .\n\\]\nThis is just the \\((i, j)\\) th entry of the matrix \\(A^{2}\\)."
  },
  {
    "objectID": "lectures/ch2_lecture2.html#adjacency-matrix-and-power",
    "href": "lectures/ch2_lecture2.html#adjacency-matrix-and-power",
    "title": "Ch2 Lecture 2",
    "section": "Adjacency matrix and power",
    "text": "Adjacency matrix and power\nResult: The \\((i, j)\\) th entry of \\(A^{r}\\) gives the number of (directed) walks of length \\(r\\) starting at vertex \\(i\\) and ending at vertex \\(j\\).\n\nTherefore: power of the \\(i\\) th vertex is the sum of all entries in the \\(i\\) th row of the matrix \\(A+A^{2}\\)."
  },
  {
    "objectID": "lectures/ch2_lecture2.html#vertex-power-in-our-teams-example",
    "href": "lectures/ch2_lecture2.html#vertex-power-in-our-teams-example",
    "title": "Ch2 Lecture 2",
    "section": "Vertex power in our teams example",
    "text": "Vertex power in our teams example\n\\[\nA+A^{2}=\\left[\\begin{array}{llllll}\n0 & 1 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 1 & 0 & 1 \\\\\n0 & 1 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 & 0 & 0\n\\end{array}\\right]+\\left[\\begin{array}{llllll}\n0 & 1 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 1 & 0 & 1 \\\\\n0 & 1 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 & 0 & 0\n\\end{array}\\right]\n\\left[\\begin{array}{llllll}\n0 & 1 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 1 & 0 & 1 \\\\\n0 & 1 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 & 0 & 0\n\\end{array}\\right]\n\\]\n\\[=\\left[\\begin{array}{llllll}\n0 & 2 & 1 & 1 & 1 & 0 \\\\\n1 & 0 & 1 & 1 & 0 & 1 \\\\\n1 & 2 & 0 & 2 & 1 & 1 \\\\\n0 & 1 & 1 & 0 & 1 & 1 \\\\\n0 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 & 0 & 0\n\\end{array}\\right] .\n\\]\n\nThe sum of each row gives the vertex power.\n\nHow can you use multiplication to find the sum of each row?\nCan multiply by a column of ones…"
  },
  {
    "objectID": "lectures/ch2_lecture2.html#now-you-try",
    "href": "lectures/ch2_lecture2.html#now-you-try",
    "title": "Ch2 Lecture 2",
    "section": "Now you try",
    "text": "Now you try\nWorking together,\n\npick a topic (sports? elections?).\nLook up data and make a graph or digraph.\nCompute the adjacency matrix.\nFind the vertex powers.\n\nDo your results make sense in context?\nDoes it make sense?\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n \n\n\n \n\n\n\n\n \n\n\n\ninsect life stages\ndying insects\nliving insects"
  },
  {
    "objectID": "lectures/ch2_lecture4.html#idea-of-mcmc",
    "href": "lectures/ch2_lecture4.html#idea-of-mcmc",
    "title": "Ch2 Lecture 4",
    "section": "Idea of MCMC",
    "text": "Idea of MCMC\n\nSuppose there’s something we want to calculate – say the average web revenue for pages every day.\nStart out simple: suppose we have a revenue amount for each page, per user. How do we calculate the average revenue per page?\nWe can figure out how many users visit each page, and then multiply that by the revenue per user for that page.\nWe can do this by calculating the stationary distribution of the Markov chain that describes the transitions between pages.\nOr, we can do it by sampling.\nWork through the notebook to see how this works in practice.\nOK, now make it more complicated.\nWe have a formula for a given webpage that tells us how much revenue it generates given the user’s age, their gender, their location, etc.\nHow can we find the average revenue per page now?\nWe need to know the distribution of users and their characteristics. We can’t just use the stationary distribution of the Markov chain anymore.\nWe could try to calculate the distribution of users, but that’s hard.\nInstead, we can use a Markov Chain Monte Carlo method to sample from the distribution of users. We can then use these samples to estimate the average revenue per page.\nTo do Gibbs sampling, we will sample from the distribution of each variable given the other variables. This is easier than sampling from the joint distribution of all the variables.\nSo for instance, we might suppose that a user is male, 25 years old, and on the webpage for the Chicago Cubs. We can specify a distribution of locations for the user’s location given this information. Sample from that distribution.\nThen, we can say ok, we’ve got a user who is 25 years old, on the webpage for the Cubs, and in Chicago. What’s the probability that the user is male?\nGo through all the variables in turn, sampling from the distribution of each given the others, including for the webpages."
  },
  {
    "objectID": "lectures/ch2_lecture4.html#intro-to-the-idea-of-a-restricted-boltzmann-machine",
    "href": "lectures/ch2_lecture4.html#intro-to-the-idea-of-a-restricted-boltzmann-machine",
    "title": "Ch2 Lecture 4",
    "section": "Intro to the idea of a Restricted Boltzmann Machine",
    "text": "Intro to the idea of a Restricted Boltzmann Machine\n\nYou’d like to learn about what sort of objects exist in the world. You have a bunch of data, but you don’t know what the objects are. You’d like to learn about the objects and the features that describe them.\nOne way to do this is to start trying to assign labels to the objects you are seeing. You can start with random labels, and then try to improve them.\nYour goal can be that if you imagine objects given the labels, you’ll end up with a distribution that looks like the data you have.\n\n\n\nRBM schematic\n\n\nYour dreams should look like the real world.\nOK, but how do you measure how well your dreams match the real world?\nYou could calculate the probability of imagining each possible image… but that’s intractable.\nInstead, you can do a Markov Chain Monte Carlo method called Gibbs sampling. The basic idea of MCMC is that you can sample from a distribution by starting at some point and then moving around in a way that the distribution of your samples will eventually match the distribution you’re interested in.\nFor Gibbs sampling, you start with a random sample, and then you update each variable in turn, given the other variables. This may be much easier to compute than needing to know the full transition matrix for every possible pair of states. (We are in high dimensions here, because every node in our network is a variable.)\nIn the case of the RBM, you update each hidden node given the visible nodes, and then you update each visible node given the hidden nodes. You keep doing this for a while, and then you have a sample from the distribution you’re interested in.\nNow you compare this sample to your data, and you adjust the weights in your network to make the sample look more like the data."
  },
  {
    "objectID": "lectures/ch2_lecture4.html#math-of-the-rbm",
    "href": "lectures/ch2_lecture4.html#math-of-the-rbm",
    "title": "Ch2 Lecture 4",
    "section": "Math of the RBM",
    "text": "Math of the RBM\n\n\n\nStates are determined by an energy function \\(E(\\mathbf{v}, \\mathbf{h})\\). \\[\nE(\\mathbf{v}, \\mathbf{h})=-\\sum_{i} a_{i} v_{i}-\\sum_{j} b_{j} h_{j}-\\sum_{i j} v_{i} W_{i j} h_{j}\n\\]\n\n\nThen the probability distribution is given by the Boltzmann distribution:\n\\(P_{\\mathrm{rbm}}(\\mathbf{v}, \\mathbf{h})=\\frac{1}{Z} e^{-E(\\mathbf{v}, \\mathbf{h})}\\) where \\(Z=\\sum_{\\mathbf{v}, \\mathbf{h}} e^{-E(\\mathbf{v}, \\mathbf{h})}\\)\n\n\nFrom https://ml-lectures.org/docs/unsupervised_learning/ml_unsupervised-1.html"
  },
  {
    "objectID": "lectures/ch2_lecture4.html#section",
    "href": "lectures/ch2_lecture4.html#section",
    "title": "Ch2 Lecture 4",
    "section": "",
    "text": "The probability of a visible vector \\(\\mathbf{v}\\) is given by marginalizing over the hidden variables:\n\\[\n\\begin{equation*}\nP_{\\mathrm{rbm}}(\\mathbf{v})=\\sum_{\\mathbf{h}} P_{\\mathrm{rbm}}(\\mathbf{v}, \\mathbf{h})=\\frac{1}{Z} \\sum_{h} e^{-E(\\mathbf{v}, \\mathbf{h})}\n\\end{equation*}\n\\]\nConveniently, this gives each visible unit an independent probability of activation:\n\\[\nP_{\\mathrm{rbm}}\\left(v_{i}=1 | \\mathbf{h}\\right)=\\sigma\\left(a_{i}+\\sum_{j} W_{i j} h_{j}\\right), \\quad i=1, \\ldots, n_{\\mathrm{v}}\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture4.html#section-1",
    "href": "lectures/ch2_lecture4.html#section-1",
    "title": "Ch2 Lecture 4",
    "section": "",
    "text": "The same is true for hidden units, given the visible units:\n\\[\n\\begin{equation*}\nP_{\\mathrm{rbm}}\\left(h_{j}=1 | \\mathbf{v}\\right)=\\sigma\\left(b_{j}+\\sum_{i} v_{i} W_{i j}\\right) \\quad j=1, \\ldots, n_{\\mathrm{h}}\n\\end{equation*}\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture4.html#training",
    "href": "lectures/ch2_lecture4.html#training",
    "title": "Ch2 Lecture 4",
    "section": "Training",
    "text": "Training\nConsider a set of binary input data \\(\\mathbf{x}_{k}, k=1, \\ldots, M\\), drawn from a probability distribution \\(P_{\\text {data }}(\\mathbf{x})\\).\nGoal: tune the parameters \\(\\{\\mathbf{a}, \\mathbf{b}, W\\}\\) such that after training \\(P_{\\mathrm{rbm}}(\\mathbf{x}) \\approx P_{\\mathrm{data}}(\\mathbf{x})\\).\n\nTo do this, we need to be able to estimate \\(P_{\\mathrm{rbm}}\\)!\nUnfortunately, this is often intractable, because it requires calculating the partition function \\(Z\\)."
  },
  {
    "objectID": "lectures/ch2_lecture4.html#details-of-the-training",
    "href": "lectures/ch2_lecture4.html#details-of-the-training",
    "title": "Ch2 Lecture 4",
    "section": "Details of the training",
    "text": "Details of the training\nWe want to maximize the log-likelihood of the data under the model: \\[\nL(\\mathbf{a}, \\mathbf{b}, W)=-\\sum_{k=1}^{M} \\log P_{\\mathrm{rbm}}\\left(\\mathbf{x}_{k}\\right)\n\\]\n\nTake derivatives of this with respect to the parameters, and use gradient descent:\n\\[\n\\begin{equation*}\n\\frac{\\partial L(\\mathbf{a}, \\mathbf{b}, W)}{\\partial W_{i j}}=-\\sum_{k=1}^{M} \\frac{\\partial \\log P_{\\mathrm{rbm}}\\left(\\mathbf{x}_{k}\\right)}{\\partial W_{i j}}\n\\end{equation*}\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture4.html#section-2",
    "href": "lectures/ch2_lecture4.html#section-2",
    "title": "Ch2 Lecture 4",
    "section": "",
    "text": "The derivative has two terms: \\[\n\\begin{equation*}\n\\frac{\\partial \\log P_{\\mathrm{rbm}}(\\mathbf{x})}{\\partial W_{i j}}=x_{i} P_{\\mathrm{rbm}}\\left(h_{j}=1 |\\mathbf{x}\\right)-\\sum_{\\mathbf{v}} v_{i} P_{\\mathrm{rbm}}\\left(h_{j}=1 | \\mathbf{v}\\right) P_{\\mathrm{rbm}}(\\mathbf{v})\n\\end{equation*}\n\\]\n\nUse this to update the weights:\n\\[\nW_{i j} \\rightarrow W_{i j}-\\eta \\frac{\\partial L(a, b, W)}{\\partial W_{i j}}\n\\]\nProblem: the second term in the derivative is intractable! It has \\(2^{n_{\\mathrm{v}}}\\) terms:\n\\[\n\\sum_{\\mathbf{v}} v_{i} P_{\\mathrm{rbm}}\\left(h_{j}=1 | \\mathbf{v}\\right) P_{\\mathrm{rbm}}(\\mathbf{v})\n\\]\nInstead, we will use Gibbs sampling to estimate \\(P_{\\mathrm{rbm}}(\\mathbf{v})\\)."
  },
  {
    "objectID": "lectures/ch2_lecture4.html#gibbs-sampling-to-the-rescue",
    "href": "lectures/ch2_lecture4.html#gibbs-sampling-to-the-rescue",
    "title": "Ch2 Lecture 4",
    "section": "Gibbs sampling to the rescue",
    "text": "Gibbs sampling to the rescue\n\nInput: Any visible vector \\(\\mathbf{v}(0)\\)\nOutput: Visible vector \\(\\mathbf{v}(r)\\)\nfor: \\(n=1 \\backslash\\) dots \\(r\\)\n\\(\\operatorname{sample} \\mathbf{h}(n)\\) from \\(P_{\\mathrm{rbm}}(\\mathbf{h} \\mathbf{v}=\\mathbf{v}(n-1))\\)\nsample \\(\\mathbf{v}(n)\\) from \\(P_{\\mathrm{rbm}}(\\mathbf{v} \\mathbf{h}=\\mathbf{h}(n))\\) end"
  },
  {
    "objectID": "lectures/ch2_lecture4.html#using-an-rbm",
    "href": "lectures/ch2_lecture4.html#using-an-rbm",
    "title": "Ch2 Lecture 4",
    "section": "Using an RBM",
    "text": "Using an RBM"
  },
  {
    "objectID": "lectures/ch2_lecture4.html#section-3",
    "href": "lectures/ch2_lecture4.html#section-3",
    "title": "Ch2 Lecture 4",
    "section": "",
    "text": "Suppose we want to solve a nonsingular linear system \\(A x=b\\) repeatedly, with different choices of \\(b\\).\n\n\nE.g. Heat flow problem, where the right-hand side is determined by the heat source term \\(f(x)\\).\n\n\n\\[\n\\begin{equation*}\n-y_{i-1}+2 y_{i}-y_{i+1}=\\frac{h^{2}}{K} f\\left(x_{i}\\right), i=1,2, \\ldots, n\n\\end{equation*}\n\\]\n\n\nPerhaps you want to experiment with different functions for the heat source term.\n\n\nWhat do we do? Each time, we create the augmented matrix \\(\\widetilde{A}=[A \\mid b]\\), then get it into reduced row echelon form.\n\n\nEach time change \\(b\\), we have to redo all the work of Gaussian or Gauss-Jordan elimination !\n\n\nEspecially frustrating because the main part of our work is the same: putting the part of \\(\\widetilde{A}\\) corresponding to the coefficient matrix \\(A\\) into reduced row echelon form."
  },
  {
    "objectID": "lectures/ch2_lecture4.html#lu-factorization-saving-that-work",
    "href": "lectures/ch2_lecture4.html#lu-factorization-saving-that-work",
    "title": "Ch2 Lecture 4",
    "section": "LU Factorization: Saving that work",
    "text": "LU Factorization: Saving that work\nGoal: Find a way to record our work on \\(A\\), so that solving a new system involves very little additional work.\n\nLU Factorization: Let \\(A\\) be an \\(n \\times n\\) matrix. An LU factorization of \\(A\\) is a pair of \\(n \\times n\\) matrices \\(L, U\\) such that\n\n\\(L\\) is lower triangular.\n\\(U\\) is upper triangular.\n\\(A=L U\\).\n\n\n\nWhy is this so wonderful? Triangular systems \\(A \\mathbf{x}=\\mathbf{b}\\) are easy to solve.\nRemember: If \\(A\\) is upper triangular, we can solve for the last variable, then the next-to-last variable, etc."
  },
  {
    "objectID": "lectures/ch2_lecture4.html#solving-an-upper-triangular-system",
    "href": "lectures/ch2_lecture4.html#solving-an-upper-triangular-system",
    "title": "Ch2 Lecture 4",
    "section": "Solving an upper triangular system",
    "text": "Solving an upper triangular system\nLet’s say we have the following system:\n\\(A x = b\\) where A is the upper-triangular matrix \\(A = \\begin{bmatrix} 2 & 1 & 0 \\\\ 0 & 1 & -1 \\\\ 0 & 0 & -1 \\end{bmatrix}\\), and we want to solve for \\(b = \\begin{bmatrix} 1 \\\\ 1 \\\\ -2 \\end{bmatrix}\\).\n\nWe form the augmented matrix \\(\\widetilde{A} = [A | b] = \\begin{bmatrix} 2 & 1 & 0 & | & 1 \\\\ 0 & 1 & -1 & | & 1 \\\\ 0 & 0 & -1 & | & -2 \\end{bmatrix}\\).\n\n\nBack substitution:\n\nLast equation: \\(-x_3 = -2\\), so \\(x_3 = 2\\).\nSubstitute this value into the second equation, \\(x_2 - x_3 = 1\\), so \\(x_2 = 3\\).\nFinally, we substitute \\(x_2\\) and \\(x_3\\) into the first equation, \\(2x_1 + x_2 = 1\\), so \\(x_1 = -1\\)."
  },
  {
    "objectID": "lectures/ch2_lecture4.html#solving-a-lower-triangular-system",
    "href": "lectures/ch2_lecture4.html#solving-a-lower-triangular-system",
    "title": "Ch2 Lecture 4",
    "section": "Solving a lower triangular system",
    "text": "Solving a lower triangular system\nIf \\(A\\) is lower triangular, we can solve for the first variable, then the second variable, etc.\nLet’s say we have the following system:\n\\(A y = b\\) where A is the lower-triangular matrix \\(A = \\begin{bmatrix} 1 & 0 & 0 \\\\ -1 & 1 & 0 \\\\ 1 & 2 & 1 \\end{bmatrix}\\), and we want to solve for \\(b = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix}\\).\n\nWe form the augmented matrix \\(\\widetilde{A} = [A | b] = \\begin{bmatrix} 1 & 0 & 0 & | & 1 \\\\ -1 & 1 & 0 & | & 0 \\\\ 1 & 2 & 1 & | & 1 \\end{bmatrix}\\).\n\n\nForward substitution:\n\nFirst equation: \\(y_1 = 1\\).\nSubstitute this value into the second equation, \\(-y_1 + y_2 = 0\\), so \\(y_2 = 1\\).\nFinally, we substitute \\(y_1\\) and \\(y_2\\) into the third equation, \\(y_1 + 2y_2 + y_3 = 1\\), so \\(y_3 = -2\\).\n\n\n\nThis was just as easy as solving the upper triangular system!"
  },
  {
    "objectID": "lectures/ch2_lecture4.html#solving-a-x-b-with-lu-factorization",
    "href": "lectures/ch2_lecture4.html#solving-a-x-b-with-lu-factorization",
    "title": "Ch2 Lecture 4",
    "section": "Solving \\(A x = b\\) with LU factorization",
    "text": "Solving \\(A x = b\\) with LU factorization\nNow suppose we want to solve \\(A x = b\\) and we know that \\(A = L U\\). The original system becomes \\(L U x = b\\).\nIntroduce an intermediate variable \\(y = U x\\). Our system is now \\(L y = b\\). Now perform these steps:\n\nForward solve: Solve lower triangular system \\(L y = b\\) for the variable \\(y\\).\nBack solve: Solve upper triangular system \\(U x = y\\) for the variable \\(x\\).\nThis does it!\n\n\nOnce we have the matrices \\(L, U\\), the right-hand sides only come when solving the two triangular systems. Easy!"
  },
  {
    "objectID": "lectures/ch2_lecture4.html#example",
    "href": "lectures/ch2_lecture4.html#example",
    "title": "Ch2 Lecture 4",
    "section": "Example",
    "text": "Example\nYou are given that\n\\[\nA=\\left[\\begin{array}{rrr}\n2 & 1 & 0 \\\\\n-2 & 0 & -1 \\\\\n2 & 3 & -3\n\\end{array}\\right]=\\left[\\begin{array}{rrr}\n1 & 0 & 0 \\\\\n-1 & 1 & 0 \\\\\n1 & 2 & 1\n\\end{array}\\right]\\left[\\begin{array}{rrr}\n2 & 1 & 0 \\\\\n0 & 1 & -1 \\\\\n0 & 0 & -1\n\\end{array}\\right] .\n\\]\nSolve this system for \\(\\mathbf{b} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix}\\).\n\nSay that y = Ux.\n\n\n\n\n\nForward solve: \\[\n\\left[\\begin{array}{rrr}\n1 & 0 & 0 \\\\\n-1 & 1 & 0 \\\\\n1 & 2 & 1\n\\end{array}\\right]\\left[\\begin{array}{l}\ny_{1} \\\\\ny_{2} \\\\\ny_{3}\n\\end{array}\\right]=\\left[\\begin{array}{l}\n1 \\\\\n0 \\\\\n1\n\\end{array}\\right]\n\\]\n\\(y_{1}=1\\), then \\(y_{2}=0+1 y_{1}=1\\), then \\(y_{3}=1-1 y_{1}-2 y_{2}=-2\\).\n\n\n\nBack solve:\n\\[\n\\left[\\begin{array}{rrr}\n2 & 1 & 0 \\\\\n0 & 1 & -1 \\\\\n0 & 0 & -1\n\\end{array}\\right]\\left[\\begin{array}{l}\nx_{1} \\\\\nx_{2} \\\\\nx_{3}\n\\end{array}\\right]=\\left[\\begin{array}{r}\n1 \\\\\n1 \\\\\n-2\n\\end{array}\\right]\n\\]\n\\(x_{3}=-2 /(-1)=2\\), then \\(x_{2}=1+x_{3}=3\\), then \\(x_{1}=\\left(1-1 x_{2}\\right) / 2=-1\\)."
  },
  {
    "objectID": "lectures/ch2_lecture4.html#when-we-can-do-lu-factorization",
    "href": "lectures/ch2_lecture4.html#when-we-can-do-lu-factorization",
    "title": "Ch2 Lecture 4",
    "section": "When we can do LU factorization",
    "text": "When we can do LU factorization\n\nNot all square matrices have LU factorizations! This one doesn’t: \\(\\left[\\begin{array}{ll}0 & 1 \\\\ 1 & 0\\end{array}\\right]\\)\nIf Gaussian elimination can be performed on the matrix \\(A\\) without row exchanges, then the factorization exists\n\n(it’s really a by-product of Gaussian elimination.)\n\nIf row exchanges are needed, there is still a factorization that will work, but it’s a bit more complicated."
  },
  {
    "objectID": "lectures/ch2_lecture4.html#intuition-behind-lu-factorization",
    "href": "lectures/ch2_lecture4.html#intuition-behind-lu-factorization",
    "title": "Ch2 Lecture 4",
    "section": "Intuition behind LU factorization",
    "text": "Intuition behind LU factorization\n\nWhen we do Gaussian elimination, we are multiplying \\(A\\) by a series of elementary matrices to get it into upper triangular form.\nCall the product of all those elementary matrices \\(\\tilde L\\).\nSo our original system \\(A x = b\\) becomes \\(\\tilde L A x = \\tilde L b\\).\nWe recognize that \\(\\tilde L A\\) will be an upper triangular matrix, because that’s what we get when we do Gaussian elimination. This is our \\(U\\).\nBut what about the other part? We can multiply both sides by the inverse of \\(\\tilde L\\) to get\n\\[\n\\begin{aligned}\n\\tilde{L}^{-1} \\tilde{L} A x &= \\tilde{L}^{-1} \\tilde L b \\\\\n\\tilde{L}^{-1} U x &= b\n\\end{aligned}\n\\]\nNow the challenge is simply to find the inverse of \\(\\tilde L\\).\nRemember that \\(\\tilde L\\) is the product of the elementary matrices that we used when doing Gaussian elimination – call them \\(E_1\\), \\(E_2\\), etc.\nSo \\(\\tilde L = E_n E_{n-1} \\ldots E_1\\).\nThe inverse of a product of matrices is the product of the inverses in reverse order: $(E_n E_{n-1} E_1)^{-1} = E_1^{-1} E_2^{-1} E_n^{-1} $.\nFortunately, the inverse of an elementary matrix is easy to find: it’s just the same matrix with the opposite sign on the entry that was used to eliminate. So if we have the matrix which will add twice the first row to the second row, its inverse will be the matrix which will subtract twice the first row from the second row.\n\\[\nE = \\begin{bmatrix} 1 & 0 & 0 \\\\ 2 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\quad E^{-1} = \\begin{bmatrix} 1 & 0 & 0 \\\\ -2 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\n\\]\nAs it happens, when we follow the steps of Gaussian elimination (with no row swaps), the product of all these inverse matrices is just a lower triangular matrix, with each entry equal to the negative of the multiplier we used in doing the associated elimination step.\nThis is our \\(L\\)!\nSo all we need to do is keep track of these multipliers as we do Gaussian elimination, and we can use them to find the inverse of \\(\\tilde L\\). If we are doing things by hand, we can even write them into the lower part of our matrix as we go, since that part will be zeroed out anyways."
  },
  {
    "objectID": "lectures/ch2_lecture4.html#example-1",
    "href": "lectures/ch2_lecture4.html#example-1",
    "title": "Ch2 Lecture 4",
    "section": "Example",
    "text": "Example\nHere we do Gaussian elimination on the matrix \\(A = \\begin{bmatrix} 2 & 1 & 0 \\\\ -2 & 0 & -1 \\\\ 2 & 3 & -3 \\end{bmatrix}\\):\n\\(\\left[\\begin{smallmatrix}2 & 1 & 0\\\\-2 & 0 & -1\\\\2 & 3 & -3\\end{smallmatrix}\\right]\\) \\(\\xrightarrow[E_{31}(-1)]{E_{21}(1)}\\) \\(\\left[\\begin{smallmatrix}2 & 1 & 0\\\\0 & 1 & -1\\\\0 & 2 & -3\\end{smallmatrix}\\right]\\) \\(\\xrightarrow[E_{32}(-2)]{\\longrightarrow}\\) \\(\\left[\\begin{smallmatrix}2 & 1 & 0\\\\0 & 1 & -1\\\\0 & 0 & -1\\end{smallmatrix}\\right]\\)\n\n\n\nLet’s put those elementary row operations into matrix form. There were three of them:\n\n\\(E_{21}(1)\\) : \\(\\begin{bmatrix} 1 & 0 & 0 \\\\ 1 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\)\n\\(E_{31}(-1)\\): \\(\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ -1 & 0 & 1 \\end{bmatrix}\\)\n\\(E_{32}(-2)\\): \\(\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & -2 & 1 \\end{bmatrix}\\)\n\n\n\n\nThe inverses of these matrices are\n\n\\(\\begin{bmatrix} 1 & 0 & 0 \\\\ -1 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\), \\(\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 1 \\end{bmatrix}\\), and \\(\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 2 & 1 \\end{bmatrix}\\)."
  },
  {
    "objectID": "lectures/ch2_lecture4.html#section-4",
    "href": "lectures/ch2_lecture4.html#section-4",
    "title": "Ch2 Lecture 4",
    "section": "",
    "text": "The product of all these matrices is\n\\[\n\\left[\\begin{matrix}1 & 0 & 0\\\\0 & 1 & 0\\\\-1 & 0 & 1\\end{matrix}\\right]\\left[\\begin{matrix}1 & 0 & 0\\\\1 & 1 & 0\\\\0 & 0 & 1\\end{matrix}\\right]\\left[\\begin{matrix}1 & 0 & 0\\\\0 & 1 & 0\\\\0 & -2 & 1\\end{matrix}\\right]=\\left[\\begin{matrix}1 & 0 & 0\\\\1 & 1 & 0\\\\-1 & -2 & 1\\end{matrix}\\right]\n\\]\nThis is a lower triangular matrix, and it is the inverse of the matrix we used to do Gaussian elimination.\nWe can also see that the entries below the diagonal are the negatives of the multipliers we used in the elimination steps."
  },
  {
    "objectID": "lectures/ch2_lecture4.html#steps-to-lu-factorization",
    "href": "lectures/ch2_lecture4.html#steps-to-lu-factorization",
    "title": "Ch2 Lecture 4",
    "section": "Steps to LU factorization",
    "text": "Steps to LU factorization\nLet \\(\\left[a_{i j}^{(k)}\\right]\\) be the matrix obtained from \\(A\\) after using the \\(k\\) th pivot to clear out entries below it.\n\n(The original matrix is \\(A=\\left[a_{i j}^{(0)}\\right]\\))\n\n\nAll the row operations we will use include ratios \\(\\left(-a_{i j} / a_{j j}\\right)\\).\n\n\nThe row-adding elementary operations are of the form\n\\(E_{i j}\\left(-a_{i j}^{(k)} / a_{j j}^{(k)}\\right)\\)\n\n\nWe can give these ratios a name: multipliers.\n\\(m_{i j}=-a_{i j}^{(k)} / a_{j j}^{(k)}\\), where \\(i&gt;j\\)\n\n\n\nIf Gaussian elimination is used without row exchanges on the nonsingular matrix \\(A\\), resulting in the upper triangular matrix \\(U\\), and if \\(L\\) is the unit lower triangular matrix whose entries below the diagonal are the negatives of the multipliers \\(m_{i j}\\), then \\(A=L U\\)."
  },
  {
    "objectID": "lectures/ch2_lecture4.html#storing-the-multipliers-as-we-go",
    "href": "lectures/ch2_lecture4.html#storing-the-multipliers-as-we-go",
    "title": "Ch2 Lecture 4",
    "section": "Storing the multipliers as we go",
    "text": "Storing the multipliers as we go\nFor efficiency, we can just “store” the multipliers in the lower triangular part of the matrix on the left as we go along, since that will be zero anyways.\n\\[\n\\left[\\begin{array}{rrr}\n(2) & 1 & 0 \\\\\n-2 & 0 & -1 \\\\\n2 & 3 & -3\n\\end{array}\\right] \\xrightarrow[E_{31}(-1)]{E_{21}(1)}\\left[\\begin{array}{rrr}\n2 & 1 & 0 \\\\\n-1 & (1) & -1 \\\\\n1 & 2 & -3\n\\end{array}\\right] \\xrightarrow[E_{32}(-2)]{\\longrightarrow}\\left[\\begin{array}{rrr}\n2 & 1 & 0 \\\\\n-1 & 1 & -1 \\\\\n1 & 2 & -1\n\\end{array}\\right] .\n\\]\n\n\ncircle the multipliers as we go along\n\nNow we read off the results from the final matrix:\n\\[\nL=\\left[\\begin{array}{rrr}\n1 & 0 & 0 \\\\\n1 & 1 & 0 \\\\\n-1 & 2 & 1\n\\end{array}\\right] \\text { and } U=\\left[\\begin{array}{rrr}\n2 & 1 & 0 \\\\\n0 & 1 & -1 \\\\\n0 & 0 & -1\n\\end{array}\\right]\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture4.html#superaugmented-matrix",
    "href": "lectures/ch2_lecture4.html#superaugmented-matrix",
    "title": "Ch2 Lecture 4",
    "section": "Superaugmented matrix",
    "text": "Superaugmented matrix\nCould we just keep track by using the superaugmented matrix, like we did last lecture? What would that look like?\npause\n\n\\(\\left[\\begin{smallmatrix}2 & 1 & 0 & 1 & 0 & 0\\\\-2 & 0 & -1 & 0 & 1 & 0\\\\2 & 3 & -3 & 0 & 0 & 1\\end{smallmatrix}\\right]\\) \\(\\xrightarrow[E_{31}(-1)]{E_{21}(1)}\\) \\(\\left[\\begin{smallmatrix}2 & 1 & 0 & 1 & 0 & 0\\\\0 & 1 & -1 & 1 & 1 & 0\\\\0 & 2 & -3 & -1 & 0 & 1\\end{smallmatrix}\\right]\\) \\(\\xrightarrow[E_{32}(-2)]{\\longrightarrow}\\) \\(\\left[\\begin{smallmatrix}2 & 1 & 0 & 1 & 0 & 0\\\\0 & 1 & -1 & 1 & 1 & 0\\\\0 & 0 & -1 & -3 & -2 & 1\\end{smallmatrix}\\right]\\)\n\n\nOur superaugmented matrix does become an upper triangular matrix on the left and a lower triangular matrix on the right.\nUnfortunately, the lower triangular matrix on the right is \\(\\tilde{L}^{-1}\\), not \\(\\tilde{L}\\).\nSo we can’t just read off \\(L\\) and \\(U\\) from the superaugmented matrix."
  },
  {
    "objectID": "lectures/ch2_lecture4.html#plu-factorization",
    "href": "lectures/ch2_lecture4.html#plu-factorization",
    "title": "Ch2 Lecture 4",
    "section": "PLU factorization",
    "text": "PLU factorization\nWhat if we need row exchanges?\n\nWe could start off by doing all the row-exchanging elementary operations that we need, and store the product of these row-exchanging matrices as a matrix \\(P\\).\nThis product is called a permutation matrix\nApplying the correct permuatation matrix \\(P\\) to \\(A\\), we get a matrix for which Gaussian elimination will succeed without further row exchanges.\n\n\nNow we have a theorem that applies to all nonsingular matrices:\n\nIf \\(A\\) is a nonsingular matrix, then there exists a permutation matrix \\(P\\), upper triangular matrix \\(U\\), and unit lower triangular matrix \\(L\\) such that \\(P A=L U\\).\n\n\n\nSo, if you’ve got a nonsingular matrix \\(A\\), you can always find a permutation matrix \\(P\\), an upper triangular matrix \\(U\\), and a unit lower triangular matrix \\(L\\) that satisfy \\(P A=L U\\). Pretty neat, huh?\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n \n\n\n \n\n\n\n\n \n\n\n\nRBM schematic"
  },
  {
    "objectID": "lectures/ch4lecture1.html#planes",
    "href": "lectures/ch4lecture1.html#planes",
    "title": "Ch4 Lecture 1",
    "section": "Planes",
    "text": "Planes\nMore copilot insanity. I wrote the first sentence, and then it autocompleted as follows:\nHave students put a dot in the center of the card and say this is the origin. . . . Have them draw a line and say this is the x-axis. . . . Have them draw a line perpendicular to the x-axis and say this is the y-axis.\n\nHave them draw a line perpendicular to the x and y axes and say this is the z-axis.\n\n\nHave them draw a line perpendicular to the x and y axes and say this is the w-axis.\n\n\nHave them draw a line perpendicular to the x, y, and w axes and say this is the v-axis.\n\n\nHave them draw a line perpendicular to the x, y, w, and v axes and say this is the u-axis.\n\n\nHave them draw a line perpendicular to the x, y, w, v, and u axes and say this is the t-axis. Have them draw a line perpendicular to the x, y, w, v, u, and t axes and say this is the s-axis. Have them draw a line perpendicular to the x, y, w, v, u, t, and s axes and say this is the r-axis. Have them draw a line perpendicular to the x, y, w, v, u, t, s, and r axes and say this is the q-axis. Have them draw a line perpendicular to the x, y, w, v, u, t, s, r, and q axes and say this is the p-axis. Have them draw a line perpendicular to the x, y, w, v, u, t, s, r, q, and p axes and say this is the o-axis. Have them draw a line perpendicular to the x, y, w, v, u, t, s, r, q, p, and o axes and say this is the n-axis. Have them draw a line perpendicular to the x, y, w, v, u, t, s, r, q, p, o, and n axes and say this is the m-axis. Have them draw a line perpendicular to the x, y, w, v, u, t, s, r, q, p, o, n, and m axes and say this is the l-axis. Have them draw a line perpendicular to the x, y, w, v, u, t, s, r, q, p, o, n, m, and l axes and say this is the k-axis. Have them draw a line perpendicular to the x, y, w, v, u, t, s…"
  },
  {
    "objectID": "lectures/ch4lecture1.html#section",
    "href": "lectures/ch4lecture1.html#section",
    "title": "Ch4 Lecture 1",
    "section": "",
    "text": "Give out toothpicks and cards and markers. Have students put a dot in the center of the card and say this is the origin. Imagine the card is an infinite plane. Now have them hold one end of their marker at the “origin”; the other end of the marker represents vector coordinates. Can they come up with a simple way of describing which points are on the plane?\nHave them poke a toothpick through the card at the origin. (This toothpick represents the normal vector to the plane.) Now can they see a simple way of describing which points are on the plane?\nWrite that the vector describing the toothpick is called \\(\\vec a\\), and we’ll assume it is of unit length. The plane is the set of all vectors \\(\\vec x\\) such that \\(\\vec a\\cdot \\vec x = 0\\). This is the equation of the plane."
  },
  {
    "objectID": "lectures/ch4lecture1.html#finding-the-normal-vector-when-we-know-two-vectors-in-the-plane",
    "href": "lectures/ch4lecture1.html#finding-the-normal-vector-when-we-know-two-vectors-in-the-plane",
    "title": "Ch4 Lecture 1",
    "section": "Finding the normal vector when we know two vectors in the plane",
    "text": "Finding the normal vector when we know two vectors in the plane\n\nNotice that the normal vector is perpendicular to the plane. It’s also going to be orthogonal to vector in the plane, and to any two vectors that lie in the plane.\n\nFor \\(\\mathbb{R}^3\\), we can find the normal vector by taking the cross product of two vectors in the plane.\nLet \\(\\mathbf{i}, \\mathbf{j}\\), and \\(\\mathbf{k}\\) represent the standard basis \\(\\mathbf{e}_{1}, \\mathbf{e}_{2}, \\mathbf{e}_{3}\\).\n\n\\[\n\\mathbf{u} \\times \\mathbf{v}=\\left(u_{2} v_{3}-u_{3} v_{2}\\right) \\mathbf{i}+\\left(u_{3} v_{1}-u_{1} v_{3}\\right) \\mathbf{j}+\\left(u_{1} v_{2}-u_{2} v_{1}\\right) \\mathbf{k}=\\left|\\begin{array}{ccc}\n\\mathbf{i} & \\mathbf{j} & \\mathbf{k} \\\\\nu_{1} & u_{2} & u_{3} \\\\\nv_{1} & v_{2} & v_{3}\n\\end{array}\\right|\n\\]\n\n\nThe cross product is orthogonal to both \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\): \\(\\mathbf{u} \\cdot \\mathbf{u} \\times \\mathbf{v}=0\\) and \\(\\mathbf{v} \\cdot \\mathbf{u} \\times \\mathbf{v}=0\\)\n\n\nWe can use the cross product to find the normal vector to the plane, simply by taking the cross product of two vectors in the plane."
  },
  {
    "objectID": "lectures/ch4lecture1.html#finding-the-normal-vector-from-the-equation-of-the-plane",
    "href": "lectures/ch4lecture1.html#finding-the-normal-vector-from-the-equation-of-the-plane",
    "title": "Ch4 Lecture 1",
    "section": "Finding the normal vector from the equation of the plane",
    "text": "Finding the normal vector from the equation of the plane\nIf the plane is specified \\(ax + by + cz = d\\), then a normal vector is \\(\\vec a = ( a, b, c )\\).\n\nCheck:\n\nFind three points on the plane.\n\nSet the first point where \\(x = 0\\) and \\(y = 0\\), so \\(z = d/c\\): \\((0, 0, d/c)\\).\nSet second point is where \\(x=0\\) and \\(z = 0\\), so \\(y=d/b\\): \\((0,d/b,0)\\).\nThe third point is \\((d/a,0,0)\\).\n\nNow find two displacement vectors in the plane:\n\n\\(\\vec u = (0, d/b, 0) - (0, 0, d/c) = (0, d/b, -d/c)\\) and\n\\(\\vec v = (d/a, 0, 0) - (0, 0, d/c)=(d/a,0,-d/c)\\)."
  },
  {
    "objectID": "lectures/ch4lecture1.html#section-1",
    "href": "lectures/ch4lecture1.html#section-1",
    "title": "Ch4 Lecture 1",
    "section": "",
    "text": "Now take the cross product of these two vectors to find the normal vector:\n\\[\n\\begin{aligned}\n\\vec u \\times \\vec v \\\\\n&= \\left|\\begin{array}{ccc}\n\\mathbf{i} & \\mathbf{j} & \\mathbf{k} \\\\\n0 & d/b & -d/c \\\\\nd/a & 0 & -d/c\n\\end{array}\\right| \\\\\n&= - \\mathbf{i} \\left( \\frac{d}{b} \\frac{d}{c} \\right) + \\mathbf{j} \\left( \\frac{d}{a} \\frac{d}{c} \\right) - \\mathbf{k} \\left( \\frac{d}{a} \\frac{d}{b} \\right) \\\\\n&= -d^2 \\left[ \\left( \\frac{1}{bc} \\right) \\mathbf{i} - \\left( \\frac{1}{ac} \\right) \\mathbf{j} + \\left( \\frac{1}{ab} \\right) \\mathbf{k} \\right]\\\\\n&= -\\frac{d^2}{abc} \\left[\\left( a \\right) \\mathbf{i} - \\left( b \\right) \\mathbf{j} + \\left( c \\right) \\mathbf{k} \\right]\\\\\n\\end{aligned}\n\\]\nThis is parallel to \\(\\vec{a} = (a, b, c)\\)."
  },
  {
    "objectID": "lectures/ch4lecture1.html#distances-from-the-plane",
    "href": "lectures/ch4lecture1.html#distances-from-the-plane",
    "title": "Ch4 Lecture 1",
    "section": "Distances from the plane",
    "text": "Distances from the plane\n\nHave them hold the marker still with its end at the origin, but with the point not on the plane. Can they describe the distance to the plane?\nNotice that it’s the same as the scalar projection of the vector onto the normal vector: projection of \\(\\vec x\\) onto \\(\\vec a\\). That is, the distance is \\(|\\vec a\\cdot \\vec x|\\). (If the normal vector is not of unit length, the distance would have been \\(|\\vec a\\cdot \\vec x|/||\\vec a||\\).)"
  },
  {
    "objectID": "lectures/ch4lecture1.html#section-2",
    "href": "lectures/ch4lecture1.html#section-2",
    "title": "Ch4 Lecture 1",
    "section": "",
    "text": "Now have them figure out the displacement vector that points from \\(\\vec x\\) to the closest point on the plane.\nIt is in the same direction as \\(\\vec a\\), and its length is \\(|\\vec a\\cdot \\vec x|/ ||\\vec a||\\). This is the projection of \\(\\vec x\\) onto the normal vector."
  },
  {
    "objectID": "lectures/ch4lecture1.html#back-onto-the-plane",
    "href": "lectures/ch4lecture1.html#back-onto-the-plane",
    "title": "Ch4 Lecture 1",
    "section": "Back onto the plane",
    "text": "Back onto the plane\n\nNow have them find the location of the point on the plane that is closest to \\(\\vec x\\).\nIt is \\(\\vec x - \\vec a |\\vec x \\cdot\\vec a|/||\\vec a||^2\\).\nThis is the projection of \\(\\vec x\\) onto the plane. We find it by subtracting the projection of \\(\\vec x\\) onto the normal vector from \\(\\vec x\\).\nProjection onto the plane is the same as the displacement vector from the point to the plane is the same as the closest point on the plane to the point."
  },
  {
    "objectID": "lectures/ch4lecture1.html#a-plane-not-at-the-origin",
    "href": "lectures/ch4lecture1.html#a-plane-not-at-the-origin",
    "title": "Ch4 Lecture 1",
    "section": "A plane not at the origin",
    "text": "A plane not at the origin\n\nHave them designate the origin as some other point, not on the card.\nCan they come up with a simple way of describing what the displacement was?\nThey can describe the displacement of the plane with a scalar multiple of the normal vector.\nCall this q \\(\\vec a\\)."
  },
  {
    "objectID": "lectures/ch4lecture1.html#section-3",
    "href": "lectures/ch4lecture1.html#section-3",
    "title": "Ch4 Lecture 1",
    "section": "",
    "text": "Now how can they describe which points are on the plane?\nThe simplest way (for my brain at least) is to imagine taking a point on the plane and subtracting the displacement vector from it. Is the new point where the plane would be if the plane were at the origin?\nSo, take \\(\\vec x\\) and subtract \\(\\vec a q\\) from it. Would this point fall in a plane at the origin? That is, is \\(\\vec a\\cdot (\\vec x - \\vec a q) = 0\\)?\nWe can simplify this: \\(\\vec a\\cdot \\vec x - \\vec a\\cdot \\vec a q = 0\\). So \\(\\vec a\\cdot \\vec x = \\vec a\\cdot \\vec a q\\). So \\(\\vec a\\cdot \\vec x = q\\).\nThis is the equation of the plane."
  },
  {
    "objectID": "lectures/ch4lecture1.html#hyperplane",
    "href": "lectures/ch4lecture1.html#hyperplane",
    "title": "Ch4 Lecture 1",
    "section": "Hyperplane",
    "text": "Hyperplane\n\nA hyperplane in \\(\\mathbb{R}^{n}\\) is the set of all \\(\\mathbf{x} \\in \\mathbb{R}^{n}\\) such that \\(\\mathbf{a} \\cdot \\mathbf{x}=b\\), where the nonzero vector \\(\\mathbf{a} \\in \\mathbb{R}^{n}\\) and scalar \\(b\\) are given.\n\n\n\nLet \\(H\\) be the hyperplane in \\(\\mathbb{R}^{n}\\) defined by the equation \\(\\mathbf{a} \\cdot \\mathbf{x}=b\\) and let \\(\\mathbf{x}_{*} \\in H\\). Then\n\n\\(\\mathbf{a}^{\\perp}=\\left\\{\\mathbf{y} \\in \\mathbb{R}^{n} \\mid \\mathbf{a} \\cdot \\mathbf{y}=0\\right\\}\\) is a subspace of \\(\\mathbb{R}^{n}\\) of dimension \\(n-1\\).\n\\(H=\\mathbf{x}_{*}+\\mathbf{a}^{\\perp}=\\left\\{\\mathbf{x}_{*}+\\mathbf{y} \\mid \\mathbf{y} \\in \\mathbf{a}^{\\perp}\\right\\}\\)."
  },
  {
    "objectID": "lectures/ch4lecture1.html#example",
    "href": "lectures/ch4lecture1.html#example",
    "title": "Ch4 Lecture 1",
    "section": "Example",
    "text": "Example\nFind an equation that defines the plane containing the three (noncollinear) points \\(P, Q\\), and \\(R\\) with coordinates \\((1,0,2),(2,1,0)\\), and \\((3,1,1)\\), respectively.\n\n\\[\n\\begin{aligned}\n& \\overrightarrow{P Q}=(2,1,0)-(1,0,2)=(1,1,-2) \\\\\n& \\overrightarrow{P R}=(3,1,1)-(1,0,2)=(2,1,-1)\n\\end{aligned}\n\\]\n\n\n\\[\n\\mathbf{u} \\times \\mathbf{v}=\\left|\\begin{array}{ccc}\n\\mathbf{i} & \\mathbf{j} & \\mathbf{k} \\\\\n1 & 1 & -2 \\\\\n2 & 1 & -1\n\\end{array}\\right|=\\mathbf{i}-3 \\mathbf{j}-\\mathbf{k}\n\\]\n\n\nThe normal vector is \\(\\mathbf{a}=(1,-3,-1)\\).\n\n\nThe equation of the plane is \\(\\mathbf{a} \\cdot \\mathbf{x}=b\\)\nWhat is \\(b\\)? We can plug in any of the points and solve for it. Let’s use \\(P\\):\n\n\n\\(b=\\mathbf{a} \\cdot \\mathbf{P}=(1,-3,-1) \\cdot(1,0,2)=1+0-2=-1\\)."
  },
  {
    "objectID": "lectures/ch4lecture1.html#projections-to-the-displaced-plane",
    "href": "lectures/ch4lecture1.html#projections-to-the-displaced-plane",
    "title": "Ch4 Lecture 1",
    "section": "Projections to the displaced plane",
    "text": "Projections to the displaced plane\n\nNow have them find the displacement vector that points from \\(\\vec x\\) to the closest point on the plane.\nThis will be the same as if we again subtract to get a plane that goes through the origin. So the displaced position is $x - q a $, and its distance from the plane that goes through the origin is this dot a: $a(x - q a)=ax - q $.\nMultiplying by \\(\\vec a\\) we get $a (ax - q) $.\nThen, to find the closest point on the plane, we subtract this from \\(\\vec x\\): $x - a (ax - q) $."
  },
  {
    "objectID": "lectures/ch4lecture1.html#summary",
    "href": "lectures/ch4lecture1.html#summary",
    "title": "Ch4 Lecture 1",
    "section": "Summary",
    "text": "Summary\n\nPlane through the origin: \\(\\vec a\\cdot \\vec x = 0\\)\nPlane displaced from the origin: \\(\\vec a\\cdot \\vec x = q\\)\nComponent of \\(\\vec x\\) along the vector \\(\\vec a\\) (scalar projection): \\(\\vec a\\cdot \\vec x / || \\vec a ||\\)\nProjection of \\(\\vec x\\) onto \\(\\vec a\\): \\(\\vec a / ||a|| \\quad \\vec a\\cdot \\vec x/||\\vec a||\\)\nProjection of \\(\\vec x\\) onto the plane for which \\(\\vec a\\) is the normal vector, displaced by \\(q \\vec a\\) from the origin: \\(\\vec x\\) - projection onto the normal vector = $x - a (ax - q) $"
  },
  {
    "objectID": "lectures/ch4lecture1.html#projection-formula-for-vectors",
    "href": "lectures/ch4lecture1.html#projection-formula-for-vectors",
    "title": "Ch4 Lecture 1",
    "section": "Projection Formula for Vectors",
    "text": "Projection Formula for Vectors\nLet \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) be vectors, \\(\\mathbf{v} \\neq \\mathbf{0}\\).\n\\[\n\\mathbf{p}=\\frac{\\mathbf{v} \\cdot \\mathbf{u}}{\\mathbf{v} \\cdot \\mathbf{v}} \\mathbf{v} \\text { and } \\quad \\mathbf{q}=\\mathbf{u}-\\mathbf{p}\n\\]\n\n\nThen \\(\\mathbf{p}\\) is parallel to \\(\\mathbf{v}, \\mathbf{q}\\) is orthogonal to \\(\\mathbf{v}\\), and \\(\\mathbf{u}=\\mathbf{p}+\\mathbf{q}\\).\n. . . We say \\(\\operatorname{proj}_{\\mathbf{v}} \\mathbf{u}=\\frac{\\mathbf{v} \\cdot \\mathbf{u}}{\\mathbf{v} \\cdot \\mathbf{v}} \\mathbf{v}\\) and \\(\\operatorname{orth}_{\\mathbf{v}} \\mathbf{u}=\\mathbf{u}-\\operatorname{proj}_{\\mathbf{v}} \\mathbf{u}\\)"
  },
  {
    "objectID": "lectures/ch4lecture1.html#taking-a-step-back-from-linear-algebra-fitting-a-line-to-data",
    "href": "lectures/ch4lecture1.html#taking-a-step-back-from-linear-algebra-fitting-a-line-to-data",
    "title": "Ch4 Lecture 1",
    "section": "Taking a step back from linear algebra: fitting a line to data",
    "text": "Taking a step back from linear algebra: fitting a line to data\n\nDraw a scatter plot of some data. Draw a line that seems to fit the data well. Make a separate plot of the residuals vs x.\nMention least squares.\nNow draw a line that doesn’t fit the data well. Make a separate plot of the residuals. Notice that now there is a relationship between the residuals and x – their correlation is not zero.\nIdea of least squares can then to be to find the line such that the residuals are uncorrelated with x – so that the slope of the residuals vs x is zero.\nThat is, we want the residuals to be orthogonal to the line."
  },
  {
    "objectID": "lectures/ch4lecture1.html#straight-lines-of-x-and-y-now-in-rn",
    "href": "lectures/ch4lecture1.html#straight-lines-of-x-and-y-now-in-rn",
    "title": "Ch4 Lecture 1",
    "section": "Straight lines of x and y, now in Rn",
    "text": "Straight lines of x and y, now in Rn\n\nHave the students draw a scatter plot of some data in x and y, which are on a straight line. Ask them what this looks like if we plot the data in \\(\\mathbb{R}^n\\). What does \\(x\\) look like now? What is its relation to \\(y\\)?\nHint: have them write the relationship between \\(x_1\\) and \\(y_1\\).\nWe can think of the the data \\(x\\) as a single point in \\(\\mathbb{R}^n\\), and the same for \\(y\\). So if there is a linear relationship between \\(x\\) and \\(y\\), then \\(\\mathbf{y}\\) a multiple of \\(\\mathbf{x}\\)."
  },
  {
    "objectID": "lectures/ch4lecture1.html#imperfect-data",
    "href": "lectures/ch4lecture1.html#imperfect-data",
    "title": "Ch4 Lecture 1",
    "section": "Imperfect data",
    "text": "Imperfect data\n\nNow have them realize what happens if the data is not on a straight line. What does the relationship between \\(x\\) and \\(y\\) look like now?\n\\(y\\) is still close to a multiple of \\(x\\), but not exactly. They aren’t exactly parallel.\nThe difference between them is the vector of residuals. The squared norm of the residuals vector is the sum of the individual residuals."
  },
  {
    "objectID": "lectures/ch4lecture1.html#guessing-at-a-fit",
    "href": "lectures/ch4lecture1.html#guessing-at-a-fit",
    "title": "Ch4 Lecture 1",
    "section": "Guessing at a fit",
    "text": "Guessing at a fit\n\nSuppose we think that \\(y\\) is approximately a multiple of \\(x\\), so that \\(\\mathbf y \\approx m \\mathbf x\\). How should we measure how good this guess is?\nIdea: calculate the residuals, which are the differences between the actual \\(y\\) and the guess \\(m x\\). Their norm should be small.\nDraw the residuals."
  },
  {
    "objectID": "lectures/ch4lecture1.html#best-guess",
    "href": "lectures/ch4lecture1.html#best-guess",
    "title": "Ch4 Lecture 1",
    "section": "Best guess",
    "text": "Best guess\n\nFor different values of \\(m\\), the residuals will be different. We want to find the value of \\(m\\) that makes the residuals as small as possible. Geometrically, when will this be?\nWhen the residuals are orthogonal to \\(x\\).\nThere is a line that contains all possible values for multiples of \\(x\\). The residuals will always be a sum of a component along this line and a component orthogonal to it. Notice that the component orthogonal to the line will always be the same – it’s the displacement from \\(y\\) to the closest point on the line. But the parallel component will change depending on what value we choose for \\(m\\).\nTo make the residuals as small as possible, we want the parallel component to be zero. That is, we want the residuals to be orthogonal to \\(x\\)."
  },
  {
    "objectID": "lectures/ch4lecture1.html#least-squares",
    "href": "lectures/ch4lecture1.html#least-squares",
    "title": "Ch4 Lecture 1",
    "section": "Least squares",
    "text": "Least squares\n\\(m \\mathbf{x}-\\mathbf{y}\\) should be orthogonal to \\(\\mathbf{x}\\).\n\\[\n\\begin{aligned}\n\\mathbf{x} \\cdot\\left(m \\mathbf{x}-\\mathbf{y}\\right) &=0 \\\\\nm \\mathbf{x} \\cdot \\mathbf{x}-\\mathbf{x} \\cdot \\mathbf{y} &=0 \\\\\nm \\mathbf{x} \\cdot \\mathbf{x} &=\\mathbf{x} \\cdot \\mathbf{y} \\\\\n\\mathbf{x} \\cdot \\mathbf{x}\\quad m  &=\\mathbf{x} \\cdot \\mathbf{y} \\\\\n\\end{aligned}\n\\]\n\nChange to matrix notation:\n\\(\\mathbf{x} \\rightarrow \\mathbf{A}\\text{,}\\qquad\\) \\(\\mathbf{y} \\rightarrow \\mathbf{b}\\text{,}\\qquad\\) \\(m \\rightarrow \\mathbf{x}\\)\n\n\nLinear equation becomes \\(\\mathbf{A} \\mathbf{x}= \\mathbf{b}\\). But it’s not consistent if the data aren’t on the straight line. Best fit equation becomes\n\n\n\\[\n\\begin{aligned}\n\\mathbf{x} \\cdot \\mathbf{x}\\quad m  &=\\mathbf{x} \\cdot \\mathbf{y} \\\\\n\\mathbf{A}^{T} \\mathbf{A} \\mathbf{x} &=\\mathbf{A}^{T} \\mathbf{b}\n\\end{aligned}\n\\]\nThese are the normal equations. We solve them to find \\(\\mathbf{x}\\) which corresponds to the best fit."
  },
  {
    "objectID": "lectures/ch4lecture1.html#normal-equations-for-a-single-predictor",
    "href": "lectures/ch4lecture1.html#normal-equations-for-a-single-predictor",
    "title": "Ch4 Lecture 1",
    "section": "Normal equations for a single predictor",
    "text": "Normal equations for a single predictor\nWhen we have a single predictor variable and a single response variable, the \\(\\mathbf{x}\\) we are trying to find is just a scalar.\n\n\\(\\mathbf{A}\\) is a column vector of the predictor variable, call it now \\(\\mathbf{a}\\)\n\n\n\\(\\mathbf{b}\\) is a column vector of the response variable.\n\n\nThe normal equations become\n\\[\n\\begin{aligned}\n\\mathbf{a}^{T} \\mathbf{a} \\mathbf{x} &=\\mathbf{a}^{T} \\mathbf{b} \\\\\n\\mathbf{a} \\cdot \\mathbf{a} \\mathbf{x} &=\\mathbf{a} \\cdot \\mathbf{b} \\\\\nx = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\mathbf{a} \\cdot \\mathbf{a}}\n\\end{aligned}\n\\]\nThis is the formula for the slope of the best fit line."
  },
  {
    "objectID": "lectures/ch4lecture1.html#example-1",
    "href": "lectures/ch4lecture1.html#example-1",
    "title": "Ch4 Lecture 1",
    "section": "Example",
    "text": "Example\nYou have a scale that measures in unknown units.\nYou have items that you know weigh approximately 2, 5, and 7 pounds (approximately.)\n\nYou measure the scale’s output for these items and get \\(0.7,2.4\\), and 3.2. What is the conversion factor from the scale’s units to pounds?\n\n\nYou can set up the system of equations:\n\\(\\mathbf{a}= \\begin{bmatrix} 2 \\\\ 5 \\\\ 7 \\end{bmatrix}\\) and \\(\\mathbf{b}= \\begin{bmatrix} 0.7 \\\\ 2.4 \\\\ 3.2 \\end{bmatrix}\\)\nCan you find \\(\\mathbf{x}\\) such that \\(\\mathbf{a} \\cdot \\mathbf{x} = \\mathbf{b}\\)?\n\n\nNo, it’s inconsistent. But you can find the best fit.\n\n\n\\[\n\\begin{aligned}\n\\mathbf{a}^{T} \\mathbf{a} \\mathbf{x} &=\\mathbf{a}^{T} \\mathbf{b} \\\\\n\\begin{bmatrix} 2 & 5 & 7 \\end{bmatrix} \\begin{bmatrix} 2 \\\\ 5 \\\\ 7 \\end{bmatrix} x &= \\begin{bmatrix} 2 & 5 & 7 \\end{bmatrix} \\begin{bmatrix} 0.7 \\\\ 2.4 \\\\ 3.2 \\end{bmatrix}\nx = \\frac{2 \\cdot 0.7 + 5 \\cdot 2.4 + 7 \\cdot 3.2}{2^2 + 5^2 + 7^2} \\approx 0.459\n\\end{aligned}\n\\]\nSo to go from pounds to the scale’s units, multiply by 0.459.\n\n\n(This is the opposite of what the book did – they found the conversion factor from the scale’s units to pounds.)"
  },
  {
    "objectID": "lectures/ch4lecture1.html#normal-equations-for-multiple-predictors",
    "href": "lectures/ch4lecture1.html#normal-equations-for-multiple-predictors",
    "title": "Ch4 Lecture 1",
    "section": "Normal equations for multiple predictors",
    "text": "Normal equations for multiple predictors\nWhen we have multiple predictors, \\(\\mathbf{A}\\) is a matrix\n\nEach column corresponds to a predictor variable.\n\n\n\\(\\mathbf{x}\\) is a column vector of the coefficients we are trying to find, one per predictor.\n\n\nWhat do the normal equations tell us? We can break them apart into one equation corresponding to each column of \\(\\mathbf{A}\\).\n\n\n\\[\n\\begin{aligned}\n\\mathbf{A}^{T} \\mathbf{A} \\mathbf{x} &=\\mathbf{A}^{T} \\mathbf{b} \\\\\n\\text{becomes} \\\\\n\\mathbf{a_i}\\cdot \\mathbf{A} \\mathbf{x} &=\\mathbf{a_i} \\cdot \\mathbf{b}\\text{ for every i} \\\\\n\\mathbf{a_i}\\cdot \\left(\\mathbf{b}-\\mathbf{Ax}\\right)&=0\\text{ for every i}\n\\end{aligned}\n\\]\n\n\nWe need to find \\(\\mathbf{x}\\) such that the residuals are orthogonal to every column of \\(\\mathbf{A}\\)."
  },
  {
    "objectID": "lectures/ch4lecture1.html#example-2",
    "href": "lectures/ch4lecture1.html#example-2",
    "title": "Ch4 Lecture 1",
    "section": "Example",
    "text": "Example\nWe have two predictors \\(a_1\\) and \\(a_2\\) and a response variable \\(b\\). We have the following data:\n\n\n\n\\(x_1\\)\n\\(x_2\\)\n\\(y\\)\n\n\n\n\n2\n1\n0\n\n\n1\n1\n0\n\n\n2\n1\n2\n\n\n\n\nWe are hoping to find a linear relationship of the form \\(\\beta_1 x_1 + \\beta_2 x_2 = y\\) for some values of \\(\\beta_1\\) and \\(\\beta_2\\).\n\n\nThis would bring us the following system of equations:\n\\[\n\\begin{aligned}\n2 \\beta_{1}+\\beta_{2} & =0 \\\\\n\\beta_{1}+\\beta_{2} & =0 \\\\\n2 \\beta_{1}+\\beta_{2} & =2 .\n\\end{aligned}\n\\]\n\n\nObviously inconsistent! (\\(2 \\beta_{1}+\\beta_{2}\\) cannot equal both 0 and 2.)\n\n\nFind the least squares solution using the normal equations:"
  },
  {
    "objectID": "lectures/ch4lecture1.html#section-5",
    "href": "lectures/ch4lecture1.html#section-5",
    "title": "Ch4 Lecture 1",
    "section": "",
    "text": "Change variable names, so the \\(\\beta\\)s are now \\(\\mathbf{x}\\), the \\(x\\)s are now the columns \\(\\mathbf{A}\\), and the \\(y\\)s are now \\(\\mathbf{b}\\).\n\\[\nA=\\left[\\begin{array}{ll}\n2 & 1 \\\\\n1 & 1 \\\\\n2 & 1\n\\end{array}\\right], \\text { and } \\mathbf{b}=\\left[\\begin{array}{l}\n0 \\\\\n0 \\\\\n2\n\\end{array}\\right]\n\\]\n\nResiduals are \\(\\mathbf{b}-\\mathbf{A} \\mathbf{x}\\).\nNormal equations are \\(\\mathbf{A}^{T} \\mathbf{A} \\mathbf{x}=\\mathbf{A}^{T} \\mathbf{b}\\)."
  },
  {
    "objectID": "lectures/ch4lecture1.html#section-6",
    "href": "lectures/ch4lecture1.html#section-6",
    "title": "Ch4 Lecture 1",
    "section": "",
    "text": "\\[\nA^{T} A=\\left[\\begin{array}{lll}\n2 & 1 & 2 \\\\\n1 & 1 & 1\n\\end{array}\\right]\\left[\\begin{array}{ll}\n2 & 1 \\\\\n1 & 1 \\\\\n2 & 1\n\\end{array}\\right]=\\left[\\begin{array}{ll}\n9 & 5 \\\\\n5 & 3\n\\end{array}\\right]\n\\]\nwith inverse \\[\n\\left(A^{T} A\\right)^{-1}=\\left[\\begin{array}{ll}\n9 & 5 \\\\\n5 & 3\n\\end{array}\\right]^{-1}=\\frac{1}{2}\\left[\\begin{array}{rr}\n3 & -5 \\\\\n-5 & 9\n\\end{array}\\right]\n\\]\n\n\\[\nA^{T} \\mathbf{b}=\\left[\\begin{array}{lll}\n2 & 1 & 2 \\\\\n1 & 1 & 1\n\\end{array}\\right]\\left[\\begin{array}{l}\n0 \\\\\n0 \\\\\n2\n\\end{array}\\right]=\\left[\\begin{array}{l}\n4 \\\\\n2\n\\end{array}\\right]\n\\]"
  },
  {
    "objectID": "lectures/ch4lecture1.html#least-squares-solution",
    "href": "lectures/ch4lecture1.html#least-squares-solution",
    "title": "Ch4 Lecture 1",
    "section": "Least Squares Solution",
    "text": "Least Squares Solution\n\\[\n\\mathbf{x}=\\left(A^{T} A\\right)^{-1} A^{T} \\mathbf{b}=\\frac{1}{2}\\left[\\begin{array}{rr}\n3 & -5 \\\\\n-5 & 9\n\\end{array}\\right]\\left[\\begin{array}{l}\n4 \\\\\n2\n\\end{array}\\right]=\\left[\\begin{array}{r}\n1 \\\\\n-1\n\\end{array}\\right]\n\\]\n\nBack in our original variables, this means the best fit estimate for \\(\\beta_1\\) is 1 and the best \\(\\beta_2\\) is -1."
  },
  {
    "objectID": "lectures/ch4lecture1.html#section-7",
    "href": "lectures/ch4lecture1.html#section-7",
    "title": "Ch4 Lecture 1",
    "section": "",
    "text": "Using 3d plotting tools, visualize what just happened in the previous example."
  },
  {
    "objectID": "lectures/ch4lecture1.html#least-squares-1",
    "href": "lectures/ch4lecture1.html#least-squares-1",
    "title": "Ch4 Lecture 1",
    "section": "Least squares",
    "text": "Least squares\n\\(m \\mathbf{x}-\\mathbf{y}\\) should be orthogonal to \\(\\mathbf{x}\\).\n\\[\n\\begin{aligned}\n\\mathbf{x} \\cdot\\left(m \\mathbf{x}-\\mathbf{y}\\right) &=0 \\\\\nm \\mathbf{x} \\cdot \\mathbf{x}-\\mathbf{x} \\cdot \\mathbf{y} &=0 \\\\\nm \\mathbf{x} \\cdot \\mathbf{x} &=\\mathbf{x} \\cdot \\mathbf{y} \\\\\n\\mathbf{x} \\cdot \\mathbf{x}\\quad m  &=\\mathbf{x} \\cdot \\mathbf{y} \\\\\n\\end{aligned}\n\\]\n\nChange to matrix notation:\n\\(\\mathbf{x} \\rightarrow \\mathbf{A}\\text{,}\\qquad\\) \\(\\mathbf{y} \\rightarrow \\mathbf{b}\\text{,}\\qquad\\) \\(m \\rightarrow \\mathbf{x}\\)\n\n\nLinear equation becomes \\(\\mathbf{A} \\mathbf{x}= \\mathbf{b}\\). But it’s not consistent if the data aren’t on the straight line. Best fit equation becomes\n\n\n\\[\n\\begin{aligned}\n\\mathbf{x} \\cdot \\mathbf{x}\\quad m  &=\\mathbf{x} \\cdot \\mathbf{y} \\\\\n\\mathbf{A}^{T} \\mathbf{A} \\mathbf{x} &=\\mathbf{A}^{T} \\mathbf{b}\n\\end{aligned}\n\\]\nThese are the normal equations. We solve them to find \\(\\mathbf{x}\\) which corresponds to the best fit."
  },
  {
    "objectID": "lectures/ch4lecture1.html#orthogonal-coordinates-theorem",
    "href": "lectures/ch4lecture1.html#orthogonal-coordinates-theorem",
    "title": "Ch4 Lecture 1",
    "section": "Orthogonal Coordinates Theorem",
    "text": "Orthogonal Coordinates Theorem\n\nIf \\(\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\ldots, \\mathbf{v}_{n}\\) are nonzero and orthogonal, and\n\\(\\mathbf{v} \\in \\operatorname{span}\\left\\{\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\ldots, \\mathbf{v}_{n}\\right\\}\\),\n\\(\\mathbf{v}\\) can be expressed uniquely (up to order) as a linear combination of \\(\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\ldots, \\mathbf{v}_{n}\\), namely\n\\[\n\\mathbf{v}=\\frac{\\mathbf{v}_{1} \\cdot \\mathbf{v}}{\\mathbf{v}_{1} \\cdot \\mathbf{v}_{1}} \\mathbf{v}_{1}+\\frac{\\mathbf{v}_{2} \\cdot \\mathbf{v}}{\\mathbf{v}_{2} \\cdot \\mathbf{v}_{2}} \\mathbf{v}_{2}+\\cdots+\\frac{\\mathbf{v}_{n} \\cdot \\mathbf{v}}{\\mathbf{v}_{n} \\cdot \\mathbf{v}_{n}} \\mathbf{v}_{n}\n\\]"
  },
  {
    "objectID": "lectures/ch4lecture1.html#proof",
    "href": "lectures/ch4lecture1.html#proof",
    "title": "Ch4 Lecture 1",
    "section": "Proof",
    "text": "Proof\nSince \\(\\mathbf{v} \\in \\operatorname{span}\\left\\{\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\ldots, \\mathbf{v}_{n}\\right\\}\\),\nwe can write \\(\\mathbf{v}\\) as a linear combination of the \\(\\mathbf{v}_{i}\\) ’s:\n\\[\n\\mathbf{v}=c_{1} \\mathbf{v}_{1}+c_{2} \\mathbf{v}_{2}+\\cdots+c_{n} \\mathbf{v}_{n}\n\\]\n\nNow take the inner product of both sides with \\(\\mathbf{v}_{k}\\):\n\n\nSince \\(\\mathbf{v}_{k} \\cdot \\mathbf{v}_{j}=0\\) if \\(j \\neq k\\),\n\\[\n\\begin{aligned}\n\\mathbf{v}_{k} \\cdot \\mathbf{v} & =\\mathbf{v}_{k} \\cdot\\left(c_{1} \\mathbf{v}_{1}+c_{2} \\mathbf{v}_{2}+\\ldots \\cdots+c_{n} \\mathbf{v}_{n}\\right) \\\\\n& =c_{1} \\mathbf{v}_{k} \\cdot \\mathbf{v}_{1}+c_{2} \\mathbf{v}_{k} \\cdot \\mathbf{v}_{2}+\\cdots+c_{n} \\mathbf{v}_{k} \\cdot \\mathbf{v}_{n}=c_{k} \\mathbf{v}_{k} \\cdot \\mathbf{v}_{k}\n\\end{aligned}\n\\]\n\n\nSince \\(\\mathbf{v}_{k} \\neq 0\\), \\(\\left\\|\\mathbf{v}_{k}\\right\\|^{2}=\\mathbf{v}_{k} \\cdot \\mathbf{v}_{k} \\neq 0\\) so we can divide::\n\\[\nc_{k}=\\frac{\\mathbf{v}_{k} \\cdot \\mathbf{v}}{\\mathbf{v}_{k} \\cdot \\mathbf{v}_{k}}\n\\]\n\n\nAny linear combination of an orthogonal set of nonzero vectors is the sum of its projections in the direction of each vector in the set."
  },
  {
    "objectID": "lectures/ch4lecture1.html#orthogonal-matrix",
    "href": "lectures/ch4lecture1.html#orthogonal-matrix",
    "title": "Ch4 Lecture 1",
    "section": "Orthogonal matrix",
    "text": "Orthogonal matrix\nSuppose \\(\\mathbf{u}_{1}, \\mathbf{u}_{2}, \\ldots, \\mathbf{u}_{n}\\) is an orthonormal basis of \\(\\mathbb{R}^{n}\\)\n\nMake these the column vectors of \\(\\mathbf{A}\\): \\(A=\\left[\\mathbf{u}_{1}, \\mathbf{u}_{2}, \\ldots, \\mathbf{u}_{n}\\right]\\)\n\n\nBecause the \\(\\mathbf{u}_{i}\\) are orthonormal, \\(\\mathbf{u}_{i}^{T} \\mathbf{u}_{j}=\\delta_{i j}\\)\n\n\nUse this to calculate \\(A^{T} A\\).\n\nits \\((i, j)\\) th entry is \\(\\mathbf{u}_{n}^{T} \\mathbf{u}_{n}\\)\n= \\(\\delta_{i j}\\)\nSo we have = \\(A^{T} A=\\left[\\delta_{i j}\\right]=I\\)\nTherefore, \\(A^{T} = A^{-1}\\)"
  },
  {
    "objectID": "lectures/ch4lecture1.html#orthogonal-matrix-1",
    "href": "lectures/ch4lecture1.html#orthogonal-matrix-1",
    "title": "Ch4 Lecture 1",
    "section": "Orthogonal matrix",
    "text": "Orthogonal matrix\n\nA square real matrix \\(Q\\) is called orthogonal if \\(Q^{T}=Q^{-1}\\).\nA square matrix \\(U\\) is called unitary if \\(U^{*}=U^{-1}\\)."
  },
  {
    "objectID": "lectures/ch4lecture1.html#example-3",
    "href": "lectures/ch4lecture1.html#example-3",
    "title": "Ch4 Lecture 1",
    "section": "Example",
    "text": "Example\nShow that the matrix \\(R(\\theta)=\\left[\\begin{array}{rr}\\cos \\theta & -\\sin \\theta \\\\ \\sin \\theta & \\cos \\theta\\end{array}\\right]\\) is orthogonal.\n\n\\[\n\\begin{aligned}\nR(\\theta)^{T} R(\\theta) & =\\left(\\left[\\begin{array}{cc}\n\\cos \\theta & -\\sin \\theta \\\\\n\\sin \\theta & \\cos \\theta\n\\end{array}\\right]\\right)^{T}\\left[\\begin{array}{cc}\n\\cos \\theta & -\\sin \\theta \\\\\n\\sin \\theta & \\cos \\theta\n\\end{array}\\right] \\\\\n& =\\left[\\begin{array}{cc}\n\\cos \\theta \\sin \\theta \\\\\n-\\sin \\theta \\cos \\theta\n\\end{array}\\right]\\left[\\begin{array}{cc}\n\\cos \\theta-\\sin \\theta \\\\\n\\sin \\theta & \\cos \\theta\n\\end{array}\\right] \\\\\n& =\\left[\\begin{array}{cc}\n\\cos ^{2} \\theta+\\sin ^{2} \\theta & \\cos \\theta \\sin \\theta-\\sin \\theta \\cos \\theta \\\\\n-\\cos \\theta \\sin \\theta+\\sin \\theta \\cos \\theta & \\sin ^{2} \\theta+\\cos ^{2} \\theta\n\\end{array}\\right]=\\left[\\begin{array}{ll}\n1 & 0 \\\\\n0 & 1\n\\end{array}\\right],\n\\end{aligned}\n\\]\n\n\nThis rotation matrix preserves vector lengths and angles between vectors (illustrate).\nThus, \\(R(\\theta)\\mathbf{x}\\cdot R(\\theta)\\mathbf{y}=\\mathbf{x} \\cdot \\mathbf{y}\\)\n\n\nThis is true in general for orthogonal matrices:\n\\[\nQ \\mathbf{x} \\cdot Q \\mathbf{y}=(Q \\mathbf{x})^{T} Q \\mathbf{y}=\\mathbf{x}^{T} Q^{T} Q \\mathbf{y}=\\mathbf{x}^{T} \\mathbf{y}=\\mathbf{x} \\cdot \\mathbf{y}\n\\]\nalso,\n\\[\n\\|Q \\mathbf{x}\\|^{2}=Q \\mathbf{x} \\cdot Q \\mathbf{x}=(Q \\mathbf{x})^{T} Q \\mathbf{x}=\\mathbf{x}^{T} Q^{T} Q \\mathbf{x}=\\mathbf{x}^{T} \\mathbf{x}=\\|\\mathbf{x}\\|^{2}\n\\]"
  },
  {
    "objectID": "submitted/projects_1/stat243_project1.html",
    "href": "submitted/projects_1/stat243_project1.html",
    "title": "Project #1 - Bobby Buyalos",
    "section": "",
    "text": "This was nice work overall. I’ve left a bunch of comments and places where I’d like to see more explanation or more testing; I hope you’ll take the opportunity to revise and resubmit!"
  },
  {
    "objectID": "submitted/projects_1/stat243_project1.html#markov-chains",
    "href": "submitted/projects_1/stat243_project1.html#markov-chains",
    "title": "Project #1 - Bobby Buyalos",
    "section": "Markov Chains:",
    "text": "Markov Chains:\nConsider our system (\\(n = 1, 2, \\ldots\\)): \\[\\begin{aligned}\n    &a_n = .5a_{n-1} + .2b_{n-1} + .3c_{n-1} \\\\\n    &b_n = .2a_{n-1} + .6b_{n-1} + .1c_{n-1} \\\\\n    &c_n = .3a_{n-1} + .2b_{n-1} + .6c_{n-1}\n\\end{aligned}\n\\]\n\n\nWhat do these variables represent? (I know from reading the project prompt, but it would be good to include this information in the text as well.)\nThis forms the transition matrix:\n\\[\n\\begin{pmatrix}\n    .5 & .2 & .3 \\\\\n    .2 & .6 & .1 \\\\\n    .3 & .2 & .6\n\\end{pmatrix}\n\\]\nLets see what the market shares will be for the next three years:\n\nimport sympy as sym\n\nP = sym.Matrix([[.5, .2 , .3 ],\n    [.2 , .6 , .1],\n    [.3 , .2 , .6]])\n\nx = sym.Matrix([[1/3], [1/3], [1/3]])\n\nfor i in range(4):\n    print(f\"\\n Year {i}:\")\n    print((P**i)*x)\n\n\n Year 0:\nMatrix([[0.333333333333333], [0.333333333333333], [0.333333333333333]])\n\n Year 1:\nMatrix([[0.333333333333333], [0.300000000000000], [0.366666666666667]])\n\n Year 2:\nMatrix([[0.336666666666667], [0.283333333333333], [0.380000000000000]])\n\n Year 3:\nMatrix([[0.339000000000000], [0.275333333333333], [0.385666666666667]])\n\n\nCampaign #1 has the following transition matrix:\n\\[\n\\begin{pmatrix}\n    .5 & .32 & .3 \\\\\n    .2 & .48 & .1 \\\\\n    .3 & .2 & .6\n\\end{pmatrix}\n\\]\nCampaign #2 has the following transition matrix:\n\\[\n\\begin{pmatrix}\n    .5 & .2 & .42 \\\\\n    .2 & .6 & .1 \\\\\n    .3 & .2 & .48\n\\end{pmatrix}\n\\]\n\nimport random\nC1 = sym.Matrix([[.5, .32 , .3 ],\n    [.2 , .48 , .1],\n    [.3 , .2 , .6]])\n\nC2 = sym.Matrix([[.5, .2 , .42 ],\n    [.2 , .6 , .1],\n    [.3 , .2 , .48]])\n\ndef generate_input_vector():\n    random1 = random.randint(1, 999)\n    random2 = random.randint(1, (1000 - random1))\n    random3 = 1000 - random1 - random2\n    return sym.Matrix([[random1/1000], [random2/1000], [random3/1000]])\n\ndef ten_year_forcast(T, x):\n    return (T**10)*x\n\n\ndef test_system():\n    P_score = 0\n    C1_score = 0\n    C2_score = 0\n    for i in range(2000):\n        x = generate_input_vector()\n        P_value = ten_year_forcast(P, x)[0]\n        C1_value = ten_year_forcast(C1, x)[0]\n        C2_value = ten_year_forcast(C2, x)[0]\n    \n        if P_value &gt;= max(C1_value, C2_value):\n            P_score += 1\n        if C1_value &gt;= max(P_value, C2_value):\n            C1_score += 1\n        if C2_value &gt;= max(C1_value, P_value):\n            C2_score += 1\n    return (f\"P: {P_score}\", f\"C1: {C1_score}\", f\"C2: {C2_score}\")\n\ntest_system()\n\n('P: 0', 'C1: 0', 'C2: 2000')\n\n\n\nFor efficiency, it would be much faster to calculate the matrices to the 10th power once and then use this for each of your 2000 random vectors! If you do this, you’ll also be able to look at the 10 year matrices and reach a conclusion about which campaign is best for the long term, even without having to do the simulation.\nThere is an issue with the random vectors here – the first element of the vector ends up being twice as large as the other two, on average.\n\nx=[]\nfor i in range(2000):\n    x.append(generate_input_vector())\nimport pylab as plt\n\n\nimport numpy as np\ny=np.array(x)\ny.shape\nnp.mean(y, axis=0)\n\narray([[0.499943500000000],\n       [0.251202500000000],\n       [0.248854000000000]], dtype=object)\n\n\n\nI would reccomended going with the second campaign. I ran the following test to come to this conclusion:\nFirst, I randomly generated 2000 inital vectors. I also set up a function that runs, for 10 iterations (i.e. 10 year forcast), the markov chains representing the cases of 1) no ad campaigns 2) the first ad campaign and 3) the third ad campaign. I ran each randomly generated inital vector each markov chain and then counted for which case ended with the highest market share.\nFor all 2,000 inital vectors, the second ad campaign ended with the highest market share. Therefore, I strongly reccomended moving forward with the second ad campaign when considering long-term interests.\nSome limitations of this study may be that it only looks at the long term since it uses a a ten year model. Therefore, if the company is looking for short term gains (next quarter, the next year, etc), then these results may not be helpful and a new study should be conducted. In addition, this model assumes that there is no counter-ad campaign from another company that may change the transition matrix. In other words, this study does not anticipate other changing factors in the market that may change the transition matrix and thus the results.\n\nI liked that you chose to use many random vectors for evaluating the different campaigns (although see the note about the random vectors above).\nI would like to see a bit more thinking about why you got the results that you did. Is there something about the transition matrices that tells you why the second campaign is the best?\nGrade: S (satisfactory)"
  },
  {
    "objectID": "submitted/projects_1/stat243_project1.html#comments",
    "href": "submitted/projects_1/stat243_project1.html#comments",
    "title": "Project #1 - Bobby Buyalos",
    "section": "Comments",
    "text": "Comments\nI liked that you chose to use the random vectors for evaluating the different campaigns."
  },
  {
    "objectID": "submitted/projects_1/stat243_project1.html#sports-ranking",
    "href": "submitted/projects_1/stat243_project1.html#sports-ranking",
    "title": "Project #1 - Bobby Buyalos",
    "section": "Sports Ranking:",
    "text": "Sports Ranking:\nWe will first incode the sports data into a graph structure and adjacency matrix.\nOur first naïve measure of rankings is through the win/loss ratio (calculated below as # wins - # losses):\n\n\n\nTeam\nRatio\n\n\n\n\n7\n4\n\n\n5\n2\n\n\n3\n1\n\n\n6\n0\n\n\n1\n-1\n\n\n2\n-2\n\n\n4\n-4\n\n\n\n\n\nIs this actually a ratio? A bit misleading to use that term!\nThen we have our matrix power ranking:\n\n\n\nTeam\nPower\n\n\n\n\n5\n16\n\n\n7\n16\n\n\n3\n10\n\n\n6\n9\n\n\n2\n6\n\n\n1\n5\n\n\n4\n5\n\n\n\nThese methods yield essentially the same rankings with only minor (at most one place) differences\n\n\nCan you comment on what about the math gives the difference? Why do teams 5 and 7 look the same for the second method, but different for the first?\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport scipy\nimport numpy as np\n\nE = [(1, 2), (7, 3), (2, 4), (4, 5), (3, 2), (5, 1), (6, 1), (7, 2), (2, 6), (3, 4), (7, 4), (5, 7), (6, 4), (3, 5), (5, 6), (7, 1), (5, 2), (7, 6), (1, 4), (6, 3)]\n\nM = [4, 8, 7, 3, 7, 23, 15, 6, 18, 13, 14, 7, 13, 7, 18, 45, 10, 19, 14, 13]\n\n\nG = nx.DiGraph()\n\n# Add nodes\nG.add_nodes_from([1, 2, 3, 4, 5, 6, 7])\n\n# Add vertices\nG.add_edges_from(E)\n\n# Draw the graph\nnx.draw_circular(G, with_labels=True)\nplt.show()\n\n\n#convert to numpy\nadj_matrix = nx.adjacency_matrix(G)\nadj_matrix = adj_matrix.toarray()\n\n#form transpose\ntranspose = adj_matrix.transpose()\n\n#calculate win-loss ratio\nfor i in range (7):\n    print(f\"Team {i + 1}'s rato: {sum(adj_matrix[i] - transpose[i])}\")\n\nprint(\"\\n\")\n\n#Calculate powers of all verticies:\npower_matrix = adj_matrix + adj_matrix.dot(adj_matrix)\nfor i in range (7):\n    print(f\"Team {i + 1}'s power: {sum(power_matrix[i])}\")\n\n\n\n\n\n\n\n\nTeam 1's rato: -1\nTeam 2's rato: -2\nTeam 3's rato: 1\nTeam 4's rato: -4\nTeam 5's rato: 2\nTeam 6's rato: 0\nTeam 7's rato: 4\n\n\nTeam 1's power: 5\nTeam 2's power: 6\nTeam 3's power: 10\nTeam 4's power: 5\nTeam 5's power: 16\nTeam 6's power: 9\nTeam 7's power: 16\n\n\nNow we will do the reverse page rank methodology (rounded to three decimal places):\n\n\n\nIndex\nValue\n\n\n\n\n5\n0.250\n\n\n7\n0.185\n\n\n3\n0.166\n\n\n6\n0.131\n\n\n4\n0.128\n\n\n2\n0.080\n\n\n1\n0.060\n\n\n\nThis ranking is very similar to the previous two, the most signifcant difference is the place of 4, who has risen to 3-rd from worst, whereas 4 was the worst team in the previous two rankings.\n\n#Create Diag. Matrix D\ndiag_vals = []\nfor i in range(7):\n    total = np.sum(transpose[i])\n    if total != 0:\n        diag_vals.append(1/total)\n    else:\n        diag_vals.append(0)\nD = np.diag(diag_vals)\n\n#form our transition matrix\ntransition = adj_matrix.dot(D)\n\n#create function to solve our system\ndef find_stable(alpha, v, P, Id):\n    to_invert = Id - alpha*P\n    inverse = to_invert.inv()\n    stable = inverse*((1-alpha)*v)\n    return stable\n\nI_7 = sym.Matrix([[1, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0], \\\n    [0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 1]])\n\nv = sym.Matrix([[1/7], [1/7], [1/7], [1/7], [1/7], [1/7], [1/7]])\n\n#et voila, we have our reverse page rank\nfind_stable(0.85, v, transition, I_7)\n\n\\(\\displaystyle \\left[\\begin{matrix}0.0601644551488991\\\\0.0802002135998523\\\\0.166343756251851\\\\0.127607872531524\\\\0.249833649654006\\\\0.130864601791489\\\\0.184985451022379\\end{matrix}\\right]\\)\n\n\n\n\nWhat is the math behind what’s happening in this code here? What is reverse page rank, and what exactly is happening in the find_stable function? How can I, the reader, know that it’s working as it should?\nNow we will do the weighted power ranking:\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport scipy\nimport numpy as np\n\nE = [(1, 2), (7, 3), (2, 4), (4, 5), (3, 2), (5, 1), (6, 1), (7, 2), (2, 6), (3, 4), (7, 4), (5, 7), (6, 4), (3, 5), (5, 6), (7, 1), (5, 2), (7, 6), (1, 4), (6, 3)]\n\nM = [4, 8, 7, 3, 7, 23, 15, 6, 18, 13, 14, 7, 13, 7, 18, 45, 10, 19, 14, 13]\n\nF = nx.DiGraph()\nnodes = range(1, 8)\nF.add_nodes_from(nodes)\n\nfor i, tuple in enumerate(E):\n    a, b = tuple\n    F.add_edge(a, b, weight = M[i])\n\nnx.draw_circular(F, with_labels=True)\nplt.show()\n\nweighted_adj = nx.adjacency_matrix(F)\nweighted_adj = weighted_adj.toarray()\nprint(weighted_adj)\n\n#power matrix:\npower_weighted_matrix = weighted_adj + weighted_adj.dot(weighted_adj)\nfor i in range (7):\n    print(f\"Team {i + 1}'s power: {sum(power_weighted_matrix[i])}\")\n\n\n\n\n\n\n\n\n[[ 0  4  0 14  0  0  0]\n [ 0  0  0  7  0 18  0]\n [ 0  7  0 13  7  0  0]\n [ 0  0  0  0  3  0  0]\n [23 10  0  0  0 18  7]\n [15  0 13 13  0  0  0]\n [45  6  8 14  0 19  0]]\nTeam 1's power: 160\nTeam 2's power: 784\nTeam 3's power: 647\nTeam 4's power: 177\nTeam 5's power: 2104\nTeam 6's power: 701\nTeam 7's power: 2089\n\n\nWe can see the power ranking of the weighted matrix:\n\n\n\nTeam\nPower\n\n\n\n\nTeam 5\n2104\n\n\nTeam 7\n2089\n\n\nTeam 2\n784\n\n\nTeam 6\n701\n\n\nTeam 3\n647\n\n\nTeam 4\n177\n\n\nTeam 1\n160\n\n\n\n\nI would like to see more explanation about what is happening mathematically in these different cases; what are each of the rankings measuring, what formulas apply, and why your code matches what these formulas say. Then, I’d like a discussion of why the different rankings give different results, and which one you think is the most accurate.\nAlso: there are a lot of places where you are doing things manually (like entering the identity matrix by hand) or using loops to do matrix computations. You’d be better off using numpy or sympy functions to do these things, as they are much faster and less error-prone (and easier to write, too!)\nGrade: R"
  },
  {
    "objectID": "submitted/projects_1/stat243_project1.html#plu-factorization",
    "href": "submitted/projects_1/stat243_project1.html#plu-factorization",
    "title": "Project #1 - Bobby Buyalos",
    "section": "PLU Factorization",
    "text": "PLU Factorization\nWe will first create the PLU_factorization algo (assuming non-zero diag entries):\n\nimport copy\ndef PLU_factorization(A):\n    \"\"\"\n    PLU factorization algo. Assuming non-zero diag. entires.\n\n    Inputs:\n    P [numpy array] - input square matrix\n\n    Outputs tuple[numpy array, numpy array, numpy array] - will reutrn P, L \n    and U matrices in that order\n    \"\"\"\n    P = copy.deepcopy(A)\n    numrows = len(P)\n    numcols = len(P[0])\n    index_list = [i for i in range(numrows)]\n    multipliers = {}\n\n    for k in range(numcols - 1):\n        if int(P[index_list[k]][k]) == 0:\n            for i in range(index_list[k], numrows):\n                if P[i][k] != 0:\n                    i_index = index_list.index(i)\n                    k_index = index_list.index(k)\n                    index_list[k_index], index_list[i_index] = index_list[i_index], index_list[k_index]\n                    break\n        #run again in case we dont find a nonzero value\n        if int(P[index_list[k]][k]) != 0:\n            for i in range(k + 1, numrows):\n                m = index_list.index(i)\n                multiplier = - P[m][k] / P[index_list[k]][k]\n                multipliers[(m, k)] = multiplier\n                P[m] = P[m] + multiplier*(P[index_list[k]])\n    \n    #Create Lower Triangular Matrix\n    zero_matrix = np.eye((numrows))\n    for tuple in multipliers:\n        x, y = tuple\n        zero_matrix[index_list[x]][y] = - multipliers[tuple]\n    \n    #Create Permutation Matrix and Upper Triangular Matrix:\n    perm_matrix = []\n    U = []\n    for i in index_list:\n        y = [0]*numrows\n        y[i] = 1\n        perm_matrix.append(y)\n        U.append(list(P[i]))\n    return (np.array(perm_matrix), zero_matrix, np.array(U))\n\nTo show that these work, we will show the LU factorization problems from HW3:\n\n#Problem 11:\ntest1 = np.array([[2, -1, 1], [2, 3, -2], [4, 2, 2]])\nP, L, U = PLU_factorization(test1)\nprint(\"PROBLEM 11\")\nprint(\"P:\", P)\nprint(\"L:\", L)\nprint(\"U:\", U)\n\nprint(\"LU = \", L.dot(U))\nprint(\"A = \", test1)\n\nPROBLEM 11\nP: [[1 0 0]\n [0 1 0]\n [0 0 1]]\nL: [[1. 0. 0.]\n [1. 1. 0.]\n [2. 1. 1.]]\nU: [[ 2 -1  1]\n [ 0  4 -3]\n [ 0  0  3]]\nLU =  [[ 2. -1.  1.]\n [ 2.  3. -2.]\n [ 4.  2.  2.]]\nA =  [[ 2 -1  1]\n [ 2  3 -2]\n [ 4  2  2]]\n\n\n\n#problem 12\ntest2 = np.array([[2, 1, 0], [-4, -1, -1], [2, 3, -3]])\nP, L, U = PLU_factorization(test2)\nprint(\"PROBLEM 12\")\nprint(\"P:\", P)\nprint(\"L:\", L)\nprint(\"U:\", U)\nprint(\"LU = \", L.dot(U))\nprint(\"A = \", test2)\nprint(\"\\n\")\n\nPROBLEM 12\nP: [[1 0 0]\n [0 1 0]\n [0 0 1]]\nL: [[ 1.  0.  0.]\n [-2.  1.  0.]\n [ 1.  2.  1.]]\nU: [[ 2  1  0]\n [ 0  1 -1]\n [ 0  0 -1]]\nLU =  [[ 2.  1.  0.]\n [-4. -1. -1.]\n [ 2.  3. -3.]]\nA =  [[ 2  1  0]\n [-4 -1 -1]\n [ 2  3 -3]]\n\n\n\n\n\n#problem 13\ntest3 = np.array([[2, 1, 3], [-4, -2, -1], [2, 3, -3]])\nP, L, U = PLU_factorization(test3)\nprint(\"PROBLEM 13\")\nprint(\"P:\", P)\nprint(\"L:\", L)\nprint(\"U:\", U)\nprint(\"LU = \", L.dot(U))\nprint(\"PA = \", P.dot(test3))\nprint(\"\\n\")\n\nPROBLEM 13\nP: [[1 0 0]\n [0 0 1]\n [0 1 0]]\nL: [[ 1.  0.  0.]\n [ 1.  1.  0.]\n [-2. -0.  1.]]\nU: [[ 2  1  3]\n [ 0  2 -6]\n [ 0  0  5]]\nLU =  [[ 2.  1.  3.]\n [ 2.  3. -3.]\n [-4. -2. -1.]]\nPA =  [[ 2  1  3]\n [ 2  3 -3]\n [-4 -2 -1]]\n\n\n\n\nNow we will code the PLU solver algo:\n\ndef PLU_solver(A, b):\n    \"\"\"\n    Given a square matrix A (non-zero diag entries) and a solution b, solve for the unknowns using\n    PLU factorization (i.e. solve for x in Ax = b)\n    \"\"\"\n    size = len(A)\n    P, L, U = PLU_factorization(A)\n    #find  Ly = Pb\n    Pb = P.dot(b)\n    y = [None]*size\n    y[0] = [(Pb[0]/L[0][0])[0]]\n    for row in range(1, size):\n        sum = 0\n        for col in range(row):\n            sum += (L[row][col])*(y[col][0])\n        y[row] = [((Pb[row] - sum) / L[row][row])[0]]\n    y = np.array(y)\n\n    #solve Ux = y\n    x = [None]*size\n    n = size - 1\n    x[n] = [(y[n] / U[n][n])[0]]\n    row = n - 1\n    while row &gt;=0:\n        sum = 0\n        for col in range(row + 1, size):\n            sum += U[row][col]*(x[col][0])\n        x[row] = [((y[row] - sum) / U[row][row])[0]]\n        row -= 1\n    x = np.array(x)\n    return x\n\nNow we will test this solver:\n\n#PLU solver test1\nA = np.array([[2, 1, 0], [-2, 0, -1], [2, 3, -3]])\nb = np.array([[1], [0], [1]])\nx = PLU_solver(A, b)\n\nprint(\"x =\", x)\nprint(\"b = \", b)\nprint(\"Ax = \", A.dot(x))\n\nx = [[-1.]\n [ 3.]\n [ 2.]]\nb =  [[1]\n [0]\n [1]]\nAx =  [[1.]\n [0.]\n [1.]]\n\n\n\n#PLU solver test2\nA = np.array([[2, 1, 3], [-4, -2, -1], [2, 3, -3]])\nb = np.array([[4], [3], [5]])\nx = PLU_solver(A, b)\n\nprint(\"x =\", x)\nprint(\"b = \", b)\nprint(\"Ax = \", A.dot(x))\n\nx = [[-4.85]\n [ 7.1 ]\n [ 2.2 ]]\nb =  [[4]\n [3]\n [5]]\nAx =  [[4.]\n [3.]\n [5.]]\n\n\nNow we will create the inverse solver algo that uses the PLU solver:\n\n#inverse Solver:\ndef find_inverse(A):\n    size = len(A)\n    output = []\n    for i in range(size):\n        ei = np.zeros((size, 1))\n        ei[i] = [1]\n        xi = PLU_solver(A, ei)\n        output.append(xi)\n    return np.hstack(output)\n\nNow we will show two tests that see that this works:\n\n#test1\nA = np.array([[2, 1, 0], [-2, 0, -1], [2, 3, -3]])\ninvA = find_inverse(A)\nprint(invA.dot(A))\n\nprint(\"\\n\")\n\n#test2\nA = np.array([[1, 2, 3], [0, 1, 4], [5, 6, 0]])\ninvA = find_inverse(A)\nprint(invA.dot(A))\n\n[[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]]\n\n\n[[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]]\n\n\n\nIt looks like you have a lot of the pieces in place here. But I’d like to see a better job testing the outputs. When you run the problems from HW3, how am I supposed to know if these outputs are what they should be? You could either write something like “this should be upper triangular, and it is, and this should be …”, and/or you could do some kind of test (I bet either numpy or sympy has a function to test whether a matrix is upper triangular.)\nIn general, I’m trying to read your code to figure out what you were doing, but I’d prefer to have you tell me what you’re doing and why you’re doing it in prose as well as in the code.\nFor the testing, I’m curious about how this algorithm compares to some built-in solvers. Could you run it many times and compare e.g. the speed to one of the built-in systems? (I’m sure yours will be slower, that’s find, just curious about the difference!)\nGrade: R"
  },
  {
    "objectID": "notebooks/pagerank_tutorial.html",
    "href": "notebooks/pagerank_tutorial.html",
    "title": "PageRank Tutorial",
    "section": "",
    "text": "import sympy as sym\n\nWe make a matrix using the sym.Matrix function.\n\nT = sym.Matrix([[1,2,3],[2,1,3],[2,1,1]])\nT\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & 2 & 3\\\\2 & 1 & 3\\\\2 & 1 & 1\\end{matrix}\\right]\\)\n\n\nWe can create symbolic variables, and put them into the matrix if we want…\n\nx1=sym.var('x1'); x2=sym.var('x2');x3=sym.var('x3');x4=sym.var('x4')\n\n\nx1\n\n\\(\\displaystyle x_{1}\\)\n\n\n\nT2 = sym.Matrix([[1,x2,3],[2,1,3],[2,x1,1]])\nT2\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & x_{2} & 3\\\\2 & 1 & 3\\\\2 & x_{1} & 1\\end{matrix}\\right]\\)\n\n\nIf we input fractions, they are by default converted to floating point numbers:\n\nT3 = sym.Matrix([[1,x2,1/3],[2,1,3],[2,x1,1]])\nT3\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & x_{2} & 0.333333333333333\\\\2 & 1 & 3\\\\2 & x_{1} & 1\\end{matrix}\\right]\\)\n\n\nIt’s nicer to input fractions and keep them as fractions:\n\nT4 = sym.Matrix([[1,x2,sym.Rational(1,3)],[2,1,3],[2,x1,1]])\nT4\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & x_{2} & \\frac{1}{3}\\\\2 & 1 & 3\\\\2 & x_{1} & 1\\end{matrix}\\right]\\)\n\n\nWe can substitute values in for variables:\n\nT4.subs(x1,5)\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & x_{2} & \\frac{1}{3}\\\\2 & 1 & 3\\\\2 & 5 & 1\\end{matrix}\\right]\\)\n\n\nWe can find reduced row echelon form:\n\nT4.rref()\n\n(Matrix([\n [1, 0, 0],\n [0, 1, 0],\n [0, 0, 1]]),\n (0, 1, 2))\n\n\nThis outputs the RREFand tells us which columns correspond to bound variables.\nLastly, we can do Gauss-Jordan elimination to solve a system of equations with a matrix and a right-hand-side:\n\nT4.gauss_jordan_solve(sym.Matrix([2,1,3]))\n\n(Matrix([\n [(17*x1 - 24*x2 - 3)/(7*x1 - 12*x2 - 1)],\n [                 -4/(7*x1 - 12*x2 - 1)],\n [(-9*x1 + 12*x2 + 3)/(7*x1 - 12*x2 - 1)]]),\n Matrix(0, 1, []))\n\n\nThat’s messy! It’s exporting several things, but the one we want is the first.\n\nT4.gauss_jordan_solve(sym.Matrix([2,1,3]))[0]\n\n\\(\\displaystyle \\left[\\begin{matrix}\\frac{17 x_{1} - 24 x_{2} - 3}{7 x_{1} - 12 x_{2} - 1}\\\\- \\frac{4}{7 x_{1} - 12 x_{2} - 1}\\\\\\frac{- 9 x_{1} + 12 x_{2} + 3}{7 x_{1} - 12 x_{2} - 1}\\end{matrix}\\right]\\)\n\n\n\n\n\nCreate a matrix representing the system of equations for the “third try” in the PageRank problem:\n\\[\n\\begin{aligned}\n& x_{1}=\\frac{x_{2}}{1}+\\frac{x_{3}}{3} \\\\\n& x_{2}=\\frac{x_{1}}{2}+\\frac{x_{3}}{3} \\\\\n& x_{3}=\\frac{x_{1}}{2}+\\frac{x_{4}}{1} \\\\\n& x_{4}=\\frac{x_{3}}{3} .\n\\end{aligned}\n\\]\nNow find the RREF form of this matrix. What does that tell you about the system?\nNext, use the gauss_jordan_solve. What’s surprising (or not) about these results?\nSubstitute a specific value to get a particular solution…"
  },
  {
    "objectID": "notebooks/pagerank_tutorial.html#set-up-the-environment.-import-the-sympy-library.",
    "href": "notebooks/pagerank_tutorial.html#set-up-the-environment.-import-the-sympy-library.",
    "title": "PageRank Tutorial",
    "section": "",
    "text": "import sympy as sym\n\nWe make a matrix using the sym.Matrix function.\n\nT = sym.Matrix([[1,2,3],[2,1,3],[2,1,1]])\nT\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & 2 & 3\\\\2 & 1 & 3\\\\2 & 1 & 1\\end{matrix}\\right]\\)\n\n\nWe can create symbolic variables, and put them into the matrix if we want…\n\nx1=sym.var('x1'); x2=sym.var('x2');x3=sym.var('x3');x4=sym.var('x4')\n\n\nx1\n\n\\(\\displaystyle x_{1}\\)\n\n\n\nT2 = sym.Matrix([[1,x2,3],[2,1,3],[2,x1,1]])\nT2\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & x_{2} & 3\\\\2 & 1 & 3\\\\2 & x_{1} & 1\\end{matrix}\\right]\\)\n\n\nIf we input fractions, they are by default converted to floating point numbers:\n\nT3 = sym.Matrix([[1,x2,1/3],[2,1,3],[2,x1,1]])\nT3\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & x_{2} & 0.333333333333333\\\\2 & 1 & 3\\\\2 & x_{1} & 1\\end{matrix}\\right]\\)\n\n\nIt’s nicer to input fractions and keep them as fractions:\n\nT4 = sym.Matrix([[1,x2,sym.Rational(1,3)],[2,1,3],[2,x1,1]])\nT4\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & x_{2} & \\frac{1}{3}\\\\2 & 1 & 3\\\\2 & x_{1} & 1\\end{matrix}\\right]\\)\n\n\nWe can substitute values in for variables:\n\nT4.subs(x1,5)\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & x_{2} & \\frac{1}{3}\\\\2 & 1 & 3\\\\2 & 5 & 1\\end{matrix}\\right]\\)\n\n\nWe can find reduced row echelon form:\n\nT4.rref()\n\n(Matrix([\n [1, 0, 0],\n [0, 1, 0],\n [0, 0, 1]]),\n (0, 1, 2))\n\n\nThis outputs the RREFand tells us which columns correspond to bound variables.\nLastly, we can do Gauss-Jordan elimination to solve a system of equations with a matrix and a right-hand-side:\n\nT4.gauss_jordan_solve(sym.Matrix([2,1,3]))\n\n(Matrix([\n [(17*x1 - 24*x2 - 3)/(7*x1 - 12*x2 - 1)],\n [                 -4/(7*x1 - 12*x2 - 1)],\n [(-9*x1 + 12*x2 + 3)/(7*x1 - 12*x2 - 1)]]),\n Matrix(0, 1, []))\n\n\nThat’s messy! It’s exporting several things, but the one we want is the first.\n\nT4.gauss_jordan_solve(sym.Matrix([2,1,3]))[0]\n\n\\(\\displaystyle \\left[\\begin{matrix}\\frac{17 x_{1} - 24 x_{2} - 3}{7 x_{1} - 12 x_{2} - 1}\\\\- \\frac{4}{7 x_{1} - 12 x_{2} - 1}\\\\\\frac{- 9 x_{1} + 12 x_{2} + 3}{7 x_{1} - 12 x_{2} - 1}\\end{matrix}\\right]\\)"
  },
  {
    "objectID": "notebooks/pagerank_tutorial.html#now-you-try",
    "href": "notebooks/pagerank_tutorial.html#now-you-try",
    "title": "PageRank Tutorial",
    "section": "",
    "text": "Create a matrix representing the system of equations for the “third try” in the PageRank problem:\n\\[\n\\begin{aligned}\n& x_{1}=\\frac{x_{2}}{1}+\\frac{x_{3}}{3} \\\\\n& x_{2}=\\frac{x_{1}}{2}+\\frac{x_{3}}{3} \\\\\n& x_{3}=\\frac{x_{1}}{2}+\\frac{x_{4}}{1} \\\\\n& x_{4}=\\frac{x_{3}}{3} .\n\\end{aligned}\n\\]\nNow find the RREF form of this matrix. What does that tell you about the system?\nNext, use the gauss_jordan_solve. What’s surprising (or not) about these results?\nSubstitute a specific value to get a particular solution…"
  },
  {
    "objectID": "lectures/ch3lecture1.html",
    "href": "lectures/ch3lecture1.html",
    "title": "Ch3 Lecture 1",
    "section": "",
    "text": "Sec \\(\\backslash\\) Meter\n0\n\\(1 / 6\\)\n\\(1 / 3\\)\n\\(1 / 2\\)\n\\(2 / 3\\)\n\\(5 / 6\\)\n1\n\n\n\n\n\\(t=240\\)\n0.0\n0.032\n1.23\n3.69\n1.23\n0.032\n0.0\n\n\n\\(t=270\\)\n0.0\n0.051\n1.21\n3.48\n1.21\n0.051\n0.0\n\n\n\n. . .\nGoal: find D.\n. . .\n\\[\n\\begin{equation*}\ny_{i, t+1}=y_{i, t}+\\frac{k D}{h^{2}}\\left(y_{i-1, t}-2 y_{i, t}+y_{i+1, t}\\right)+k f_{i, t}\n\\end{equation*}\n\\]\n. . .\n\n\n\\[\n\\begin{equation*}\ny_{i, t+1}=y_{i, t}+d\\left(y_{i-1, t}-2 y_{i, t}+y_{i+1, t}\\right)\n\\end{equation*}\n\\]\n\\(y_{0,t+1} = 0\\) \\(y_{1,t+1}=y_{1,t}+d\\left(0-2 y_{1,t}+y_{2,t}\\right)\\) \\(y_{3,t+1}=y_{3,t}+d\\left(y_{2,t}-2 y_{3,t}+y_{4,t}\\right)\\) … \\(y_{6,t+1}=0\\)\n. . .\nWe want to solve these for \\(y_{i,t}\\) given \\(y_{i,t+1}\\).\n\\[\n\\begin{equation*}\ny_{i, t+1}-d\\left(y_{i-1, t}-2 y_{i, t}+y_{i+1, t}\\right)=y_{i, t}\n\\end{equation*}\n\\]\n\\[\n\\approx y_{i, t+1}-d\\left(y_{i-1, t+1}-2 y_{i, t+1}+y_{i+1, t+1}\\right) ?\n\\]\n\n\n\nIn matrix form, this becomes \\(y_{t+1}=y_{t}+ d My_{t}\\):\n\\[\nM=\\left[\\begin{array}{ccccccc}\n-2 & 1 & 0 & 0 & 0 & 0 & 0 \\\\\n1 & -2 & 1 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & -2 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & -2 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & -2 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 1 & -2 & 1 \\\\\n0 & 0 & 0 & 0 & 0 & 1 & -2\n\\end{array}\\right]\n\\]\n. . .\n\\[\ny_{t}=y_{t+1}- d My_{t} \\overset{?}\\approx y_{t+1}- d My_{t+1}\n\\]\nTurns out it works… but why?\n. . .\n\n\n\n\n\\(y_{t+1}=y_{t}+ dMy_{t}\\)\nis the same as \\(y_{t+1}=Iy_{t}+ dMy_{t}\\)\nin other words, \\(y_{t+1}=(I+dM)y_{t}\\)\n\n. . .\nHow do we solve this for \\(y_{t}\\), given \\(y_{t+1}\\)?\n. . .\n\\[\ny_{t+1}=(I+dM)y_{t} \\Rightarrow y_{t}=(I+dM)^{-1}y_{t+1}\n\\]\n. . .\nTwo different approaches – one correct, the other one gave approximately the right answer. Why?\n\n\n\n\\[\ny_{t+1}=(I+dM)y_{t} \\Rightarrow y_{t}=(I+dM)^{-1}y_{t+1}\n\\]\n\\[\ny_{t}=y_{t+1}- dMy_{t} \\overset{?}\\approx y_{t+1}- d My_{t+1}\n\\]\n. . .\nConnection: we will use math to show that the two are approximately equal.\n. . .\nRemember Taylor series?\nFor small \\(x\\), we can approximate \\(\\frac{1}{1+x}\\) as \\(1-x+x^2-x^3+\\ldots\\)\n. . .\nIf we have a matrix \\(A\\) such that \\(X = I + A\\) is invertible, then \\(X^{-1} = I - A + A^2 - A^3 + \\ldots\\)\n. . .\nSo to first order in d,\n\\[\ny_{t}=(I+dM)^{-1}y_{t+1} \\approx \\left(I-dM\\right) y_{t+1}=y_{t+1}-d M y_{t+1}\n\\]\n. . .\nNow \\(y_{t+1}=y_{t} + dM y_{t}\\). Plugging this in,\n\\(y_{t}=(I+dM)^{-1}y_{t+1} \\approx y_{t+1}-d M \\left(y_{t}+d M y_{t}\\right)\\approx y_{t+1}-d M y_{t}\\)\n\n\n\nThe problem was much easier to solve once we put it in matrix form."
  },
  {
    "objectID": "course_docs/syllabus.html",
    "href": "course_docs/syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "(Subject to change)\n\n\n\n\n\n\n\n\nWeek\nTopics\nReading\n\n\n\n\n1\nLinear Systems of Equations\nCh. 1\n\n\n2\nMatrix math\n1.3-1.4, 2.1-2.3\n\n\n3\nMatrix math, continued\n2.3-2.5\n\n\n4\n** No class on Monday, April 8 ** Factorization\n2.8\n\n\n5\nVector spaces? Least Squares\n3, 4.2\n\n\n6\nComputer graphics, QR factorization\n4.2, 4.4\n\n\n7\nData compression, wavelets, discrete dynamical systems\n4.4, 5.3\n\n\n8\nPCA, SVD\n5.4, 5.6\n\n\n9\nFourier Transform\n6.6"
  },
  {
    "objectID": "course_docs/syllabus.html#textbook",
    "href": "course_docs/syllabus.html#textbook",
    "title": "Syllabus",
    "section": "Textbook:",
    "text": "Textbook:\nOur textbook is Applied Linear Algebra and Matrix Analysis, 2nd Ed, by Thomas Shores. You can download this textbook for free from the Springer website."
  },
  {
    "objectID": "course_docs/syllabus.html#course-components",
    "href": "course_docs/syllabus.html#course-components",
    "title": "Syllabus",
    "section": "Course components:",
    "text": "Course components:\n\nWeekly homework (problem sets)\nThese will be graded with a standard point scale.\n\n\nThree “project packs”.\nFor each pack, you’ll choose 3 of 4 mini projects to complete. Projects are slightly longer and more open-ended than homework problems. Projects will be graded on an ESMU scale. (This stands for Excellent, Meets Expectations, Revision necessary, and Ungradeable.)\nFor each of the first two project packs, you’ll have the chance to revise and resubmit up to two projects, so long as they were initially graded with an M or R. For the third project pack, you won’t be able to revise and resubmit projects (because this will be due at the end of the quarter).\n\n\nFinal exam\nThere will be a final exam during exam week, in two parts. The first part will be similar to the problems in the problem sets. The second part will be a critical analysis of some applications. You’ll have to suggest directions that you might take in an analysis and look for potential weaknesses in a provided analysis. However, if you’ve gotten at least two Es on your project packs, you won’t have to take the final exam.\nWe will not have a midterm exam."
  },
  {
    "objectID": "course_docs/syllabus.html#course-grading",
    "href": "course_docs/syllabus.html#course-grading",
    "title": "Syllabus",
    "section": "Course grading",
    "text": "Course grading\nThe final grade will be determined by the following weights:\n\nProblem sets: 20%\nFirst part of final exam: 15%\nSecond part of final exam: 15% OR skipped.\nProject packs: 50% OR 65% if the second part of the final exam is skipped.\n\n\n\n\n\n\n\nGrading details\n\n\n\n\n\nProject pack grades are calculated as follows. There are 90 points possible in the project packs, 10 for each project that you complete.\nYou’ll get 10 points for an E or M grade, and 5 points for an R grade. You’ll get 0 points for a U grade. If you revise and resubmit, you’ll get the full points for the grade you receive on the resubmission. If you have at least two E’s from the first two project packs, you may choose to skip the second part of the final exam; in this case, the project packs will be worth 65% of the final grade.\nThe grading for the final project pack will be done differently. You’ll still get 10 points for an E or an M. However, if you get an R on one project and an E on another, the E can be used to bring the R up to an M. So if your grades are E, R, M, this would be changed to M, M, M, for full credit. If your grades are M, R, R, you’d get 20/30 points for this project pack. If your grades are E, R, R, this would be changed to M, M, R, and you’d get 25/30 points. If you get an E on one project and a U on another, the E can be used to bring the U up to an R.\n\n\n\n\n\n\n\n\n\nGrading examples\n\n\n\n\n\nLet’s look at two example students.\nStudent A has the following grades going into the final exam: Problem sets: 90% Project Pack 1: E, S, S (after revision) -&gt; 30/30 Project Pack 2: E, E, S (after revision) -&gt; 30/30\nThe student chooses to skip the second part of the final exam.\nHer grade on the first part of the final exam is 80%. Her grades for the third Project Pack are R, E, R. -&gt; 25/30.\nWhen combined, this student’s project pack grades are 85/90, or 94.4%.\nHer final course grade is 94.40.65+90.20+80*.15 = 91.36%.\nStudent B has the following grades going into the final exam: Problem sets: 80% Project Pack 1: E, S, R (after revision) -&gt; 25/30 Project Pack 2: S, R, R (after revision) -&gt; 20/30\nThis student does not have the option to skip the second part of the final exam.\nHer grade on the first part of the final exam is 87%. Her grade on the second part of the final exam is 90%. Her grade on the third project pack is E, R, R, which becomes S, S, R, for 25/30.\nWhen combined, this student’s project pack grades are 70/90, or 77.8%.\nHer final course grade is 77.80.50+80.20+87.15+90.15 = 81.45%."
  },
  {
    "objectID": "notebooks/mcmc_pagerank.html",
    "href": "notebooks/mcmc_pagerank.html",
    "title": "MCMC Pagerank",
    "section": "",
    "text": "# Do imports\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport scipy\nimport numpy as np\n# Create a directed graph\nG = nx.DiGraph()\n\n# Add nodes\nG.add_nodes_from([1, 2, 3, 4, 5, 6])\n\n# Add vertices\nedges = [(1, 2), (1, 3), (3, 2), (3, 1), (3, 4), (5, 6), (6, 5),(3,2),(2,4),(4,1)]\nG.add_edges_from(edges)\n\n# Draw the graph\nnx.draw_circular(G, with_labels=True)\nplt.show()\n# Since the graph G is a DiGraph (Directed Graph), we can use the adjacency_matrix method from networkx to get the adjacency matrix\n# Then, we normalize each row to sum to 1 to get the transition matrix\n\n# Get the adjacency matrix (returns a scipy sparse matrix)\nadj_matrix = nx.adjacency_matrix(G)\n\n# Convert to a dense numpy array\nadj_matrix = adj_matrix.toarray()\n\n# Calculate the sum of each row\nrow_sums = adj_matrix.sum(axis=1)\n\n# Normalize the matrix (this is the transition matrix)\ntransition_matrix = adj_matrix / row_sums[:, np.newaxis]\nprint(transition_matrix.T)\n\n[[0.         0.         0.33333333 1.         0.         0.        ]\n [0.5        0.         0.33333333 0.         0.         0.        ]\n [0.5        0.         0.         0.         0.         0.        ]\n [0.         1.         0.33333333 0.         0.         0.        ]\n [0.         0.         0.         0.         0.         1.        ]\n [0.         0.         0.         0.         1.         0.        ]]"
  },
  {
    "objectID": "notebooks/mcmc_pagerank.html#if-not-doing-pagerank-initializing-a-random-stochastic-matrix",
    "href": "notebooks/mcmc_pagerank.html#if-not-doing-pagerank-initializing-a-random-stochastic-matrix",
    "title": "MCMC Pagerank",
    "section": "If not doing pagerank: Initializing a random stochastic matrix",
    "text": "If not doing pagerank: Initializing a random stochastic matrix\n\nimport numpy as np\n\n# Create a 6x6 matrix with random values\nmatrix = np.random.rand(6, 6)\n\n# Normalize the matrix to make it a stochastic matrix\nstochastic_matrix = matrix / matrix.sum(axis=1, keepdims=True)\n\n# Square the matrix to increase the difference between values\nmatrix_squared = np.square(stochastic_matrix)\n\n# Normalize the squared matrix to make it a stochastic matrix\nstochastic_matrix_squared = matrix_squared / matrix_squared.sum(axis=1, keepdims=True)"
  },
  {
    "objectID": "notebooks/mcmc_pagerank.html#finding-the-stationary-state-of-the-matrix",
    "href": "notebooks/mcmc_pagerank.html#finding-the-stationary-state-of-the-matrix",
    "title": "MCMC Pagerank",
    "section": "Finding the stationary state of the matrix",
    "text": "Finding the stationary state of the matrix\n\n\n\n# Normalize the teleportation matrix\nteleportation = np.ones(np.shape(transition_matrix))\nrow_sums = teleportation.sum(axis=1)\nteleportation = teleportation / row_sums[:, np.newaxis]\n\nalpha = .15\n\ntransition_matrix_combined = (1-alpha)*transition_matrix + alpha*teleportation\n\n\n\n# Use the transition matrix from the pagerank\nstochastic_matrix = transition_matrix_combined\n\n# Compute the eigenvalues and right eigenvectors\neigenvalues, eigenvectors = np.linalg.eig(np.transpose(stochastic_matrix))\n\n# Find the index of the eigenvalue equal to 1\nindex = np.where(np.isclose(eigenvalues, 1))\n\n# The corresponding eigenvector is the stationary distribution\nstationary_distribution = np.real_if_close(eigenvectors[:, index].flatten())\n\n# Normalize the stationary distribution so it sums to 1\nstationary_distribution /= np.sum(stationary_distribution)\n\nprint(stationary_distribution)\nprint(index)\n\n[0.2154013  0.14956679 0.11654555 0.18515302 0.16666667 0.16666667]\n(array([0]),)\n\n\n\nDouble-check that the stationary state really is stationary\n\n# Multiply the stationary distribution by the transition matrix\nresult = np.dot(stationary_distribution, stochastic_matrix)\n\n# Check if the result is the same as the stationary distribution\nis_stationary = np.allclose(result, stationary_distribution)\n\nprint(is_stationary)\n\nTrue"
  },
  {
    "objectID": "notebooks/mcmc_pagerank.html#doing-a-mcmc-simulation-of-the-system",
    "href": "notebooks/mcmc_pagerank.html#doing-a-mcmc-simulation-of-the-system",
    "title": "MCMC Pagerank",
    "section": "Doing a MCMC simulation of the system",
    "text": "Doing a MCMC simulation of the system\n\nSimulate the states\n\n# Initialize the state\nstate = np.random.choice(6)\n\n# Number of passes\nnum_passes = 1000\n\n# Store the states\n# Initialize the state variable\nstates = np.zeros(num_passes)\n\nfor _ in range(num_passes):\n    # Choose the next state based on the probabilities in the transition matrix\n    # The column in the transition matrix corresponding to our current state gives the probabilities\n    state = np.random.choice(6, p=stochastic_matrix[state])\n    states[_]=(state)\n\n# Convert the states to a numpy array\nstates = np.array(states)\n\n\n\nPlot the emperical distribution\n\nimport matplotlib.pyplot as plt\n\nplt.hist(states, bins=6, alpha=0.5)\nplt.xlabel('State')\nplt.ylabel('Frequency')\nplt.title('Histogram of States')\nplt.show()"
  },
  {
    "objectID": "notebooks/mcmc_pagerank.html#comparing-the-theoretical-and-emperical-results",
    "href": "notebooks/mcmc_pagerank.html#comparing-the-theoretical-and-emperical-results",
    "title": "MCMC Pagerank",
    "section": "Comparing the theoretical and emperical results",
    "text": "Comparing the theoretical and emperical results\n\n# Plotting the histogram of states\nplt.hist(states, bins=6, density=True, alpha=0.7, label='Empirical')\n\n# Plotting the stationary distribution\nplt.plot(stationary_distribution, label='Theoretical', marker='o')\n\nplt.title('Comparison of Empirical and Theoretical Distributions')\nplt.xlabel('States')\nplt.ylabel('Probability')\nplt.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\n\nWatching the emperical distribution develop over time\n\nimport numpy as np\nfrom IPython.display import HTML\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\n\n# Use stochastic_matrix as the transition matrix\nQ = stochastic_matrix\n\n# Initialize the figure\nfig, ax = plt.subplots()\nnum_frames=100;\n# Initialize an empty array for the empirical distribution\nempirical_distribution = np.zeros((num_frames, len(Q[0])))\n\n# Initialize the state variable\nstate = 0\n\n# Function to update the empirical distribution\n# Modify the update function to run 10 iterations without updating i\ndef update(i):\n    global state, empirical_distribution\n    empirical_distribution[i] = empirical_distribution[i-1]\n    for _ in range(10):  # Run 10 iterations\n        state = np.random.choice(range(len(Q[state])), p=Q[state])\n        empirical_distribution[i, state] += 1\n    # Plot the distribution\n    ax.clear()\n    ax.bar(range(len(Q[state])), empirical_distribution[i] )\n\n\n# Create the animation\nani = animation.FuncAnimation(fig, update, frames=range(1, num_frames), repeat=False)\n\n# Use the HTML object to display the animation\nHTML(ani.to_jshtml())\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "notebooks/mcmc_pagerank.html#adding-teleportation",
    "href": "notebooks/mcmc_pagerank.html#adding-teleportation",
    "title": "MCMC Pagerank",
    "section": "Adding teleportation",
    "text": "Adding teleportation"
  },
  {
    "objectID": "notebooks/turing_patterns.html",
    "href": "notebooks/turing_patterns.html",
    "title": "Turing Patterns",
    "section": "",
    "text": "Adapted from ijmbarr"
  },
  {
    "objectID": "notebooks/turing_patterns.html#reaction-diffusion-equations",
    "href": "notebooks/turing_patterns.html#reaction-diffusion-equations",
    "title": "Turing Patterns",
    "section": "Reaction Diffusion Equations",
    "text": "Reaction Diffusion Equations\nReaction-Diffusion equations are a class of partial differential equations whose dynamics are governed by two terms: a diffusion part and a reaction part. We will be looking at the two component case, which takes the form\n\\(\\frac{\\partial a(x,t)}{\\partial t} = D_{a}\\frac{\\partial^{2} a(x,t)}{\\partial x^{2}} + R_{a}(a(x,t),b(x,t))\\)\n\\(\\frac{\\partial b(x,t)}{\\partial t} = D_{b}\\frac{\\partial^{2} b(x,t)}{\\partial x^{2}} + R_{b}(a(x,t),b(x,t))\\)\nWhere the \\(a(x,t)\\) and \\(b(x,t)\\) describe the concentration of chemicals \\(a\\) and \\(b\\) at position \\(x\\) and time \\(t\\). The functions \\(R_{a}\\) and \\(R_{b}\\) determine how the concentrations change due to interspecies reactions and \\(D_{a}\\) and \\(D_{b}\\) are the diffusion coefficients.\n\nDiffusion\nThe diffusion part of the equations causes areas of high concentration to spread out to areas of low concentration, while conserving the total amount of the chemical. To get a feel for what’s happening, let’s focus the equation\n$ = D_{a} $\nIf this looks familiar, it is because it appears in a number of different areas of science and maths. A few are: the diffusion equation, the heat equation and Brownian motion.\nIt has an analytic solution, which for initial Gaussian distribution is\n$ a(x,t) = (-)$\nA “spreading out”, or diffusion over time, as the name suggests.\n\n\nSimulating equations\nIf you’ve got this far, you might have noticed that this is written in a jupyter notebook. This allows us to mix code with writing, so we don’t just have to look at an equation to understand how it behaves - we can simulate it directly.\nTo simulate the PDEs, I’m going to use the explicit finite-difference method. It is not the best numerical method to use, but it is easy to code, and provided we keep the time step small enough it will be stable.\nUnder this scheme, we approximate the time derivative as\n\\[\n\\frac{\\partial a(x,t)}{\\partial t} \\approx \\frac{1}{dt}(a_{x,t+1} - a_{x,t})\n\\]\nAnd the spatial part of the derivative (which is usually know as the Laplacian) as\n\\[\n\\frac{\\partial^{2} a(x,t)}{\\partial x^{2}} \\approx \\frac{1}{dx^{2}}(a_{x+1,t} + a_{x-1,t} - 2a_{x,t})\n\\]\nPutting it all together, for the diffusion part of the equation we get.\n\\[\na_{x,t+1} = a_{x,t} + dt\\left(  \\frac{D_{a}}{dx^{2}}(a_{x+1,t} + a_{x-1,t} - 2a_{x,t})  \\right)\n\\]\nLet’s start by defining some functions to take care of the spatial derivatives. We will be using periodic boundary conditions throughout our exploration, which are easy to implement in numpy using its roll function.\n\n\nimport httpimport\nwith httpimport.github_repo('ijmbarr','turing-patterns'):\n  import tutils\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# I'm using seaborn for it's fantastic default styles\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\n\n%matplotlib inline\n%load_ext autoreload\n%autoreload 2\n\nfrom tutils import BaseStateSystem\n\n\ndef laplacian1D(a, dx):\n    return (\n        - 2 * a\n        + np.roll(a,1,axis=0)\n        + np.roll(a,-1,axis=0)\n    ) / (dx ** 2)\n\ndef laplacian2D(a, dx):\n    return (\n        - 4 * a\n        + np.roll(a,1,axis=0)\n        + np.roll(a,-1,axis=0)\n        + np.roll(a,+1,axis=1)\n        + np.roll(a,-1,axis=1)\n    ) / (dx ** 2)\n\nTo aid in generating visualisations, I’ve written a base class which updates and plots a system based on some notion of state. There’s nothing specific about the reaction diffusion equations encoded in it, so I’m not going to go into any detail about it. You can find the full code for it, along with this notebook on github here. It uses matplotlib’s animation functionality to plot the output as a gif.\nAll together we can simulate the effect of the diffusion equation:\n\nclass OneDimensionalDiffusionEquation(BaseStateSystem):\n    def __init__(self, D):\n        self.D = D\n        self.width = 1000\n        self.dx = 10 / self.width\n        self.dt = 0.9 * (self.dx ** 2) / (2 * D)\n        self.steps = int(0.1 / self.dt)\n\n    def initialise(self):\n        self.t = 0\n        self.X = np.linspace(-5,5,self.width)\n        self.a = np.exp(-self.X**2)\n\n    def update(self):\n        for _ in range(self.steps):\n            self.t += self.dt\n            self._update()\n\n    def _update(self):\n        La = laplacian1D(self.a, self.dx)\n        delta_a = self.dt * (self.D * La)\n        self.a += delta_a\n\n    def draw(self, ax):\n        ax.clear()\n        ax.plot(self.X,self.a, color=\"r\")\n        ax.set_ylim(0,1)\n        ax.set_xlim(-5,5)\n        ax.set_title(\"t = {:.2f}\".format(self.t))\n\none_d_diffusion = OneDimensionalDiffusionEquation(D=1)\n\none_d_diffusion.plot_time_evolution(\"diffusion.gif\")\n\nMovieWriter imagemagick unavailable; using Pillow instead.\n\n\n\n\n\ndiffusion\n\n\nAs expected - the diffusion equation causes the the concentration to “spread out”.\n\n\nReaction\nInteractions between the two chemical components are introduced via the functions \\(R_{a}\\) and \\(R_{b}\\). These functions only depend on the local concentration of each of the chemicals. Their exact form will depend on the chemicals involved, but it is possible to show that Turing patterns are observed for a whole class of different equations.\nThe one thing we will require from these equations is that they reach a stable equilibrium when the concentrations of the chemicals involved are completely homogeneous. This means that there exists concentrations of \\(a\\) and \\(b\\) such that\n\\(R_a(a_{0}, b_{0}) = 0\\)\n\\(R_{b}(a_{0}, b_{0}) = 0\\)\nThe fact that we require this to hold will make the later instability more suprising.\nFor the reaction equations, I’m going to use the FitzHugh–Nagumo equation\n\\(R_a(a, b) = a - a^{3} - b + \\alpha\\)\n\\(R_{b}(a, b) = \\beta (a - b)\\)\nWhere \\(\\alpha\\) and \\(\\beta\\) are constants.\nLet’s see how it behaves\n\nclass ReactionEquation(BaseStateSystem):\n    def __init__(self, Ra, Rb):\n        self.Ra = Ra\n        self.Rb = Rb\n        self.dt = 0.01\n        self.steps = int(0.1 / self.dt)\n\n    def initialise(self):\n        self.t = 0\n        self.a = 0.1\n        self.b = 0.7\n        self.Ya = []\n        self.Yb = []\n        self.X = []\n\n    def update(self):\n        for _ in range(self.steps):\n            self.t += self.dt\n            self._update()\n\n    def _update(self):\n        delta_a = self.dt * self.Ra(self.a,self.b)\n        delta_b = self.dt * self.Rb(self.a,self.b)\n\n        self.a += delta_a\n        self.b += delta_b\n\n    def draw(self, ax):\n        ax.clear()\n\n        self.X.append(self.t)\n        self.Ya.append(self.a)\n        self.Yb.append(self.b)\n\n        ax.plot(self.X,self.Ya, color=\"r\", label=\"A\")\n        ax.plot(self.X,self.Yb, color=\"b\", label=\"B\")\n        ax.legend()\n\n        ax.set_ylim(0,1)\n        ax.set_xlim(0,5)\n        ax.set_xlabel(\"t\")\n        ax.set_ylabel(\"Concentrations\")\n\nalpha, beta =  0.2, 5\n\ndef Ra(a,b): return a - a ** 3 - b + alpha\ndef Rb(a,b): return (a - b) * beta\n\none_d_reaction = ReactionEquation(Ra, Rb)\none_d_reaction.plot_time_evolution(\"reaction.gif\", n_steps=50)\n\nMovieWriter imagemagick unavailable; using Pillow instead.\n\n\n\n\n\nreaction\n\n\nThe system is stable, and stabilises to \\(a = b = \\sqrt[3]{\\alpha}\\)."
  },
  {
    "objectID": "notebooks/turing_patterns.html#full-model",
    "href": "notebooks/turing_patterns.html#full-model",
    "title": "Turing Patterns",
    "section": "Full Model",
    "text": "Full Model\nWe now have two parts to the reaction diffusion equation: a diffusion term that “spreads” out concentration and a reaction part the equalises the two concentrations. It feels like these two together should create a stable system, so it is surprising that we can end up with patterns.\nBut enough with the preliminaries, let’s take a look at some of the patterns formed.\n\ndef random_initialiser(shape):\n    return(\n        np.random.normal(loc=0, scale=0.05, size=shape),\n        np.random.normal(loc=0, scale=0.05, size=shape)\n    )\n\nclass OneDimensionalRDEquations(BaseStateSystem):\n    def __init__(self, Da, Db, Ra, Rb,\n                 initialiser=random_initialiser,\n                 width=1000, dx=1,\n                 dt=0.1, steps=1):\n\n        self.Da = Da\n        self.Db = Db\n        self.Ra = Ra\n        self.Rb = Rb\n\n        self.initialiser = initialiser\n        self.width = width\n        self.dx = dx\n        self.dt = dt\n        self.steps = steps\n\n    def initialise(self):\n        self.t = 0\n        self.a, self.b = self.initialiser(self.width)\n\n    def update(self):\n        for _ in range(self.steps):\n            self.t += self.dt\n            self._update()\n\n    def _update(self):\n\n        # unpack so we don't have to keep writing \"self\"\n        a,b,Da,Db,Ra,Rb,dt,dx = (\n            self.a, self.b,\n            self.Da, self.Db,\n            self.Ra, self.Rb,\n            self.dt, self.dx\n        )\n\n        La = laplacian1D(a, dx)\n        Lb = laplacian1D(b, dx)\n\n        delta_a = dt * (Da * La + Ra(a,b))\n        delta_b = dt * (Db * Lb + Rb(a,b))\n\n        self.a += delta_a\n        self.b += delta_b\n\n    def draw(self, ax):\n        ax.clear()\n        ax.plot(self.a, color=\"r\", label=\"A\")\n        ax.plot(self.b, color=\"b\", label=\"B\")\n        ax.legend()\n        ax.set_ylim(-1,1)\n        ax.set_title(\"t = {:.2f}\".format(self.t))\n\nDa, Db, alpha, beta = 1, 100, -0.005, 10\n\ndef Ra(a,b): return a - a ** 3 - b + alpha\ndef Rb(a,b): return (a - b) * beta\n\nwidth = 100\ndx = 1\ndt = 0.001\n\nOneDimensionalRDEquations(\n    Da, Db, Ra, Rb,\n    width=width, dx=dx, dt=dt,\n    steps=100\n).plot_time_evolution(\"1dRD.gif\", n_steps=150)\n\nMovieWriter imagemagick unavailable; using Pillow instead.\n\n\n\n\n\nreaction\n\n\nA pattern of stripes.\nWe can see that same behavior in two dimensions:\n\nclass TwoDimensionalRDEquations(BaseStateSystem):\n    def __init__(self, Da, Db, Ra, Rb,\n                 initialiser=random_initialiser,\n                 width=1000, height=1000,\n                 dx=1, dt=0.1, steps=1):\n\n        self.Da = Da\n        self.Db = Db\n        self.Ra = Ra\n        self.Rb = Rb\n\n        self.initialiser = initialiser\n        self.width = width\n        self.height = height\n        self.shape = (width, height)\n        self.dx = dx\n        self.dt = dt\n        self.steps = steps\n\n    def initialise(self):\n        self.t = 0\n        self.a, self.b = self.initialiser(self.shape)\n\n    def update(self):\n        for _ in range(self.steps):\n            self.t += self.dt\n            self._update()\n\n    def _update(self):\n\n        # unpack so we don't have to keep writing \"self\"\n        a,b,Da,Db,Ra,Rb,dt,dx = (\n            self.a, self.b,\n            self.Da, self.Db,\n            self.Ra, self.Rb,\n            self.dt, self.dx\n        )\n\n        La = laplacian2D(a, dx)\n        Lb = laplacian2D(b, dx)\n\n        delta_a = dt * (Da * La + Ra(a,b))\n        delta_b = dt * (Db * Lb + Rb(a,b))\n\n        self.a += delta_a\n        self.b += delta_b\n\n    def draw(self, ax):\n        ax[0].clear()\n        ax[1].clear()\n\n        ax[0].imshow(self.a, cmap='jet')\n        ax[1].imshow(self.b, cmap='brg')\n\n        ax[0].grid(visible=False)\n        ax[1].grid(visible=False)\n\n        ax[0].set_title(\"A, t = {:.2f}\".format(self.t))\n        ax[1].set_title(\"B, t = {:.2f}\".format(self.t))\n\n    def initialise_figure(self):\n        fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12,6))\n        return fig, ax\n\nDa, Db, alpha, beta = 1, 100, -0.015, 10\n\ndef Ra(a,b): return a - a ** 3 - b + alpha\ndef Rb(a,b): return (a - b) * beta\n\nwidth = 100\ndx = 1\ndt = 0.001\n\nTwoDimensionalRDEquations(\n    Da, Db, Ra, Rb,\n    width=width, height=width,\n    dx=dx, dt=dt, steps=100\n).plot_evolution_outcome(\"2dRD.png\", n_steps=150)\n\n\n\n\nreaction\n\n\n(It is possible to animate the formation of these patterns using a similar method call as in the one-dimensional case above. The resulting gifs are quite impressive, but also large, so they are not directly included here. However, if you click on the image it will take you to the animation)\nIt turns out this behaviour is common to a lot of reaction-diffusion equations, not just the ones presented here.\nSo how does this happen? To understand, we are going to need to do some maths."
  },
  {
    "objectID": "notebooks/turing_patterns.html#stability-analysis",
    "href": "notebooks/turing_patterns.html#stability-analysis",
    "title": "Turing Patterns",
    "section": "Stability Analysis",
    "text": "Stability Analysis\nLet’s return to the original equations. We’ve seen that for certain conditions, pattern formations occurs.\nUnfortunately, solving the equations directly for non-linear reaction functions is often not possible. Instead we can look at what happens when the the system is perturbed slightly from equilibrium.\n\nLinearising the Equations\nWe start by assuming there is some concentrations, \\(a_{0}\\) and \\(b_{0}\\), for which the system is stable. This means that - \\(R_{a}(a_{0},b_{0}) = 0\\) - \\(R_{b}(a_{0},b_{0}) = 0\\)\nAround these solutions, we look at the time dependence of small perturbations around these values - \\(x = a - a_{0}\\) - \\(y = b - b_{0}\\)\nAnd linearise the reaction equations - \\(R_{a}(a,b) \\approx r_{aa}x + r_{ab}y\\) - \\(R_{b}(a,b) \\approx r_{ba}x + r_{bb}y\\)\nwhere \\(r_{ij} = \\frac{\\partial R_{i}}{\\partial j}\\).\nThese approximations give us a set of linear equations, written in vector form as\n\\(\\dot{\\mathbf{x}} = D\\nabla^{2} \\mathbf{x} + R \\mathbf{x}\\)\nWhere \\(R = \\left(\\begin{matrix} r_{11} & r_{12} \\\\ r_{21} & r_{22}\\end{matrix}\\right)\\) and \\(D = \\left(\\begin{matrix} D_{a} & 0 \\\\ 0 & D_{b}\\end{matrix}\\right)\\)\n\n\nFourier Transform\nIf we impose periodic boundary conditions to this equation, a natural solution can be found by applying a Fourier Transformation to \\(\\mathbf{x}\\). If we call the reciprocal coordinate \\(k\\), and the Fourier transform of \\(\\mathbf{x}\\) as \\(\\tilde{\\mathbf{x}}\\), then the transformed equation is\n\\(\\dot{\\tilde{\\mathbf{x}}} = (R - k^{2}D) \\tilde{\\mathbf{x}}\\)\nWhich, has solutions of the form\n\\(\\tilde{\\mathbf{x}}(t) = \\tilde{\\mathbf{x}}(0) e^{\\omega t}\\)\nTo find \\(\\omega\\) we plug this solution back into our transformed equation to get\n\\(\\omega \\tilde{\\mathbf{x}} = (R - k^{2}D) \\tilde{\\mathbf{x}}\\)\nShowing that \\(\\omega\\) is just the eigenvalue of \\((R - k^{2}D)\\).\n\n\nStability\nWe now have an equation for the time dependence of our system in the Fourier domain. Using it, we can now discuss what we mean by stability. Our system is considered stable if small perturbations around the homogeneous do not cause it to move further away from the stable solutions.\nIn terms of our solution, \\(\\tilde{\\mathbf{x}}(0) e^{\\omega t}\\), stability means that the values of \\(\\omega\\) does not have positive real parts for all values of \\(k\\). If \\(\\omega\\) is negative, the perturbation will decay away. If \\(\\omega\\) is imaginary, it will oscillate around the stable state. However, if it is positive and real any small perturbation will grow exponentially, until a high order term of the reaction equation becomes important.\nTo find \\(\\omega\\), we need to solve the equation\n$(R - k^{2}D - I) = 0 $\nWriting \\(J = R - k^{2}D\\), this equitation takes the form\n\\(\\omega^{2} - \\omega\\hbox{Tr}(J) + \\hbox{Det}(J) = 0\\)\nSolving for \\(\\omega\\), we get\n\\(\\omega = \\frac{1}{2}(\\hbox{Tr}(J) \\pm \\sqrt{\\hbox{Tr}(J)^{2} - 4 \\hbox{Det}(J) })\\)\n\n\nConditions for (in)Stability\nFrom our initial assumption that there was a stable homogeneous state, we require that \\(\\omega\\) has negative real parts where \\(k = 0\\), which corresponds to the spatially homogeneous solution. For this to be true, we require that\n\n\\(\\hbox{Tr}(R) &lt; 0\\)\n\\(\\hbox{det}(R) &gt; 0\\)\n\nOr, in terms of the components of \\(R\\): - \\(r_{aa} + r_{bb} &lt; 0\\) - \\(r_{aa}r_{bb} - r_{ab}r_{ba} &gt; 0\\)\nFor an instability to now occur at finite wavelength, we need one of the following conditions to hold: - \\(\\hbox{Tr}(J) &gt; 0\\) - \\(\\hbox{det}(J) &lt; 0\\)\nBecause \\(\\hbox{Tr}(J) = \\hbox{Tr}(R) - k^{2}(d_{a} + d_{b})\\), the first condition cannot hold for any real \\(k\\). This means the we require the second to hold, or, after a bit of algebra\n\\(k^{4}d_{a}d_{b} - k^{2}(d_{a}r_{bb} + d_{b}r_{aa}) + (r_{aa}r_{bb} - r_{ab}r_{ba}) &lt; 0\\)\nfor some real value of \\(k\\). Once again, we need to solve a quadratic equation. To do this we note that because \\(k\\) needs to be real, \\(k^{2}\\) must be positive. This means that at least one root of quadratic equation in \\(k^{2}\\) needs to be positive and real. This condition is only met when\n\\(d_{b}r_{aa} + d_{a}r_{bb} &gt; 2\\sqrt{d_{a}d_{b}(r_{aa}r_{bb} - r_{ab}r_{ba})} &gt; 0\\)\nAnd that’s it. We’ve derived that conditions for the diffusion-reaction equations to be stable to small perturbations.\nWe can write the complete requirements as\n\n\\(r_{aa} + r_{bb} &lt; 0\\)\n\\(r_{aa}r_{bb} - r_{ab}r_{ba} &gt; 0\\)\n\\(d_{b}r_{aa} + d_{a}r_{bb} &gt; 2\\sqrt{d_{a}d_{b}(r_{aa}r_{bb} - r_{ab}r_{ba})} &gt; 0\\)\n\n\n\nDiscussion\nWe’ve waded through a lot of maths to reach this point. Let’s take stock of what we can make of the conditions for instability that we have derived.\nGiven the diffusion coefficients are positive, from the first and third equation above, we know that \\(r_{aa}\\) and \\(r_{bb}\\) have to have different signs, and the component with the negative sign has to be larger. Combining this we the second equation we know that either \\(r_{ab}\\) or \\(r_{ba}\\) has to be negative, and the other has to be positive. This two facts show why some systems like this are called activator-inhibitor system: the presence of one component increase the production of itself - the other inhibits the production of both.\nWith this information, the third equation shows us that the diffusion coefficients need to be different. Specifically, the diffusion coefficient of the inhibitor has to be larger then that of the activator.\nTogether, this gives us a hand-waving explanation of what might be going on: Consider starting with a small increase in the activator at some point. This in turn creates an increase in the inhibitor at that point. Both of these chemical diffuse to nearby points, but the inhibitor diffuses faster, lowering the activator concentration of nearby points. This lowering of the activator concentration at nearby points, which in turn lowers the inhibitor concentration. The disturbance spreads out like a wave.\nIt is a lot easier to get a feel for what’s going on once we visualise the this perturbation:\n\nDa, Db, alpha, beta = 1, 100, -0.005, 10\n\ndef Ra(a,b): return a - a ** 3 - b + alpha\ndef Rb(a,b): return (a - b) * beta\n\ndef initalise_bump(shape):\n    a  = np.zeros(shape)\n    if len(a.shape) == 1:\n        a[int(a.shape[0] / 2)] = 0.3\n    elif len(a.shape) == 2:\n        a[int(a.shape[0] / 2), int(a.shape[1] / 2)] = 0.3\n\n    return(\n        a,\n        np.zeros(shape)\n    )\n\nwidth = 100\ndx = 1\ndt = 0.001\n\nOneDimensionalRDEquations(\n    Da, Db, Ra, Rb,\n    initialiser=initalise_bump,\n    width=width, dx=dx, dt=dt,\n    steps=250\n).plot_time_evolution(\"1dRD_initial_bump.gif\", n_steps=150)\n\n\n\n\nreaction\n\n\n\n\nExploring the Parameter Space\nIt is interesting to ask how the parameters of the equations affect the pattern formation. To get a feel for what this dependence is, we can investigate the amplitude of the instability for different frequencies. Close to equilibrium, we expect the frequency with the largest real component to dominate the dynamics of the system.\nCalculating this value isn’t mathematically difficult: we have an equation of the amplitude \\(\\omega\\) in terms of the frequency \\(k\\) above. Differentiating an equation of this size by hand can become tedious due to its size, and it is easy to introduce errors. And why carry out simple replacement steps when we have a computer to hand?\nIn the following, I use the computer algebra system sympy to find the largest real mode of the system\n\nimport sympy as sp\nsp.init_printing()\n\n# define symbols\na,b,alpha,beta = sp.symbols(\"a,b,alpha,beta\")\nRaa, Rab, Rba, Rbb = sp.symbols(\"Raa, Rab, Rba, Rbb\")\nDa, Db = sp.symbols(\"Da, Db\")\nk2 = sp.Symbol(\"k2\")\n\n# create matricies\nR = sp.Matrix(\n    [[Raa, Rab],\n     [Rba, Rbb]]\n)\n\nD = sp.Matrix(\n    [[Da, 0],\n     [0, Db]]\n)\n\nJ = (R - k2 * D)\n\n# define our solution for omega\nomega = sp.Matrix.trace(J) + sp.sqrt(sp.Matrix.trace(J) ** 2 - 4 * sp.Matrix.det(J))\nomega\n\n\\[- Da k_{2} - Db k_{2} + Raa + Rbb + \\sqrt{- 4 Da Db k_{2}^{2} + 4 Da Rbb k_{2} + 4 Db Raa k_{2} - 4 Raa Rbb + 4 Rab Rba + \\left(- Da k_{2} - Db k_{2} + Raa + Rbb\\right)^{2}}\\]\n\n\n\n# find the maximum value of omega for k^{2}\nomega_prime = sp.diff(omega, k2)\nsols = sp.solvers.solve(omega_prime, k2)\nmax_k2 = sols[0]\n\nmax_k2\n\n\\[\\frac{1}{Da Db \\left(Da - Db\\right)} \\left(Da Db Raa - Da Db Rbb - Da \\sqrt{- Da Db Rab Rba} - Db \\sqrt{- Da Db Rab Rba}\\right)\\]\n\n\n\ndef get_length_scale(al, be, da, db):\n\n    # rewritng in terms of Da, Db, alpha, beta\n    Ra = a - a**3 - b + alpha\n    Rb = beta * (a - b)\n\n    # we expand around the homogenius steady state\n    zero_subs = [\n        (a, sp.sign(alpha) * sp.cbrt(sp.Abs(alpha))),\n        (b, sp.sign(alpha) * sp.cbrt(sp.Abs(alpha)))\n    ]\n\n    # and linearise\n    R_prime_subs = {\n        (Raa, sp.diff(Ra, a)),\n        (Rab, sp.diff(Ra, b)),\n        (Rba, sp.diff(Rb, a)),\n        (Rbb, sp.diff(Rb, b))\n    }\n\n    # putting in our numeric values we get\n    vals = [\n        (alpha, al),\n        (beta, be),\n        (Da, da),\n        (Db, db)\n    ]\n\n    # substitute it all in to find the maximum k\n    max_k2 = [sp.N(\n        sol\n        .subs(R_prime_subs)\n        .subs(zero_subs)\n        .subs(vals)\n    ) for sol in sols]\n\n    k2 = max(sp.re(sym) for sym in max_k2)\n\n    # if k2 is negative, no real solution exists\n    if k2 &lt;= 0:\n        return -1\n\n    # convert k2 into length units\n    return sp.N(2 * sp.pi / sp.sqrt(k2))\n\nget_length_scale(-0.005, 10, 1, 100)\n\n\\[13.6336345738685\\]\n\n\nWhich looks about right when we compare it to our simulations above - the spatial variation is about 14 length units.\nLet’s use it to see how the length scale of the system changes as a function of \\(\\alpha\\) and \\(\\beta\\):\n\nbetas = [(1 + i) * 0.125 for i in range(0,40)]\nalphas = [i * 0.05 for i in range(-20,20)]\n\noutcomes = np.zeros((len(betas), len(alphas)))\n\nfor x,al in enumerate(alphas):\n    for y,be in enumerate(betas):\n        outcomes[y,x] = get_length_scale(al, be, 1, 100)\n\n\nplt.figure(figsize=(8,8))\n\no = outcomes\no[outcomes == 0] = -1\n\nplt.imshow(o, cmap=\"hsv\", interpolation=\"none\")\nplt.colorbar()\n\nplt.xticks(*zip(*[(n,l) for n,l in enumerate(alphas) if n % 5 == 0]), rotation=90)\nplt.yticks(*zip(*[(n,l) for n,l in enumerate(betas) if n % 5 == 0]), rotation=0)\n\nplt.title(\"Lengthscale\", fontsize=20)\nplt.xlabel(r\"$\\alpha$\", fontsize=18)\nplt.ylabel(r\"$\\beta$\", fontsize=18);\n\n\n\n\n\n\n\n\nAnd take a look at the long length scale we should see when we use a low value of \\(\\beta\\):\n\nDa, Db, alpha, beta = 1, 100, 0.01, 0.25\n\ndef Ra(a,b): return a - a ** 3 - b + alpha\ndef Rb(a,b): return (a - b) * beta\n\nwidth = 200\ndx = 1\ndt = 0.001\n\nTwoDimensionalRDEquations(\n    Da, Db, Ra, Rb,\n    width=width, height=width,\n    dx=dx, dt=dt, steps=150\n).plot_evolution_outcome(\"2dRD_small_beta.png\", n_steps=100)\n\n\n\n\nreaction\n\n\n\n\nInitial Conditions\nAdjusting the parameters gives us control over the length scale of the system. We can also gain some control over the system by adjusting the initial conditions of the system. For example, we can impose symmetries on the system by making the initial conditions symmetric:\n\nfrom scipy.ndimage.interpolation import rotate\n\ndef average_rotate(a, degree):\n    \"\"\"\n    Takes a 2d array a, and produces the average arrays,\n    having rotated it degree times. The resulting shape\n    has approximate degree-fold rotational symmetry.\n    \"\"\"\n    theta = 360 / degree\n\n    a = np.mean([rotate(a, theta * i, reshape=False) for i in range(degree)], axis=0)\n\n    return a\n\n\ndef random_symmetric_initialiser(shape, degree):\n    \"\"\"\n    Random initialiser with degree-fold symmetry.\n    \"\"\"\n\n    a = np.random.normal(loc=0, scale=0.05, size=shape)\n    b = np.random.normal(loc=0, scale=0.05, size=shape)\n\n    return (\n        average_rotate(a, degree),\n        average_rotate(b, degree)\n    )\n\nDa, Db, alpha, beta = 1, 100, 0.01, 1\n\ndef Ra(a,b): return a - a ** 3 - b + alpha\ndef Rb(a,b): return (a - b) * beta\n\nwidth = 200\ndx = 1\ndt = 0.001\n\n# three fold\nthree_fold_initialiser = lambda shape: random_symmetric_initialiser(shape, 3)\n\nTwoDimensionalRDEquations(\n    Da, Db, Ra, Rb,\n    initialiser=three_fold_initialiser,\n    width=width, height=width,\n    dx=dx, dt=dt, steps=250\n).plot_evolution_outcome(\"2dRD_3_fold_sym.png\", n_steps=150)\n\n# five fold\nfive_fold_initialiser = lambda shape: random_symmetric_initialiser(shape, 5)\n\nTwoDimensionalRDEquations(\n    Da, Db, Ra, Rb,\n    initialiser=five_fold_initialiser,\n    width=width, height=width,\n    dx=dx, dt=dt, steps=250\n).plot_evolution_outcome(\"2dRD_5_fold_sym.png\", n_steps=150)\n\nThree fold rotational symmetry:\n\n\n\nreaction\n\n\nFive fold rotational symmetry:\n\n\n\nreaction\n\n\nThe symmetry isn’t perfect due to implementing it one a square grid, but it works quite well.\n\n\nSpatial Effects\nAnother interesting extension to these systems is to see what happens when we allow the parameters of the system to vary in space. In our implementation, this is just a matter of making the parameters a grid as well:\n\nDa, Db, alpha, beta = 1, 100, 0.01, 10\n\nwidth = 200\ndx = 1\ndt = 0.001\n\nx,y = np.mgrid[0:width,0:width]\nbeta = 0.1 + 5 * (1 + np.sin(2 * np.pi * y / 50)) * (1 + np.sin(2 * np.pi * x / 50))\n\n\ndef Ra(a,b): return a - a ** 3 - b + alpha\ndef Rb(a,b): return (a - b) * beta\n\nTwoDimensionalRDEquations(\n    Da, Db, Ra, Rb,\n    width=width, height=width,\n    dx=dx, dt=dt, steps=100\n).plot_evolution_outcome(\"2dRD_spatial.png\", n_steps=200)\n\n\n\n\nreaction"
  },
  {
    "objectID": "notebooks/turing_patterns.html#similar-systems",
    "href": "notebooks/turing_patterns.html#similar-systems",
    "title": "Turing Patterns",
    "section": "Similar Systems",
    "text": "Similar Systems\nTuring patterns are only one part of pattern generation from non-linear PDEs. Given we have the tools to explore these systems, it is worth looking at another simple system in which pattern formation occurs: the Gray-Scott equations. As described in the paper Complex Patterns in a Simple System, for an interesting set of initial conditions the Gray-Scott equations result in a range complex patterns being formed.\nWhile these equations are reaction-diffusion equations, the patterns that end up being formed are not from Turing instabilities. Instead they come from the initial conditions of the system: a small region in the centre of the region is perturbed. As this perturbation spreads out, strange patterns are formed.\nTo visualise it, we just need to pass another initialisation function of our previous class, and put in new reaction equations. The parameters for the interesting systems are taken from here. I’m not currently aware of any theory that gives a full explanation of these patterns.\n\ndef gs_initialiser(shape):\n        a = np.ones(shape)\n        b = np.zeros(shape)\n        centre = int(shape[0] / 2)\n\n        a[centre-20:centre+20,centre-20:centre+20] = 0.5\n        b[centre-20:centre+20,centre-20:centre+20] = 0.25\n\n        a += np.random.normal(scale=0.05, size=shape)\n        b += np.random.normal(scale=0.05, size=shape)\n\n        return a,b\n\n# interesting parameters taken from http://www.aliensaint.com/uo/java/rd/\nparams = [\n    [0.16, 0.08, 0.035, 0.065],\n    [0.14, 0.06, 0.035, 0.065],\n    [0.16, 0.08, 0.06, 0.062],\n    [0.19, 0.05, 0.06, 0.062],\n    [0.16, 0.08, 0.02, 0.055],\n    [0.16, 0.08, 0.05, 0.065],\n    [0.16, 0.08, 0.054, 0.063],\n    [0.16, 0.08, 0.035, 0.06]\n]\n\nfor i, param in enumerate(params):\n\n    Da, Db, f, k = param\n\n    def Ra(a,b): return - a*b*b + f*(1-a)\n    def Rb(a,b): return + a*b*b - (f+k)*b\n\n    width = 200\n    dx = 1\n    dt = 1\n\n    TwoDimensionalRDEquations(\n        Da, Db, Ra, Rb,\n        initialiser=gs_initialiser,\n        width=width, height=width,\n        dx=dx, dt=dt, steps=200\n    ).plot_evolution_outcome(\"gs_{}.png\".format(i), n_steps=100)\n\n\n\n\nreaction\n\n\n\n\n\nreaction\n\n\n\n\n\nreaction\n\n\n\n\n\nreaction\n\n\n\n\n\nreaction\n\n\n\n\n\nreaction\n\n\n\n\n\nreaction\n\n\n\n\n\nreaction"
  },
  {
    "objectID": "notebooks/turing_patterns.html#conclusion",
    "href": "notebooks/turing_patterns.html#conclusion",
    "title": "Turing Patterns",
    "section": "Conclusion",
    "text": "Conclusion\nIt’s been interesting to play with these non-linear systems. One of the main challenges I’ve found with looking at them is how you find “interesting” parameters. I have some ideas about how we might search for these automatically, but that will have to wait for another post.\nThe complexity that can arise from these simple systems surprised me at fist, but it has been shown that it is possible to encode cellular automata in reaction diffusion systems. This suggests that the patterns formed by reaction-diffusion systems can be as complex as those produced by any Turing-complete system, and that any attempt to provide a quantitative explanation for their full behaviour will eventually run into some undecidable statement.\nAnd with that connection between Turing’s work on reaction-diffusion equations and his more well known work on computability, I think it’s time to end this post."
  },
  {
    "objectID": "notebooks/turing_patterns.html#code",
    "href": "notebooks/turing_patterns.html#code",
    "title": "Turing Patterns",
    "section": "Code",
    "text": "Code\nThis post was created from a jupyter notebook. You can find it here."
  },
  {
    "objectID": "submitted/projects_1/First_Projects.html",
    "href": "submitted/projects_1/First_Projects.html",
    "title": "Markov Chain",
    "section": "",
    "text": "Very nice work overall! There’s a bit more I’d like to see on the second project – and I’ve got a few small questions about the first one, if you’ve got time.\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport scipy\nimport numpy as np\nfrom scipy.stats import norm\n\n\nMarkov Chain\nThe transition matrix under this context will be \\(\\begin{bmatrix}\n0.5 & 0.2 & 0.3\\\\\n0.2 & 0.6 & 0.1\\\\\n0.3 & 0.2 & 0.6\\\\\n\\end{bmatrix}\\).\nNow, let us put this transition matrix into code. We will set the initial state of the system to be\n\\[\\begin{bmatrix}\n\\frac{1}{3}\\\\\n\\frac{1}{3}\\\\\n\\frac{1}{3}\n\\end{bmatrix}\\]\nto denote all three companies having equal shares of the market.\n\n\nI appreciate having the explanation of what this initial state means, right here when you first present it.\n\n# Code in this transition matrix and the initial state (Equal shares of customers)\nmarkovTransition = np.array([[0.5, 0.2, 0.3], [0.2, 0.6, 0.1], [0.3, 0.2, 0.6]])\ninitialState = np.array([[1/3], [1/3], [1/3]])\niteration = initialState\n\n# Test three iterations\nfor i in range(3):\n  iteration = markovTransition.dot(iteration)\n  print(f\"{i + 1} Year Later: \\n Percent of customer shares for A: {np.round(iteration[0, 0] * 100, decimals=2)}% \\n Percent of customer shares for B: {np.round(iteration[1, 0] * 100, decimals=2)}% \\n Percent of customer shares for C: {np.round(iteration[2, 0] * 100, decimals=2)}%\")\n\n1 Year Later: \n Percent of customer shares for A: 33.33% \n Percent of customer shares for B: 30.0% \n Percent of customer shares for C: 36.67%\n2 Year Later: \n Percent of customer shares for A: 33.67% \n Percent of customer shares for B: 28.33% \n Percent of customer shares for C: 38.0%\n3 Year Later: \n Percent of customer shares for A: 33.9% \n Percent of customer shares for B: 27.53% \n Percent of customer shares for C: 38.57%\n\n\nWe can see from the results that B begins to lose shares of the market, C gains a considerable amount of shares (around 2%), while A gains small amount of shares (around 0.6%) in three years.\nNow, let us test the effects of the two advertising campaigns.\nAdvertising Campaign 1 convinces 20% of customers who would have to stay in B to switch to A. This can be represented in the modified transition matrix: \\(\\begin{bmatrix}\n0.5 & 0.32 & 0.3\\\\\n0.2 & 0.48 & 0.1\\\\\n0.3 & 0.2 & 0.6\\\\\n\\end{bmatrix}\\).\nAdvertising Campaign 2 convinces 20% of customers who would have to stay in C to switch to A. This can be represented in the modified transition matrix: \\(\\begin{bmatrix}\n0.5 & 0.2 & 0.42\\\\\n0.2 & 0.6 & 0.1\\\\\n0.3 & 0.2 & 0.48\\\\\n\\end{bmatrix}\\).\nTo view the effectiveness of the campaigns, I will iterate these markov chains over times and create a Gaussian Fit.\n\n# Code in the advertising campaign transition matrices.\ntransition1 = np.array([[0.5, 0.32, 0.3], [0.2, 0.48, 0.1], [0.3, 0.2, 0.6]])\ntransition2 = np.array([[0.5, 0.2, 0.42], [0.2, 0.6, 0.1], [0.3, 0.2, 0.48]])\n\n# Let us iterate over random distribution vectors for the two transition matrices\n# to view the resulting market shares after 3 years, 50 years, and 100 years.\nnp.random.seed(10)\nruns = 10000\n\n# I am defining a function that takes in the two transition matrices and the\n# number of years to iterate and outputs Gaussian fits for the shares for A\ndef advertisingGaussian(runs, years, transitionMatrix1, transitionMatrix2):\n  resultForAFrom1 = []\n  resultForAFrom2 = []\n  for run in range(runs):\n    # We do experiment a total of variable \"runs\" times.\n    # Create random initial distribution vector for the intial market share.\n    mold = np.random.random((3,))\n    initialState = mold / mold.sum()\n\n    state = initialState\n    # Now we do the actual iteration for the two matrices and append the results.\n    for year in range(years):\n      state = transition1.dot(state)\n    resultForAFrom1.append(state[0])\n\n    state = initialState\n    for year in range(years):\n      state = transition2.dot(state)\n    resultForAFrom2.append(state[0])\n\n  # Now we can fit the two sets of data for the market share of A\n  fig, axs = plt.subplots(2, 1, figsize=(6, 6))\n\n  # Determine the random color of the histograms\n  randomColorNumber = np.random.rand(3,)\n\n\n  # Fitting the 1 results\n  mu1, sigma1 = norm.fit(resultForAFrom1)\n\n  # Plotting the advertising campaign 1 results\n  axs[0].hist(resultForAFrom1, bins=100, density=True, alpha=0.7, color=randomColorNumber)\n  axs[0].set_title(f'The Distribution of Market Shares of A with Advertising Campaign 1 after {years} years, iterated for {runs} times.\\n(Estimated market share value of A for Campaign 1: {np.round(mu1, decimals=4)})', pad=20)\n  axs[0].set_xlabel('Market Share Distribution for A')\n  axs[0].set_ylabel('Density')\n\n  xmin1, xmax1 = axs[0].get_xlim()\n  x1 = np.linspace(xmin1, xmax1, 100)\n  p1 = norm.pdf(x1, mu1, sigma1)\n  axs[0].plot(x1, p1, 'k', linewidth=2, label='Gaussian Fit')\n\n\n  # Fitting the 2 results\n  mu2, sigma2 = norm.fit(resultForAFrom2)\n\n  # Plotting the advertising campaign 2 results\n  axs[1].hist(resultForAFrom2, bins=100, density=True, alpha=0.7, color=randomColorNumber)\n  axs[1].set_title(f'The Distribution of Market Shares of A with Advertising Campaign 2 after {years} years, iterated for {runs} times.\\n(Estimated market share value of A for Campaign 2: {np.round(mu2, decimals=4)})', pad=20)\n  axs[1].set_xlabel('Market Share Distribution for A')\n  axs[1].set_ylabel('Density')\n\n  xmin2, xmax2 = axs[1].get_xlim()\n  x2 = np.linspace(xmin2, xmax2, 100)\n  p2 = norm.pdf(x2, mu2, sigma2)\n  axs[1].plot(x2, p2, 'k', linewidth=2, label='Gaussian Fit')\n\n  # Showing the plot\n  plt.tight_layout()\n  plt.show()\n\n# Now call the function\n\nadvertisingGaussian(runs, 3, transition1, transition2)\nprint(\"\\n\")\nadvertisingGaussian(runs, 10, transition1, transition2)\nprint(\"\\n\")\nadvertisingGaussian(runs, 50, transition1, transition2)\nprint(\"\\n\")\nadvertisingGaussian(runs, 100, transition1, transition2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/Users/kendra/Library/Python/3.8/lib/python/site-packages/numpy/lib/histograms.py:883: RuntimeWarning: divide by zero encountered in divide\n  return n/db/n.sum(), bin_edges\n/Users/kendra/Library/Python/3.8/lib/python/site-packages/numpy/lib/histograms.py:883: RuntimeWarning: invalid value encountered in divide\n  return n/db/n.sum(), bin_edges\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet us analyze these results. Each pair of histogram denotes the distribution of the market shares for A after some years. The estimated market share values after the set amount of years is denoted under the title. This estimated value is the mu parameter for the Gaussian fit, denoting the value with the greatest probabiltiy distribution.\nFor Advertising Campaign 1, we see that for 1000 random initial distributions of market shares will tend with each iteration towards a stationary state of 0.38 for the market share of A. This suggests that Advertising Campaign 1 will allow A to stabilize at 0.3806 marketshares. This seems to be a very good strategy.\nFor Advertising Campaign 2, we can see that the most probable market share distribution for also increases. With long iterations under numerous random intial states, A seems to always achieve the stationary state of 0.39 for its market share.\nEven after three years, markov simulation suggests that Advertising Campaign 2 performs better than Advertising Campaign 1, as the market share of A under Advertising Campaign 2 is bigger than the market share of A under Advertising Campaign 1. The later iterations also suggests the same thing. Therefore, my model suggests that Advertising Campaign 2 is a more effective choice both for long term and short term increase in market share for A.\n\nI agree with these conclusions.\n\nCan you discuss why these results had to happen, given the specific transition matrices you had? For instance, you are multiplying by the same matrices many times. What do you see if you simply look at the matrix exponents?\n\nAlso: there seems to be a problem with the histograms I see after 50 and 100 years. What’s going on? Did you see this on your end? Is there something else you could do to represent these data better?\n\nIt is important to note the limitations within this model. First, it simplifies the dynamics of the market. By iterating the static transition matrices over long periods of time, we are assuming that the market does not change. In other words, company B and company C are doing nothing to regain their lost customers. After all, it is quite unreasonable to assume that things will always stay the same in 100 years. The effectiveness of the advertising campaign also can change, which is not reflected in the static transition matrices. Therefore, this model may be effect in approximating the effects of the advertising campaigns for a very short period of iteration before competitors B and C have a chance to react or implement their own strategies.\nNow, perhaps we wish to go a step further and analyze how much of an effect gaining shares from B or C will have on the long term share of A. Then, we can simply construct functions of A market share with respect to the gaining of the market share of B or C.\n\n# The variable parameter will be a number between 0 and 1, denoting up till\n# 100% of the customer that was suppose to remain with B going to A\n# Iterations will control how many years passes.\ndef functionB(parameter, iterations, initialState):\n  change = parameter * 0.4\n  transitionMatrix = np.array([[0.5, 0.2 + change, 0.3], [0.2, 0.6 - change, 0.1], [0.3, 0.2, 0.6]])\n  # We will start with the assumption for equal shares.\n  for year in range(iterations):\n    initialState = transitionMatrix.dot(initialState)\n  return initialState[0]\n\ndef functionC(parameter, iterations, initialState):\n  change = parameter * 0.4\n  transitionMatrix = np.array([[0.5, 0.2, 0.3 + change], [0.2, 0.6, 0.1], [0.3, 0.2, 0.6 - change]])\n  # We will start with the assumption for equal shares\n  for year in range(iterations):\n    initialState = transitionMatrix.dot(initialState)\n  return initialState[0]\n\n# Setting the initial state to be all companies having equal shares\ninitialState = np.ones((3,1)) / 3\n\n# Readying the plot\nxValues = np.linspace(0,1, num=101)\nBValues = []\nCValues = []\n\nfor x in xValues:\n  BValues.append(functionB(x, 50, initialState))\n  CValues.append(functionC(x, 50, initialState))\n\n# Plotting\nplt.title(\"Market Share of A after 50 years with Respect to Change in Parameter x\")\nplt.plot(xValues, BValues, label=\"Parameter under Advertisement 1 (B to A)\")\nplt.plot(xValues, CValues, label=\"Parameter under Advertisement 2 (C to A)\")\nplt.xlabel(\"Parameter (Percent Change / 100)\")\nplt.ylabel(\"Market Share for A\")\nplt.legend(loc=\"upper left\", frameon=True)\n\n\n\n\n\n\n\n\nThe above graph works is generated by the following method. We introduce the parameter variable that ranges from 0 to 1. For each transition matrix representing the advertisement matrix, the parameter looks at the percentage of customers going to A rather than staying in B/C from the effects of the campaign and multiplies it by the parameter. For example, if the parameter is 1, than for Advertisement 1, 100% of the customers that was originally going to stay with B will go to A, resulting in the following transition matrix: \\(\\begin{bmatrix}\n0.5 & 0.8 & 0.3\\\\\n0.2 & 0.0 & 0.1\\\\\n0.3 & 0.2 & 0.6\\\\\n\\end{bmatrix}\\).\n\n\nI love this - it’s a very concrete example which makes it very clear what you are doing.\nNow, with the modified transition matrix by the parameter, the initial state of all companies having equal shares\n\\[\\begin{bmatrix}\n\\frac{1}{3}\\\\\n\\frac{1}{3}\\\\\n\\frac{1}{3}\n\\end{bmatrix}\\]\nis initialized. We apply the initial state to the modified transition matrix for 50 times. The resulting market share value of A is then recorded and plotted.\nLooking at the graph, if the resources needed to convert customers from C to A is the same as the resources needed to convert customers from B to A, then it is more effective to convert customers from C since the increase in the market share of A is more for C than B as we increase the parameter.\n\nThis is nicely done and very clear overall. I think you could push a little more on the why; why does Campaign 2 work better, how can you tell just from looking at the transition matrix?\nGrade: E\n\n\n\nSports Ranking\nWe will first rank the teams based on their win-loss ratio. This will be the simplest way to rank them. In fact, we can create a digraph with the given edges before doing so. This allows us to create an adjacency matrix. The sum of the \\(i^{th}\\) row of the matrix gives us the number of wins for team \\(i\\), while the sum of the \\(i^{th}\\) column of the matrix gives us the number of losses for team \\(i\\). Therefore, to account for both wins and losses of each team, we can create a ranking score for each team determined by their number of wins subtracted by their number of losses. Using the adjacency matrix of the graph we created makes our life a lot easier, so I am introducing the graph before the simple ranking.\n\n\nNot really important, but what you do isn’t actually a win-loss ratio…\n\nsportsGraph = nx.DiGraph()\n\nsportsGraph.add_nodes_from([1, 2, 3, 4, 5, 6, 7])\n\nedges = [\n\n    (1, 2), (7, 3), (2, 4), (4, 5), (3, 2), (5, 1), (6, 1), (7, 2), (2, 6),\n\n    (3, 4), (7, 4), (5, 7), (6, 4), (3, 5), (5, 6), (7, 1), (5, 2), (7, 6),\n\n    (1, 4), (6, 3)\n\n]\n\nsportsGraph.add_edges_from(edges)\n\n# Let us plot the graph representing the wins and losses of the 7 teams.\nnx.draw_circular(sportsGraph, with_labels=True)\nplt.show()\n\n# To calculate the win-losses for the teams, we can do it by hand, but it is easier to analyze the adjacency matrix of the graph.\nadj_matrix = nx.adjacency_matrix(sportsGraph)\nadj_matrix = adj_matrix.toarray()\n\n# Define the win and loss arrays, where the index+1 denotes the team number:\nwins = np.zeros(7)\nlosses = np.zeros(7)\n\n# The ith column of the adjacency matrix is the number of losses team i experienced. The ith row is the number of wins team i experienced.\nfor i in range(len(wins)):\n  wins[i] = adj_matrix[i,:].sum()\n\nfor i in range(len(losses)):\n  losses[i] = adj_matrix[:, i].sum()\n\n# We define a very simple score that takes into account the number of wins and the number of losses for each team. The higher the winLossScore, the better the ranking.\nwinLossScore = wins - losses\n\nsimpleSort = np.argsort(winLossScore)[::-1]\nwLSorted = np.sort(winLossScore)[::-1]\n\nprint(\"\\n\")\nprint(\"Simple ranking of the teams based on win/loss:\")\n\nfor i in range(len(simpleSort)):\n  print(f\"{i+1}: Team {simpleSort[i] + 1} with a win - loss score of {wLSorted[i]}\")\n\n\n\n\n\n\n\n\n\n\nSimple ranking of the teams based on win/loss:\n1: Team 7 with a win - loss score of 4.0\n2: Team 5 with a win - loss score of 2.0\n3: Team 3 with a win - loss score of 1.0\n4: Team 6 with a win - loss score of 0.0\n5: Team 1 with a win - loss score of -1.0\n6: Team 2 with a win - loss score of -2.0\n7: Team 4 with a win - loss score of -4.0\n\n\nWe can see that the win-loss score gives us fairly neat ranking. Although we didn’t see the same scores on different teams, we can account for it by developing a system. If two teams have the same score, then we look at their total wins. The team with the most wins will be ranked higher. If they have the same wins, they have to have the same losses (since they have the same win-loss score). In this case, we will have to devise more sophisticated methods for ranking. This is what we do next. We will use the vertex power to rank the teams. Remember, the vertex power is the number of connections of a vertex with length two of less. This allows us to sort of consider the “weight” of the team wins. If team 1 wins team 2, and team 2 wins team 3, then we consider team 1 to be better than team 3 and count that as a “win” for team 1 (lucky!)\n\n\nI’m not sure what you mean by this: ” If they have the same wins, they have to have the same losses (since they have the same win-loss score). In this case, we will have to devise more sophisticated methods for ranking. ”\nThen, we can rank the teams based on this power concept. The team with the highest power will be first place.\n\n# Next, we use the vertex power to rank\n\n# Calculate the power matrix\npowerMatrix = adj_matrix + adj_matrix.dot(adj_matrix)\n\n# This will include all values for the powers of each team i (index)\npowers = np.zeros(7)\n\nfor i in range(len(powers)):\n  powers[i] = powerMatrix[i, :].sum()\n\n# Now, sort the powers\npowerSort = np.argsort(powers)[::-1]\npowersSorted = np.sort(powers)[::-1]\n\n# Printing the sort\nprint(\"Power Ranking of the teams\")\nfor i in range(len(powerSort)):\n  print(f\"{i+1}: Team {powerSort[i] + 1} with a power value of {powersSorted[i]}\")\n\nPower Ranking of the teams\n1: Team 7 with a power value of 16.0\n2: Team 5 with a power value of 16.0\n3: Team 3 with a power value of 10.0\n4: Team 6 with a power value of 9.0\n5: Team 2 with a power value of 6.0\n6: Team 4 with a power value of 5.0\n7: Team 1 with a power value of 5.0\n\n\nWe can see that, compared to the simple ranking based on win-loss scores, the power ranking has all of the places the same except for a switch in the place of team 1, 2, 4. This is interesting. This means that although team 2 has a lower win-loss score than team 1, it has a higher power than team 1. We also note that the power ranking is not satisfactory, as there are two pairs of teams (7 and 5, 4 and 1) that has the same power.\n\n\nWhy does Team 2 have a higher power than Team 1? Can you explain it concretely in terms of games that it won?\nTherefore, we should go further and try the Reverse PageRank method. Remember that the Reverse PageRank uses the adjoint of our original adjacency matrix. So, instead of the original PageRank ranking pages based on ingoing links, we now rank based on outgoing links. In our context, this is what we wish, since the outgoing links from team 1 to team 2 means that team 1 won over team 2. Also, Reverse PageRank involves the inclusion of a teleportation vector. This teleportation vector in this context allows us to encode in some “luck” parameter to all of the teams. Maybe team 1 won team 2 because a member in team 2 had a stomache. The teleportation vector then represents a global correction to the weighing of all wins.\n\n\nThe “luck” idea is an interesting way of describing the teleportation vector; I hadn’t thought of it like this before. I think you could dig in a bit: it gives a little bit of weight to the worldview that the teams really are equally good, and that the observed wins and losses were just due to luck. I think you could\nNow, let’s do the reverse pagerank.\n\ntransitionMatrix = np.zeros(np.shape(adj_matrix))\n\n# Since we are doing the transpose of the adjacent matrix, we can do a small\n# shortcut and simply normalize the column vectors of the adjacement matrix.\n# This is because the transition matrix is the \"normalized\" transpose of the\n# adjacent matrix. We start with the transpose of the adjacent matrix. So this\n# ends up canceling out.\nfor j in range(np.shape(adj_matrix)[1]):\n  sumBuffer = 0\n  for i in range(np.shape(adj_matrix)[0]):\n    sumBuffer += adj_matrix[i,j]\n  for m in range(np.shape(adj_matrix)[0]):\n    if adj_matrix[m,j] &gt; 0:\n      transitionMatrix[m,j] = adj_matrix[m,j] / sumBuffer\n\n# Now, we solve the PageRank equation with our obtained transitionMatrix\nalpha = 0.85\n\nmatrixToSolve = np.eye(7) - alpha * transitionMatrix\ninverse = np.linalg.inv(matrixToSolve)\noneMinusAlphaV = (1 - alpha) * (1/7 * np.ones((7,1)))\nanswer = inverse.dot(oneMinusAlphaV)\n\n#Now we sort the values based on descending order\nreverseIndex = np.argsort(answer, axis=0).flatten()[::-1]\nreverseSort = np.sort(answer, axis=0).flatten()[::-1]\n\n#Printing the results\nprint(\"The ReversePage Algorithm Ranking: \")\nfor i in range(len(answer)):\n  print(f\"{i+1}: Team {reverseIndex[i] + 1} with the value of {np.round(reverseSort[i], decimals=2)}\")\n\nThe ReversePage Algorithm Ranking: \n1: Team 5 with the value of 0.25\n2: Team 7 with the value of 0.18\n3: Team 3 with the value of 0.17\n4: Team 6 with the value of 0.13\n5: Team 4 with the value of 0.13\n6: Team 2 with the value of 0.08\n7: Team 1 with the value of 0.06\n\n\nLet us think about the results of this ReversePage compared to the power ranking. We can see now that when we consider the “stable state”, which is what we determine to be the ranking, Team 5 has higher “ranking score” than Team 7. This suggests that even though Team 5 and Team 7 have the same power, the influence, or the “quality” of the wins for Team 5 is higher than Team 7.\n\n\nI really want to see the why here! In English, why is the quality of the wins for Team 5 higher than Team 7?\nAnother interesting result to recognize is the change in the ranking place for Team 4 and Team 6. Even though the “quality” of their wins are the same, the power, or the amount of their wins differ.\nNow, let us add actual weights to the wins with M.\n\nM = [4, 8, 7, 3, 7, 23, 15, 6, 18, 13, 14, 7, 13, 7, 18, 45, 10, 19, 14, 13]\n\n# Administer the weighing\nindex = 0\nnewMatrix = np.zeros(np.shape(adj_matrix))\nfor coordinate in edges:\n  newMatrix[coordinate[0] - 1, coordinate[1] - 1] = adj_matrix[coordinate[0] - 1, coordinate[1] - 1] * M[index]\n  index += 1\n\n# Calculate the new power matrix\nnewPowerMatrix = newMatrix + newMatrix.dot(newMatrix)\n\n# This will include all values for the powers of each team i (index)\nnewPower = np.zeros(7)\n\nfor i in range(len(newPower)):\n  newPower[i] = newPowerMatrix[i, :].sum()\n\n# Now, sort the powers\nnewPowerSort = np.argsort(newPower)[::-1]\nnewPowersSorted = np.sort(newPower)[::-1]\n\n# Printing the sort\nprint(\"Power ranking with added weighing: \")\nfor i in range(len(newPowerSort)):\n  print(f\"{i+1}: Team {newPowerSort[i] + 1} with a power value of {newPowersSorted[i]}\")\n\nPower ranking with added weighing: \n1: Team 5 with a power value of 2104.0\n2: Team 7 with a power value of 2089.0\n3: Team 2 with a power value of 784.0\n4: Team 6 with a power value of 701.0\n5: Team 3 with a power value of 647.0\n6: Team 4 with a power value of 177.0\n7: Team 1 with a power value of 160.0\n\n\nThe higher the power value for the teams, we can assume that the significance of their win is larger. This means that we know although team 5 had less wins than team 7, team 5’s wins were just so much more rewarding and meaningful. The same could be said with the other teams.\n\nYou’ve done a nice job here, clearly explaining what you did and formatting the output nicely. All that’s missing to make this an E grade is an intuitive explanation in English of why the different algorithms are giving different results in a few specific cases.\n\nGrade: M\n\n\n\nLU Factorization\nLet us construct the Gaussian Elimination Algorithm and record all row exchanges with an index array. The overall idea is as follows: For a given matrix A, we first look at the first column of A and find the first nonzero component for this column vector. If this is not the first row of the column vector, we swap it with the first row of the column vector and record this swap with the index array. Then, we find all of the necessary LU factorization factors needed to make the row values of the column vector below 0, recording the multipliers in an identity matrix. Then, we move on to the 2nd column vector of A and do the same thing.\n\ndef PLUSystemSolver(matrix):\n  # Converting to Numpy array\n  matrix = np.array(matrix)\n  matrixSize = np.shape(matrix)\n\n  # Define the row index vector\n  rowIndexArray = np.array(range(0, matrixSize[0]))\n\n  # Defining the mold for the lower triangular matrix L\n  L = np.eye(matrixSize[0])\n\n  # Due to the construction of the algorthm below, we need to temporary save all of the multipliers and its corresponding\n  # coordinate in the L matrix. This is because we have potential row switches, which will mess the row orders up.\n  multiplierSave = []\n  coordinateSave = []\n\n  # Now let the algorthm begin\n  # We need to iterate over all column vectors\n  for columnIndex in range(matrixSize[1] - 1):\n    # For each column vector, iterate over the row components starting from the rowIndex as the columnIndex (indicating the diagonal)\n    for rowIndexSub in range(columnIndex, matrixSize[0]):\n      # Checking for nonzero \"pivots\"\n      if (np.absolute(matrix[rowIndexArray[rowIndexSub], columnIndex]) &gt; 0):\n        # Swapping the first nonzero pivot. DON'T SWITCH THE ACTUAL ROWS\n        rowIndexArray[rowIndexArray[columnIndex]], rowIndexArray[rowIndexSub] = rowIndexArray[rowIndexSub], rowIndexArray[rowIndexArray[columnIndex]]\n        break\n    # Now we do the subtraction algorthm on the column. Defining the value to subtract others.\n    pivotVal = matrix[rowIndexArray[columnIndex], columnIndex]\n    # We first make sure that the column index is not out of bounds (it is not the last column vector)\n    if columnIndex != (matrixSize[1] - 1):\n      for rowIndexSub in range(columnIndex + 1, matrixSize[0]):\n        # If the iterated value is nonzero, we do the subtraction\n        currentVal = matrix[rowIndexArray[rowIndexSub], columnIndex]\n        # Then we do the subtraction and find the multipliers\n        if currentVal != 0:\n          multiplier = (currentVal / pivotVal)\n          # Subtracting\n          matrix[rowIndexArray[rowIndexSub]] = matrix[rowIndexArray[rowIndexSub]] - (matrix[rowIndexArray[columnIndex]] * (multiplier))\n          # Recording the values of the multiplier and coordinates\n          coordinateSave.append([rowIndexSub, columnIndex])\n          multiplierSave.append(multiplier)\n  # Now we input the multipliers to the correct positions\n  for i in range(len(coordinateSave)):\n    L[rowIndexArray[coordinateSave[i][0]], coordinateSave[i][1]] = multiplierSave[i]\n\n  # Construction the Permutation Matrix\n  P = np.zeros(np.shape(matrix))\n  for i in range(len(rowIndexArray)):\n    mold = np.zeros(np.shape(matrix)[1])\n    mold[[rowIndexArray[i]]] = 1\n    P[[i]] = mold\n\n  # Finally, we switch the row position for U\n  matrixCopy = np.copy(matrix)\n  for i in range(matrixSize[0]):\n    matrix[[i]] = matrixCopy[[rowIndexArray[i]]]\n\n  # Returning the output\n  return P, L, matrix\n\n# Testing\nA = np.array([[2, 1, 0], [-4, -1, -1], [2, 3, -3]])\nA2 = np.array([[2, 1, 3],[-4, -2, -1],[2, 3, -3]])\nB1 = PLUSystemSolver(A)\nB2 = PLUSystemSolver(A2)\n\nprint(\"Starting Matrix: \\n\", A)\nprint(\"P Matrix: \\n\", B1[0])\nprint(\"L Matrix: \\n\", B1[1])\nprint(\"U Matrix: \\n\", B1[2])\nprint(\"\\n\")\nprint(\"Starting Matrix: \\n\", A2)\nprint(\"P Matrix: \\n\", B2[0])\nprint(\"L Matrix: \\n\", B2[1])\nprint(\"U Matrix: \\n\", B2[2])\n\nStarting Matrix: \n [[ 2  1  0]\n [-4 -1 -1]\n [ 2  3 -3]]\nP Matrix: \n [[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]]\nL Matrix: \n [[ 1.  0.  0.]\n [-2.  1.  0.]\n [ 1.  2.  1.]]\nU Matrix: \n [[ 2  1  0]\n [ 0  1 -1]\n [ 0  0 -1]]\n\n\nStarting Matrix: \n [[ 2  1  3]\n [-4 -2 -1]\n [ 2  3 -3]]\nP Matrix: \n [[1. 0. 0.]\n [0. 0. 1.]\n [0. 1. 0.]]\nL Matrix: \n [[ 1.  0.  0.]\n [ 1.  1.  0.]\n [-2.  0.  1.]]\nU Matrix: \n [[ 2  1  3]\n [ 0  2 -6]\n [ 0  0  5]]\n\n\nWe have successfully implemented the PLU function. This function takes the matrix in question and returns a tuple of (P matrix, L matrix, U matrix). We can see its success in finding the PLU matrices for HW3 Problem 12 and Problem 13.\n\n\nI’d like to see more on the testing and the outputs here. How do we know that the product of the PLU matrices really is the starting matrix? How do we know that U and L are really upper and lower diagonal? (We can see this by eye, but you could say so, or you could test for it.)\n\nIs it possible that your algorithm fails on a matrix that is different from these two?\nNow, let us implement the module that uses this PLU function to solve general linear systems. We will assume that the diagonals for the upper triangular and lower triangular matrices are nonzero. This systemSolver algorithm will take in the square nonsingular matrix and b in the form of a list with the correct dimension (3 numbers in a list for a 3 by 3 nonsingular square matrix).\n\n# Let us first define an upper triangular system solver.\n# b will be a one dimensional array.\ndef upperSolve(matrix, b):\n  # We are assuming that the matrix input is an upper triangular matrix.\n  # The matrix has to be square, so the size we can just get a single number.\n  matrixSize = np.shape(matrix)[0]\n  # Construct an empty solution vector\n  solution = np.zeros(matrixSize)\n\n  # We iterate in the reverse order on the rows\n  for rowIndex in reversed(range(matrixSize)):\n    # The value pointer is the value of the diagonal.\n    valuePointer = matrix[rowIndex, rowIndex]\n    if rowIndex == matrixSize - 1:\n      solution[matrixSize - 1] = b[matrixSize - 1] / valuePointer\n      continue\n\n    solution[rowIndex] = b[rowIndex] / matrix[rowIndex, rowIndex]\n    for index in range(rowIndex + 1, matrixSize):\n      solution[rowIndex] -= matrix[rowIndex, index] * solution[index] / matrix[rowIndex, rowIndex]\n\n  return solution\n\ndef lowerSolve(matrix, b):\n  # This will be similar to the upperSolve algorithm\n  # We are assuming that the matrix input is an upper triangular matrix.\n  # The matrix has to be square, so the size we can just get a single number.\n  matrixSize = np.shape(matrix)[0]\n  # Construct an empty solution vector\n  solution = np.zeros(matrixSize)\n\n  # We iterate in the reverse order on the rows\n  for rowIndex in range(matrixSize):\n    # The value pointer is the value of the diagonal.\n    valuePointer = matrix[rowIndex, rowIndex]\n    if rowIndex == 0:\n      solution[0] = b[0] / valuePointer\n      continue\n\n    solution[rowIndex] = b[rowIndex] / matrix[rowIndex, rowIndex]\n    for index in (range(0, rowIndex)):\n      solution[rowIndex] -= matrix[rowIndex, index] * solution[index] / matrix[rowIndex, rowIndex]\n\n  return solution\n\n# After defining the triangular solvers, we can carry on solving the entire system.\ndef systemSolver(P, L, U, b):\n  matrixSize = np.shape(U)\n\n  # We have (P^-1)LUx = b, so we should first solve for z, where (P^-1)z = b.\n  z = np.linalg.solve(P, b)\n\n  # Then, we have z = LUx. So let y = Ux. Then Ly = z is a system and we solve for y.\n  y = lowerSolve(L, z)\n\n  # Finally, we have Ux = y.\n  x = upperSolve(U, y)\n\n  return x\n\nThe Linear System solver utilizing PLU factorization has been implemented. To test the accuracy of the algorithm, we can generate random matrices with random solutions.\n\n# From ChatGBT\ndef generate_random_nonsingular_matrix(n):\n    while True:\n        A = np.random.rand(n, n)  # Generate a random n x n matrix\n        if np.linalg.det(A) != 0:  # Check if the determinant is non-zero\n            return A\n\n# Let us time this test. You will see the point soon!\nimport time\n\nstart = time.perf_counter()\nmatrixCreationTime = 0\n\n# Now let us test it\ntrueTimes = 0\nfalseTimes = 0\n\n# Testing for 10000 times\nfor i in range(10000):\n  # Generating random 1 x 10 vector for b and 10 * 10 nonsingular matrices\n  random = np.random.uniform(0, 100000, size=10)\n\n  # I am also timing how long the matrix creation and PLU calculation is\n  start_1 = time.perf_counter()\n  randomSquare = generate_random_nonsingular_matrix(10)\n  end_1 = time.perf_counter()\n\n  P, L, U = PLUSystemSolver(randomSquare)\n\n  matrixCreationTime += end_1 - start_1\n\n  # Test case\n  if systemSolver(P, L, U, random).all() == np.linalg.solve(randomSquare, random).all():\n    trueTimes += 1\n  else:\n    falseTimes = 0\n    print(randomSquare, random)\n\nprint(\"Number of Times the Algorithm is Correct: \", trueTimes)\nprint(\"Number of Times the Algorithm is Incorrect: \", falseTimes)\n\nend = time.perf_counter()\nduration = end - start\nprint(f\"The total test took {duration:.6f} seconds. \\n The matrix creation took a total of {matrixCreationTime:.6f} seconds.\")\nprint(f\"Test time subtracted by matrix creation time is {duration - matrixCreationTime:.6f}\")\n\nNumber of Times the Algorithm is Correct:  10000\nNumber of Times the Algorithm is Incorrect:  0\nThe total test took 2.916151 seconds. \n The matrix creation took a total of 0.066211 seconds.\nTest time subtracted by matrix creation time is 2.849939\n\n\nWe see from the results of the test. After generating 10000 random 1 x 10 vectors for b and 10 * 10 nonsingular matrices, the algorithm always matches the linalg solution. We can conclude with confidence that our algorithm works well.\n\n\nI’m quite surprised that your algorithm exactly matches the linag one; I would have thought you’d need to allow for some rounding error. Neat!\nLet us try something. Instead of creating a random matrix everytime, let us just use the same matrix. Everything else is the same.\n\ntrueTimes = 0\nfalseTimes = 0\nrandomSquare = generate_random_nonsingular_matrix(10)\nP, L, U = PLUSystemSolver(randomSquare)\n\nstart = time.perf_counter()\n# Testing for 10000 times\nfor i in range(10000):\n  # Generating random 1 x 10 vector for b and 10 * 10 nonsingular matrices\n  random = np.random.uniform(0, 100000, size=10)\n\n  # Keeping the time counters to keep the timing same as last block's test.\n  start_1 = time.perf_counter()\n  end_1 = time.perf_counter()\n  matrixCreationTime += end_1 - start_1\n\n  # Test case\n  if systemSolver(P, L, U, random).all() == np.linalg.solve(randomSquare, random).all():\n    trueTimes += 1\n  else:\n    falseTimes = 0\n    print(randomSquare, random)\n\nprint(\"Number of Times the Algorithm is Correct: \", trueTimes)\nprint(\"Number of Times the Algorithm is Incorrect: \", falseTimes)\n\nend = time.perf_counter()\nduration = end - start\nprint(f\"The test took {duration:.6f} seconds.\")\n\nNumber of Times the Algorithm is Correct:  10000\nNumber of Times the Algorithm is Incorrect:  0\nThe test took 0.883356 seconds.\n\n\nThe difference in the times of these tests is noticeable. The second test, which only calculates P, L, U once is noticeably faster than the first test. My hypothesis was that the second test where we did not create random matrices everytime and used the same matrix could be noticeably shorter. This makes sense, as the advantage of PLU is the fact that once we have PLU, we quickly calculate the solutions corresponding to PLU for any b.\n\n\nThis is awesome.\nNow, we can create our final inverse matrix finder module! The inverseFinder takes in nonsingular square matrices and output the inverse :)\n\ndef inverseFinder(matrix):\n  matrixSize = np.shape(matrix)[0]\n  # First, let us generate the standard coordinate basis that will serve as the\n  # b values to the system\n  bSet = []\n  for i in range(matrixSize):\n    e = np.zeros(matrixSize)\n    e[i] = 1\n    bSet.append(e)\n\n  P, L, U = PLUSystemSolver(matrix)\n\n  # Now we can obtain the solutino set\n  solutionSet = []\n  for b in bSet:\n    solutionSet.append(systemSolver(P, L, U, b))\n\n  transpose = np.array(solutionSet)\n  answer = transpose.T\n  return answer\n\n# Simple test\nprint(inverseFinder(A))\nprint(np.linalg.inv(A))\n\n[[-3.  -1.5  0.5]\n [ 7.   3.  -1. ]\n [ 5.   2.  -1. ]]\n[[-3.  -1.5  0.5]\n [ 7.   3.  -1. ]\n [ 5.   2.  -1. ]]\n\n\nYay, the inverse finder is finished. Similar to the systemSolver algorithm, let us test it more extensively by generating random nonsingular square matrices.\n\ntrueTimes = 0\nfalseTimes = 0\n\n# Testing for 10000 times\nfor i in range(10000):\n  # Generating random 1 x 10 vector for b and 10 * 10 nonsingular matrices\n  randomSquare = generate_random_nonsingular_matrix(10)\n  # Test case\n  if np.linalg.inv(randomSquare).all() != None:\n    if inverseFinder(randomSquare).all() == np.linalg.inv(randomSquare).all():\n      trueTimes += 1\n    else:\n      falseTimes = 0\n  else:\n    print(randomSquare)\n\nprint(\"Number of Times the Algorithm is Correct: \", trueTimes)\nprint(\"Number of Times the Algorithm is Incorrect: \", falseTimes)\n\nNumber of Times the Algorithm is Correct:  10000\nNumber of Times the Algorithm is Incorrect:  0\n\n\nThis code will take a little while to run. But after it finishes, we see that the inverseFinder algorithm works very well! There is no time when the algorithm is wrong during this test. It is important to note the limitations of the algorithm systemSolver and inverseFInder as they can only take square nonsingular matrices. In other words, matrices with a nonzero determinant. However, this is expected as the inverse can only be found for matrices with zero determinant. Therefore, the only big limitation will be for systemSolver. We cannot use this algorithm to solve singular matrices or matrix systems with infinite solutions. However, it is also important to note the strength of these algorithms. It uses a relatively clear and straigtforward process to find the system solution and the inverses. Once the PLU matrices are found, the solutions for the linear system corresponding to PLU for any b can be found with small computing power.\n\nThis is excellent. You’ve created the algorithms and tested them well, including cleverly measuring the effects of using the decomposition for repeated solving. Great work.\nGrade: E"
  },
  {
    "objectID": "submitted/projects_1/24320_project1.html",
    "href": "submitted/projects_1/24320_project1.html",
    "title": "Markov Chains",
    "section": "",
    "text": "Code\nimport sympy as sp\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n\n\nCode\n# Markov chain\nM = sp.Matrix([\n    [0.5, 0.2, 0.3],\n    [0.2, 0.6, 0.1],\n    [0.3, 0.2, 0.6]\n])\n\n\n\n\nCode\norig = sp.Matrix([sp.Rational(1,3), sp.Rational(1,3), sp.Rational(1,3)])\n(M**3)*orig\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0.339\\\\0.275333333333333\\\\0.385666666666667\\end{matrix}\\right]\\)\n\n\nAs demonstrated, after 3 years, the Markov chain slightly favors C, with B being the least favored\n\n\nCode\nplan1 = sp.Matrix([\n    [0.5, 0.2 + .6*0.2, 0.3],\n    [0.2, 0.6*.8, 0.1],\n    [0.3, 0.2, 0.6]\n])\nplan2 = sp.Matrix([\n    [0.5, 0.2, 0.3+0.6*.2],\n    [0.2, 0.6, 0.1],\n    [0.3, 0.2, 0.6*0.8]\n])\nplan1\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0.5 & 0.32 & 0.3\\\\0.2 & 0.48 & 0.1\\\\0.3 & 0.2 & 0.6\\end{matrix}\\right]\\)\n\n\n\n\nCode\nequal = sp.Matrix([sp.Rational(1,3), sp.Rational(1,3), sp.Rational(1,3)])\nweight_a = sp.Matrix([sp.Rational(5,10), sp.Rational(25,100), sp.Rational(25,100)])\nweight_b = sp.Matrix([sp.Rational(25,100), sp.Rational(5,10), sp.Rational(25,100)])\nweight_c = sp.Matrix([sp.Rational(25,100), sp.Rational(25,100), sp.Rational(5,10)])\n\n\n\n\nWould be really useful to have comments here. What are equal, weight_a, weight_b, and weight_c? I figured it it out but it took me a minute…\n\n\nCode\n# Varying states for plan 1\nequal1 = [equal, (plan1 ** 1)*equal, (plan1 ** 5)*equal, (plan1 ** 10)*equal]\nweight_a1 = [weight_a, (plan1 ** 1)*weight_a, (plan1 ** 5)*weight_a, (plan1 ** 10)*weight_a]\nweight_b1 = [weight_b, (plan1 ** 1)*weight_b, (plan1 ** 5)*weight_b, (plan1 ** 10)*weight_b]\nweight_c1 = [weight_c, (plan1 ** 1)*weight_c, (plan1 ** 5)*weight_c, (plan1 ** 10)*weight_c]\nplan1_states = [equal1, weight_a1, weight_b1, weight_c1]\n# Varying states for plan 2\nequal2 = [equal, (plan2 ** 1)*equal, (plan2 ** 5)*equal, (plan2 ** 10)*equal]\nweight_a2 = [weight_a, (plan2 ** 1)*weight_a, (plan2 ** 5)*weight_a, (plan2 ** 10)*weight_a]\nweight_b2 = [weight_b, (plan2 ** 1)*weight_b, (plan2 ** 5)*weight_b, (plan2 ** 10)*weight_b]\nweight_c2 = [weight_c, (plan2 ** 1)*weight_c, (plan2 ** 5)*weight_c, (plan2 ** 10)*weight_c]\nplan2_states = [equal2, weight_a2, weight_b2, weight_c2]\n\n\n::: column-margin I’d like to see you rewrite this using a function that takes in the plan matrix and the initial state, and returns the state after 1, 5, and 10 years. This will make it easier to read your code to understand what it’s doing (and it’s easier for you to write in general!).\n\n\nCode\ntime = [0,1,5,10]\ncompanies = ['A', 'B', 'C']\n\n\n\n\nCode\n# Find company max distribution\ndef plan_max(company, plan):\n  result = plan[0][3][company]\n  dis = 0\n  if result &lt; weight_a1[3][company]:\n    result = plan[1][3][company]\n    dis = 1\n  if result &lt; weight_b1[3][company]:\n    result = plan[2][3][company]\n    dis = 2\n  if result &lt; weight_c1[3][company]:\n    result = plan[3][3][company]\n    dis = 3\n  return result, dis\n\n\n\n\nI can’t understand what this is doing, because I don’t know what the ‘plan’ variable will be when I call this function. I’m trying to look below to see but it’s hard to follow.\n\n\nCode\n# A's performance under different ways, original plan\nbase = sp.Matrix([((M**10)*equal)[0], ((M**10)*weight_a)[0], ((M**10)*weight_b)[0], ((M**10)*weight_c)[0]])\nbase_equal = [equal, (M ** 1)*equal, (M ** 5)*equal, (M ** 10)*equal]\nbase_a = [weight_a, (M ** 1)*weight_a, (M ** 5)*weight_a, (M ** 10)*weight_a]\nbase_b = [weight_b, (M ** 1)*weight_b, (M ** 5)*weight_b, (M ** 10)*weight_b]\nbase_c = [weight_c, (M ** 1)*weight_c, (M ** 5)*weight_c, (M ** 10)*weight_c]\n\nplan1_dis = sp.Matrix([((plan1**10)*equal)[0], ((plan1**10)*weight_a)[0], ((plan1**10)*weight_b)[0], ((plan1**10)*weight_c)[0]])\nplan2_dis = sp.Matrix([((plan2**10)*equal)[0], ((plan2**10)*weight_a)[0], ((plan2**10)*weight_b)[0], ((plan2**10)*weight_c)[0]])\n\n\n\n\nCode\nsum(plan1_dis - base)\n\n\n\\(\\displaystyle 0.156465181152655\\)\n\n\n\n\nCode\nsum(plan2_dis - base)\n\n\n\\(\\displaystyle 0.194330292122828\\)\n\n\n\n\nCode\nplan_max(0, plan2_states)\n\n\n(0.390033780497374, 0)\n\n\n\n\nCode\nplan2_dis\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0.390033780497374\\\\0.390040976888677\\\\0.390005782377759\\\\0.390054582225684\\end{matrix}\\right]\\)\n\n\nAs demonstrated, we can see that overall, Plan 2 performs the best in terms of Company A’s long term market-share. This is based 4 distributions: equal distribution, and 50%, 25%, 25% market shares for each company. Overall, Plan 2 performs the best when the distribution is equal. We will now overlay Plan 2’s best performance distribution compared to its worst performance distribution.\n\n\nCode\n# Plan 1: Equal distribution\nmarket = equal2\nfor i in range(3):\n  plt.plot(time, [market[0][i], market[1][i], market[2][i], market[3][i]], label=f\"Company {companies[i]}: Equal\")\nmarket = weight_b2\nfor i in range(3):\n  plt.plot(time, [market[0][i], market[1][i], market[2][i], market[3][i]], label=f\"Company {companies[i]}: Weight C\", linestyle=\":\")\nplt.ylabel('Customer Market Share: Equal Distribution')\nplt.xlabel('Years')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThis is a really nice plot!\nAs demonstrated, Company A’s performance evens out through all distributions. We will now overlay a few distributions comparing the original plan to Plan 2.\n\n\nCode\nmarket = equal2\nfor i in range(3):\n  plt.plot(time, [market[0][i], market[1][i], market[2][i], market[3][i]], label=f\"Company {companies[i]}: Equal 2\")\nmarket = base_equal\nfor i in range(3):\n  plt.plot(time, [market[0][i], market[1][i], market[2][i], market[3][i]], label=f\"Company {companies[i]}: Base\", linestyle=\":\")\nplt.ylabel('Customer Market Share: Equal Distribution')\nplt.xlabel('Years')\nplt.legend(loc=3)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nmarket = weight_a2\nfor i in range(3):\n  plt.plot(time, [market[0][i], market[1][i], market[2][i], market[3][i]], label=f\"Company {companies[i]}: Weight A 2\")\nmarket = base_a\nfor i in range(3):\n  plt.plot(time, [market[0][i], market[1][i], market[2][i], market[3][i]], label=f\"Company {companies[i]}: Base\", linestyle=\":\")\nplt.ylabel('Customer Market Share: Equal Distribution')\nplt.xlabel('Years')\nplt.legend(loc=3)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nmarket = weight_b2\nfor i in range(3):\n  plt.plot(time, [market[0][i], market[1][i], market[2][i], market[3][i]], label=f\"Company {companies[i]}: Weight B 2\")\nmarket = base_b\nfor i in range(3):\n  plt.plot(time, [market[0][i], market[1][i], market[2][i], market[3][i]], label=f\"Company {companies[i]}: Base\", linestyle=\":\")\nplt.ylabel('Customer Market Share: Equal Distribution')\nplt.xlabel('Years')\nplt.legend(loc=3)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nmarket = weight_c2\nfor i in range(3):\n  plt.plot(time, [market[0][i], market[1][i], market[2][i], market[3][i]], label=f\"Company {companies[i]}: Weight C 2\")\nmarket = base_c\nfor i in range(3):\n  plt.plot(time, [market[0][i], market[1][i], market[2][i], market[3][i]], label=f\"Company {companies[i]}: Base\", linestyle=\":\")\nplt.ylabel('Customer Market Share: Equal Distribution')\nplt.xlabel('Years')\nplt.legend(loc=3)\nplt.show()\n\n\n\n\n\n\n\n\n\nAs demonstrated, A performs consistently better under Plan 2 than the original plan. Thus, we can see that it would be ideal for the company to undergo marketing strategies to implement plan 2. However, we must consider the market distributions: all distributions provided assume no company has more than 50% of the market share. However, as displayed below, even if we start with 0 market share, our new marketing strategy will propel us to the largest market holder within a 10 year period.\n\n\nCode\n(plan2 ** 10) * sp.Matrix([0.0, 0.99, .01])\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0.389923740012833\\\\0.278200270113832\\\\0.331875989873334\\end{matrix}\\right]\\)\n\n\n\nI suspect your conclusions may be correct here – but I can’t tell for sure because I don’t know what the code is doing.\nI get it; this is how my code often looks as well. It’s been a real challenge over the years to train myself to write code that is clear and easy to understand. It’s a skill that takes time to develop, but it’s worth it.\nWhen you want to be able to come back and understand what you’ve done, and especially if you want others to understand what you’ve done, your code will need a bunch of changes.\nFirst, you should look for areas of repeated code and make these into functions.\nThen you should think about how to logically order the functions, and how to name everything so that it’s very clear how your code corresponds to what you are trying to do mathematically. It may be helpful to first write (in Markdown cells) what you are computing, with formulas, and then give the code.\nFinally, you should add comments to your code to explain what each part is doing.\nIf you use an editor that supports CoPilot or a similar AI code editing tool, this can be very helpful: you can take a bit of code and ask the AI to “make this more efficient”. You’ll want to make sure then that you go over it carefully to make sure it’s still correct and that you understand what it’s doing; the AI can do a good job too with “explain this”. For my code, I’ve been using VSCode with the Jupyter extension and the CoPilot plugin, all free. (Alternatively, you could use Quarto, which is a document format that lets you write in Markdown with Python code in blocks. I find it easier to think when I don’t have to put every bit of prose inside its own separate block, but YMMV. For writing these comments, I converted your homework file to Quarto; if you click on “Code” and then “View Source” at the top of this document, you can see what it looks like.)\nOnce you have gotten the code into shape, it would be great to look again at your conclusions and see if you can understand what about the matrices tells you that e.g. Plan 2 really is best for your company.\nGrade: R\n\n\nSports Ranking\n\n\nCode\n# Do imports\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport scipy\nimport numpy as np\n\n\n\n\nCode\n# Create a directed graph\nG = nx.DiGraph()\n\n# Add nodes\nG.add_nodes_from([1, 2, 3, 4, 5, 6, 7])\n\n# Add vertices\nedges = [(1,2),(7,3),(2,4),(4,5),(3,2),(5,1),(6,1),(3,1),(7,2),(2,6), (3, 4), (7,4),(5,7),(6,4),(3,5),(5,6),(7,1),(5,2),(7,6),(1,4),(6,3)]\nG.add_edges_from(edges)\n\n# Draw the graph\nnx.draw_circular(G, with_labels=True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nadj_matrix = nx.adjacency_matrix(G).toarray()\nadj_matrix\n\n\narray([[0, 1, 0, 1, 0, 0, 0],\n       [0, 0, 0, 1, 0, 1, 0],\n       [1, 1, 0, 1, 1, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0],\n       [1, 1, 0, 0, 0, 1, 1],\n       [1, 0, 1, 1, 0, 0, 0],\n       [1, 1, 1, 1, 0, 1, 0]])\n\n\n\n\nCode\n# Power Matrix\npower_matrix = adj_matrix + adj_matrix ** 2\npower_matrix\n\n\narray([[0, 2, 0, 2, 0, 0, 0],\n       [0, 0, 0, 2, 0, 2, 0],\n       [2, 2, 0, 2, 2, 0, 0],\n       [0, 0, 0, 0, 2, 0, 0],\n       [2, 2, 0, 0, 0, 2, 2],\n       [2, 0, 2, 2, 0, 0, 0],\n       [2, 2, 2, 2, 0, 2, 0]])\n\n\n\n\nCode\n# Calculating team ranking with the power matrix\npower_sum = [sum(power_matrix[i]) for i in range(7)]\nrrank = np.argsort(power_sum)\nrank = np.flip(rrank + 1)\nrank\n\n\narray([7, 5, 3, 6, 2, 1, 4])\n\n\n\n\nCode\n# Calculating team ranking with reverse pagerank: calculate diagonal matrix D\nadjT = adj_matrix.T\nD = np.zeros((7, 7))\nfor i in range(7):\n  D[i][i] = 1 / sum(adjT[i])\n\n\n\n\nCode\n# Determine P = ATD -&gt; (AT)TD = AD with alpha a = 0.85, teleporation vector 1/7 e_7)\nP = sp.Matrix(sp.Matrix(adj_matrix) * sp.Matrix(D))\na = 0.85\nv = sp.Matrix(np.ones(7) * 1/7)\n\n\n\n\nCode\n# Solve for x\nM = (sp.eye(7) - a * P)\nb = (1 - a) * v\nsol = M.solve(b)\nx = np.array([sol[i] for i in range(7)], dtype='float')\nx\n\n\narray([0.05966732, 0.07966581, 0.17626964, 0.12535158, 0.24452472,\n       0.13033224, 0.18418869])\n\n\n\n\nCode\n# Find ranking\npr_rank = np.flip(np.argsort(x))+1\npr_rank\n\n\narray([5, 7, 3, 6, 4, 2, 1])\n\n\n\n\nCode\n# Create a weighted directed graph\nG = nx.DiGraph()\n\n# Add nodes\nG.add_nodes_from([1, 2, 3, 4, 5, 6, 7])\n\n# Add vertices\nedges = [(1,2),(7,3),(2,4),(4,5),(3,2),(5,1),(6,1),(3,1),(7,2),(2,6), (3, 4), (7,4),(5,7),(6,4),(3,5),(5,6),(7,1),(5,2),(7,6),(1,4),(6,3)]\nlength = len(edges)\nweight = [4,8,7,3,7,7,23,15,6,18,13,14,7,13,7,18,45,10,19,14,13]\nfor i in range(length):\n  G.add_edge(edges[i][0], edges[i][1], weight=weight[i])\n\npos = nx.spring_layout(G, seed=7)  # positions for all nodes - seed for reproducibility\n\n# nodes\nnx.draw_networkx_nodes(G, pos, node_size=700)\n\n# edges\nnx.draw_networkx_edges(G, pos, edgelist=edges, width=1)\n\n# node labels\nnx.draw_networkx_labels(G, pos, font_size=20, font_family=\"sans-serif\")\n# edge weight labels\nedge_labels = nx.get_edge_attributes(G, \"weight\")\nnx.draw_networkx_edge_labels(G, pos, edge_labels)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Replace adjacency booleans with weights\nweight_adj = adj_matrix\nfor i in range(length):\n  weight_adj[edges[i][0]-1, edges[i][1]-1] = weight[i]\nweight_adj\n\n\narray([[ 0,  4,  0, 14,  0,  0,  0],\n       [ 0,  0,  0,  7,  0, 18,  0],\n       [15,  7,  0, 13,  7,  0,  0],\n       [ 0,  0,  0,  0,  3,  0,  0],\n       [ 7, 10,  0,  0,  0, 18,  7],\n       [23,  0, 13, 13,  0,  0,  0],\n       [45,  6,  8, 14,  0, 19,  0]])\n\n\n\n\nCode\nweight_power = weight_adj + weight_adj**2\nweight_power_sum = [sum(weight_power[i]) for i in range(7)]\nrrank_weight = np.argsort(weight_power_sum)\nrank_weight = np.flip(rrank_weight + 1)\nrank_weight\n\n\narray([7, 6, 5, 3, 2, 1, 4])\n\n\n\n\nCode\n# Calculating team ranking with reverse pagerank: calculate diagonal matrix D\nadjT_weight = weight_adj.T\nD = np.zeros((7, 7))\nfor i in range(7):\n  D[i][i] = 1 / sum(adjT_weight[i])\n# Determine P = ATD -&gt; (AT)TD = AD with alpha a = 0.85, teleporation vector 1/7 e_7)\nP = sp.Matrix(sp.Matrix(weight_adj) * sp.Matrix(D))\na = 0.85\nv = sp.Matrix(np.ones(7) * 1/7)\n# Solve for x\nM = (sp.eye(7) - a * P)\nb = (1 - a) * v\nsol = M.solve(b)\nx = np.array([sol[i] for i in range(7)], dtype='float')\n# Find ranking\npr_rank = np.flip(np.argsort(x))+1\npr_rank\n\n\narray([5, 3, 7, 6, 4, 2, 1])\n\n\n\nHere the code is a bit more straightforward, so I’m more able to follow along. You also have a bit more commenting in the code, which is helpful, but it could still use more.\nHowever, we don’t have any Markdown blocks at all to describe your thinking and how it relates mathematically to the text, etc.\nYou should start with an introduction to the problem, what you are trying to learn. Then you should describe for each step what you are trying to accomplish and how it differs from previous steps. Then, once you have a result, you’ll want to explain what it means and how it relates to the original problem.\nGrade: R\n\n\n\nIso Rank\n\n\nCode\n# Construct adjacency matrices for A,B,C,D,E and 1,2,3,4,5\n# G1 = A,B,C,D,E\n# G2 = 1,2,3,4,5\nnp_G1 = np.matrix([\n    [0,1,1,0,1],\n    [1,0,0,1,0],\n    [1,0,0,0,0],\n    [0,1,0,0,0],\n    [1,0,0,0,0]\n], dtype='float')\nnp_G2 = np.matrix([\n    [0,0,1,0,0],\n    [0,0,1,0,0],\n    [1,1,0,1,0],\n    [0,0,1,0,1],\n    [0,0,0,1,0]\n], dtype='float')\nG1T = np_G1.transpose()\nG2T = np_G2.transpose()\nfor i in range(5):\n  G1T[i] /= np.sum(G1T[i])\n  G2T[i] /= np.sum(G2T[i])\nG1 = sp.Matrix(np.transpose(G1T))\nG2 = sp.Matrix(np.transpose(G2T))\nG2\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0 & 0 & 0.333333333333333 & 0 & 0\\\\0 & 0 & 0.333333333333333 & 0 & 0\\\\1.0 & 1.0 & 0 & 0.5 & 0\\\\0 & 0 & 0.333333333333333 & 0 & 1.0\\\\0 & 0 & 0 & 0.5 & 0\\end{matrix}\\right]\\)\n\n\n\n\nCode\n# Construct G2 x G1 (Outer x Inner) surfing matrix\nM = sp.zeros(25)\nfor h in range(5):\n  for k in range(5):\n    for i in range(5):\n      for j in range(5):\n        M[i+5*(h), j+5*(k)] = G2[h, k] * G1[i, j]\nM\n\n\n\\(\\displaystyle \\left[\\begin{array}{ccccccccccccccccccccccccc}0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.166666666666667 & 0.333333333333333 & 0 & 0.333333333333333 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.111111111111111 & 0 & 0 & 0.333333333333333 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.111111111111111 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.166666666666667 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.111111111111111 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.166666666666667 & 0.333333333333333 & 0 & 0.333333333333333 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.111111111111111 & 0 & 0 & 0.333333333333333 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.111111111111111 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.166666666666667 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.111111111111111 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & 0.5 & 1.0 & 0 & 1.0 & 0 & 0.5 & 1.0 & 0 & 1.0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.25 & 0.5 & 0 & 0.5 & 0 & 0 & 0 & 0 & 0\\\\0.333333333333333 & 0 & 0 & 1.0 & 0 & 0.333333333333333 & 0 & 0 & 1.0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.166666666666667 & 0 & 0 & 0.5 & 0 & 0 & 0 & 0 & 0 & 0\\\\0.333333333333333 & 0 & 0 & 0 & 0 & 0.333333333333333 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.166666666666667 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & 0.5 & 0 & 0 & 0 & 0 & 0.5 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.25 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\0.333333333333333 & 0 & 0 & 0 & 0 & 0.333333333333333 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.166666666666667 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.166666666666667 & 0.333333333333333 & 0 & 0.333333333333333 & 0 & 0 & 0 & 0 & 0 & 0 & 0.5 & 1.0 & 0 & 1.0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.111111111111111 & 0 & 0 & 0.333333333333333 & 0 & 0 & 0 & 0 & 0 & 0 & 0.333333333333333 & 0 & 0 & 1.0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.111111111111111 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.333333333333333 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.166666666666667 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.5 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.111111111111111 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.333333333333333 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.25 & 0.5 & 0 & 0.5 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.166666666666667 & 0 & 0 & 0.5 & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.166666666666667 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.25 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.166666666666667 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\end{array}\\right]\\)\n\n\n\n\nCode\n# Calculate x given alpha = 0.85, v = 1/25 * e25\na = 0.85\nv = 1/25 * sp.Matrix(np.ones(25))\nmat = (sp.eye(25) - a * M)\nb = (1-a)*v\nx = mat.solve(b)\nx\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0.0422740152656166\\\\0.032389985878358\\\\0.0191626479069361\\\\0.0183187399484339\\\\0.0191626479069361\\\\0.0422740152656166\\\\0.032389985878358\\\\0.0191626479069361\\\\0.0183187399484339\\\\0.0191626479069361\\\\0.139369213132264\\\\0.0869558114007097\\\\0.0422740152656167\\\\0.0466847222520773\\\\0.0422740152656166\\\\0.0869558114007097\\\\0.0618975729669319\\\\0.0323899858783579\\\\0.0318586781711893\\\\0.0323899858783579\\\\0.0466847222520773\\\\0.0318586781711893\\\\0.0183187399484339\\\\0.019153234255473\\\\0.0183187399484339\\end{matrix}\\right]\\)\n\n\n\n\nCode\nimport math\niso = sp.zeros(5,5)\nfor i in range(5):\n  for j in range(5):\n    iso[i, j] = x[i + 5 * j]\ntemp = iso\niso\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0.0422740152656166 & 0.0422740152656166 & 0.139369213132264 & 0.0869558114007097 & 0.0466847222520773\\\\0.032389985878358 & 0.032389985878358 & 0.0869558114007097 & 0.0618975729669319 & 0.0318586781711893\\\\0.0191626479069361 & 0.0191626479069361 & 0.0422740152656167 & 0.0323899858783579 & 0.0183187399484339\\\\0.0183187399484339 & 0.0183187399484339 & 0.0466847222520773 & 0.0318586781711893 & 0.019153234255473\\\\0.0191626479069361 & 0.0191626479069361 & 0.0422740152656166 & 0.0323899858783579 & 0.0183187399484339\\end{matrix}\\right]\\)\n\n\nRow Labels: A,B,C,D,E. Column Labels: 1,2,3,4,5. Thus, the best match is immediately A:3\n\n\nCode\ntemp.row_del(0)\ntemp.col_del(2)\ntemp\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0.032389985878358 & 0.032389985878358 & 0.0618975729669319 & 0.0318586781711893\\\\0.0191626479069361 & 0.0191626479069361 & 0.0323899858783579 & 0.0183187399484339\\\\0.0183187399484339 & 0.0183187399484339 & 0.0318586781711893 & 0.019153234255473\\\\0.0191626479069361 & 0.0191626479069361 & 0.0323899858783579 & 0.0183187399484339\\end{matrix}\\right]\\)\n\n\nNext match is clearly B:4\n\n\nCode\ntemp.row_del(0)\ntemp.col_del(2)\ntemp\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0.0191626479069361 & 0.0191626479069361 & 0.0183187399484339\\\\0.0183187399484339 & 0.0183187399484339 & 0.019153234255473\\\\0.0191626479069361 & 0.0191626479069361 & 0.0183187399484339\\end{matrix}\\right]\\)\n\n\nNext choice C:1, C:2, E:1, E:2. We choose E:1\n\n\nCode\ntemp.row_del(2)\ntemp.col_del(0)\ntemp\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0.0191626479069361 & 0.0183187399484339\\\\0.0183187399484339 & 0.019153234255473\\end{matrix}\\right]\\)\n\n\nWe round-off with C:2, D:5. Thus, our Final matching is A:3, B:4, C:2, D:5, E:1. This is 100% accurate with our original graph, thus we have found a bijective mapping.\nWe now consider Figure 2.12 with e = {B,C} removed\n\n\nCode\n# G1 = P, G2 = Q\nnp_P = np.matrix([\n    [0,1,1],\n    [1,0,1],\n    [1,1,0]\n], dtype='float')\nnp_Q = np.matrix([\n    [0,1,0,1,0],\n    [1,0,0,1,0],\n    [0,0,0,1,0],\n    [1,1,1,0,1],\n    [0,0,0,1,0]\n], dtype='float')\nPT = np_P.transpose()\nQT = np_Q.transpose()\nfor i in range(5):\n  QT[i] /= np.sum(QT[i])\nfor i in range(3):\n  PT[i] /= np.sum(PT[i])\nP = sp.Matrix(np.transpose(PT))\nQ = sp.Matrix(np.transpose(QT))\nP\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0 & 0.5 & 0.5\\\\0.5 & 0 & 0.5\\\\0.5 & 0.5 & 0\\end{matrix}\\right]\\)\n\n\n\n\nCode\n# Contruct N = Q x P\nN = sp.zeros(15)\nfor h in range(5):\n  for k in range(5):\n    for i in range(3):\n      for j in range(3):\n        N[i+3*(h), j+3*(k)] = Q[h, k] * P[i, j]\nN\n\n\n\\(\\displaystyle \\left[\\begin{array}{ccccccccccccccc}0 & 0 & 0 & 0 & 0.25 & 0.25 & 0 & 0 & 0 & 0 & 0.125 & 0.125 & 0 & 0 & 0\\\\0 & 0 & 0 & 0.25 & 0 & 0.25 & 0 & 0 & 0 & 0.125 & 0 & 0.125 & 0 & 0 & 0\\\\0 & 0 & 0 & 0.25 & 0.25 & 0 & 0 & 0 & 0 & 0.125 & 0.125 & 0 & 0 & 0 & 0\\\\0 & 0.25 & 0.25 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.125 & 0.125 & 0 & 0 & 0\\\\0.25 & 0 & 0.25 & 0 & 0 & 0 & 0 & 0 & 0 & 0.125 & 0 & 0.125 & 0 & 0 & 0\\\\0.25 & 0.25 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.125 & 0.125 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.125 & 0.125 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.125 & 0 & 0.125 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.125 & 0.125 & 0 & 0 & 0 & 0\\\\0 & 0.25 & 0.25 & 0 & 0.25 & 0.25 & 0 & 0.5 & 0.5 & 0 & 0 & 0 & 0 & 0.5 & 0.5\\\\0.25 & 0 & 0.25 & 0.25 & 0 & 0.25 & 0.5 & 0 & 0.5 & 0 & 0 & 0 & 0.5 & 0 & 0.5\\\\0.25 & 0.25 & 0 & 0.25 & 0.25 & 0 & 0.5 & 0.5 & 0 & 0 & 0 & 0 & 0.5 & 0.5 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.125 & 0.125 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.125 & 0 & 0.125 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.125 & 0.125 & 0 & 0 & 0 & 0\\end{array}\\right]\\)\n\n\n\n\nCode\n# Solve for X\na = 0.85\nv = 1/15 * sp.Matrix(np.ones(15))\nmat = (sp.eye(15) - a * N)\nb = (1-a)*v\nx = mat.solve(b)\nx\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0.0649589820860539\\\\0.0649589820860539\\\\0.0649589820860539\\\\0.0649589820860539\\\\0.0649589820860539\\\\0.0649589820860539\\\\0.037351414699481\\\\0.037351414699481\\\\0.037351414699481\\\\0.128712539762264\\\\0.128712539762264\\\\0.128712539762264\\\\0.037351414699481\\\\0.037351414699481\\\\0.037351414699481\\end{matrix}\\right]\\)\n\n\n\n\nCode\niso = sp.zeros(3,5)\nfor i in range(3):\n  for j in range(5):\n    iso[i, j] = x[i + 3 * j]\ntemp = iso\niso\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0.0649589820860539 & 0.0649589820860539 & 0.037351414699481 & 0.128712539762264 & 0.037351414699481\\\\0.0649589820860539 & 0.0649589820860539 & 0.037351414699481 & 0.128712539762264 & 0.037351414699481\\\\0.0649589820860539 & 0.0649589820860539 & 0.037351414699481 & 0.128712539762264 & 0.037351414699481\\end{matrix}\\right]\\)\n\n\nColumn (Top) Labels: A, B, C, D, E. Row (Side) Labels: 1, 2, 3. Best matchings: [(D:1 or D:2 or D:3), ((A:2 or A:3 or B:2 or B:3) or (A:1 or A:3 or B:1 or B:3) or (A:1 or A:2 or B:1 or B:2)), choose the remaining A or B)]\n\n\nCode\n# Solve for A, B\nA_adj = np.matrix([\n    [0,1,0],\n    [0,0,1],\n    [1,0,0]\n], dtype='float')\n# The fourth row of Q should be the zero vector, we elect to replace with the one\n# vector to find our correction vector\nB_adj = np.matrix([\n    [0,0,1,0,0],\n    [1,0,0,0,0],\n    [0,1,0,1,0],\n    [1,1,1,1,1],\n    [0,1,0,0,0]\n], dtype='float')\nfor i in range(5):\n  B_adj[i] /= np.sum(B_adj[i])\nfor i in range(3):\n  A_adj[i] /= np.sum(A_adj[i])\nA = A_adj.transpose()\nB = B_adj.transpose()\n\n\n\n\nCode\n# Construct C = B x A\n# Contruct N = Q x P\nC = sp.zeros(15)\nfor h in range(5):\n  for k in range(5):\n    for i in range(3):\n      for j in range(3):\n        C[i+3*(h), j+3*(k)] = B[h, k] * A[i, j]\nC\n\n\n\\(\\displaystyle \\left[\\begin{array}{ccccccccccccccc}0 & 0 & 0 & 0 & 0 & 1.0 & 0 & 0 & 0 & 0 & 0 & 0.2 & 0 & 0 & 0\\\\0 & 0 & 0 & 1.0 & 0 & 0 & 0 & 0 & 0 & 0.2 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 1.0 & 0 & 0 & 0 & 0 & 0 & 0.2 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.5 & 0 & 0 & 0.2 & 0 & 0 & 1.0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0.5 & 0 & 0 & 0.2 & 0 & 0 & 1.0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.5 & 0 & 0 & 0.2 & 0 & 0 & 1.0 & 0\\\\0 & 0 & 1.0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.2 & 0 & 0 & 0\\\\1.0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.2 & 0 & 0 & 0 & 0 & 0\\\\0 & 1.0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.2 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.5 & 0 & 0 & 0.2 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0.5 & 0 & 0 & 0.2 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.5 & 0 & 0 & 0.2 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.2 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.2 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.2 & 0 & 0 & 0 & 0\\end{array}\\right]\\)\n\n\n\n\nCode\n# Solve for X\na = 0.85\nv = 1/15 * sp.Matrix(np.ones(15))\nmat = (sp.eye(15) - a * C)\nb = (1-a)*v\nx = mat.solve(b)\nx\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0.0850597851234713\\\\0.0850597851234713\\\\0.0850597851234713\\\\0.076434208276694\\\\0.076434208276694\\\\0.076434208276694\\\\0.0923915254432319\\\\0.0923915254432319\\\\0.0923915254432319\\\\0.0593571064016549\\\\0.0593571064016549\\\\0.0593571064016549\\\\0.0200907080882813\\\\0.0200907080882813\\\\0.0200907080882813\\end{matrix}\\right]\\)\n\n\n\n\nCode\niso = sp.zeros(3,5)\nfor i in range(3):\n  for j in range(5):\n    iso[i, j] = x[i + 3 * j]\ntemp = iso\niso\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0.0850597851234713 & 0.076434208276694 & 0.0923915254432319 & 0.0593571064016549 & 0.0200907080882813\\\\0.0850597851234713 & 0.076434208276694 & 0.0923915254432319 & 0.0593571064016549 & 0.0200907080882813\\\\0.0850597851234713 & 0.076434208276694 & 0.0923915254432319 & 0.0593571064016549 & 0.0200907080882813\\end{matrix}\\right]\\)\n\n\nColumn Labels: A,B,C,D,E Row Labels: 1,2,3 Possible matchings: [(C:1 or C:2 or C:3), ((A:2 or A:3), (A:1 or A:3), (A:1 or A:2), choose B:k where k is the remaining row. The best matching is [(C:1 or C:2 or C:3), (A:3 or A:1 or A:2), (B:2 or B:3 or B:1)] choosing each one respectively (i.e. choosing C:1 (index 0) chooses A:3 and B:2)\n\nSame as above!\nGrade: R"
  },
  {
    "objectID": "submitted/projects_1/xw.html",
    "href": "submitted/projects_1/xw.html",
    "title": "Projects 1",
    "section": "",
    "text": "Hi Xiao,\nI think you’ve done a nice job with the coding aspect of these reports. Everything you do is straightforward enough that I can clearly follow it. (For students who are less skilled programmers, they sometimes make very convoluted code which is really hard for me to understand.)\nHowever, I’d like to see much more explanation of what you’re doing and why, from a mathematical perspective. For instance, in the diffusion problem, you could start with a section describing the problem setup and what you are trying to find. Then each time you put a bit of code, you should explain what that code is doing, perhaps giving (in Markdown) the formula that you are implementing.\nIt takes a little while to get used to writing in Markdown when you are used to just commenting within your code; you can be more vebose and write in full sentences, use Latex expressions, etc.\nThen, when you make plots, they should be clearly labeled and you should explain what you are seeing in the plot.\nFinally, I’d like to see you interacting a bit with the ideas that explain why you get the results you get. For instance, in the Markov Chain problem, you could explain how you can tell from the intial matrices which companies are likely to be more profitable in the long run.\n(Regarding the Markov Chain problem – you state that the system will always converge to the same stationary state, which I think is true but in this report I’d like to see you try it by using a few different initial states. Or do a mathematical proof if you have a good idea of how to do that!)\nI’m going to give you a grade of R for each of these projects; I hope you will be able to revise and resubmit them, because the underlying work here is quite good.\n\n\nDiffusion\n\n\nCode\nimport sympy as sym\nimport numpy as np\nimport pandas as pd\nsb = sym.Symbol\n\n\n\n\n\n\n\nCode\nN=7\nM = sym.zeros(N)\nfor i in range(N):\n  if (i&gt;0):\n      M[i-1,i]=1\n  M[i,i]=-2\n  if (i&lt;N-1):\n      M[i+1,i]=1\n  # M[0, i] = 0\n  # M[6, i] = 0\n#   M[i, 0] = 0\n#   M[i, 6] = 0\n# M[5, 6] = 0\nM\n\n\n\\(\\displaystyle \\left[\\begin{matrix}-2 & 1 & 0 & 0 & 0 & 0 & 0\\\\1 & -2 & 1 & 0 & 0 & 0 & 0\\\\0 & 1 & -2 & 1 & 0 & 0 & 0\\\\0 & 0 & 1 & -2 & 1 & 0 & 0\\\\0 & 0 & 0 & 1 & -2 & 1 & 0\\\\0 & 0 & 0 & 0 & 1 & -2 & 1\\\\0 & 0 & 0 & 0 & 0 & 1 & -2\\end{matrix}\\right]\\)\n\n\n\n\nCode\nt270 = sym.Matrix([0.0, 0.051, 1.21, 3.48, 1.21, 0.051, 0.0])\n# t270 = sym.Matrix([0.051, 1.21, 3.48, 1.21, 0.051])\n\n\n\n\nCode\nt240 = sym.Matrix([0.0, 0.032, 1.23, 3.69, 1.23, 0.032, 0.0])\n\n\nGoing forward from 240 to 270\n\n\nCode\nDh2 = 0.00004 * 36\n\ncurrent = t240\nfor step in range(240, 271):\n    next_step = current + M*current * Dh2\n    if step % 10 == 0:\n        print(step, current)\n        print('\\n')\n    current = next_step\n\n\n240 Matrix([[0], [0.0320000000000000], [1.23000000000000], [3.69000000000000], [1.23000000000000], [0.0320000000000000], [0]])\n\n\n250 Matrix([[0.000562471243392380], [0.0486924553785851], [1.24759546552771], [3.62029245984376], [1.24759546552771], [0.0486924553785851], [0.000562471243392380]])\n\n\n260 Matrix([[0.00134480689310895], [0.0651618382102242], [1.26395165777253], [3.55305081122375], [1.26395165777253], [0.0651618382102242], [0.00134480689310895]])\n\n\n270 Matrix([[0.00233753410690556], [0.0814003695987503], [1.27913496114619], [3.48817057984740], [1.27913496114619], [0.0814003695987503], [0.00233753410690556]])\n\n\n\n\nBackwards from 270 to 240\n\n\nCode\ncurrent.shape\n\n\n(7, 1)\n\n\n\n\nCode\nDh2 = 0.00004 * 36\n\ncurrent = t270\nfor step in range(270, -1, -1):\n    prev_step = (sym.eye(current.shape[0]) + M* Dh2).inv()*current \n    if step % 40 == 0:\n        print(step, current)\n        print('\\n')\n    current = sym.Matrix(np.maximum(prev_step, 0))\nprint(step, current)\ninit_state = current\n\n\n240 Matrix([[0], [0.00212689488945927], [1.15621985395050], [3.68754809461451], [1.15621985395048], [0.00212689488945964], [0]])\n\n\n200 Matrix([[9.47143103797723e-5], [0], [1.06279149507993], [4.00254036435968], [1.06279149507989], [0], [9.47143103797697e-5]])\n\n\n160 Matrix([[0.000194835210686209], [0], [0.937128531983600], [4.36938971176471], [0.937128531983537], [0], [0.000194835210686203]])\n\n\n120 Matrix([[0.000294410849364334], [0], [0.771792042493187], [4.79882186271244], [0.771792042493099], [0], [0.000294410849364323]])\n\n\n80 Matrix([[0.000389481701770366], [0], [0.557712169649991], [5.30388057840637], [0.557712169649879], [0], [0.000389481701770348]])\n\n\n40 Matrix([[0.000474717247747708], [0], [0.283827585787304], [5.90043223273764], [0.283827585787171], [0], [0.000474717247747682]])\n\n\n0 Matrix([[0.000543077482862398], [8.59904987581519e-5], [0], [6.60726130357147], [0], [8.59904987583709e-5], [0.000543077482862360]])\n\n\n0 Matrix([[0.000544502698002186], [9.92726600399977e-5], [0], [6.62637281796703], [0], [9.92726600402174e-5], [0.000544502698002148]])\n\n\n\n\nCode\nimport warnings\nwarnings.filterwarnings(\"ignore\")\ncurrent = init_state\nplot_df = pd.DataFrame([], columns=['0', '1/6', '1/3', '1/2'], dtype=np.float64)\nfor step in range(0, 301):\n    plot_df.loc[step] = current[:4]\n    next_step = current + M*current * Dh2\n    if step==0 or step ==210 or step == 300:\n        print(step, current)\n        print('\\n')\n    current = next_step\nplot_df.loc[step] = current[:4]\n\n\n0 Matrix([[0.000544502698002186], [9.92726600399977e-5], [0], [6.62637281796703], [0], [9.92726600402174e-5], [0.000544502698002148]])\n\n\n210 Matrix([[0.0171961512440258], [0.170658419488325], [1.14714251754697], [3.95457410693089], [1.14714251754697], [0.170658419488326], [0.0171961512440258]])\n\n\n300 Matrix([[0.0390928199813232], [0.277467835539847], [1.32429430615571], [3.33577189136941], [1.32429430615571], [0.277467835539847], [0.0390928199813232]])\n\n\n\n\n\n\nCode\nplot_df = plot_df.astype(np.float_)\n\n\n\n\nCode\nplot_df.plot(kind='line')\n\n\n\n\n\n\n\n\n\nIt is expected that the center curve looks like an logarithmic decay, the 1/3 curve looks like a logarithmic increase which I suppose will decrease at the point. Using Linear Algebra does seem like a much easier approach than the actual Physics of computing the diffusion constant as well as the intial concentration.\n\n\nMarkov\n\n\nCode\nimport sympy as sym\nsb = sym.Symbol\nimport pandas as pd\n\n\n\n\nCode\nM = sym.Matrix([[.5, .2, .3],[.2, .6, .1], [.3, .2, .6]])\nM\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0.5 & 0.2 & 0.3\\\\0.2 & 0.6 & 0.1\\\\0.3 & 0.2 & 0.6\\end{matrix}\\right]\\)\n\n\n3 years later\n\n\nCode\nM**3 * sym.Matrix([0.33333, 0.33333, 0.33333])\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0.33899661\\\\0.27533058\\\\0.38566281\\end{matrix}\\right]\\)\n\n\nfirst campaign\n\n\nCode\nMc1 = sym.Matrix([[.5, .32, .3],[.2, .48, .1], [.3, .2, .6]])\nMc1\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0.5 & 0.32 & 0.3\\\\0.2 & 0.48 & 0.1\\\\0.3 & 0.2 & 0.6\\end{matrix}\\right]\\)\n\n\n\n\nCode\nMc1**50 * sym.Matrix([0.33333, 0.33333, 0.33333])\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0.380562995951417\\\\0.22266983805668\\\\0.396757165991903\\end{matrix}\\right]\\)\n\n\n\n\nCode\nMc1**50 * sym.Matrix([0, 1, 0])\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0.380566801619433\\\\0.222672064777328\\\\0.396761133603239\\end{matrix}\\right]\\)\n\n\n\nsecond campaign below\n\n\nCode\nMc2 = sym.Matrix([[.5, .32, .3+.6*.2],[.2, .48, .1], [.3, .2, .6*.8]])\nMc2\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0.5 & 0.32 & 0.42\\\\0.2 & 0.48 & 0.1\\\\0.3 & 0.2 & 0.48\\end{matrix}\\right]\\)\n\n\n\n\nCode\nMc2**50 * sym.Matrix([0.33333, 0.33333, 0.33333])\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0.431422288077188\\\\0.230872949689869\\\\0.337694762232943\\end{matrix}\\right]\\)\n\n\n\n\nCode\nMc2**50 * sym.Matrix([0, 1, 0])\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0.431426602343211\\\\0.230875258442453\\\\0.337698139214335\\end{matrix}\\right]\\)\n\n\n\n\nReport\nIt is better to run campaign 2 as the steady state vector for campaign 2 is 43% vs 38% for campaign 1. It does not matter what the starting state is the end state after many iterations would be the same. As such the second campaign has insurance company A with a higher market share.\nBelow is the chart for the net gain our company A gains from either company B for marketing campaign 1 or company C for marketing campaign 2 for each subsequent iteration.\nImplicit assumption of this model is that the effectiveness of each marketing campaign in driving consumer behavior remains constant across each time period. This is most likely not the case in reality. In either case though it looks like marketing campaign 2 is the better campaign unless company B has a lot more customers than company C in the initial period.\n\n\nCode\nc1_state = sym.Matrix([0.33333, 0.33333, 0.33333])\nc2_state = sym.Matrix([0.33333, 0.33333, 0.33333])\ngains = pd.DataFrame([], columns=['Percentage from B', 'Percentage from C'])\ngain_from_b = 0\ngain_from_c = 0\nfor step in range(21):\n    gains.loc[int(step)] = (gain_from_b, gain_from_c)\n    next_state_c1 = Mc1 * c1_state\n    next_state_c2 = Mc2 * c2_state\n    gain_from_b = next_state_c1[1] * Mc1[0, 1] - c1_state[0] * Mc1[1, 0]\n    gain_from_c = next_state_c2[2] * Mc2[0, 2] - c2_state[0] * Mc1[2, 0]\n    c1_state = next_state_c1\n    c2_state = next_state_c2\n\n\n\n\nCode\ngains = gains.astype(float)\n\n\n\n\nCode\ngains.plot(kind='line')\n\n\n\n\n\n\n\n\n\n\n\n\nRanking\n\n\nCode\nimport sympy as sym\nimport numpy as np\nimport pandas as pd\nimport networkx as nx\nsb = sym.Symbol\nsm = sym.Matrix\n\n\n\n\nCode\nG = nx.DiGraph()\nG.add_nodes_from([1, 2, 3, 4, 5, 6, 7])\nG.add_edges_from([(1, 2), (7, 3), (2, 4), (4, 5), (3, 2), (5, 1), (6, 1), (3, 1), (7, 2), (2, 6), (3, 4), (7, 4)\n                  , (5, 7), (6, 4), (3, 5), (5, 6), (7, 1), (5, 2), (7, 6), (1, 4), (6, 3)])\n\n\n\n\nCode\nnx.draw(G, with_labels=True)\n\n\n\n\n\n\n\n\n\n\nadjacency matrix\n\n\nCode\nadj_mat = nx.to_numpy_array(G)\nadj_mat\n\n\narray([[0., 1., 0., 1., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 1., 0.],\n       [1., 1., 0., 1., 1., 0., 0.],\n       [0., 0., 0., 0., 1., 0., 0.],\n       [1., 1., 0., 0., 0., 1., 1.],\n       [1., 0., 1., 1., 0., 0., 0.],\n       [1., 1., 1., 1., 0., 1., 0.]])\n\n\n\n\nwin-loss record\n\n\nCode\nwins = adj_mat.sum(axis=1)\nwins\n\n\narray([2., 2., 4., 1., 4., 3., 5.])\n\n\n\n\nCode\nlosses = adj_mat.sum(axis=0)\nlosses\n\n\narray([4., 4., 2., 5., 2., 3., 1.])\n\n\n\n\nCode\nteam = pd.DataFrame({'wins': wins, 'losses': losses}, index=range(1, 8))\nteam.sort_values(['wins', 'losses'], ascending=[False, True])\n\n\n\n\n\n\n\n\n\n\nwins\nlosses\n\n\n\n\n7\n5.0\n1.0\n\n\n3\n4.0\n2.0\n\n\n5\n4.0\n2.0\n\n\n6\n3.0\n3.0\n\n\n1\n2.0\n4.0\n\n\n2\n2.0\n4.0\n\n\n4\n1.0\n5.0\n\n\n\n\n\n\n\n\n\n\nVertex Power\n\n\nCode\npower_rank = (adj_mat + adj_mat**2).sum(axis=1)\npd.Series(power_rank, index=range(1, 8)).sort_values(ascending=False)\n\n\n7    10.0\n3     8.0\n5     8.0\n6     6.0\n1     4.0\n2     4.0\n4     2.0\ndtype: float64\n\n\n\n\nReverse PageRank\n\n\nCode\ncol_sums = adj_mat.sum(axis=0)\nP = np.zeros(adj_mat.shape)\nfor i in range(adj_mat.shape[0]):\n    P[i, :] = adj_mat[i, :] / col_sums\npd.DataFrame(P)\n\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n\n\n\n\n0\n0.00\n0.25\n0.0\n0.2\n0.0\n0.000000\n0.0\n\n\n1\n0.00\n0.00\n0.0\n0.2\n0.0\n0.333333\n0.0\n\n\n2\n0.25\n0.25\n0.0\n0.2\n0.5\n0.000000\n0.0\n\n\n3\n0.00\n0.00\n0.0\n0.0\n0.5\n0.000000\n0.0\n\n\n4\n0.25\n0.25\n0.0\n0.0\n0.0\n0.333333\n1.0\n\n\n5\n0.25\n0.00\n0.5\n0.2\n0.0\n0.000000\n0.0\n\n\n6\n0.25\n0.25\n0.5\n0.2\n0.0\n0.333333\n0.0\n\n\n\n\n\n\n\n\n\n\nCode\nalpha = 0.85\nv = sym.ones(adj_mat.shape[0])[:, 0] / adj_mat.shape[0]\nv\n\n\n\\(\\displaystyle \\left[\\begin{matrix}\\frac{1}{7}\\\\\\frac{1}{7}\\\\\\frac{1}{7}\\\\\\frac{1}{7}\\\\\\frac{1}{7}\\\\\\frac{1}{7}\\\\\\frac{1}{7}\\end{matrix}\\right]\\)\n\n\n\n\nCode\npg_mat = (sym.eye(P.shape[0]) - alpha * P)\npg_mat\n\n\n\\(\\displaystyle \\left[\\begin{matrix}1.0 & -0.2125 & 0 & -0.17 & 0 & 0 & 0\\\\0 & 1.0 & 0 & -0.17 & 0 & -0.283333333333333 & 0\\\\-0.2125 & -0.2125 & 1.0 & -0.17 & -0.425 & 0 & 0\\\\0 & 0 & 0 & 1.0 & -0.425 & 0 & 0\\\\-0.2125 & -0.2125 & 0 & 0 & 1.0 & -0.283333333333333 & -0.85\\\\-0.2125 & 0 & -0.425 & -0.17 & 0 & 1.0 & 0\\\\-0.2125 & -0.2125 & -0.425 & -0.17 & 0 & -0.283333333333333 & 1.0\\end{matrix}\\right]\\)\n\n\n\n\nCode\npd.Series(list(pg_mat.gauss_jordan_solve(v* (1-alpha))[0]), index=range(1, 8))\n\n\n1    0.0596673237707704\n2    0.0796658079275577\n3     0.176269636041093\n4     0.125351577397605\n5     0.244524719927139\n6     0.130332241204918\n7     0.184188693730917\ndtype: object\n\n\n\n\nWeighted graph power ranking\n\n\nCode\nGw = nx.DiGraph()\nGw.add_nodes_from([1, 2, 3, 4, 5, 6, 7])\nGw.add_weighted_edges_from([(1, 2, 4), (7, 3, 8), (2, 4, 7), (4, 5, 3), (3, 2, 7), (5, 1, 7), (6, 1, 23)\n                            , (3, 1, 15), (7, 2, 6), (2, 6, 18), (3, 4, 13), (7, 4, 14)\n                  , (5, 7, 7), (6, 4, 13), (3, 5, 7), (5, 6, 18), (7, 1, 45), (5, 2, 10), (7, 6, 19), (1, 4, 13), (6, 3, 13)])\nadj_matw = nx.to_numpy_array(Gw)\nadj_matw\n\n\narray([[ 0.,  4.,  0., 13.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  7.,  0., 18.,  0.],\n       [15.,  7.,  0., 13.,  7.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  3.,  0.,  0.],\n       [ 7., 10.,  0.,  0.,  0., 18.,  7.],\n       [23.,  0., 13., 13.,  0.,  0.,  0.],\n       [45.,  6.,  8., 14.,  0., 19.,  0.]])\n\n\n\n\nCode\npd.Series((adj_matw + adj_matw**2).sum(axis=1), index=range(1, 8))\n\n\n1     202.0\n2     398.0\n3     534.0\n4      12.0\n5     564.0\n6     916.0\n7    2774.0\ndtype: float64"
  },
  {
    "objectID": "submitted/projects_1/sy_project1.html",
    "href": "submitted/projects_1/sy_project1.html",
    "title": "Projects 1",
    "section": "",
    "text": "Hi Sally, this work is very nice. I especially like how well organized and documented your code is.\nYou’ll see that I mention a few times, I’d also like for you to explain what you are doing in Markdown, outside of the code. That just makes it easier on the reader.\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport gravis as gv\nfrom collections import defaultdict\n\n\n\nLU Factorization\n\n\nCode\n# LU Factorization Part 1. PLU Factorization Program \n\ndef PLU_factorization(A):\n    n = len(A)\n    L = np.eye(n)  # Initialize L as identity matrix\n    U = np.copy(A)  # To derive U from A, begin with partial pivoting then Gaussian elimination...\n    indx = np.arange(1, n + 1)  # Initialize indx array with 1, 2, ..., n \n    \n    for k in range(n - 1): \n        # Partial pivoting\n        max_index = np.argmax(np.abs(U[k:, k])) + k # Search for the largest entry in the kth column\n        \n        if max_index != k: # Check if the (k,k)th entry is the largest entry or not\n            # If the (k,k)th entry is the largest entry in the column, can just select it as pivot\n            # If not, then exchange the kth row with the row with kth row's largest entry in both U and L\n            # So that the new (k,k)th entry, which is the largest in the column, is a more proper pivot \n            U[[k, max_index]] = U[[max_index, k]]\n            L[[k, max_index], :k] = L[[max_index, k], :k]\n            \n            # Update indx array to reflect the row exchange \n            indx[k], indx[max_index] = indx[max_index], indx[k] \n        \n        for i in range(k + 1, n):\n            # Compute multipliers and perform Gaussian elimination\n            L[i, k] = U[i, k] / U[k, k]\n            U[i, k:] -= L[i, k] * U[k, k:]\n            \n    P = np.eye(n)[indx - 1] # Derive permutation matrix P from indx array\n    return L, U, indx, P\n\n# Testing this program module with random nonsingular matrix \nA = np.array([[2, 1, -1],\n              [2, 3, 3],\n              [4, 1, -3]], dtype=float)\n\nL, U, indx, P = PLU_factorization(A)\n\nprint(\"Lower triangular matrix L:\")\nprint(L)\nprint(\"\\nUpper triangular matrix U:\")\nprint(U)\nprint(\"\\nPermuted index array indx:\")\nprint(indx)\nprint(\"\\nPermutatation matrix P: \")\nprint(P)\n\n\nLower triangular matrix L:\n[[1.  0.  0. ]\n [0.5 1.  0. ]\n [0.5 0.2 1. ]]\n\nUpper triangular matrix U:\n[[ 4.   1.  -3. ]\n [ 0.   2.5  4.5]\n [ 0.   0.  -0.4]]\n\nPermuted index array indx:\n[3 2 1]\n\nPermutatation matrix P: \n[[0. 0. 1.]\n [0. 1. 0.]\n [1. 0. 0.]]\n\n\n\n\nCode\n# LU Factorization Part 2. Linear Systems Solver \n\ndef solve_linear_system(A, b):\n    n = A.shape[0]\n    \n    # First perform PLU factorization \n    L, U, indx, P = PLU_factorization(A)\n    \n    # Permute vector b using P \n    b_permuted = P.dot(b)\n    \n    # Solve Ly = Pb for y using forward substitution\n    y = np.zeros(n)\n    for i in range(n):\n        y[i] = b_permuted[i] - L[i, :i].dot(y[:i])\n    \n    # Solve Ux = y for x using backward substitution\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = (y[i] - U[i, i+1:].dot(x[i+1:])) / U[i, i]\n    return x\n\n# Testing \nA = np.array([[2, 1, -1],\n              [2, 3, 3],\n              [4, 1, -3]], dtype=float)\nb = np.array([1, 2, 3])\nx = solve_linear_system(A, b)\n\nprint(\"Solution to the linear system Ax = b:\")\nprint(x)\n\n\nSolution to the linear system Ax = b:\n[ 2.5 -2.5  1.5]\n\n\n\n\nCode\n# LU Factorization Part 3. Matrix Inverse Solver\n\ndef matrix_inverse(A): \n    n = A.shape[0] \n    \n    # Initialize the matrix inverse as empty to update with x_i's later \n    A_inv = np.zeros_like(A, dtype=float)\n    \n    # Perform PLU factorization\n    L, U, indx, P = PLU_factorization(A)\n    \n    # Solve Ax_i = e_i for i =1, 2, ..., n just as described in the project description\n    for i in range(n):\n        e_i = np.eye(n)[:, i]  # e_i is the ith column of n by n identity matrix\n        \n        # Permute e_i according to the permutation matrix P\n        e_i_permuted = P.dot(e_i)\n        \n        # Solve linear system Ax_i = e_i for x_i\n        y = np.zeros(n)\n        for j in range(n):\n            y[j] = e_i_permuted[j] - L[j, :j].dot(y[:j])\n        \n        x_i = np.zeros(n)\n        for j in range(n - 1, -1, -1):\n            x_i[j] = (y[j] - U[j, j+1:].dot(x_i[j+1:])) / U[j, j]\n        \n        # Store x_i as the ith column vector (from left to right) of the matrix inverse\n        A_inv[:, i] = x_i \n    \n    return A_inv # Now the matrix inverse is updated with x_i's obtained from the linear systems solver\n\n# Testing \nA = np.array([[2, 1, -1],\n              [2, 3, 3],\n              [4, 1, -3]], dtype=float)\nA_inv = matrix_inverse(A)\n\nprint(\"Inverse of matrix A:\")\nprint(A_inv)\n\n# Or (easier in practice, when n is small)\nA = np.array([[2, 1, -1],\n              [2, 3, 3],\n              [4, 1, -3]], dtype=float)\nx_1 = solve_linear_system(A, np.array([1, 0, 0]))\nx_2 = solve_linear_system(A, np.array([0, 1, 0]))\nx_3 = solve_linear_system(A, np.array([0, 0, 1]))\nprint(\"This way we get A's inverse as follows (which yields the same results): \")\nprint(np.column_stack((x_1, x_2, x_3)))\n\n\nInverse of matrix A:\n[[-3.   0.5  1.5]\n [ 4.5 -0.5 -2. ]\n [-2.5  0.5  1. ]]\nThis way we get A's inverse as follows (which yields the same results): \n[[-3.   0.5  1.5]\n [ 4.5 -0.5 -2. ]\n [-2.5  0.5  1. ]]\n\n\n\nThis is nice clean code, well documented and easy to follow.\nI would like to see you explain what you are doing in Markdown, outside of the code (for this and all of the projects); you can introduce the problem, put in any formulas you need, describe what is happening mathematically, etc.\nFor this project, I’d like to see you test your algorithms on a few more matrices. Can you add an automated way to test if they do what they say they do? Can you test if the U matrix is truly upper triangular, if the L matrix is truly lower triangular, if the product of L and U is equal to the original matrix, etc.? What about for the inverse and the linear equation solvers – can you test if they are indeed giving correct answers?\nGrade: R\n\n\n\nMarkov Chains\n\n\nCode\n# Markov Chains Part 1. Transition Matrix and Market Share Predicts\n\nP = np.array([[0.5, 0.2, 0.3],\n              [0.2, 0.6, 0.1],\n              [0.3, 0.2, 0.6]])\n\n# Check if P is correctly modeled so that this is indeed a markov chain\nif sum(P[0])+sum(P[1])+sum(P[2]) != 3:\n    print(\"transition matrix not right\")\nelse: print(\"transition matrix correctly modeled\")\n\n# Initial state vector \nx_0 = np.array([1/3, 1/3, 1/3])\n\n# Use the transition matrix to calculate market shares after three years\nfinal_market_shares = np.linalg.matrix_power(P, 3) @ x_0\nprint(\"Market shares three years from now is:\")\nprint(final_market_shares)\n\n# Test for different initial state vectors to see if there's a difference\nfinal_market_shares_A_dominant = np.linalg.matrix_power(P, 3) @ np.array([2/3, 1/6, 1/6])\nprint(\"Had the initial state been (2/3, 1/6, 1/6), market shares three years from now would be:\")\nprint(final_market_shares_A_dominant)\n\nfinal_market_shares_B_dominant = np.linalg.matrix_power(P, 3) @ np.array([1/6, 2/3, 1/6])\nprint(\"Had the initial state been (1/6, 2/3, 1/6), market shares three years from now would be:\")\nprint(final_market_shares_B_dominant)\n\nfinal_market_shares_C_dominant = np.linalg.matrix_power(P, 3) @ np.array([1/6, 1/6, 2/3])\nprint(\"Had the initial state been (1/6, 1/6, 2/3), market shares three years from now would be:\")\nprint(final_market_shares_C_dominant)\n\n\ntransition matrix correctly modeled\nMarket shares three years from now is:\n[0.339      0.27533333 0.38566667]\nHad the initial state been (2/3, 1/6, 1/6), market shares three years from now would be:\n[0.345      0.26916667 0.38583333]\nHad the initial state been (1/6, 2/3, 1/6), market shares three years from now would be:\n[0.3265     0.30666667 0.36683333]\nHad the initial state been (1/6, 1/6, 2/3), market shares three years from now would be:\n[0.3455     0.25016667 0.40433333]\n\n\n\n\nCode\n# Markov Chains Part 2. Advertising Campaigns\n\n# To analyze the effect of each advertising plan on company A's market shares\n# Modify the transition matrix accordingly \n# Switching an additional 20% of company B's remaining customers to A means\n# the transition probability from B to A becomes 0.2 + 0.2*0.6 = 0.32\n# the transition probability from B to B becomes 0.6*(1-0.2) = 0.48\n# Switching an additional 20% of company C's remaining customers to A means\n# the transition probability from C to A becomes 0.3 + 0.2*0.6 = 0.42\n# the transition probability from C to C becomes 0.6*(1-0.2) = 0.48\n\nP1 = np.array([[0.5, 0.32, 0.3],\n              [0.2, 0.48, 0.1],\n              [0.3, 0.2, 0.6]])\n\nP2 = np.array([[0.5, 0.2, 0.42],\n              [0.2, 0.6, 0.1],\n              [0.3, 0.2, 0.48]])\n\n# Now we predict market shares 3 years from now using different initial conditions\nx_0_equal_share = np.array([1/3, 1/3, 1/3])\nx_0_A_dominant = np.array([2/3, 1/6, 1/6])\nx_0_B_dominant = np.array([1/6, 2/3, 1/6])\nx_0_C_dominant = np.array([1/6, 1/6, 2/3])\n\n# Use the transition matrices to calculate market shares after 3 years under plan 1\nfinal_market_shares_1_equal_share = np.linalg.matrix_power(P1, 3) @ x_0_equal_share\nprint(\"Advertising plan 1 with equal-share initial condition, market shares 3 years from now is:\")\nprint(final_market_shares_1_equal_share)\n\nfinal_market_shares_1_A_dominant = np.linalg.matrix_power(P1, 3) @ x_0_A_dominant\nprint(\"Advertising plan 1 with the A-dominant initial condition, market shares 3 years from now is:\")\nprint(final_market_shares_1_A_dominant)\n\nfinal_market_shares_1_B_dominant = np.linalg.matrix_power(P1, 3) @ x_0_B_dominant\nprint(\"Advertising plan 1 with the B-dominant initial condition, market shares 3 years from now is:\")\nprint(final_market_shares_1_B_dominant)\n\nfinal_market_shares_1_C_dominant = np.linalg.matrix_power(P1, 3) @ x_0_C_dominant\nprint(\"Advertising plan 1, under the C-dominant initial condition, market shares 3 years from now is:\")\nprint(final_market_shares_1_C_dominant)\n\n# If we compare these predicted results with those from part 1, \n# We may infer that the market share of company A is likely to be improved by plan 1\n# Because under all four initial state vectors we chose, \n# Company A's market share after 3 years is higher than that in the case of no advertising plan\n# We can see this if we compare the first entries in the final_share vectors\n\n# However, under different initial conditions, size of increase is different\nincrease_equal_share = final_market_shares_1_equal_share[0] - final_market_shares[0]\nprint(\"The increase in A's market share resulted from plan 1 under equal-share condition is:\")\nprint(increase_equal_share)\nincrease_A_dominant = final_market_shares_1_A_dominant[0] - final_market_shares_A_dominant[0]\nprint(\"The increase in A's market share resulted from plan 1 under A-dominant condition is:\")\nprint(increase_A_dominant)\nincrease_B_dominant = final_market_shares_1_B_dominant[0] - final_market_shares_B_dominant[0]\nprint(\"The increase in A's market share resulted from plan 1 under B-dominant condition is:\")\nprint(increase_B_dominant)\nincrease_C_dominant = final_market_shares_1_C_dominant[0] - final_market_shares_C_dominant[0]\nprint(\"The increase in A's market share resulted from plan 1 under C-dominant condition is:\")\nprint(increase_C_dominant)\n\n\n# Use the transition matrices to calculate market shares after 3 years under plan 2\nfinal_market_shares_2_equal_share = np.linalg.matrix_power(P2, 3) @ x_0_equal_share\nprint(\"Advertising plan 2 with equal-share initial condition, market shares 3 years from now is:\")\nprint(final_market_shares_2_equal_share)\n\nfinal_market_shares_2_A_dominant = np.linalg.matrix_power(P2, 3) @ x_0_A_dominant\nprint(\"Advertising plan 2 with the A-dominant initial condition, market shares 3 years from now is:\")\nprint(final_market_shares_2_A_dominant)\n\nfinal_market_shares_2_B_dominant = np.linalg.matrix_power(P2, 3) @ x_0_B_dominant\nprint(\"Advertising plan 2 with the B-dominant initial condition, market shares 3 years from now is:\")\nprint(final_market_shares_2_B_dominant)\n\nfinal_market_shares_2_C_dominant = np.linalg.matrix_power(P2, 3) @ x_0_C_dominant\nprint(\"Advertising plan 2, under the C-dominant initial condition, market shares 3 years from now is:\")\nprint(final_market_shares_2_C_dominant)\n\n# If we compare these predicted results with those from part 1, \n# We may infer that the market share of company A is also likely to be improved by plan 2\n# Because under all four initial state vectors we chose, \n# Company A's market share after 3 years is higher than that in the case of no advertising plan\n# We can see this if we compare the first entries in the final_share vectors\n\n# However, under different initial conditions, size of increase is different\nincrease2_equal_share = final_market_shares_2_equal_share[0] - final_market_shares[0]\nprint(\"The increase in A's market share resulted from plan 2 under equal-share condition is:\")\nprint(increase2_equal_share)\nincrease2_A_dominant = final_market_shares_2_A_dominant[0] - final_market_shares_A_dominant[0]\nprint(\"The increase in A's market share resulted from plan 2 under A-dominant condition is:\")\nprint(increase2_A_dominant)\nincrease2_B_dominant = final_market_shares_2_B_dominant[0] - final_market_shares_B_dominant[0]\nprint(\"The increase in A's market share resulted from plan 2 under B-dominant condition is:\")\nprint(increase2_B_dominant)\nincrease2_C_dominant = final_market_shares_2_C_dominant[0] - final_market_shares_C_dominant[0]\nprint(\"The increase in A's market share resulted from plan 2 under C-dominant condition is:\")\nprint(increase2_C_dominant)\n\n\nAdvertising plan 1 with equal-share initial condition, market shares 3 years from now is:\n[0.380696   0.22771733 0.39158667]\nAdvertising plan 1 with the A-dominant initial condition, market shares 3 years from now is:\n[0.383008   0.22699867 0.38999333]\nAdvertising plan 1 with the B-dominant initial condition, market shares 3 years from now is:\n[0.380852   0.24227467 0.37687333]\nAdvertising plan 1, under the C-dominant initial condition, market shares 3 years from now is:\n[0.378228   0.21387867 0.40789333]\nThe increase in A's market share resulted from plan 1 under equal-share condition is:\n0.04169600000000001\nThe increase in A's market share resulted from plan 1 under A-dominant condition is:\n0.03800800000000004\nThe increase in A's market share resulted from plan 1 under B-dominant condition is:\n0.05435200000000001\nThe increase in A's market share resulted from plan 1 under C-dominant condition is:\n0.032728000000000035\nAdvertising plan 2 with equal-share initial condition, market shares 3 years from now is:\n[0.387496   0.28205333 0.33045067]\nAdvertising plan 2 with the A-dominant initial condition, market shares 3 years from now is:\n[0.392888   0.27432667 0.33278533]\nAdvertising plan 2 with the B-dominant initial condition, market shares 3 years from now is:\n[0.369708   0.31122667 0.31906533]\nAdvertising plan 2, under the C-dominant initial condition, market shares 3 years from now is:\n[0.399892   0.26060667 0.33950133]\nThe increase in A's market share resulted from plan 2 under equal-share condition is:\n0.048495999999999984\nThe increase in A's market share resulted from plan 2 under A-dominant condition is:\n0.047887999999999986\nThe increase in A's market share resulted from plan 2 under B-dominant condition is:\n0.043208000000000024\nThe increase in A's market share resulted from plan 2 under C-dominant condition is:\n0.054391999999999996\n\n\n\n\nCode\n# Summary of results for Markov Chain Problem Part 2:\n# Based on the four different initial state vectors I chose and the 3-year timeline\n# We may argue that plan 2 is more attractive \n# Because the size of the increases in A's market share under plan 2\n# is higher than that of plan 1 under 3 out of the 4 initial state vectors I chose\n# My results say that as long as initially, B is NOT particularly dominant in the market \n# Then the increase in A's market share due to plan 2 would be bigger than that of plan 1\n# Which argues, in most cases, in favor of plan 2 given the fact that plan 1&2 cost the same\n\n# However, one may argue in favor of plan 1 if B is currently \n# as dominant in the market as in initial state vector (1/6, 2/3, 1/6)\n# Since this initial state vector is rather arbitrarily chosen\n# this could provoke further interest in the \"threshold\" that company B's market share has \n# to exceed in order for us to argue in favor of plan 1 instead of plan 2\n# Just as given in this project's description for further analysis\n# We shall analyze the effect of increasing B's initial market share on the size of the increases \n\n# With the results at hand, we may create a simple dot plot\n# Where we plot the size of the increases against B's market share for both plans\n\n# Data points for Plan 1 \nplan1_data_points = [(1/6, 0.038), (1/3, 0.042), (2/3, 0.054)] \n# we use the equal-share, A-dominant, and B-dominant initial state vectors\nplan1_x_values, plan1_y_values = zip(*plan1_data_points)\n\n# Data points for Plan 2\nplan2_data_points = [(1/6, 0.048), (1/3, 0.048), (2/3, 0.043)]\nplan2_x_values, plan2_y_values = zip(*plan2_data_points)\n\n# Create the dot plot and connect the points for Plan 1\nplt.figure(figsize=(8, 4))  \nplt.scatter(plan1_x_values, plan1_y_values, color='blue', label='Plan 1 Data Points')  \nplt.plot(plan1_x_values, plan1_y_values, linestyle='-', marker='o', color='blue')  \nplt.scatter(plan2_x_values, plan2_y_values, color='red', label='Plan 2 Data Points')  \nplt.plot(plan2_x_values, plan2_y_values, linestyle='-', marker='o', color='red') \nplt.xlabel('Company B market share')  \nplt.ylabel('Size of the increases in company A market share')    \nplt.legend()  \nplt.grid(True)  \nplt.show() \n\n# To decide whether it is worthwhile to gain customers from B (plan 1) or C (plan 2)\n# just as suggested by the plot (size of increase due to plan 2 is NOT always above that of plan 1)\n# we shall consider how much market share does B occupy initially\n# because whether the size of the increase of A's market share due to plan 1 exceeds that of plan 2 \n# changes when the initial market share of B exceeds a certain limit/threshold\n\n\n\n\n\n\n\n\n\n\nI really like how you dug into the issue of the threshold for the initial market share of B. It’s great that you noticed a discrepancy in the results and thought about how the initial state might affect the outcome. This is a very insightful analysis that adds depth to your work.\nHow do you think the results might change if you looked further into the future, beyond the 3-year timeline? Would the threshold for the initial market share of B change if you considered a longer time horizon? Would the initial state cease to matter at all?\nI also appreciate the way you’ve presented your results in a clear and concise manner. Great work!\nGrade: E\n\n\n\nCode\n# Sports Ranking Part 1. Ranking by Win/Loss Record\n# Given series of (i, j) pairs representing game results\nE = [(1, 2), (7, 3), (2, 4), (4, 5), (3, 2), (5, 1), (6, 1), (3, 1), (7, 2),\n                (2, 6), (3, 4), (7, 4), (5, 7), (6, 4), (3, 5), (5, 6), (7, 1), (5, 2),\n                (7, 6), (1, 4), (6, 3)]\n\n# Initialize a dictionary to record the win/loss record for each team\nteam_stats = defaultdict(lambda: {'wins': 0, 'losses': 0})\n\nfor winner, loser in E:\n    team_stats[winner]['wins'] += 1\n    team_stats[loser]['losses'] += 1\n\n# Rank the teams based on win/loss records\nranked_teams = sorted(team_stats.items(), key=lambda x: (x[1]['wins'], -x[1]['losses']), reverse=True)\n\n# Print the ranked teams\nprint(\"Ranking of teams based on win/loss record:\")\nfor rank, (team, stats) in enumerate(ranked_teams, start=1):\n    print(f\"{rank}. Team {team}: Wins = {stats['wins']}, Losses = {stats['losses']}\")\n\n\nRanking of teams based on win/loss record:\n1. Team 7: Wins = 5, Losses = 1\n2. Team 3: Wins = 4, Losses = 2\n3. Team 5: Wins = 4, Losses = 2\n4. Team 6: Wins = 3, Losses = 3\n5. Team 1: Wins = 2, Losses = 4\n6. Team 2: Wins = 2, Losses = 4\n7. Team 4: Wins = 1, Losses = 5\n\n\n\n\nCode\n# Then plot the digraph and give the adjacency matrix \nG = nx.DiGraph()\nG.add_node(\"1\")\nG.add_node(\"2\")\nG.add_node(\"3\")\nG.add_node(\"4\")\nG.add_node(\"5\")\nG.add_node(\"6\")\nG.add_node(\"7\")\nG.add_edge(\"1\",\"2\")\nG.add_edge(\"7\",\"3\")\nG.add_edge(\"2\",\"4\")\nG.add_edge(\"4\",\"5\")\nG.add_edge(\"3\",\"2\")\nG.add_edge(\"5\",\"1\")\nG.add_edge(\"6\",\"1\")\nG.add_edge(\"3\",\"1\")\nG.add_edge(\"7\",\"2\")\nG.add_edge(\"2\",\"6\")\nG.add_edge(\"3\",\"4\")\nG.add_edge(\"7\",\"4\")\nG.add_edge(\"5\",\"7\")\nG.add_edge(\"6\",\"4\")\nG.add_edge(\"3\",\"5\")\nG.add_edge(\"5\",\"6\")\nG.add_edge(\"7\",\"1\")\nG.add_edge(\"5\",\"2\")\nG.add_edge(\"7\",\"6\")\nG.add_edge(\"1\",\"4\")\nG.add_edge(\"6\",\"3\")\npos = nx.spring_layout(G)\nnx.draw_networkx_nodes(G, pos)\nnx.draw_networkx_labels(G, pos)\nnx.draw_networkx_edges(G, pos, edge_color='r', arrows = True)\nplt.show()\n\n# Now construct the adjacency matrix \nijpairs = [(1, 2), (7, 3), (2, 4), (4, 5), (3, 2), (5, 1), (6, 1), (3, 1), (7, 2),\n         (2, 6), (3, 4), (7, 4), (5, 7), (6, 4), (3, 5), (5, 6), (7, 1), (5, 2),\n         (7, 6), (1, 4), (6, 3)]\n\n# Determine the number of teams\nnum_teams = max(max(pair) for pair in ijpairs)\n\n# Initialize the adjacency matrix with zeros\nadjacency_matrix = np.zeros((num_teams, num_teams), dtype=int)\n\n# Fill in the adjacency matrix based on the pairs\n# For instance, (1,2) means the (1,2)th entry of adjacency matrix is 1,\n# meanwhile the (2,1)th entry is 0\n\nfor i, j in ijpairs:\n    adjacency_matrix[i - 1, j - 1] = 1  # Subtract 1 to convert team numbers to array indices\n\nprint(\"Adjacency matrix:\")\nprint(adjacency_matrix)\n\n\n\n\n\n\n\n\n\nAdjacency matrix:\n[[0 1 0 1 0 0 0]\n [0 0 0 1 0 1 0]\n [1 1 0 1 1 0 0]\n [0 0 0 0 1 0 0]\n [1 1 0 0 0 1 1]\n [1 0 1 1 0 0 0]\n [1 1 1 1 0 1 0]]\n\n\n\n\nCode\n# Sports Ranking Part 2. Example 2.26 Power Ranking Method\n\n# Find A+A^2\nA = adjacency_matrix\nA_squared = np.linalg.matrix_power(A, 2)\nA_plus_A_squared = A + A_squared\n\nprint (A_plus_A_squared)\n\n# Now we may compute the power of each vertex by summing over all entries in the ith row of A+A^2\nrow_sum_1 = np.sum(A_plus_A_squared[0])\nrow_sum_2 = np.sum(A_plus_A_squared[1])\nrow_sum_3 = np.sum(A_plus_A_squared[2])\nrow_sum_4 = np.sum(A_plus_A_squared[3])\nrow_sum_5 = np.sum(A_plus_A_squared[4])\nrow_sum_6 = np.sum(A_plus_A_squared[5])\nrow_sum_7 = np.sum(A_plus_A_squared[6])\n\nprint(\"Power of vertex 1 is:\")\nprint(row_sum_1)\nprint(\"Power of vertex 2 is:\")\nprint(row_sum_2)\nprint(\"Power of vertex 3 is:\")\nprint(row_sum_3)\nprint(\"Power of vertex 4 is:\")\nprint(row_sum_4)\nprint(\"Power of vertex 5 is:\")\nprint(row_sum_5)\nprint(\"Power of vertex 6 is:\")\nprint(row_sum_6)\nprint(\"Power of vertex 7 is:\")\nprint(row_sum_7)\n\n# When we print the results and rank the 7 teams based on which one has the greatest power\n# We get power ranking : team 7 &gt; team 5 &gt; team 3 &gt; team 6 &gt; team 2 &gt; team 1 = team 4\n\n\n[[0 1 0 2 1 1 0]\n [1 0 1 2 1 1 0]\n [2 3 0 3 2 2 1]\n [1 1 0 0 1 1 1]\n [3 3 2 4 0 3 1]\n [2 2 1 3 2 0 0]\n [3 3 2 5 2 2 0]]\nPower of vertex 1 is:\n5\nPower of vertex 2 is:\n6\nPower of vertex 3 is:\n13\nPower of vertex 4 is:\n5\nPower of vertex 5 is:\n16\nPower of vertex 6 is:\n10\nPower of vertex 7 is:\n17\n\n\n\n\nCode\n# Sports Ranking Part 3. Example 2.47 Reverse PageRank Method\n\n# As in example 2.47, write out the page ranking transition matrix P = (A^T)^TD\n# By Theorem 2.7, D is a diagonal matrix whose ith entry is 1/(sum of all entries in the ith row of A)\nD = np.array([[1/2, 0, 0, 0, 0, 0, 0],\n              [0, 1/2, 0, 0, 0, 0, 0],\n              [0, 0, 1/4, 0, 0, 0, 0],\n              [0, 0, 0, 1, 0, 0, 0],\n              [0, 0, 0, 0, 1/4, 0, 0],\n              [0, 0, 0, 0, 0, 1/3, 0],\n              [0, 0, 0, 0, 0, 0, 1/5]])\nP = A @ D\n# Select teleportation vector v\nv = np.array([1/7, 1/7, 1/7, 1/7, 1/7, 1/7, 1/7])\n# Choose teleportation parameter alpha as in example 2.47\nalpha = 0.85\n# Use the reverse PageRank method to solve for x \nx = (1-alpha)*np.linalg.inv((np.eye(7) - alpha*P))@v\nprint(x)\n\n# This rank suggests team 7 &gt; team 3 &gt; team 5 &gt; team 6 &gt; team 1 &gt; team 2 &gt; team 4\n\n\n[0.14823866 0.1450318  0.26648395 0.07667243 0.25997111 0.20622941\n 0.32629959]\n\n\n\n\nCode\n# Sports Ranking Part 4. Power Ranking Weighted by Game Margins\n\n# Create a weighted digraph based on additional information M\nedges = [(\"1\",\"2\", 4),\n         (\"7\",\"3\", 8),\n         (\"2\",\"4\", 7),\n         (\"4\",\"5\", 3),\n         (\"3\",\"2\", 7),\n         (\"5\",\"1\", 7),\n         (\"6\",\"1\", 23),\n         (\"3\",\"1\", 15),\n         (\"7\",\"2\", 6),\n         (\"2\",\"6\", 18),\n         (\"3\",\"4\", 13),\n         (\"7\",\"4\", 14),\n         (\"5\",\"7\", 7),\n         (\"6\",\"4\", 13),\n         (\"3\",\"5\", 7),\n         (\"5\",\"6\", 18),\n         (\"7\",\"1\", 45),\n         (\"5\",\"2\", 10),\n         (\"7\",\"6\", 19),\n         (\"1\",\"4\", 14),\n         (\"6\",\"3\", 13),\n        ]\nG = nx.DiGraph()\nfor source, target, strength in edges:\n    G.add_edge(source, target, strength = strength)\ngv.d3(G, show_edge_label = True, edge_label_data_source = 'strength')\n\n\n\n  \n\n  \n    \n\n    \n      \n        \n        \n          \n            Details for selected element\n          \n          \n          \n        \n      \n    \n\n    \n      \n        \n        \n          General\n        \n        \n          \n          \n            \n              App state\n            \n            \n              Reset\n            \n          \n          \n          \n            \n              Display mode\n            \n            \n              Enter full screen\n            \n          \n          \n          \n            \n              Export\n            \n            \n              SVG\n              PNG\n              JPG\n            \n          \n        \n        \n        \n          Data selection\n        \n        \n          \n          \n            \n              Graph\n            \n            \n              \n            \n          \n          \n          \n            \n              Node label text\n            \n            \n              \n            \n          \n          \n          \n            \n              Edge label text\n            \n            \n              \n            \n          \n          \n          \n            Node size\n          \n          \n            \n              \n            \n            \n              \n              Normalize\n            \n            \n              \n                Minimum\n                \n                \n              \n              \n                Maximum\n                \n                \n              \n            \n          \n          \n          \n            Edge size\n          \n          \n            \n              \n            \n            \n              \n              Normalize\n            \n            \n              \n                Minimum\n                \n                \n              \n              \n                Maximum\n                \n                \n              \n            \n          \n        \n        \n        \n          Nodes\n        \n        \n          \n          \n            \n              Visibility\n            \n            \n              \n                \n                Show nodes\n              \n            \n          \n          \n          \n            \n              Size\n            \n            \n              \n                Scaling factor\n                \n                \n              \n            \n          \n          \n          \n            \n              Position\n            \n            \n              Release fixed nodes\n            \n          \n          \n          \n            \n              Drag behavior\n            \n            \n              \n                \n                Fix node position\n              \n            \n          \n          \n          \n            \n              Hover behavior\n            \n            \n              \n                \n                Show neighborhood\n              \n              \n                \n                Show tooltips (if provided)\n              \n            \n          \n        \n        \n        \n          \n            Node images\n          \n          \n            \n            \n              \n                Visibility\n              \n              \n                \n                  \n                  Show node images\n                \n              \n            \n            \n            \n              \n                Size\n              \n              \n                Scaling factor\n                \n                \n              \n            \n          \n        \n        \n        \n          Node labels\n        \n        \n          \n          \n            \n              Visibility\n            \n            \n              \n                \n                Show node labels\n              \n              \n                \n                Show borders\n              \n            \n          \n          \n          \n            \n              Size\n            \n            \n              Scaling factor\n              \n              \n            \n          \n          \n          \n            \n              Rotation\n            \n            \n              Angle\n              \n              \n            \n          \n        \n        \n        \n          Edges\n        \n        \n          \n          \n            \n              Visibility\n            \n            \n              \n                \n                Show edges\n              \n            \n          \n          \n          \n            \n              Size\n            \n            \n              Scaling factor\n              \n              \n            \n          \n          \n          \n            \n              Form\n            \n            \n              Curvature\n              \n              \n            \n          \n          \n          \n            \n              Hover behavior\n            \n            \n              \n                \n                Show tooltips (if provided)\n              \n            \n          \n        \n        \n        \n          Edge labels\n        \n        \n          \n          \n            \n              Visibility\n            \n            \n              \n                \n                Show edge labels\n              \n              \n                \n                Show borders\n              \n            \n          \n          \n          \n            \n              Size\n            \n            \n              Scaling factor\n              \n              \n            \n          \n          \n          \n            \n              Rotation\n            \n            \n              Angle\n              \n              \n            \n          \n        \n        \n        \n          Layout algorithm\n        \n        \n\n          \n          \n            \n              Simulation\n            \n            \n              \n                \n                Active\n              \n            \n          \n          \n          \n            \n              Many-body force\n            \n            \n              \n                \n                On\n              \n              \n                \n                  Strength\n                  \n                  \n                \n                \n                  Theta\n                  \n                  \n                \n                \n                  \n                    \n                    Use minimum distance\n                  \n                  \n                    Min\n                    \n                    \n                  \n                \n                \n                  \n                    \n                    Use maximum distance\n                  \n                  \n                    Max\n                    \n                    \n                  \n                \n              \n            \n          \n          \n          \n            \n              Links force\n            \n            \n              \n                \n                On\n                \n              \n              \n                \n                  Distance\n                  \n                  \n                \n                \n                  Strength\n                  \n                  \n                \n              \n            \n          \n          \n          \n            \n              Collision force\n            \n            \n              \n                \n                On\n              \n              \n                \n                  Radius\n                  \n                  \n                \n                \n                  Strength\n                  \n                  \n                \n              \n            \n          \n          \n          \n            \n              x-positioning force\n            \n            \n              \n                \n                On\n              \n              \n                \n                  Strength\n                  \n                  \n                \n              \n            \n          \n          \n          \n            \n              y-positioning force\n            \n            \n              \n                \n                On\n              \n              \n                \n                  Strength\n                  \n                  \n                \n              \n            \n          \n          \n          \n            \n              Centering force\n            \n            \n              \n                \n                On\n              \n            \n          \n        \n      \n    \n  \n\n  \n\n\n\n\n\nIt turns out to be pretty easy to get the weighted adjacency matrix from the weighted digraph above. Just needs a little bit of tweaking to get it into the same order you have the teams in. (Also, I added “todense” so that it doesn’t come out as a sparse matrix.)\n\n\nCode\nnx.adjacency_matrix(G,weight='strength',nodelist=[str(i) for i in range(1, 8)]).todense()\n\n\narray([[ 0,  4,  0, 14,  0,  0,  0],\n       [ 0,  0,  0,  7,  0, 18,  0],\n       [15,  7,  0, 13,  7,  0,  0],\n       [ 0,  0,  0,  0,  3,  0,  0],\n       [ 7, 10,  0,  0,  0, 18,  7],\n       [23,  0, 13, 13,  0,  0,  0],\n       [45,  6,  8, 14,  0, 19,  0]])\n\n\n\n\nCode\n# Now construct the \"weighted\" adjacency matrix \nijpairs = [(1, 2), (7, 3), (2, 4), (4, 5), (3, 2), (5, 1), (6, 1), (3, 1), (7, 2),\n         (2, 6), (3, 4), (7, 4), (5, 7), (6, 4), (3, 5), (5, 6), (7, 1), (5, 2),\n         (7, 6), (1, 4), (6, 3)]\nmargins = [4, 8, 7, 3, 7, 7, 23, 15, 6, 18, 13, 14, 7, 13, 7, 18, 45, 10, 19, 14, 13]\n\n# Determine the number of teams\nnum_teams = max(max(pair) for pair in ijpairs)\n\n# Initialize the weighted adjacency matrix with zeros\nweighted_adjacency_matrix = np.zeros((num_teams, num_teams), dtype=int)\n\n# Fill in the weighted adjacency matrix based on pairs and margins\nfor pair, margin in zip(ijpairs, margins):\n    i, j = pair\n    weighted_adjacency_matrix[i - 1, j - 1] = margin  # Subtract 1 to convert team numbers to array indices\n\n# Let B denote this \"weighted\" adjacency matrix\nB = weighted_adjacency_matrix\nprint(\"The weighted adjacency matrix B is therefore given by: \")\nprint(B)\n\n# We then do the 'power ranking' again with B\n# Find B+B^2\nB_squared = np.linalg.matrix_power(B, 2)\nB_plus_B_squared = B + B_squared\n\n\n# Now we may compute the power of each vertex by summing over all entries in the ith row of A+A^2\nrow_sum_1_weighted = np.sum(B_plus_B_squared[0])\nrow_sum_2_weighted = np.sum(B_plus_B_squared[1])\nrow_sum_3_weighted = np.sum(B_plus_B_squared[2])\nrow_sum_4_weighted = np.sum(B_plus_B_squared[3])\nrow_sum_5_weighted = np.sum(B_plus_B_squared[4])\nrow_sum_6_weighted = np.sum(B_plus_B_squared[5])\nrow_sum_7_weighted = np.sum(B_plus_B_squared[6])\n\nprint(\"Power of vertex 1 is:\")\nprint(row_sum_1_weighted)\nprint(\"Power of vertex 2 is:\")\nprint(row_sum_2_weighted)\nprint(\"Power of vertex 3 is:\")\nprint(row_sum_3_weighted)\nprint(\"Power of vertex 4 is:\")\nprint(row_sum_4_weighted)\nprint(\"Power of vertex 5 is:\")\nprint(row_sum_5_weighted)\nprint(\"Power of vertex 6 is:\")\nprint(row_sum_6_weighted)\nprint(\"Power of vertex 7 is:\")\nprint(row_sum_7_weighted)\n\n# Therefore, we end up with the following \"weighted\" ranking:\n# team 7 &gt; team 5 &gt; team 6 &gt; team 2 &gt; team 3 &gt; team 1 &gt; team 4\n\n\nThe weighted adjacency matrix B is therefore given by: \n[[ 0  4  0 14  0  0  0]\n [ 0  0  0  7  0 18  0]\n [15  7  0 13  7  0  0]\n [ 0  0  0  0  3  0  0]\n [ 7 10  0  0  0 18  7]\n [23  0 13 13  0  0  0]\n [45  6  8 14  0 19  0]]\nPower of vertex 1 is:\n160\nPower of vertex 2 is:\n928\nPower of vertex 3 is:\n820\nPower of vertex 4 is:\n129\nPower of vertex 5 is:\n1944\nPower of vertex 6 is:\n1048\nPower of vertex 7 is:\n2361\n\n\n\nI think that these results are correct. I’d like to see a bit more discussion about the implications of these results. For example, what do the rankings tell us about the teams? How can you explain in English why the rankings change from one method to the next? Which method do you think is best?\nGrade: M"
  },
  {
    "objectID": "HW/projects_2.html#modeling-with-directed-graphs",
    "href": "HW/projects_2.html#modeling-with-directed-graphs",
    "title": "Projects 2",
    "section": "1.1 1. Modeling with Directed Graphs",
    "text": "1.1 1. Modeling with Directed Graphs\nProject Descriptions: This project introduces more applications of digraphs as mathematical modeling tools. You are given that the digraph \\(G\\) has vertex set \\(V=\\{1,2,3,4,5,6\\}\\) and edge set\n\\[\nE=\\{(1,2),(2,3),(3,4),(4,2),(1,4),(3,1),(3,6),(6,3),(4,5),(5,6)\\}\n\\]\nAddress the following points regarding \\(G\\).\n\nDraw a picture of this digraph. You may leave space in your report and draw this by hand, or if you prefer, you may use the computer drawing applications available to you on your system.\nExhibit the incidence matrix \\(A\\) of this digraph and find a basis for \\(\\mathcal{N}(A)\\) using its reduced row echelon form. Some of the basis elements may be algebraic but not directed loops. Use this basis to find a basis of directed loops (e.g., non-directed basis element \\(\\mathbf{c}_{1}\\) might be replaced by directed \\(\\mathbf{c}_{1}+\\mathbf{c}_{2}\\) ).\n\nHints:\n\nA cycle is a group of nodes that connect around in a circle. (The term “loop” is usually reserved for cases where a node connects back to itself with a single edge, although our textbook author uses “loop” broadly to mean “cycle”.)\nIf a graph includes only a cycle, each node must have either a) one incoming and one outgoing edge or b) no edges at all\nIn incidence matrices, incoming edges are represented with -1 and outgoing edges with +1.\nIn an incidence matrix, each row corresponds to all the edges coming in and out of a particular node\n\nTherefore in an the incidence matrix of a digraph which is a cycle, every row must sum to zero\n\nTo find all of the cycles in a graph, we can look for combinations of edges within the graph which are cycles. This corresponds to combinations of columns in the incidence matrix for which all rows sum to zero.\n\nThis is just the nullspace of the incidence graph\n\nthe nullspace is the basis for all solutions to the equation Ax=0\neach column in the nullspace tells us a set of coefficients which can multiply the columns of the incidence matrix so that the rows can sum to zero.\nExample: if our nullspace has a column vector [0,1,0,1]^T, this tells us that column 2 + column 4 of the incidence matrix will sum to zero.\n\nOf course, when we calculate a nullspace, we allow for columns to be added or subtracted. This corresponds to having negative numbers in the column vectors of the nullspace.\nIf we subtract a column in the incidence matrix, this is like switching the directionality of an edge.\nWe can decide if we are interested in allowing cycles formed in this manner!\n\nEach column of the nullspace will be tell the indices of the columns in the incidence graph which represent edges making up a single cycle. Together, they make up a basis for forming all of the cycles present in the graph as a whole.\nA reminder, an “algebraic loop” is a cycle where we don’t care about the directionality of the edges, while a “directed loop” is one where we do respect the directionality.\n\nPick three of the following to complete:\n\nThink of the digraph as representing an electrical circuit where an edge represents some electrical object like a resistor or capacitor. Each node represents the circuit space between these objects. We can attach a potential value to each node, and a voltage, or potential difference, to each edge. The voltage across an edge is the potential value of the head node minus the potential value of the tail node. If we have specified the potentials, then for any cycle in our graph, the sum of the voltages around the cycle will automatically be zero. (This result is called Kirchhoff’s second law of electrical circuits.) However, if we don’t specify the potentials but instead specify the voltages across the edges, then it is possible to choose voltages such that the sum of voltages around a cycle is not zero. This is equivalent to choosing voltages which cannot correspond to any actual set of potentials.\n\nYour goal here is to find conditions that a vector \\(\\mathbf{b}\\) must satisfy in order for it to be a vector of voltages for some unknown distribution of potentials on the nodes. Use the fact (p. 422) that \\(A \\mathbf{x}=\\mathbf{b}\\) implies that for all \\(\\mathbf{y} \\in \\mathcal{N}\\left(A^{T}\\right), \\mathbf{y}^{T} \\mathbf{b}=0\\).\n\nAssume that across each edge of a circuit a current flows. Thus, we can assign to each edge a “weight,” namely the current flow along the edge. This is an example of a weighted digraph. However, not just any set of current weights will do, since Kirchhoff’s first law of circuits says that the total flow of current in and out of any node should be 0 . Use this law to find a matrix condition that must be satisfied by the currents and solve it to exhibit some current flows.\nThink of the digraph as representing a directed communications network. Here loops determine which nodes have bidirectional communication since any two nodes of a loop can only communicate with each other by way of a loop. By examining only a basis of directed loops how could you determine which nodes in the network can communicate with each other?\nThink of vertices of the digraph as representing airports and edges representing flight connections between airports for Gamma Airlines. Suppose further that for each connection there is a maximum number of daily flights that will be allowed by the destination airport from an origin airport and that, in the order that the edges in \\(E\\) are listed above, these limits are\n\\[\nM=\\{4,3,8,7,2,6,7,10,5,8\\} .\n\\]\n\nNow suppose that Gamma wants to maximize the flow of flights into airport 1 and out of airport 6 . Count inflows into an airport as positive and outflows as negative. Assume that the net in/outflow of Gamma flights at each airport 1 to 5 is zero, while the net inflow of such flights into airport 1 matches the net outflow from 6 .\n\nDescribe the problem of maximizing this inflow to airport 1 as a linear programming problem and express it in a standard form (block matrices are helpful.) Note that the appropriate variables are all outflows from one airport to another, i.e., along edges, together with the net inflow into airport 1.\nSolve the problem of part (a). Also solve the reverse problem: Maximize inflow into airport 6 and matching outflow from 1. Explain and justify your answers.\n\n\nWith the same limits on allowable flights into airports as in item 5, suppose that Gamma Airlines wants to determine an allocation of planes that will maximize their profits, given the following constraints: (1) Airports 1 and 6 have repair facilities for their planes, so no limit is placed on the inflow or outflow of their planes other than the airport limits. (2) Flights through airports 2-5 of Gamma planes are pass through, i.e., inflow and outflow must match. (3) Gamma has 32 planes available for this network of airports. (4) The profits per flight in thousands are, in the order that the edges in \\(E\\) are listed above,\n\n\\[\nP=\\{5,6,7,9,10,8,9,5,6,10\\}\n\\] (a) Set this problem up as a linear programming problem in standard form. Clearly identify the variables and explain how the constraints follow.\n\nSolve this problem explicitly and specify the operations taken to do so. Example 3.56 is instructive for this problem, so be aware of it. Use a technology tool that allows you to use elementary operations (ALAMA calculator has this capability)."
  },
  {
    "objectID": "HW/projects_2.html#project-5-image-compression-and-edge-detection",
    "href": "HW/projects_2.html#project-5-image-compression-and-edge-detection",
    "title": "Projects 2",
    "section": "Project 5 Image Compression and Edge Detection",
    "text": "Project 5 Image Compression and Edge Detection\nProblem Description: In this report you will test some limits of data compression by experimenting with an interesting image of your own choosing. It could be a photograph you have taken or some reasonably complex image from the internet that piqued your interest. You are to transform the image into suitable format and then see how much you can can compress that data storage requirements for that image while losing an acceptable amount of detail.\nImplementation Notes: First, you must convert the image to a grayscale format without layers with pixels stored as unsigned eight bit integers. (We are not going to deal with the additional details of color images.) For this you will need an image manipulation program such as the GNU program Gimp or commercial software such as Adobe Photoshop. Figure 4.7(a) is an example of the sort of file you with which you will start experimenting. Secondly, you will need a technology tool capable of importing standard flattened image grayscale images (such as .png, etc.) into matrices and vice versa. The freely available \\(\\mathrm{R}\\) programming language and Octave, as well as commercial Matlab and others are perfectly capable of these tasks. For the record, Figure 4.7 and its relatives in this chapter were converted from .pdf into grayscale .png images via Gimp and read, manipulated and written as matrices via Octave.\nNext, you must apply the Haar wavelet transform repeatedly to your initial image until you reach a blur that is unacceptably far from the initial image. These computations will require a mild bit of programming on your part with the technology tool of your choice. Each application of the transform will reduce storage requirements by a factor of four. How much are you able to save? Of course, “unacceptably far” is in the eye of the beholder, but here’s a pretty reasonable case: Start with a some text and compress it until you can no longer read the text.\nA picture is worth a thousand words, so be lavish with them in your write-up. Consider the amount of savings if, in addition to saving the blurs in all their detail, you were to to save a very good approximation to the edges portion of the transformed picture. For example, consider what you might achieve by first applying some thresholding condition to edge portions of the picture that sets all pixels below a certain level to zero, then accounting for the large number of resulting zeros by some compression technique. You might even suggest a format for such a compression format."
  },
  {
    "objectID": "HW/projects_2.html#least-squares",
    "href": "HW/projects_2.html#least-squares",
    "title": "Projects 2",
    "section": "2.2 3. Least Squares",
    "text": "2.2 3. Least Squares\nPick a league in a sport that you like. For example, you could pick the WNBA for basketball, or a soccer league. Pick a point halfway through last year’s season. You will use the data from the games played up to that point to predict the outcome of the season.\nStart by making a table of scores from the games played thus far: The \\((i, j)\\) th entry is team \\(i\\) ’s score in the game with team \\(j\\). (See below for an example.)\nThen, you will want to use the ideas of least squares and graph theory to predict team ratings and point spreads for the remaining games. The idea is to set up a system of equations that will give you the best fit to the observed scores. You will need to use the ideas of least squares to solve this system of equations. (See below for more details.)\nFinally, write a brief report on your project that describes the problem, your solution to it, its limitations, and the ideas behind it.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\mathrm{CU}\\)\n\\(\\mathrm{IS}\\)\n\\(\\mathrm{KS}\\)\n\\(\\mathrm{KU}\\)\n\\(\\mathrm{MU}\\)\n\\(\\mathrm{NU}\\)\n\\(\\mathrm{OS}\\)\n\\(\\mathrm{OU}\\)\n\n\n\n\n\\(\\mathrm{CU}\\)\n\n24\n\n21\n45\n\n21\n14\n\n\n\\(\\mathrm{IS}\\)\n12\n\n\n42\n21\n16\n\n7\n\n\n\\(\\mathrm{KS}\\)\n\n\n\n12\n21\n3\n27\n24\n\n\n\\(\\mathrm{KU}\\)\n9\n14\n30\n\n\n10\n\n14\n\n\n\\(\\mathrm{MU}\\)\n8\n3\n52\n\n\n18\n21\n\n\n\n\\(\\mathrm{NU}\\)\n\n51\n48\n63\n26\n\n63\n\n\n\n\\(\\mathrm{OS}\\)\n41\n\n45\n\n49\n42\n\n28\n\n\n\\(\\mathrm{OU}\\)\n17\n35\n70\n63\n\n\n31\n\n\n\n\nImplementation Notes: You will need to set up a suitable system of equations, form the normal equations, and have a technology tool solve the problem. The equations in question are formed by letting the variables be a vector x of “potentials” \\(x(i)\\), one for each team \\(i\\), so that the “potential differences” best approximate the actual score differences (i.e., point spreads) of the games. To find the vector \\(\\mathbf{x}\\) of potentials you solve the system \\(A \\mathbf{x}=\\mathbf{b}\\), where \\(\\mathbf{b}\\) is the vector of observed potential differences. N.B: the matrix \\(A\\) is not the table given above. You will get one equation for each game played. For example, by checking the \\((1,2)\\) th and \\((2,1)\\) th entries, we see that \\(\\mathrm{CU}\\) beat IS by a score of 24 to 12 . So the resulting equation for this game is \\(x(1)-x(2)=24-12=12\\). Ideally, the resulting potentials would give numbers that would enable you to predict the point spread of an as yet unplayed game: all you would have to do to determine the spread for team \\(i\\) versus team \\(j\\) is calculate the difference \\(x(j)-x(i)\\). Of course, it doesn’t really work out this way, but this is a reasonable use of the known data. When you set up this system, you obtain an inconsistent system. This is where least squares enter the picture. You will need to set up and solve the normal equations, one way or another. You might notice that the null space of the resulting coefficient matrix is nontrivial, so this matrix does not have full column rank. This makes sense: potentials are unique only up to a constant. To fix this, you could arbitrarily fix the value of one team’s potential, that is, set the weakest team’s potential value to zero by adding one additional equation to the system of the form \\(x(i)=0\\)."
  },
  {
    "objectID": "HW/projects_2.sol.html#modeling-with-directed-graphs",
    "href": "HW/projects_2.sol.html#modeling-with-directed-graphs",
    "title": "Projects 2 Solutions",
    "section": "1.1 1. Modeling with Directed Graphs",
    "text": "1.1 1. Modeling with Directed Graphs\nProject Descriptions: This project introduces more applications of digraphs as mathematical modeling tools. You are given that the digraph \\(G\\) has vertex set \\(V=\\{1,2,3,4,5,6\\}\\) and edge set\n\\[\nE=\\{(1,2),(2,3),(3,4),(4,2),(1,4),(3,1),(3,6),(6,3),(4,5),(5,6)\\}\n\\]\nAddress the following points regarding \\(G\\).\n\nDraw a picture of this digraph. You may leave space in your report and draw this by hand, or if you prefer, you may use the computer drawing applications available to you on your system.\nExhibit the incidence matrix \\(A\\) of this digraph and find a basis for \\(\\mathcal{N}(A)\\) using its reduced row echelon form. Some of the basis elements may be algebraic but not directed loops. Use this basis to find a basis of directed loops (e.g., non-directed basis element \\(\\mathbf{c}_{1}\\) might be replaced by directed \\(\\mathbf{c}_{1}+\\mathbf{c}_{2}\\) ).\n\nHints:\n\nA cycle is a group of nodes that connect around in a circle. (The term “loop” is usually reserved for cases where a node connects back to itself with a single edge, although our textbook author uses “loop” broadly to mean “cycle”.)\nIf a graph includes only a cycle, each node must have either a) one incoming and one outgoing edge or b) no edges at all\nIn incidence matrices, incoming edges are represented with -1 and outgoing edges with +1.\nIn an incidence matrix, each row corresponds to all the edges coming in and out of a particular node\n\nTherefore in an the incidence matrix of a digraph which is a cycle, every row must sum to zero\n\nTo find all of the cycles in a graph, we can look for combinations of edges within the graph which are cycles. This corresponds to combinations of columns in the incidence matrix for which all rows sum to zero.\n\nThis is just the nullspace of the incidence graph\n\nthe nullspace is the basis for all solutions to the equation Ax=0\neach column in the nullspace tells us a set of coefficients which can multiply the columns of the incidence matrix so that the rows can sum to zero.\nExample: if our nullspace has a column vector [0,1,0,1]^T, this tells us that column 2 + column 4 of the incidence matrix will sum to zero.\n\nOf course, when we calculate a nullspace, we allow for columns to be added or subtracted. This corresponds to having negative numbers in the column vectors of the nullspace.\nIf we subtract a column in the incidence matrix, this is like switching the directionality of an edge.\nWe can decide if we are interested in allowing cycles formed in this manner!\n\nEach column of the nullspace will be tell the indices of the columns in the incidence graph which represent edges making up a single cycle. Together, they make up a basis for forming all of the cycles present in the graph as a whole.\nA reminder, an “algebraic loop” is a cycle where we don’t care about the directionality of the edges, while a “directed loop” is one where we do respect the directionality.\n\nPick three of the following to complete:\n\nThink of the digraph as representing an electrical circuit where an edge represents some electrical object like a resistor or capacitor. Each node represents the circuit space between these objects. We can attach a potential value to each node, and a voltage, or potential difference, to each edge. The voltage across an edge is the potential value of the head node minus the potential value of the tail node. If we have specified the potentials, then for any cycle in our graph, the sum of the voltages around the cycle will automatically be zero. (This result is called Kirchhoff’s second law of electrical circuits.) However, if we don’t specify the potentials but instead specify the voltages across the edges, then it is possible to choose voltages such that the sum of voltages around a cycle is not zero. This is equivalent to choosing voltages which cannot correspond to any actual set of potentials.\n\nYour goal here is to find conditions that a vector \\(\\mathbf{b}\\) must satisfy in order for it to be a vector of voltages for some unknown distribution of potentials on the nodes. Use the fact (p. 422) that \\(A \\mathbf{x}=\\mathbf{b}\\) implies that for all \\(\\mathbf{y} \\in \\mathcal{N}\\left(A^{T}\\right), \\mathbf{y}^{T} \\mathbf{b}=0\\).\n\nAssume that across each edge of a circuit a current flows. Thus, we can assign to each edge a “weight,” namely the current flow along the edge. This is an example of a weighted digraph. However, not just any set of current weights will do, since Kirchhoff’s first law of circuits says that the total flow of current in and out of any node should be 0 . Use this law to find a matrix condition that must be satisfied by the currents and solve it to exhibit some current flows.\nThink of the digraph as representing a directed communications network. Here loops determine which nodes have bidirectional communication since any two nodes of a loop can only communicate with each other by way of a loop. By examining only a basis of directed loops how could you determine which nodes in the network can communicate with each other?\nThink of vertices of the digraph as representing airports and edges representing flight connections between airports for Gamma Airlines. Suppose further that for each connection there is a maximum number of daily flights that will be allowed by the destination airport from an origin airport and that, in the order that the edges in \\(E\\) are listed above, these limits are\n\\[\nM=\\{4,3,8,7,2,6,7,10,5,8\\} .\n\\]\n\nNow suppose that Gamma wants to maximize the flow of flights into airport 1 and out of airport 6 . Count inflows into an airport as positive and outflows as negative. Assume that the net in/outflow of Gamma flights at each airport 1 to 5 is zero, while the net inflow of such flights into airport 1 matches the net outflow from 6 .\n\nDescribe the problem of maximizing this inflow to airport 1 as a linear programming problem and express it in a standard form (block matrices are helpful.) Note that the appropriate variables are all outflows from one airport to another, i.e., along edges, together with the net inflow into airport 1.\nSolve the problem of part (a). Also solve the reverse problem: Maximize inflow into airport 6 and matching outflow from 1. Explain and justify your answers.\n\n\nWith the same limits on allowable flights into airports as in item 5, suppose that Gamma Airlines wants to determine an allocation of planes that will maximize their profits, given the following constraints: (1) Airports 1 and 6 have repair facilities for their planes, so no limit is placed on the inflow or outflow of their planes other than the airport limits. (2) Flights through airports 2-5 of Gamma planes are pass through, i.e., inflow and outflow must match. (3) Gamma has 32 planes available for this network of airports. (4) The profits per flight in thousands are, in the order that the edges in \\(E\\) are listed above,\n\n\\[\nP=\\{5,6,7,9,10,8,9,5,6,10\\}\n\\] (a) Set this problem up as a linear programming problem in standard form. Clearly identify the variables and explain how the constraints follow.\n\nSolve this problem explicitly and specify the operations taken to do so. Example 3.56 is instructive for this problem, so be aware of it. Use a technology tool that allows you to use elementary operations (ALAMA calculator has this capability)."
  },
  {
    "objectID": "HW/projects_2.sol.html#project-5-image-compression-and-edge-detection",
    "href": "HW/projects_2.sol.html#project-5-image-compression-and-edge-detection",
    "title": "Projects 2 Solutions",
    "section": "Project 5 Image Compression and Edge Detection",
    "text": "Project 5 Image Compression and Edge Detection\nProblem Description: In this report you will test some limits of data compression by experimenting with an interesting image of your own choosing. It could be a photograph you have taken or some reasonably complex image from the internet that piqued your interest. You are to transform the image into suitable format and then see how much you can can compress that data storage requirements for that image while losing an acceptable amount of detail.\nImplementation Notes: First, you must convert the image to a grayscale format without layers with pixels stored as unsigned eight bit integers. (We are not going to deal with the additional details of color images.) For this you will need an image manipulation program such as the GNU program Gimp or commercial software such as Adobe Photoshop. Figure 4.7(a) is an example of the sort of file you with which you will start experimenting. Secondly, you will need a technology tool capable of importing standard flattened image grayscale images (such as .png, etc.) into matrices and vice versa. The freely available \\(\\mathrm{R}\\) programming language and Octave, as well as commercial Matlab and others are perfectly capable of these tasks. For the record, Figure 4.7 and its relatives in this chapter were converted from .pdf into grayscale .png images via Gimp and read, manipulated and written as matrices via Octave.\nNext, you must apply the Haar wavelet transform repeatedly to your initial image until you reach a blur that is unacceptably far from the initial image. These computations will require a mild bit of programming on your part with the technology tool of your choice. Each application of the transform will reduce storage requirements by a factor of four. How much are you able to save? Of course, “unacceptably far” is in the eye of the beholder, but here’s a pretty reasonable case: Start with a some text and compress it until you can no longer read the text.\nA picture is worth a thousand words, so be lavish with them in your write-up. Consider the amount of savings if, in addition to saving the blurs in all their detail, you were to to save a very good approximation to the edges portion of the transformed picture. For example, consider what you might achieve by first applying some thresholding condition to edge portions of the picture that sets all pixels below a certain level to zero, then accounting for the large number of resulting zeros by some compression technique. You might even suggest a format for such a compression format."
  },
  {
    "objectID": "HW/projects_2.sol.html#least-squares",
    "href": "HW/projects_2.sol.html#least-squares",
    "title": "Projects 2 Solutions",
    "section": "2.2 3. Least Squares",
    "text": "2.2 3. Least Squares\nPick a league in a sport that you like. For example, you could pick the WNBA for basketball, or a soccer league. Pick a point halfway through last year’s season. You will use the data from the games played up to that point to predict the outcome of the season.\nStart by making a table of scores from the games played thus far: The \\((i, j)\\) th entry is team \\(i\\) ’s score in the game with team \\(j\\). (See below for an example.)\nThen, you will want to use the ideas of least squares and graph theory to predict team ratings and point spreads for the remaining games. The idea is to set up a system of equations that will give you the best fit to the observed scores. You will need to use the ideas of least squares to solve this system of equations. (See below for more details.)\nFinally, write a brief report on your project that describes the problem, your solution to it, its limitations, and the ideas behind it.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\mathrm{CU}\\)\n\\(\\mathrm{IS}\\)\n\\(\\mathrm{KS}\\)\n\\(\\mathrm{KU}\\)\n\\(\\mathrm{MU}\\)\n\\(\\mathrm{NU}\\)\n\\(\\mathrm{OS}\\)\n\\(\\mathrm{OU}\\)\n\n\n\n\n\\(\\mathrm{CU}\\)\n\n24\n\n21\n45\n\n21\n14\n\n\n\\(\\mathrm{IS}\\)\n12\n\n\n42\n21\n16\n\n7\n\n\n\\(\\mathrm{KS}\\)\n\n\n\n12\n21\n3\n27\n24\n\n\n\\(\\mathrm{KU}\\)\n9\n14\n30\n\n\n10\n\n14\n\n\n\\(\\mathrm{MU}\\)\n8\n3\n52\n\n\n18\n21\n\n\n\n\\(\\mathrm{NU}\\)\n\n51\n48\n63\n26\n\n63\n\n\n\n\\(\\mathrm{OS}\\)\n41\n\n45\n\n49\n42\n\n28\n\n\n\\(\\mathrm{OU}\\)\n17\n35\n70\n63\n\n\n31\n\n\n\n\nImplementation Notes: You will need to set up a suitable system of equations, form the normal equations, and have a technology tool solve the problem. The equations in question are formed by letting the variables be a vector x of “potentials” \\(x(i)\\), one for each team \\(i\\), so that the “potential differences” best approximate the actual score differences (i.e., point spreads) of the games. To find the vector \\(\\mathbf{x}\\) of potentials you solve the system \\(A \\mathbf{x}=\\mathbf{b}\\), where \\(\\mathbf{b}\\) is the vector of observed potential differences. N.B: the matrix \\(A\\) is not the table given above. You will get one equation for each game played. For example, by checking the \\((1,2)\\) th and \\((2,1)\\) th entries, we see that \\(\\mathrm{CU}\\) beat IS by a score of 24 to 12 . So the resulting equation for this game is \\(x(1)-x(2)=24-12=12\\). Ideally, the resulting potentials would give numbers that would enable you to predict the point spread of an as yet unplayed game: all you would have to do to determine the spread for team \\(i\\) versus team \\(j\\) is calculate the difference \\(x(j)-x(i)\\). Of course, it doesn’t really work out this way, but this is a reasonable use of the known data. When you set up this system, you obtain an inconsistent system. This is where least squares enter the picture. You will need to set up and solve the normal equations, one way or another. You might notice that the null space of the resulting coefficient matrix is nontrivial, so this matrix does not have full column rank. This makes sense: potentials are unique only up to a constant. To fix this, you could arbitrarily fix the value of one team’s potential, that is, set the weakest team’s potential value to zero by adding one additional equation to the system of the form \\(x(i)=0\\)."
  },
  {
    "objectID": "HW/projects_2.sol.html#image-compression-and-edge-detection",
    "href": "HW/projects_2.sol.html#image-compression-and-edge-detection",
    "title": "Projects 2 Solutions",
    "section": "2.1 2. Image Compression and Edge Detection",
    "text": "2.1 2. Image Compression and Edge Detection\nProblem Description: In this report you will test some limits of data compression by experimenting with an interesting image of your own choosing. It could be a photograph you have taken or some reasonably complex image from the internet that piqued your interest. You are to transform the image into suitable format and then see how much you can can compress that data storage requirements for that image while losing an acceptable amount of detail.\nImplementation Notes: First, you must convert the image to a grayscale format without layers with pixels stored as unsigned eight bit integers. (We are not going to deal with the additional details of color images.) For this you will need an image manipulation program such as the GNU program Gimp or commercial software such as Adobe Photoshop. Figure 4.7(a) is an example of the sort of file you with which you will start experimenting. Secondly, you will need a technology tool capable of importing standard flattened image grayscale images (such as .png, etc.) into matrices and vice versa. The freely available \\(\\mathrm{R}\\) programming language and Octave, as well as commercial Matlab and others are perfectly capable of these tasks. For the record, Figure 4.7 and its relatives in this chapter were converted from .pdf into grayscale .png images via Gimp and read, manipulated and written as matrices via Octave.\nNext, you must apply the Haar wavelet transform repeatedly to your initial image until you reach a blur that is unacceptably far from the initial image. These computations will require a mild bit of programming on your part with the technology tool of your choice. Each application of the transform will reduce storage requirements by a factor of four. How much are you able to save? Of course, “unacceptably far” is in the eye of the beholder, but here’s a pretty reasonable case: Start with a some text and compress it until you can no longer read the text.\nA picture is worth a thousand words, so be lavish with them in your write-up. Consider the amount of savings if, in addition to saving the blurs in all their detail, you were to to save a very good approximation to the edges portion of the transformed picture. For example, consider what you might achieve by first applying some thresholding condition to edge portions of the picture that sets all pixels below a certain level to zero, then accounting for the large number of resulting zeros by some compression technique. You might even suggest a format for such a compression format."
  },
  {
    "objectID": "HW/projects_2.html#image-compression-and-edge-detection",
    "href": "HW/projects_2.html#image-compression-and-edge-detection",
    "title": "Projects 2",
    "section": "2.1 2. Image Compression and Edge Detection",
    "text": "2.1 2. Image Compression and Edge Detection\nProblem Description: In this report you will test some limits of data compression by experimenting with an interesting image of your own choosing. It could be a photograph you have taken or some reasonably complex image from the internet that piqued your interest. You are to transform the image into suitable format and then see how much you can can compress that data storage requirements for that image while losing an acceptable amount of detail.\nImplementation Notes: First, you must convert the image to a grayscale format without layers with pixels stored as unsigned eight bit integers. (We are not going to deal with the additional details of color images.) For this you will need an image manipulation program such as the GNU program Gimp or commercial software such as Adobe Photoshop. Figure 4.7(a) is an example of the sort of file you with which you will start experimenting. Secondly, you will need a technology tool capable of importing standard flattened image grayscale images (such as .png, etc.) into matrices and vice versa. The freely available \\(\\mathrm{R}\\) programming language and Octave, as well as commercial Matlab and others are perfectly capable of these tasks. For the record, Figure 4.7 and its relatives in this chapter were converted from .pdf into grayscale .png images via Gimp and read, manipulated and written as matrices via Octave.\nNext, you must apply the Haar wavelet transform repeatedly to your initial image until you reach a blur that is unacceptably far from the initial image. These computations will require a mild bit of programming on your part with the technology tool of your choice. Each application of the transform will reduce storage requirements by a factor of four. How much are you able to save? Of course, “unacceptably far” is in the eye of the beholder, but here’s a pretty reasonable case: Start with a some text and compress it until you can no longer read the text.\nA picture is worth a thousand words, so be lavish with them in your write-up. Consider the amount of savings if, in addition to saving the blurs in all their detail, you were to to save a very good approximation to the edges portion of the transformed picture. For example, consider what you might achieve by first applying some thresholding condition to edge portions of the picture that sets all pixels below a certain level to zero, then accounting for the large number of resulting zeros by some compression technique. You might even suggest a format for such a compression format."
  },
  {
    "objectID": "HW/HW4.html",
    "href": "HW/HW4.html",
    "title": "Homework 4",
    "section": "",
    "text": "1 \nFor the following matrices find the null space of \\(I-A\\) and find state vectors with nonnegative entries that sum to 1 in the null space, if any. Are these matrices stable (yes/no)?\n\n\\(A=\\left[\\begin{array}{rrr}0.5 & 0 & 1 \\\\ 0.5 & 0.5 & 0 \\\\ 0 & 0.5 & 0\\end{array}\\right]\\)\n\\(A=\\left[\\begin{array}{ll}0 & 1 \\\\ 1 & 0\\end{array}\\right]\\)\n\n\n\n2 \nFind bases for the row, column, and null space of each of the following matrices. (a) \\([2,0,-1]\\) (b) \\(\\left[\\begin{array}{lllll}1 & 2 & 0 & 0 & 1 \\\\ 1 & 2 & 1 & 1 & 1 \\\\ 3 & 6 & 2 & 2 & 3\\end{array}\\right]\\) (c) \\(\\left[\\begin{array}{ccccc}1 & 2 & 0 & 4 & 0 \\\\ 1 & 3 & 5 & 2 & 1 \\\\ 2 & 3 & -5 & 10 & 0 \\\\ 2 & 4 & 0 & 8 & 1\\end{array}\\right]\\) (d) \\(\\left[\\begin{array}{rrr}2 & -3 & -1 \\\\ 0 & 2 & 0 \\\\ 2 & 4 & 1\\end{array}\\right]\\)\n\n\n3 \nFind all possible linear combinations with value zero of the following sets of vectors and the dimension of the space spanned by them.\n\n\\((0,1,1),(2,0,1),(2,2,3),(0,2,2)\\) in \\(\\mathbb{R}^{3}\\).\n\\(x, x^{2}+x, x^{2}-x\\) in \\(\\mathcal{R}_{2}\\).\n\\((1,1,2,2),(0,2,0,2),(1,0,2,1),(2,1,4,4)\\) in \\(\\mathbb{R}^{4}\\).\n\n\n\n4 \nLet \\(A=\\left[\\begin{array}{lllll}0 & 1 & 0 & 1 & 2 \\\\ 1 & 0 & 2 & 1 & 2 \\\\ 2 & 2 & 4 & 4 & 8\\end{array}\\right]\\). Use the column space algorithm on the matrix \\([A I]\\) to find a basis \\(B\\) of \\(\\mathcal{C}(A)\\) and to expand it to a basis of \\(\\mathbb{R}^{3}\\).\n\n\n5 \nFor the following orthogonal pairs \\(\\mathbf{u}, \\mathbf{v}\\) and matrix \\(M=\\left[\\begin{array}{rrr}1 & 0 & -1 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1\\end{array}\\right]\\), determine whether \\(M \\mathbf{u}\\) and \\(M \\mathbf{v}\\) are orthogonal.\n\n\\((2,1,1),(1,0,-2)\\)\n\\((0,1,1),(1,-1,1)\\)\n\\((3,1,-2),(1,3,3)\\)\n\n\n\n6 \nFind equations for the following planes in \\(\\mathbb{R}^{3}\\).\n\nThe plane containing the points \\((1,1,2),(-1,3,2),(2,4,3)\\).\nThe plane containing the points \\((-2,1,1)\\) and \\((0,1,2)\\) and orthogonal to the plane \\(2 x-y+z=3\\).\n\n\n\n7 \nFind equations for the following hyperplanes in \\(\\mathbb{R}^{4}\\).\n\nThe plane parallel to the plane \\(2 x_{1}+x_{2}-3 x_{3}+x_{4}=2\\) and containing the point \\((2,1,1,3)\\).\nThe plane through the origin and orthogonal to the vector \\((1,0,2,1)\\).\n\n\n\n8 \nFor each pair \\(A, \\mathbf{b}\\), solve the normal equations for the system \\(A \\mathbf{x}=\\mathbf{b}\\) and find the residual vector and its norm. Are there any genuine solutions to the system?\n\n\\(\\left[\\begin{array}{ll}1 & 3 \\\\ 1 & 0\\end{array}\\right],\\left[\\begin{array}{l}1 \\\\ 3\\end{array}\\right]\\)\n\\(\\left[\\begin{array}{rr}2 & -2 \\\\ 1 & 1 \\\\ 3 & 1\\end{array}\\right],\\left[\\begin{array}{r}2 \\\\ -1 \\\\ 1\\end{array}\\right]\\)\n\\(\\left[\\begin{array}{rrr}0 & 2 & 2 \\\\ 1 & 1 & 0 \\\\ -1 & 1 & 2 \\\\ 1 & -2 & -3\\end{array}\\right],\\left[\\begin{array}{l}3 \\\\ 1 \\\\ 0 \\\\ 0\\end{array}\\right]\\)\n\n\n\n9 \n(Text retrieval) You are given the following term-by-document matrix, that is, a matrix whose \\((i, j)\\) th entry is the number of times term \\(t_{i}\\) occurs in document \\(D_{j}\\). Columns of this matrix are document vectors, as are queries. We measure the quality of a match between query and document by the cosine of the angle \\(\\theta\\) between the two vectors, larger cosine being better. Which of the following nine documents \\(D_{i}\\) matches the query \\((0,1,0,1,1)\\) above the threshold value \\(\\cos \\theta \\geq 0.5\\) ? Which is the best match to the query?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(D_{1}\\)\n\\(D_{2}\\)\n\\(D_{3}\\)\n\\(D_{4}\\)\n\\(D_{5}\\)\n\\(D_{6}\\)\n\\(D_{7}\\)\n\\(D_{8}\\)\n\\(D_{9}\\)\n\n\n\n\n\\(t_{1}\\)\n1\n1\n2\n0\n1\n0\n1\n0\n1\n\n\n\\(t_{2}\\)\n0\n1\n0\n1\n0\n1\n1\n0\n0\n\n\n\\(t_{3}\\)\n0\n2\n0\n2\n0\n1\n0\n1\n1\n\n\n\\(t_{4}\\)\n1\n0\n1\n0\n1\n0\n2\n1\n0\n\n\n\\(t_{5}\\)\n1\n2\n1\n0\n0\n1\n0\n0\n1"
  },
  {
    "objectID": "HW/HW4.sol.html",
    "href": "HW/HW4.sol.html",
    "title": "Homework 4 Solutions",
    "section": "",
    "text": "1 \nFor the following matrices find the null space of \\(I-A\\) and find state vectors with nonnegative entries that sum to 1 in the null space, if any. Are these matrices stable (yes/no)?\n\n\\(A=\\left[\\begin{array}{rrr}0.5 & 0 & 1 \\\\ 0.5 & 0.5 & 0 \\\\ 0 & 0.5 & 0\\end{array}\\right]\\)\n\\(A=\\left[\\begin{array}{ll}0 & 1 \\\\ 1 & 0\\end{array}\\right]\\)\n\n\nExercise 3.4.7\n\n\\(\\operatorname{span}\\{(2,2,1)\\},\\left(\\frac{2}{5}, \\frac{2}{5}, \\frac{1}{5}\\right)\\), yes\n\\(\\operatorname{span}\\{(1,1)\\},\\left(\\frac{1}{2}, \\frac{1}{2}\\right)\\), no\n\n\n\n\n2 \nFind bases for the row, column, and null space of each of the following matrices. (a) \\([2,0,-1]\\) (b) \\(\\left[\\begin{array}{lllll}1 & 2 & 0 & 0 & 1 \\\\ 1 & 2 & 1 & 1 & 1 \\\\ 3 & 6 & 2 & 2 & 3\\end{array}\\right]\\) (c) \\(\\left[\\begin{array}{ccccc}1 & 2 & 0 & 4 & 0 \\\\ 1 & 3 & 5 & 2 & 1 \\\\ 2 & 3 & -5 & 10 & 0 \\\\ 2 & 4 & 0 & 8 & 1\\end{array}\\right]\\) (d) \\(\\left[\\begin{array}{rrr}2 & -3 & -1 \\\\ 0 & 2 & 0 \\\\ 2 & 4 & 1\\end{array}\\right]\\)\n\nExercise 3.6.5\nBases for row, column, and null spaces:\n\n\\(\\{(2,0,-1)\\},\\{1\\}\\), \\(\\left\\{\\left(\\frac{1}{2}, 0,1\\right),(0,1,0)\\right\\}\\)\n\\((1,2,0,0,1),(0,0,1,1,0)\\},\\{(1,1,3),(0,1,2)\\}\\), \\(\\{(-2,1,0,0,0),(0,0,-1,1,0),(-1,0,0,0,1)\\}\\)\n\\(\\{(1,0,-10,8,0),(0,1,5,-2,0),(0,0,0,0,1)\\}\\), \\(\\{(1,1,2,2),(2,3,3,4),(0,1,0,1)\\},\\{(10,-5,1,0,0),(-8,2,0,1,0)\\}\\) (d) \\(\\left\\{\\mathbf{e}_{1}, \\mathbf{e}_{2}, \\mathbf{e}_{3}\\right\\}\\), \\(\\left\\{\\mathbf{e}_{1}, \\mathbf{e}_{2}, \\mathbf{e}_{3}\\right\\},\\{\\}\\)\n\n\n\n\n3 \nFind all possible linear combinations with value zero of the following sets of vectors and the dimension of the space spanned by them.\n\n\\((0,1,1),(2,0,1),(2,2,3),(0,2,2)\\) in \\(\\mathbb{R}^{3}\\).\n\\(x, x^{2}+x, x^{2}-x\\) in \\(\\mathcal{R}_{2}\\).\n\\((1,1,2,2),(0,2,0,2),(1,0,2,1),(2,1,4,4)\\) in \\(\\mathbb{R}^{4}\\).\n\n\nExercise 3.6.7\n\n\\(c_{1} \\mathbf{v}_{1}+c_{2} \\mathbf{v}_{2}+c_{3} \\mathbf{v}_{3}+c_{4} \\mathbf{v}_{4}=\\mathbf{0}\\), where \\(c_{1}=-2 c_{3}-2 c_{4}\\), \\(c_{2}=-c_{3}\\), and \\(c_{3}, c_{4}\\) are free, dim \\(\\operatorname{span}\\left\\{\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\mathbf{v}_{3}, \\mathbf{v}_{4}\\right\\}=2\\)\n\\(c_{1} x+\\) \\(c_{2}\\left(x^{2}+x\\right)+c_{3}\\left(x^{2}-x\\right)=0\\) where \\(c_{1}=2 c_{3}, c_{2}=-c_{3}\\), and \\(c_{3}\\) is free, dim span \\(\\left\\{x, x^{2}+x, x^{2}-x\\right\\}=2\\)\n\\(c_{1} \\mathbf{v}_{1}+c_{2} \\mathbf{v}_{2}+c_{3} \\mathbf{v}_{3}+c_{4} \\mathbf{v}_{4}=\\mathbf{0}\\), where \\(c_{1}=-c_{3}, c_{2}=\\frac{1}{2} c_{3}, c_{4}=0\\) and \\(c_{3}\\) is free, dim \\(\\operatorname{span}\\left\\{\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\mathbf{v}_{3}, \\mathbf{v}_{4}\\right\\}=3\\)\n\n\n\n\n4 \nLet \\(A=\\left[\\begin{array}{lllll}0 & 1 & 0 & 1 & 2 \\\\ 1 & 0 & 2 & 1 & 2 \\\\ 2 & 2 & 4 & 4 & 8\\end{array}\\right]\\). Use the column space algorithm on the matrix \\([A I]\\) to find a basis \\(B\\) of \\(\\mathcal{C}(A)\\) and to expand it to a basis of \\(\\mathbb{R}^{3}\\).\n\nExercise 3.6.13\n\\([A I]\\) has RREF \\(\\left[\\begin{array}{llllllcc}1 & 0 & 2 & 1 & 2 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 1 & 2 & 0 & -1 & \\frac{1}{2} \\\\ 0 & 0 & 0 & 0 & 0 & 1 & 1 & -\\frac{1}{2}\\end{array}\\right]\\), so a basis of \\(\\mathcal{C}(A)\\) is \\(B=\\)\n\\(\\{(0,1,2),(1,0,2)\\}\\) and can be expanded to the basis \\(\\{(0,1,2),(1,0,2),(1,0,0)\\}\\) of \\(\\mathbb{R}^{3}\\) according to the column space algorithm.\n\n\n\n5 \nFor the following orthogonal pairs \\(\\mathbf{u}, \\mathbf{v}\\) and matrix \\(M=\\left[\\begin{array}{rrr}1 & 0 & -1 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1\\end{array}\\right]\\), determine whether \\(M \\mathbf{u}\\) and \\(M \\mathbf{v}\\) are orthogonal.\n\n\\((2,1,1),(1,0,-2)\\)\n\\((0,1,1),(1,-1,1)\\)\n\\((3,1,-2),(1,3,3)\\)\n\n\n4.2.7\n\n\\((M \\mathbf{u}) \\cdot(M \\mathbf{v})=1\\), no\n\\((M \\mathbf{u}) \\cdot(M \\mathbf{v})=0\\), yes\n\\((M \\mathbf{u})\\). \\((M \\mathbf{v})=-13\\), no\n\n\n\n\n6 \nFind equations for the following planes in \\(\\mathbb{R}^{3}\\).\n\nThe plane containing the points \\((1,1,2),(-1,3,2),(2,4,3)\\).\nThe plane containing the points \\((-2,1,1)\\) and \\((0,1,2)\\) and orthogonal to the plane \\(2 x-y+z=3\\).\n\n\nExercise 4.2.9\n\n\\(x+y-4 z=-6\\)\n\\(x-2 z=-4\\)\n\n\n\n\n7 \nFind equations for the following hyperplanes in \\(\\mathbb{R}^{4}\\).\n\nThe plane parallel to the plane \\(2 x_{1}+x_{2}-3 x_{3}+x_{4}=2\\) and containing the point \\((2,1,1,3)\\).\nThe plane through the origin and orthogonal to the vector \\((1,0,2,1)\\).\n\n\nExercise 4.2.10\n\n\\(2 x_{1}+x_{2}-3 x_{3}+x_{4}=5\\)\n\\(x_{1}+2 x_{3}+x_{4}=0\\)\n\n\n\n\n8 \nFor each pair \\(A, \\mathbf{b}\\), solve the normal equations for the system \\(A \\mathbf{x}=\\mathbf{b}\\) and find the residual vector and its norm. Are there any genuine solutions to the system?\n\n\\(\\left[\\begin{array}{ll}1 & 3 \\\\ 1 & 0\\end{array}\\right],\\left[\\begin{array}{l}1 \\\\ 3\\end{array}\\right]\\)\n\\(\\left[\\begin{array}{rr}2 & -2 \\\\ 1 & 1 \\\\ 3 & 1\\end{array}\\right],\\left[\\begin{array}{r}2 \\\\ -1 \\\\ 1\\end{array}\\right]\\)\n\\(\\left[\\begin{array}{rrr}0 & 2 & 2 \\\\ 1 & 1 & 0 \\\\ -1 & 1 & 2 \\\\ 1 & -2 & -3\\end{array}\\right],\\left[\\begin{array}{l}3 \\\\ 1 \\\\ 0 \\\\ 0\\end{array}\\right]\\)\n\n\nExercise 4.2.11\n\n\\(\\mathbf{x}=\\left(3,-\\frac{2}{3}\\right), \\mathbf{b}-A \\mathbf{x}=\\mathbf{0},\\|\\mathbf{b}-A \\mathbf{x}\\|=0\\), yes\n\\(\\mathbf{x}=\\) \\(\\frac{1}{21}(9,-14), \\mathbf{b}-A \\mathbf{x}=\\frac{1}{21}(-4,-16,8),\\|\\mathbf{b}-A \\mathbf{x}\\|=\\frac{\\sqrt{336}}{21}\\), по\n\\(\\mathbf{x}=\\) \\(\\left(x_{3}+\\frac{12}{13},-x_{3}+\\frac{23}{26}, x_{3}\\right)\\) where \\(x_{3}\\) is free, \\(\\mathbf{b}-A \\mathbf{x}=\\frac{1}{26}(32,-21,1,22),\\|\\mathbf{b}-A \\mathbf{x}\\|=\\) \\(\\frac{\\sqrt{1950}}{26}\\), no\n\n\n\n\n9 \n(Text retrieval) You are given the following term-by-document matrix, that is, a matrix whose \\((i, j)\\) th entry is the number of times term \\(t_{i}\\) occurs in document \\(D_{j}\\). Columns of this matrix are document vectors, as are queries. We measure the quality of a match between query and document by the cosine of the angle \\(\\theta\\) between the two vectors, larger cosine being better. Which of the following nine documents \\(D_{i}\\) matches the query \\((0,1,0,1,1)\\) above the threshold value \\(\\cos \\theta \\geq 0.5\\) ? Which is the best match to the query?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(D_{1}\\)\n\\(D_{2}\\)\n\\(D_{3}\\)\n\\(D_{4}\\)\n\\(D_{5}\\)\n\\(D_{6}\\)\n\\(D_{7}\\)\n\\(D_{8}\\)\n\\(D_{9}\\)\n\n\n\n\n\\(t_{1}\\)\n1\n1\n2\n0\n1\n0\n1\n0\n1\n\n\n\\(t_{2}\\)\n0\n1\n0\n1\n0\n1\n1\n0\n0\n\n\n\\(t_{3}\\)\n0\n2\n0\n2\n0\n1\n0\n1\n1\n\n\n\\(t_{4}\\)\n1\n0\n1\n0\n1\n0\n2\n1\n0\n\n\n\\(t_{5}\\)\n1\n2\n1\n0\n0\n1\n0\n0\n1\n\n\n\n\nExercise 4.2.14\nDocument 7 is the best match, with a cosine of \\(\\frac{3}{\\sqrt{6} \\sqrt{3}} \\approx 0.70711\\)."
  },
  {
    "objectID": "submitted/projects_1/project1.html",
    "href": "submitted/projects_1/project1.html",
    "title": "Project 1",
    "section": "",
    "text": "Here is the system of equations that represents the scenario.\n\nHere is the transition matrix. The market shares after 3 years are shown below given that the initial market shares are equal, which meant that the initial vector is [1/3, 1/3, 1/3].\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0.5 & 0.2 & 0.3\\\\0.2 & 0.6 & 0.1\\\\0.3 & 0.2 & 0.6\\end{matrix}\\right]\\)\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0.339\\\\0.275333333333333\\\\0.385666666666667\\end{matrix}\\right]\\)\n\n\n\n\n\nAs seen in the below graphs, the effect of launching campaign 2 is better than campaign 1. Note that campaign 0 shows what would happen if no campiagn is launched. In that scenario, company C would have the highest market share whereas we would be stuck in the middle. Launching campaign 1 would not change this situation but we would be closer to C. However, launching campaign 2 would allow us to surpass C and have the highest market share, bringing us to 0.4.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou might have noticed that we’re assuming that the market shares are equal at the beginning. However, it is shown that the market shares would be the same after a couple years regardless of the initial market shares. An example is seen below where the initial market shares are [0, 0, 1].\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFurther analysis shows that there is a diminishing marginal return if more money is spent on a campaign to take more customers away from B or C. The is shown as the curves are concave down. Additionally, these graphs also show that spending money on taking C’s customers is more valuable than taking B’s because A increases around 0.15 for the former and 0.1 for the latter. Taking C’s customers also makes A the market leader by a significant margin. It should be noted that this also assumes the amount of money it takes to take B’s customers is the same as taking C’s customers.\nAdditionally, although the initial market share doesn’t matter, it is important to note that we’re assuming these events are independent of each other. For example, perhaps customers are more likely to follow the pack if a majority of the population are using companies which means that the market would be more volatile than my simulations suggest. This would also mean that it’s more important for us to have the highest market share at the beginning, making initial market shares more important.\n\nThis is excellent. You’ve found a good balance between what to show and what to hide (in code), so that the reader can understand your point without getting bogged down in details. But your code, when I look at it, is well organized and sufficiently commented.\n(Just FYI, one option, if you’d like to let the reader see your code if they like, is to put this code in the yaml at the top of the document:\nformat:\n  html:\n    code-fold: show\n    code-tools: \n      source: true\n      toggle: true\nThen if you remove the “echo: false” from your code chunks, the code will be hidden by default, but the reader can click to see it. I’ve done this for one of your code chunks below, just so you can see what it looks like.)\nRegarding this project, the only thing that I might suggest would be to discuss why these results had to happen, regardless of initial state given the specific transition matrices you had? For instance, can you look at the matrix you calculate after three years and tell from this which company will do best?\nGrade: E"
  },
  {
    "objectID": "submitted/projects_1/project1.html#introduction",
    "href": "submitted/projects_1/project1.html#introduction",
    "title": "Project 1",
    "section": "",
    "text": "Here is the system of equations that represents the scenario.\n\nHere is the transition matrix. The market shares after 3 years are shown below given that the initial market shares are equal, which meant that the initial vector is [1/3, 1/3, 1/3].\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0.5 & 0.2 & 0.3\\\\0.2 & 0.6 & 0.1\\\\0.3 & 0.2 & 0.6\\end{matrix}\\right]\\)\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0.339\\\\0.275333333333333\\\\0.385666666666667\\end{matrix}\\right]\\)"
  },
  {
    "objectID": "submitted/projects_1/project1.html#recommendation",
    "href": "submitted/projects_1/project1.html#recommendation",
    "title": "Project 1",
    "section": "",
    "text": "As seen in the below graphs, the effect of launching campaign 2 is better than campaign 1. Note that campaign 0 shows what would happen if no campiagn is launched. In that scenario, company C would have the highest market share whereas we would be stuck in the middle. Launching campaign 1 would not change this situation but we would be closer to C. However, launching campaign 2 would allow us to surpass C and have the highest market share, bringing us to 0.4.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou might have noticed that we’re assuming that the market shares are equal at the beginning. However, it is shown that the market shares would be the same after a couple years regardless of the initial market shares. An example is seen below where the initial market shares are [0, 0, 1].\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFurther analysis shows that there is a diminishing marginal return if more money is spent on a campaign to take more customers away from B or C. The is shown as the curves are concave down. Additionally, these graphs also show that spending money on taking C’s customers is more valuable than taking B’s because A increases around 0.15 for the former and 0.1 for the latter. Taking C’s customers also makes A the market leader by a significant margin. It should be noted that this also assumes the amount of money it takes to take B’s customers is the same as taking C’s customers.\nAdditionally, although the initial market share doesn’t matter, it is important to note that we’re assuming these events are independent of each other. For example, perhaps customers are more likely to follow the pack if a majority of the population are using companies which means that the market would be more volatile than my simulations suggest. This would also mean that it’s more important for us to have the highest market share at the beginning, making initial market shares more important.\n\nThis is excellent. You’ve found a good balance between what to show and what to hide (in code), so that the reader can understand your point without getting bogged down in details. But your code, when I look at it, is well organized and sufficiently commented.\n(Just FYI, one option, if you’d like to let the reader see your code if they like, is to put this code in the yaml at the top of the document:\nformat:\n  html:\n    code-fold: show\n    code-tools: \n      source: true\n      toggle: true\nThen if you remove the “echo: false” from your code chunks, the code will be hidden by default, but the reader can click to see it. I’ve done this for one of your code chunks below, just so you can see what it looks like.)\nRegarding this project, the only thing that I might suggest would be to discuss why these results had to happen, regardless of initial state given the specific transition matrices you had? For instance, can you look at the matrix you calculate after three years and tell from this which company will do best?\nGrade: E"
  },
  {
    "objectID": "submitted/projects_1/project1.html#win-loss-ratio",
    "href": "submitted/projects_1/project1.html#win-loss-ratio",
    "title": "Project 1",
    "section": "Win Loss Ratio",
    "text": "Win Loss Ratio\n\n\narray([[0., 1., 0., 1., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 1., 0.],\n       [1., 1., 0., 1., 1., 0., 0.],\n       [0., 0., 0., 0., 1., 0., 0.],\n       [1., 1., 0., 0., 0., 1., 1.],\n       [1., 0., 1., 1., 0., 0., 0.],\n       [1., 1., 1., 1., 0., 1., 0.]])\n\n\n\n\n\n\n\n\n\n\nWins\nLosses\nRatio\n\n\nTeam\n\n\n\n\n\n\n\n7\n5.0\n1.0\n5.0\n\n\n3\n4.0\n2.0\n2.0\n\n\n5\n4.0\n2.0\n2.0\n\n\n6\n3.0\n3.0\n1.0\n\n\n1\n2.0\n4.0\n0.5\n\n\n2\n2.0\n4.0\n0.5\n\n\n4\n1.0\n5.0\n0.2"
  },
  {
    "objectID": "submitted/projects_1/project1.html#vertex-power",
    "href": "submitted/projects_1/project1.html#vertex-power",
    "title": "Project 1",
    "section": "Vertex Power",
    "text": "Vertex Power\n\n\nCode\n# directed graph \nG = nx.DiGraph() \nrows, columns = M.shape\nfor i in range(rows): \n for j in range(columns): \n   if M[i][j] == 1: \n      G.add_edge(i+1,j+1) \nnx.draw(G, with_labels=True, node_size = 5000, node_color = 'lightblue', font_size = 20, font_color = 'black')\n\nMsquared = np.dot(M,M)\n\nvertex_power_matrix = Msquared + M\nvertex_power = np.sum(vertex_power_matrix, axis=1)\ndf = pd.DataFrame({'Team': team, 'Vertex Power': vertex_power}).sort_values(by='Vertex Power', ascending=False).set_index('Team')\ndf\n\n\n\n\n\n\n\n\n\n\nVertex Power\n\n\nTeam\n\n\n\n\n\n7\n17.0\n\n\n5\n16.0\n\n\n3\n13.0\n\n\n6\n10.0\n\n\n2\n6.0\n\n\n1\n5.0\n\n\n4\n5.0"
  },
  {
    "objectID": "submitted/projects_1/project1.html#reverse-page-rank",
    "href": "submitted/projects_1/project1.html#reverse-page-rank",
    "title": "Project 1",
    "section": "Reverse Page Rank",
    "text": "Reverse Page Rank\nNote that I used alpha = 0.85 and the teleportation vector is [1/7, 1/7, 1/7, 1/7, 1/7, 1/7, 1/7].\n\n\nCode\n# reverse graph\n\nT = M.transpose()\nD = np.identity(7)\nfor i in range(7):\n    D[i,i] = 1/(np.sum(T[i],axis = 0))\n\n# transition matrix\nP = np.dot(T.transpose(),D)\n\n\n# teleportation vector\nv = np.array([1/7, 1/7, 1/7, 1/7, 1/7, 1/7, 1/7])\na = 0.85\n\nlhs = (np.identity(7) - a * P)\nrhs = (1-a) * v\n\nx = np.linalg.solve(lhs, rhs)\ndf = pd.DataFrame({'Team': team, 'Page Rank': x}).sort_values(by='Page Rank', ascending=False).set_index('Team')\ndf\n\n\n\n\n\n\n\n\n\n\nPage Rank\n\n\nTeam\n\n\n\n\n\n5\n0.244525\n\n\n7\n0.184189\n\n\n3\n0.176270\n\n\n6\n0.130332\n\n\n4\n0.125352\n\n\n2\n0.079666\n\n\n1\n0.059667"
  },
  {
    "objectID": "submitted/projects_1/project1.html#weighted-vertex-power",
    "href": "submitted/projects_1/project1.html#weighted-vertex-power",
    "title": "Project 1",
    "section": "Weighted Vertex Power",
    "text": "Weighted Vertex Power\nNote I couldn’t figure out how to rename the nodes to 1-7 instead of 0-6.\n\n\nCode\n# weighted adjacency matrix\nM = np.zeros((7,7))\nM[0,1] = 4\nM[6,2] = 8\nM[1,3] = 7\nM[3,4] = 3\nM[2,1] = 7\nM[4,0] = 7\nM[5,0] = 23\nM[2,0] = 15\nM[6,1] = 6\nM[1,5] = 18\nM[2,3] = 13\nM[6,3] = 14\nM[4,6] = 7\nM[5,3] = 13\nM[2,4] = 7\nM[4,5] = 18\nM[6,0] = 45\nM[4,1] = 10\nM[6,5] = 19\nM[0,3] = 14\nM[5,2] = 13\n\ndisplay(M)\n\nG = nx.from_numpy_array(np.matrix(M), create_using=nx.DiGraph)\n\n\nlayout = nx.spring_layout(G)\n\nnx.draw(G, layout, with_labels=True, node_size = 1000, node_color = 'lightblue', font_size = 20, font_color = 'black')\nlabels = nx.get_edge_attributes(G, \"weight\")\nnx.draw_networkx_edge_labels(G, pos=layout, edge_labels=labels)\nplt.show()\n\nMsquared = np.dot(M,M)\n\nvertex_power_matrix = Msquared + M\nvertex_power = np.sum(vertex_power_matrix, axis=1)\ndf = pd.DataFrame({'Team': team, 'Vertex Power': vertex_power}).sort_values(by='Vertex Power', ascending=False).set_index('Team')\ndf\n\n\narray([[ 0.,  4.,  0., 14.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  7.,  0., 18.,  0.],\n       [15.,  7.,  0., 13.,  7.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  3.,  0.,  0.],\n       [ 7., 10.,  0.,  0.,  0., 18.,  7.],\n       [23.,  0., 13., 13.,  0.,  0.,  0.],\n       [45.,  6.,  8., 14.,  0., 19.,  0.]])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVertex Power\n\n\nTeam\n\n\n\n\n\n7\n2361.0\n\n\n5\n1944.0\n\n\n6\n1048.0\n\n\n2\n928.0\n\n\n3\n820.0\n\n\n1\n160.0\n\n\n4\n129.0\n\n\n\n\n\n\n\n\n\nThis is a good start. Your code seems good – but I’d like to also see some writing: describing the problem, explanation about what is happening mathematically in these different cases, and why your code matches what these formulas say. Then, it would be good to see a brief discussion of why the different rankings give different results, and which one you think is the most accurate.\nGrade: R"
  },
  {
    "objectID": "HW/HW4.nosol.html",
    "href": "HW/HW4.nosol.html",
    "title": "Homework 4",
    "section": "",
    "text": "1 \nFor the following matrices find the null space of \\(I-A\\) and find state vectors with nonnegative entries that sum to 1 in the null space, if any. Are these matrices stable (yes/no)?\n\n\\(A=\\left[\\begin{array}{rrr}0.5 & 0 & 1 \\\\ 0.5 & 0.5 & 0 \\\\ 0 & 0.5 & 0\\end{array}\\right]\\)\n\\(A=\\left[\\begin{array}{ll}0 & 1 \\\\ 1 & 0\\end{array}\\right]\\)\n\n\n\n2 \nFind bases for the row, column, and null space of each of the following matrices. (a) \\([2,0,-1]\\) (b) \\(\\left[\\begin{array}{lllll}1 & 2 & 0 & 0 & 1 \\\\ 1 & 2 & 1 & 1 & 1 \\\\ 3 & 6 & 2 & 2 & 3\\end{array}\\right]\\) (c) \\(\\left[\\begin{array}{ccccc}1 & 2 & 0 & 4 & 0 \\\\ 1 & 3 & 5 & 2 & 1 \\\\ 2 & 3 & -5 & 10 & 0 \\\\ 2 & 4 & 0 & 8 & 1\\end{array}\\right]\\) (d) \\(\\left[\\begin{array}{rrr}2 & -3 & -1 \\\\ 0 & 2 & 0 \\\\ 2 & 4 & 1\\end{array}\\right]\\)\n\n\n3 \nFind all possible linear combinations with value zero of the following sets of vectors and the dimension of the space spanned by them.\n\n\\((0,1,1),(2,0,1),(2,2,3),(0,2,2)\\) in \\(\\mathbb{R}^{3}\\).\n\\(x, x^{2}+x, x^{2}-x\\) in \\(\\mathcal{P}_{2}\\).\n\\((1,1,2,2),(0,2,0,2),(1,0,2,1),(2,1,4,4)\\) in \\(\\mathbb{R}^{4}\\).\n\n\n\n4 \nLet \\(A=\\left[\\begin{array}{lllll}0 & 1 & 0 & 1 & 2 \\\\ 1 & 0 & 2 & 1 & 2 \\\\ 2 & 2 & 4 & 4 & 8\\end{array}\\right]\\). Use the column space algorithm on the matrix \\([A I]\\) to find a basis \\(B\\) of \\(\\mathcal{C}(A)\\) and to expand it to a basis of \\(\\mathbb{R}^{3}\\).\n\n\n5 \nFor the following orthogonal pairs \\(\\mathbf{u}, \\mathbf{v}\\) and matrix \\(M=\\left[\\begin{array}{rrr}1 & 0 & -1 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1\\end{array}\\right]\\), determine whether \\(M \\mathbf{u}\\) and \\(M \\mathbf{v}\\) are orthogonal. (a) \\((2,1,1),(1,0,-2)\\) (b) \\((0,1,1),(1,-1,1)\\) (c) \\((3,1,-2),(1,3,3)\\)\n\n\n6 \nFind equations for the following planes in \\(\\mathbb{R}^{3}\\).\n\nThe plane containing the points \\((1,1,2),(-1,3,2),(2,4,3)\\).\nThe plane containing the points \\((-2,1,1)\\) and \\((0,1,2)\\) and orthogonal to the plane \\(2 x-y+z=3\\).\n\n\n\n7 \nFind equations for the following hyperplanes in \\(\\mathbb{R}^{4}\\).\n\nThe plane parallel to the plane \\(2 x_{1}+x_{2}-3 x_{3}+x_{4}=2\\) and containing the point \\((2,1,1,3)\\).\nThe plane through the origin and orthogonal to the vector \\((1,0,2,1)\\).\n\n\n\n8 \nFor each pair \\(A, \\mathbf{b}\\), solve the normal equations for the system \\(A \\mathbf{x}=\\mathbf{b}\\) and find the residual vector and its norm. Are there any genuine solutions to the system? (a) \\(\\left[\\begin{array}{ll}1 & 3 \\\\ 1 & 0\\end{array}\\right],\\left[\\begin{array}{l}1 \\\\ 3\\end{array}\\right]\\) (b) \\(\\left[\\begin{array}{rr}2 & -2 \\\\ 1 & 1 \\\\ 3 & 1\\end{array}\\right],\\left[\\begin{array}{r}2 \\\\ -1 \\\\ 1\\end{array}\\right]\\) (c) \\(\\left[\\begin{array}{rrr}0 & 2 & 2 \\\\ 1 & 1 & 0 \\\\ -1 & 1 & 2 \\\\ 1 & -2 & -3\\end{array}\\right],\\left[\\begin{array}{l}3 \\\\ 1 \\\\ 0 \\\\ 0\\end{array}\\right]\\) ::: solutions Exercise 4.2.11 :::\n\n\n9 \n(Text retrieval) You are given the following term-by-document matrix, that is, a matrix whose \\((i, j)\\) th entry is the number of times term \\(t_{i}\\) occurs in document \\(D_{j}\\). Columns of this matrix are document vectors, as are queries. We measure the quality of a match between query and document by the cosine of the angle \\(\\theta\\) between the two vectors, larger cosine being better. Which of the following nine documents \\(D_{i}\\) matches the query \\((0,1,0,1,1)\\) above the threshold value \\(\\cos \\theta \\geq 0.5\\) ? Which is the best match to the query?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(D_{1}\\)\n\\(D_{2}\\)\n\\(D_{3}\\)\n\\(D_{4}\\)\n\\(D_{5}\\)\n\\(D_{6}\\)\n\\(D_{7}\\)\n\\(D_{8}\\)\n\\(D_{9}\\)\n\n\n\n\n\\(t_{1}\\)\n1\n1\n2\n0\n1\n0\n1\n0\n1\n\n\n\\(t_{2}\\)\n0\n1\n0\n1\n0\n1\n1\n0\n0\n\n\n\\(t_{3}\\)\n0\n2\n0\n2\n0\n1\n0\n1\n1\n\n\n\\(t_{4}\\)\n1\n0\n1\n0\n1\n0\n2\n1\n0\n\n\n\\(t_{5}\\)\n1\n2\n1\n0\n0\n1\n0\n0\n1"
  },
  {
    "objectID": "lectures/ch4lecture2.html#normal-equations-for-multiple-predictors",
    "href": "lectures/ch4lecture2.html#normal-equations-for-multiple-predictors",
    "title": "Ch4 Lecture 2",
    "section": "Normal equations for multiple predictors",
    "text": "Normal equations for multiple predictors\nWhen we have multiple predictors, \\(\\mathbf{A}\\) is a matrix\n\nEach column corresponds to a predictor variable.\n\n\n\\(\\mathbf{x}\\) is a column vector of the coefficients we are trying to find, one per predictor.\n\n\nWhat do the normal equations tell us? We can break them apart into one equation corresponding to each column of \\(\\mathbf{A}\\).\n\n\n\\[\n\\begin{aligned}\n\\mathbf{A}^{T} \\mathbf{A} \\mathbf{x} &=\\mathbf{A}^{T} \\mathbf{b} \\\\\n\\text{becomes} \\\\\n\\mathbf{a_i}\\cdot \\mathbf{A} \\mathbf{x} &=\\mathbf{a_i} \\cdot \\mathbf{b}\\text{ for every i} \\\\\n\\mathbf{a_i}\\cdot \\left(\\mathbf{b}-\\mathbf{Ax}\\right)&=0\\text{ for every i}\n\\end{aligned}\n\\]\n\n\nWe need to find \\(\\mathbf{x}\\) such that the residuals are orthogonal to every column of \\(\\mathbf{A}\\)."
  },
  {
    "objectID": "lectures/ch4lecture2.html#example",
    "href": "lectures/ch4lecture2.html#example",
    "title": "Ch4 Lecture 2",
    "section": "Example",
    "text": "Example\nWe have two predictors \\(a_1\\) and \\(a_2\\) and a response variable \\(b\\). We have the following data:\n\n\n\n\\(x_1\\)\n\\(x_2\\)\n\\(y\\)\n\n\n\n\n2\n1\n0\n\n\n1\n1\n0\n\n\n2\n1\n2\n\n\n\n\nWe are hoping to find a linear relationship of the form \\(\\beta_1 x_1 + \\beta_2 x_2 = y\\) for some values of \\(\\beta_1\\) and \\(\\beta_2\\).\n\n\nThis would bring us the following system of equations:\n\\[\n\\begin{aligned}\n2 \\beta_{1}+\\beta_{2} & =0 \\\\\n\\beta_{1}+\\beta_{2} & =0 \\\\\n2 \\beta_{1}+\\beta_{2} & =2 .\n\\end{aligned}\n\\]\n\n\nObviously inconsistent! (\\(2 \\beta_{1}+\\beta_{2}\\) cannot equal both 0 and 2.)\n\n\nFind the least squares solution using the normal equations:"
  },
  {
    "objectID": "lectures/ch4lecture2.html#section-1",
    "href": "lectures/ch4lecture2.html#section-1",
    "title": "Ch4 Lecture 2",
    "section": "",
    "text": "Change variable names, so the \\(\\beta\\)s are now \\(\\mathbf{x}\\), the \\(x\\)s are now the columns \\(\\mathbf{A}\\), and the \\(y\\)s are now \\(\\mathbf{b}\\).\n\\[\nA=\\left[\\begin{array}{ll}\n2 & 1 \\\\\n1 & 1 \\\\\n2 & 1\n\\end{array}\\right], \\text { and } \\mathbf{b}=\\left[\\begin{array}{l}\n0 \\\\\n0 \\\\\n2\n\\end{array}\\right]\n\\]\n\nResiduals are \\(\\mathbf{b}-\\mathbf{A} \\mathbf{x}\\).\nNormal equations are \\(\\mathbf{A}^{T} \\mathbf{A} \\mathbf{x}=\\mathbf{A}^{T} \\mathbf{b}\\)."
  },
  {
    "objectID": "lectures/ch4lecture2.html#section-2",
    "href": "lectures/ch4lecture2.html#section-2",
    "title": "Ch4 Lecture 2",
    "section": "",
    "text": "\\[\nA^{T} A=\\left[\\begin{array}{lll}\n2 & 1 & 2 \\\\\n1 & 1 & 1\n\\end{array}\\right]\\left[\\begin{array}{ll}\n2 & 1 \\\\\n1 & 1 \\\\\n2 & 1\n\\end{array}\\right]=\\left[\\begin{array}{ll}\n9 & 5 \\\\\n5 & 3\n\\end{array}\\right]\n\\]\nwith inverse \\[\n\\left(A^{T} A\\right)^{-1}=\\left[\\begin{array}{ll}\n9 & 5 \\\\\n5 & 3\n\\end{array}\\right]^{-1}=\\frac{1}{2}\\left[\\begin{array}{rr}\n3 & -5 \\\\\n-5 & 9\n\\end{array}\\right]\n\\]\n\n\\[\nA^{T} \\mathbf{b}=\\left[\\begin{array}{lll}\n2 & 1 & 2 \\\\\n1 & 1 & 1\n\\end{array}\\right]\\left[\\begin{array}{l}\n0 \\\\\n0 \\\\\n2\n\\end{array}\\right]=\\left[\\begin{array}{l}\n4 \\\\\n2\n\\end{array}\\right]\n\\]"
  },
  {
    "objectID": "lectures/ch4lecture2.html#least-squares-solution",
    "href": "lectures/ch4lecture2.html#least-squares-solution",
    "title": "Ch4 Lecture 2",
    "section": "Least Squares Solution",
    "text": "Least Squares Solution\n\\[\n\\mathbf{x}=\\left(A^{T} A\\right)^{-1} A^{T} \\mathbf{b}=\\frac{1}{2}\\left[\\begin{array}{rr}\n3 & -5 \\\\\n-5 & 9\n\\end{array}\\right]\\left[\\begin{array}{l}\n4 \\\\\n2\n\\end{array}\\right]=\\left[\\begin{array}{r}\n1 \\\\\n-1\n\\end{array}\\right]\n\\]\n\nBack in our original variables, this means the best fit estimate for \\(\\beta_1\\) is 1 and the best \\(\\beta_2\\) is -1."
  },
  {
    "objectID": "lectures/ch4lecture2.html#orthogonal-coordinates-theorem",
    "href": "lectures/ch4lecture2.html#orthogonal-coordinates-theorem",
    "title": "Ch4 Lecture 2",
    "section": "Orthogonal Coordinates Theorem",
    "text": "Orthogonal Coordinates Theorem\n\nIf \\(\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\ldots, \\mathbf{v}_{n}\\) are nonzero and orthogonal, and\n\\(\\mathbf{v} \\in \\operatorname{span}\\left\\{\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\ldots, \\mathbf{v}_{n}\\right\\}\\),\n\\(\\mathbf{v}\\) can be expressed uniquely (up to order) as a linear combination of \\(\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\ldots, \\mathbf{v}_{n}\\), namely\n\\[\n\\mathbf{v}=\\frac{\\mathbf{v}_{1} \\cdot \\mathbf{v}}{\\mathbf{v}_{1} \\cdot \\mathbf{v}_{1}} \\mathbf{v}_{1}+\\frac{\\mathbf{v}_{2} \\cdot \\mathbf{v}}{\\mathbf{v}_{2} \\cdot \\mathbf{v}_{2}} \\mathbf{v}_{2}+\\cdots+\\frac{\\mathbf{v}_{n} \\cdot \\mathbf{v}}{\\mathbf{v}_{n} \\cdot \\mathbf{v}_{n}} \\mathbf{v}_{n}\n\\]"
  },
  {
    "objectID": "lectures/ch4lecture2.html#proof",
    "href": "lectures/ch4lecture2.html#proof",
    "title": "Ch4 Lecture 2",
    "section": "Proof",
    "text": "Proof\nSince \\(\\mathbf{v} \\in \\operatorname{span}\\left\\{\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\ldots, \\mathbf{v}_{n}\\right\\}\\),\nwe can write \\(\\mathbf{v}\\) as a linear combination of the \\(\\mathbf{v}_{i}\\) ’s:\n\\[\n\\mathbf{v}=c_{1} \\mathbf{v}_{1}+c_{2} \\mathbf{v}_{2}+\\cdots+c_{n} \\mathbf{v}_{n}\n\\]\n\nNow take the inner product of both sides with \\(\\mathbf{v}_{k}\\):\n\n\nSince \\(\\mathbf{v}_{k} \\cdot \\mathbf{v}_{j}=0\\) if \\(j \\neq k\\),\n\\[\n\\begin{aligned}\n\\mathbf{v}_{k} \\cdot \\mathbf{v} & =\\mathbf{v}_{k} \\cdot\\left(c_{1} \\mathbf{v}_{1}+c_{2} \\mathbf{v}_{2}+\\ldots \\cdots+c_{n} \\mathbf{v}_{n}\\right) \\\\\n& =c_{1} \\mathbf{v}_{k} \\cdot \\mathbf{v}_{1}+c_{2} \\mathbf{v}_{k} \\cdot \\mathbf{v}_{2}+\\cdots+c_{n} \\mathbf{v}_{k} \\cdot \\mathbf{v}_{n}=c_{k} \\mathbf{v}_{k} \\cdot \\mathbf{v}_{k}\n\\end{aligned}\n\\]\n\n\nSince \\(\\mathbf{v}_{k} \\neq 0\\), \\(\\left\\|\\mathbf{v}_{k}\\right\\|^{2}=\\mathbf{v}_{k} \\cdot \\mathbf{v}_{k} \\neq 0\\) so we can divide::\n\\[\nc_{k}=\\frac{\\mathbf{v}_{k} \\cdot \\mathbf{v}}{\\mathbf{v}_{k} \\cdot \\mathbf{v}_{k}}\n\\]\n\n\nAny linear combination of an orthogonal set of nonzero vectors is the sum of its projections in the direction of each vector in the set."
  },
  {
    "objectID": "lectures/ch4lecture2.html#orthogonal-matrix",
    "href": "lectures/ch4lecture2.html#orthogonal-matrix",
    "title": "Ch4 Lecture 2",
    "section": "Orthogonal matrix",
    "text": "Orthogonal matrix\nSuppose \\(\\mathbf{u}_{1}, \\mathbf{u}_{2}, \\ldots, \\mathbf{u}_{n}\\) is an orthonormal basis of \\(\\mathbb{R}^{n}\\)\n\nMake these the column vectors of \\(\\mathbf{A}\\): \\(A=\\left[\\mathbf{u}_{1}, \\mathbf{u}_{2}, \\ldots, \\mathbf{u}_{n}\\right]\\)\n\n\nBecause the \\(\\mathbf{u}_{i}\\) are orthonormal, \\(\\mathbf{u}_{i}^{T} \\mathbf{u}_{j}=\\delta_{i j}\\)\n\n\nUse this to calculate \\(A^{T} A\\).\n\nits \\((i, j)\\) th entry is \\(\\mathbf{u}_{n}^{T} \\mathbf{u}_{n}\\)\n= \\(\\delta_{i j}\\)\nSo we have = \\(A^{T} A=\\left[\\delta_{i j}\\right]=I\\)\nTherefore, \\(A^{T} = A^{-1}\\)"
  },
  {
    "objectID": "lectures/ch4lecture2.html#orthogonal-matrix-1",
    "href": "lectures/ch4lecture2.html#orthogonal-matrix-1",
    "title": "Ch4 Lecture 2",
    "section": "Orthogonal matrix",
    "text": "Orthogonal matrix\n\nA square real matrix \\(Q\\) is called orthogonal if \\(Q^{T}=Q^{-1}\\).\nA square matrix \\(U\\) is called unitary if \\(U^{*}=U^{-1}\\)."
  },
  {
    "objectID": "lectures/ch4lecture2.html#example-1",
    "href": "lectures/ch4lecture2.html#example-1",
    "title": "Ch4 Lecture 2",
    "section": "Example",
    "text": "Example\nShow that the matrix \\(R(\\theta)=\\left[\\begin{array}{rr}\\cos \\theta & -\\sin \\theta \\\\ \\sin \\theta & \\cos \\theta\\end{array}\\right]\\) is orthogonal.\n\n\\[\n\\begin{aligned}\nR(\\theta)^{T} R(\\theta) & =\\left(\\left[\\begin{array}{cc}\n\\cos \\theta & -\\sin \\theta \\\\\n\\sin \\theta & \\cos \\theta\n\\end{array}\\right]\\right)^{T}\\left[\\begin{array}{cc}\n\\cos \\theta & -\\sin \\theta \\\\\n\\sin \\theta & \\cos \\theta\n\\end{array}\\right] \\\\\n& =\\left[\\begin{array}{cc}\n\\cos \\theta \\sin \\theta \\\\\n-\\sin \\theta \\cos \\theta\n\\end{array}\\right]\\left[\\begin{array}{cc}\n\\cos \\theta-\\sin \\theta \\\\\n\\sin \\theta & \\cos \\theta\n\\end{array}\\right] \\\\\n& =\\left[\\begin{array}{cc}\n\\cos ^{2} \\theta+\\sin ^{2} \\theta & \\cos \\theta \\sin \\theta-\\sin \\theta \\cos \\theta \\\\\n-\\cos \\theta \\sin \\theta+\\sin \\theta \\cos \\theta & \\sin ^{2} \\theta+\\cos ^{2} \\theta\n\\end{array}\\right]=\\left[\\begin{array}{ll}\n1 & 0 \\\\\n0 & 1\n\\end{array}\\right],\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lectures/ch4lecture2.html#rigidity-of-orthogonal-and-unitary-matrices",
    "href": "lectures/ch4lecture2.html#rigidity-of-orthogonal-and-unitary-matrices",
    "title": "Ch4 Lecture 2",
    "section": "Rigidity of orthogonal (and unitary) matrices",
    "text": "Rigidity of orthogonal (and unitary) matrices\nThis rotation matrix preserves vector lengths and angles between vectors (illustrate by hand).\nThus, \\(R(\\theta)\\mathbf{x}\\cdot R(\\theta)\\mathbf{y}=\\mathbf{x} \\cdot \\mathbf{y}\\)\n\nThis is true in general for orthogonal matrices:\n\\[\nQ \\mathbf{x} \\cdot Q \\mathbf{y}=(Q \\mathbf{x})^{T} Q \\mathbf{y}=\\mathbf{x}^{T} Q^{T} Q \\mathbf{y}=\\mathbf{x}^{T} \\mathbf{y}=\\mathbf{x} \\cdot \\mathbf{y}\n\\]\nalso,\n\\[\n\\|Q \\mathbf{x}\\|^{2}=Q \\mathbf{x} \\cdot Q \\mathbf{x}=(Q \\mathbf{x})^{T} Q \\mathbf{x}=\\mathbf{x}^{T} Q^{T} Q \\mathbf{x}=\\mathbf{x}^{T} \\mathbf{x}=\\|\\mathbf{x}\\|^{2}\n\\]"
  },
  {
    "objectID": "lectures/ch4lecture2.html#gram-schmidt-algorithm",
    "href": "lectures/ch4lecture2.html#gram-schmidt-algorithm",
    "title": "Ch4 Lecture 2",
    "section": "Gram-Schmidt Algorithm",
    "text": "Gram-Schmidt Algorithm\nLet \\(\\mathbf{w}_{1}, \\mathbf{w}_{2}, \\ldots, \\mathbf{w}_{n}\\) be linearly independent vectors in a standard space.\n\nDefine vectors \\(\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\ldots, \\mathbf{v}_{n}\\) recursively:\nTake each \\(\\mathbf{w}_{k}\\) and subtract off the projection of \\(\\mathbf{w}_{k}\\) onto each of the previous vectors \\(\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\ldots, \\mathbf{v}_{k-1}\\)\n\n\n\\[\n\\mathbf{v}_{k}=\\mathbf{w}_{k}-\\frac{\\mathbf{v}_{1} \\cdot \\mathbf{w}_{k}}{\\mathbf{v}_{1} \\cdot \\mathbf{v}_{1}} \\mathbf{v}_{1}-\\frac{\\mathbf{v}_{2} \\cdot \\mathbf{w}_{k}}{\\mathbf{v}_{2} \\cdot \\mathbf{v}_{2}} \\mathbf{v}_{2}-\\cdots-\\frac{\\mathbf{v}_{k-1} \\cdot \\mathbf{w}_{k}}{\\mathbf{v}_{k-1} \\cdot \\mathbf{v}_{k-1}} \\mathbf{v}_{k-1}, \\quad k=1, \\ldots, n\n\\]"
  },
  {
    "objectID": "lectures/ch4lecture2.html#section-3",
    "href": "lectures/ch4lecture2.html#section-3",
    "title": "Ch4 Lecture 2",
    "section": "",
    "text": "\\[\n\\mathbf{v}_{k}=\\mathbf{w}_{k}-\\frac{\\mathbf{v}_{1} \\cdot \\mathbf{w}_{k}}{\\mathbf{v}_{1} \\cdot \\mathbf{v}_{1}} \\mathbf{v}_{1}-\\frac{\\mathbf{v}_{2} \\cdot \\mathbf{w}_{k}}{\\mathbf{v}_{2} \\cdot \\mathbf{v}_{2}} \\mathbf{v}_{2}-\\cdots-\\frac{\\mathbf{v}_{k-1} \\cdot \\mathbf{w}_{k}}{\\mathbf{v}_{k-1} \\cdot \\mathbf{v}_{k-1}} \\mathbf{v}_{k-1}, \\quad k=1, \\ldots, n\n\\]\n\nThen\n\nThe vectors \\(\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\ldots, \\mathbf{v}_{k}\\) form an orthogonal set.\nFor each index \\(k=1, \\ldots, n\\),\n\n\n\n\\[\n\\operatorname{span}\\left\\{\\mathbf{w}_{1}, \\mathbf{w}_{2}, \\ldots, \\mathbf{w}_{k}\\right\\}=\\operatorname{span}\\left\\{\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\ldots, \\mathbf{v}_{k}\\right\\}\n\\]"
  },
  {
    "objectID": "lectures/ch4lecture2.html#example-2",
    "href": "lectures/ch4lecture2.html#example-2",
    "title": "Ch4 Lecture 2",
    "section": "Example",
    "text": "Example\nLet \\(V=\\mathcal{C}(A)\\) with the standard inner product and compute an orthonormal basis of \\(V\\), where\n\\[\nA=\\left[\\begin{array}{rrrr}\n1 & 2 & 0 & -1 \\\\\n1 & -1 & 3 & 2 \\\\\n1 & -1 & 3 & 2 \\\\\n-1 & 1 & -3 & 1\n\\end{array}\\right]\n\\]\n\n\nWe need to check that the columns of \\(A\\) are linearly independent. We can do that by checking the rank of \\(A\\). - One way to do this is to row reduce \\(A\\) to see if we get a row of zeros. If we do, then the rank is less than 4 and the columns are linearly dependent. If we don’t, then the rank is 4 and the columns are linearly independent. - Another way is to compute the determinant of \\(A^{T}A\\). If the determinant is nonzero, then the columns are linearly independent. If the determinant is zero, then the columns are linearly dependent.\nBut we can see right away that it’s not full rank, since we have a repeated row!\n\nRREF of \\(A\\) is:\n\\[\nR=\\left[\\begin{array}{rrrr}\n1 & 0 & 2 & 0 \\\\\n0 & 1 & -1 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0\n\\end{array}\\right]\n\\]\n\n\nThe independent columns are columns 1, 2, and 4.\n\n\nSo let these be our \\(\\mathbf{w}_{1}, \\mathbf{w}_{2}, \\mathbf{w}_{3}\\)."
  },
  {
    "objectID": "lectures/ch4lecture2.html#finding-orthonormal-basis",
    "href": "lectures/ch4lecture2.html#finding-orthonormal-basis",
    "title": "Ch4 Lecture 2",
    "section": "Finding orthonormal basis",
    "text": "Finding orthonormal basis\nNow we need to normalize these vectors to get an orthonormal basis.\n\n\\(\\mathbf{u}_{1}=\\frac{1}{\\|\\mathbf{v}_{1}\\|} \\mathbf{v}_{1}=\\frac{1}{2}(1,1,1,-1)\\)\n\n\n\\(\\mathbf{u}_{2}=\\frac{1}{\\|\\mathbf{v}_{2}\\|} \\mathbf{v}_{2}=\\frac{1}{\\sqrt(108)}(9,-3,-3,3)\\)\netc."
  },
  {
    "objectID": "lectures/ch4lecture2.html#how-to-find-q-and-r",
    "href": "lectures/ch4lecture2.html#how-to-find-q-and-r",
    "title": "Ch4 Lecture 2",
    "section": "How to find \\(Q\\) and \\(R\\)",
    "text": "How to find \\(Q\\) and \\(R\\)\n\nStart with the columns of A, \\(A=\\left[\\mathbf{w}_{1}, \\mathbf{w}_{2}, \\mathbf{w}_{3}\\right]\\). (For now assume they are linearly independent.)\nDo Gram-Schmidt on the columns of \\(A\\):\n\n\n\\[\n\\begin{aligned}\n& \\mathbf{v}_{1}=\\mathbf{w}_{1} \\\\\n& \\mathbf{v}_{2}=\\mathbf{w}_{2}-\\frac{\\mathbf{v}_{1} \\cdot \\mathbf{w}_{2}}{\\mathbf{v}_{1} \\cdot \\mathbf{v}_{1}} \\mathbf{v}_{1} \\\\\n& \\mathbf{v}_{3}=\\mathbf{w}_{3}-\\frac{\\mathbf{v}_{1} \\cdot \\mathbf{w}_{3}}{\\mathbf{v}_{1} \\cdot \\mathbf{v}_{1}} \\mathbf{v}_{1}-\\frac{\\mathbf{v}_{2} \\cdot \\mathbf{w}_{3}}{\\mathbf{v}_{2} \\cdot \\mathbf{v}_{2}} \\mathbf{v}_{2} .\n\\end{aligned}\n\\]\n\n\n\nSolve these equations for \\(\\mathbf{w}_{1}, \\mathbf{w}_{2}, \\mathbf{w}_{3}\\):\n\n\n\n\\[\n\\begin{aligned}\n& \\mathbf{w}_{1}=\\mathbf{v}_{1} \\\\\n& \\mathbf{w}_{2}=\\frac{\\mathbf{v}_{1} \\cdot \\mathbf{w}_{2}}{\\mathbf{v}_{1} \\cdot \\mathbf{v}_{1}} \\mathbf{v}_{1}+\\mathbf{v}_{2} \\\\\n& \\mathbf{w}_{3}=\\frac{\\mathbf{v}_{1} \\cdot \\mathbf{w}_{3}}{\\mathbf{v}_{1} \\cdot \\mathbf{v}_{1}} \\mathbf{v}_{1}+\\frac{\\mathbf{v}_{2} \\cdot \\mathbf{w}_{3}}{\\mathbf{v}_{2} \\cdot \\mathbf{v}_{2}} \\mathbf{v}_{2}+\\mathbf{v}_{3} .\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lectures/ch4lecture2.html#section-4",
    "href": "lectures/ch4lecture2.html#section-4",
    "title": "Ch4 Lecture 2",
    "section": "",
    "text": "Step 1: \\(\\mathbf{v}_{1}=\\mathbf{w}_{1}=(1,1,1,-1)\\)\n\nStep 2: \\(\\mathbf{v}_{2}=\\mathbf{w}_{2}-\\frac{\\mathbf{v}_{1} \\cdot \\mathbf{w}_{2}}{\\mathbf{v}_{1} \\cdot \\mathbf{v}_{1}} \\mathbf{v}_{1}\\) . . .\n\\(=(2,-1,-1,1)-\\frac{-1}{4}(1,1,1,-1)=\\frac{1}{4}(9,-3,-3,3)\\)\n\n\nStep 3: \\(\\mathbf{v}_{3}=\\mathbf{w}_{3}-\\frac{\\mathbf{v}_{1} \\cdot \\mathbf{w}_{3}}{\\mathbf{v}_{1} \\cdot \\mathbf{v}_{1}} \\mathbf{v}_{1}-\\frac{\\mathbf{v}_{2} \\cdot \\mathbf{w}_{3}}{\\mathbf{v}_{2} \\cdot \\mathbf{v}_{2}} \\mathbf{v}_{2}\\)\n\n\n\\(=(-1,2,2,1)-\\frac{2}{4}(1,1,1,-1)-\\frac{-18}{108}(9,-3,-3,3)=(0,1,1,2)\\)"
  },
  {
    "objectID": "lectures/ch4lecture2.html#normalizing-q",
    "href": "lectures/ch4lecture2.html#normalizing-q",
    "title": "Ch4 Lecture 2",
    "section": "Normalizing \\(Q\\)",
    "text": "Normalizing \\(Q\\)\nWe can normalize the columns of \\(Q\\) by dividing each column by its length.\n\nSet \\(\\mathbf{q}_{j}=\\mathbf{v}_{j} /\\left\\|\\mathbf{v}_{j}\\right\\|\\)..\n\nStart with \\(\\mathbf{v}_{j}=\\mathbf{q}_{j}\\left\\|\\mathbf{v}_{j}\\right\\|\\)\nThen \\[\n\\left[\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\mathbf{v}_{3}\\right]=\\left[\\mathbf{q}_{1}, \\mathbf{q}_{2}, \\mathbf{q}_{3}\\right]\\left[\\begin{array}{ccc}\n\\left\\|\\mathbf{v}_{1}\\right\\| & 0 & 0 \\\\\n0 & \\left\\|\\mathbf{v}_{2}\\right\\| & 0 \\\\\n0 & 0 & \\left\\|\\mathbf{v}_{3}\\right\\|\n\\end{array}\\right]\n\\]\n\n\n\n\\[\n\\begin{aligned}\nA & =\\left[\\mathbf{q}_{1}, \\mathbf{q}_{2}, \\mathbf{q}_{3}\\right]\\left[\\begin{array}{ccc}\n\\left\\|\\mathbf{v}_{1}\\right\\| & 0 & 0 \\\\\n0 & \\left\\|\\mathbf{v}_{2}\\right\\| & 0 \\\\\n0 & 0 & \\left\\|\\mathbf{v}_{3}\\right\\|\n\\end{array}\\right]\\left[\\begin{array}{ccc}\n1 & \\frac{\\mathbf{v}_{1} \\cdot \\mathbf{w}_{2}}{\\mathbf{v}_{1} \\cdot \\mathbf{v}_{1}} & \\frac{\\mathbf{v}_{1} \\cdot \\mathbf{w}_{3}}{\\mathbf{v}_{1} \\cdot \\mathbf{v}_{1}} \\\\\n0 & 1 & \\frac{\\mathbf{v}_{2} \\cdot \\mathbf{w}_{3}}{\\mathbf{v}_{2}, \\mathbf{v}_{2}} \\\\\n0 & 0 & 1\n\\end{array}\\right]\n\\end{aligned}\n\\]\n\n\n\\[\n\\begin{aligned}\n& =\\left[\\mathbf{q}_{1}, \\mathbf{q}_{2}, \\mathbf{q}_{3}\\right]\\left[\\begin{array}{ccc}\n\\left\\|\\mathbf{v}_{1}\\right\\| & \\frac{\\mathbf{v}_{1} \\cdot \\mathbf{w}_{2}}{\\left\\|\\mathbf{v}_{1}\\right\\|} & \\frac{\\mathbf{v}_{1} \\cdot \\mathbf{w}_{3}}{\\left\\|\\mathbf{v}_{1}\\right\\|} \\\\\n0 & \\left\\|\\mathbf{v}_{2}\\right\\| & \\frac{\\mathbf{v}_{2} \\cdot \\mathbf{w}_{3}}{\\left\\|\\mathbf{v}_{2}\\right\\|} \\\\\n0 & 0 & \\left\\|\\mathbf{v}_{3}\\right\\|\n\\end{array}\\right] .\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lectures/ch4lecture2.html#final-form",
    "href": "lectures/ch4lecture2.html#final-form",
    "title": "Ch4 Lecture 2",
    "section": "Final form",
    "text": "Final form\n\\[\n\\begin{aligned}\nA & =\\left[\\mathbf{q}_{1}, \\mathbf{q}_{2}, \\mathbf{q}_{3}\\right]\\left[\\begin{array}{ccc}\n\\left\\|\\mathbf{v}_{1}\\right\\| & \\frac{\\mathbf{v}_{1} \\cdot \\mathbf{w}_{2}}{\\left\\|\\mathbf{v}_{1}\\right\\|} & \\frac{\\mathbf{v}_{1} \\cdot \\mathbf{w}_{3}}{\\left\\|\\mathbf{v}_{1}\\right\\|} \\\\\n0 & \\left\\|\\mathbf{v}_{2}\\right\\| & \\frac{\\mathbf{v}_{2} \\cdot \\mathbf{w}_{3}}{\\left\\|\\mathbf{v}_{2}\\right\\|} \\\\\n0 & 0 & \\left\\|\\mathbf{v}_{3}\\right\\|\n\\end{array}\\right] \\\\[10pt]\n&= \\left[\\mathbf{q}_{1}, \\mathbf{q}_{2}, \\mathbf{q}_{3}\\right]\\left[\\begin{array}{ccc}\n\\left\\|\\mathbf{v}_{1}\\right\\| & \\mathbf{q}_{1} \\cdot \\mathbf{w}_{2} & \\mathbf{q}_{1} \\cdot \\mathbf{w}_{3} \\\\\n0 & \\left\\|\\mathbf{v}_{2}\\right\\| & \\mathbf{q}_{2} \\cdot \\mathbf{w}_{3} \\\\\n0 & 0 & \\left\\|\\mathbf{v}_{3}\\right\\|\n\\end{array}\\right]\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lectures/ch4lecture2.html#example-3",
    "href": "lectures/ch4lecture2.html#example-3",
    "title": "Ch4 Lecture 2",
    "section": "Example",
    "text": "Example\nFind the QR factorization of the matrix\n\\[\nA=\\left[\\begin{array}{rrr}\n1 & 2 & -1 \\\\\n1 & -1 & 2 \\\\\n1 & -1 & 2 \\\\\n-1 & 1 & 1\n\\end{array}\\right]\n\\]\n\nWe already found the Gram-Schmidt orthogonalization of this matrix:"
  },
  {
    "objectID": "lectures/ch4lecture2.html#section-5",
    "href": "lectures/ch4lecture2.html#section-5",
    "title": "Ch4 Lecture 2",
    "section": "",
    "text": "\\[\n\\begin{aligned}\n& \\mathbf{w}_{1}=\\mathbf{v}_{1} \\\\\n& \\mathbf{w}_{2}=\\frac{\\mathbf{v}_{1} \\cdot \\mathbf{w}_{2}}{\\mathbf{v}_{1} \\cdot \\mathbf{v}_{1}} \\mathbf{v}_{1}+\\mathbf{v}_{2} \\\\\n& \\mathbf{w}_{3}=\\frac{\\mathbf{v}_{1} \\cdot \\mathbf{w}_{3}}{\\mathbf{v}_{1} \\cdot \\mathbf{v}_{1}} \\mathbf{v}_{1}+\\frac{\\mathbf{v}_{2} \\cdot \\mathbf{w}_{3}}{\\mathbf{v}_{2} \\cdot \\mathbf{v}_{2}} \\mathbf{v}_{2}+\\mathbf{v}_{3} .\n\\end{aligned}\n\\]\nIn matrix form, these become:\n\n\\[\nA=\\left[\\mathbf{w}_{1}, \\mathbf{w}_{2}, \\mathbf{w}_{3}\\right]=\\left[\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\mathbf{v}_{3}\\right]\\left[\\begin{array}{ccc}\n1 & \\frac{\\mathbf{v}_{1} \\cdot \\mathbf{w}_{2}}{\\mathbf{v}_{1} \\cdot \\mathbf{v}_{1}} & \\frac{\\mathbf{v}_{1} \\cdot \\mathbf{w}_{3}}{\\mathbf{v}_{1} \\cdot \\mathbf{v}_{1}} \\\\\n0 & 1 & \\frac{\\mathbf{v}_{2} \\cdot \\mathbf{w}_{3}}{\\mathbf{v}_{2} \\cdot \\mathbf{v}_{2}} \\\\\n0 & 0 & 1\n\\end{array}\\right]\n\\]\n\nWe are almost there! We have an upper triangular matrix and we have an orthogonal matrix; it’s just not normalized yet."
  },
  {
    "objectID": "lectures/ch4lecture2.html#solving-a-linear-system-with-qr",
    "href": "lectures/ch4lecture2.html#solving-a-linear-system-with-qr",
    "title": "Ch4 Lecture 2",
    "section": "Solving a linear system with QR",
    "text": "Solving a linear system with QR\nWe would like to solve the system \\(A \\mathbf{x}=\\mathbf{b}\\).\n\n\\(Q R \\mathbf{x}=\\mathbf{b}\\).\n\n\nMultiply both sides by \\(Q^{T}\\):\nSince \\(Q\\) is orthogonal, \\(Q^{T} Q=I\\) and \\(Q^{T} Q R \\mathbf{x}=R \\mathbf{x}=Q^{T} \\mathbf{b}\\).\n\n\nSuppose we have \\(\\mathbf{b}=(1,1,1,1)\\). Then we are trying to solve\n\\[\\left[\\begin{array}{rrr}\n2 & -1 / 2 & 1 \\\\\n0 & \\frac{3}{2} \\sqrt{3} & -\\sqrt{3} \\\\\n0 & 0 & \\sqrt{6}\n\\end{array}\\right]\\mathbf{x}=\\left[\\begin{array}{rrr}\n1 / 2 & 3 /(2 \\sqrt{3}) & 0 \\\\\n1 / 2 & -1 /(2 \\sqrt{3}) & 1 / \\sqrt{6} \\\\\n1 / 2 & -1 /(2 \\sqrt{3}) & 1 / \\sqrt{6} \\\\\n-1 / 2 & 1 /(2 \\sqrt{3}) & 2 / \\sqrt{6}\n\\end{array}\\right]^T \\left[\\begin{array}{rrr}1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{array}\\right]\n\\]"
  },
  {
    "objectID": "lectures/ch4lecture2.html#least-squares-with-qr-factorization",
    "href": "lectures/ch4lecture2.html#least-squares-with-qr-factorization",
    "title": "Ch4 Lecture 2",
    "section": "Least Squares with QR factorization",
    "text": "Least Squares with QR factorization\nWhat if we have a system that’s not consistent?\n\n\nSolve a related problem, just by multiplying both sides of the equation by \\(A^{T}\\) to get the Normal equations: \\[\nA^{T} A \\mathbf{x}=R^{T} Q^{T} Q R \\mathbf{x}=R^{T} I R \\mathbf{x}=R^{T} R \\mathbf{x}=A^{T} \\mathbf{b}=R^{T} Q^{T} \\mathbf{b} .\n\\]\nNotice that we can invert \\(R\\) because its diagonal entries are nonzero. So we can cancel it out to find\n\\[\n\\begin{aligned}\nR^{T} R \\mathbf{x}&=R^{T} Q^{T} \\mathbf{b} \\\\\nR \\mathbf{x}&= Q^{T} \\mathbf{b}\n\\end{aligned}\n\\]\nThis is just what we solved above!\n\npause\n\n\nThe QR factorization can be used to solve the least squares problem \\(A \\mathbf{x} \\approx \\mathbf{b}\\)."
  },
  {
    "objectID": "lectures/ch4lecture2.html#image-compression-idea",
    "href": "lectures/ch4lecture2.html#image-compression-idea",
    "title": "Ch4 Lecture 2",
    "section": "Image compression idea",
    "text": "Image compression idea\n\n\nHow can we represent this image in a way that captures the most important features?\nSimple way is pixel by pixel… but that’s a lot of data!\nWe note that the high frequency information is pretty sparse, the low frequency information more dense. So maybe separate these out and use different techniques for each."
  },
  {
    "objectID": "lectures/ch4lecture2.html#haaar-wavelet-transform",
    "href": "lectures/ch4lecture2.html#haaar-wavelet-transform",
    "title": "Ch4 Lecture 2",
    "section": "Haaar wavelet transform",
    "text": "Haaar wavelet transform\nIn 1D, N data points \\(\\left\\{x_{k}\\right\\}_{k=1}^{N}\\)\n\nCan average terms:\n\\[\n\\begin{equation*}\ny_{k}=\\frac{1}{2}\\left(x_{k}+x_{k-1}\\right), \\quad k \\in \\mathbb{Z}\n\\end{equation*}\n\\]\n\nThis is the average of two adjacent points.\nWe end up with as many points as we started with, but now they are “averaged” points, so we’ve lost information.\nCan’t recover the original data from this, but we can get a sense of the “low frequency” information.\n\n\n\nAt the same time, we can apply a difference filter (“unsmoothing”):\n\\[\n\\begin{equation*}\nz_{k}=\\frac{1}{2}\\left(x_{k}-x_{k-1}\\right), \\quad k=1, \\ldots, N\n\\end{equation*}\n\\]\n\nNow we can fully recover the original data, because \\(y_{k}+z_{k}=x_{k}, k=1, \\ldots, N\\)\n\n\n\nIdea: take our data, apply both filters to it and keep the results."
  },
  {
    "objectID": "lectures/ch4lecture2.html#example-4",
    "href": "lectures/ch4lecture2.html#example-4",
    "title": "Ch4 Lecture 2",
    "section": "Example",
    "text": "Example\n\\(g(t)=\\frac{3}{2}+\\cos \\left(\\frac{\\pi}{4} t\\right)-\\frac{1}{4} \\cos \\left(\\frac{7 \\pi}{4} t\\right), 0 \\leq t \\leq 8\\)\n\nSample at \\(t_{k}=k / 5, k=0,1, \\ldots, 40\\)\n\n\n\n\n\nx: black, y: red, z: green\n\n\n\nBut now we have twice as much data as we started with! We can just drop every other point “downsample”, and still be able to fully recover the original data."
  },
  {
    "objectID": "lectures/ch4lecture2.html#almost-orthogonal",
    "href": "lectures/ch4lecture2.html#almost-orthogonal",
    "title": "Ch4 Lecture 2",
    "section": "Almost orthogonal",
    "text": "Almost orthogonal\n\\[\n\\begin{gather}\nA_{6} A_{6}^{T}=\\frac{1}{4}\\left[\\begin{array}{rrrrrr}\n1 & 1 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 1 & 1 \\\\\n-1 & 1 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & -1 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & -1 & 1\n\\end{array}\\right]\\left[\\begin{array}{cccccc}\n1 & 0 & 0 & -1 & 0 & 0 \\\\\n1 & 0 & 0 & 1 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & -1 & 0 \\\\\n0 & 1 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & -1 \\\\\n0 & 0 & 1 & 0 & 0 & 1\n\\end{array}\\right] \\\\[10pt] =\\frac{1}{2}\\left[\\begin{array}{llllll}\n1 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 1\n\\end{array}\\right]=\\frac{1}{2} I_{6}\n\\end{gather}\n\\]"
  },
  {
    "objectID": "lectures/ch4lecture2.html#haar-wavelet-transform-matrix",
    "href": "lectures/ch4lecture2.html#haar-wavelet-transform-matrix",
    "title": "Ch4 Lecture 2",
    "section": "Haar Wavelet Transform Matrix",
    "text": "Haar Wavelet Transform Matrix\nCan make them orthogonal by dividing by \\(\\sqrt{2}\\):\n\\[\n\\begin{equation*}\ny_{k}=\\frac{\\sqrt{2}}{2}\\left(x_{k}+x_{k-1}\\right), \\quad k \\in \\mathbb{Z}\n\\end{equation*}\n\\]\nand\n\\[\n\\begin{equation*}\nz_{k}=\\frac{\\sqrt{2}}{2}\\left(x_{k}-x_{k-1}\\right), \\quad k \\in \\mathbb{Z}\n\\end{equation*}\n\\]\n\n\\[\nW_{N}=\\frac{\\sqrt{2}}{2}\\left[\\begin{array}{ccccccc}\n1 & 1 & 0 & 0 & \\ldots & 0 & 0  \\\\\n0 & 0 & 1 & 1 & \\ldots & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n0 & 0 & 0 & 0 & \\ldots & 1 & 1 \\\\\n\\ldots & \\ldots & \\ldots & \\ldots & \\ldots & \\ldots \\\\\n-1 & 1 & 0 & 0 & \\ldots & 0 & 0 \\\\\n0 & 0 & -1 & 1 & \\ldots & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n0 & 0 & 0 & 0 & \\ldots & -1 & 1\n\\end{array}\\right]=\\frac{\\sqrt{2}}{2}\\left[\\begin{array}{c}\nU \\\\\n\\ldots \\\\\nL\n\\end{array}\\right]\n\\]"
  },
  {
    "objectID": "lectures/ch4lecture2.html#moving-to-2d",
    "href": "lectures/ch4lecture2.html#moving-to-2d",
    "title": "Ch4 Lecture 2",
    "section": "Moving to 2D",
    "text": "Moving to 2D\nFor a 2D image represented pixel-by-pixel in the matrix \\(\\mathbf{A}\\), we can apply the 1D transform to each row and then to each column: \\(W_{m} A W_{n}^{T}\\)\n\nResult ends up in block form:\n\\[\nW_{m} A W_{n}^{T}=2\\left[\\begin{array}{ll}\nB & V  \\\\\nH & D\n\\end{array}\\right]\n\\]\n\n\n\n\n\n\n\n\n\n\nOriginal image\n\n\n\n\n\n\n\nAfter transform\n\n\n\n\n\n\n\nEnlarged blur\n\n\n\n\n\n\n\nEdge image\n\n\n\n\n\n\nFigure 1: Effects of Haar wavelet transform on an image.\n\n\n\n\n\n\\(B\\) represents the blurred image of \\(A\\), while \\(V, H\\) and \\(D\\) represent edges of the image \\(A\\) along vertical, horizontal and diagonal directions, respectively.\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n \n\n\n \n\n\n\n\n \n\n\n\nx: black, y: red, z: green\nOriginal image\nAfter transform\nEnlarged blur\nEdge image"
  },
  {
    "objectID": "lectures/ch4lecture2.html#section-6",
    "href": "lectures/ch4lecture2.html#section-6",
    "title": "Ch4 Lecture 2",
    "section": "",
    "text": "\\[\n\\begin{aligned}\n\\mathbf{v}_{1} & =\\mathbf{w}_{1}=(1,1,1,-1), \\\\\n\\mathbf{v}_{2} & =\\mathbf{w}_{2}-\\frac{\\mathbf{v}_{1} \\cdot \\mathbf{w}_{2}}{\\mathbf{v}_{1} \\cdot \\mathbf{v}_{1}} \\mathbf{v}_{1} \\\\\n& =(2,-1,-1,1)-\\frac{-1}{4}(1,1,1,-1)=\\frac{1}{4}(9,-3,-3,3), \\\\\n\\mathbf{v}_{3} & =\\mathbf{w}_{3}-\\frac{\\mathbf{v}_{1} \\cdot \\mathbf{w}_{3}}{\\mathbf{v}_{1} \\cdot \\mathbf{v}_{1}} \\mathbf{v}_{1}-\\frac{\\mathbf{v}_{2} \\cdot \\mathbf{w}_{3}}{\\mathbf{v}_{2} \\cdot \\mathbf{v}_{2}} \\mathbf{v}_{2} \\\\\n& =(-1,2,2,1)-\\frac{2}{4}(1,1,1,-1)-\\frac{-18}{108}(9,-3,-3,3) \\\\\n& =\\frac{1}{4}(-4,8,8,4)-\\frac{1}{4}(2,2,2,-2)+\\frac{1}{4}(6,-2,-2,2)=(0,1,1,2) .\n\\end{aligned}\n\\]\n\n\\[\n\\begin{aligned}\n& \\mathbf{u}_{1}=\\frac{\\mathbf{v}_{1}}{\\left\\|\\mathbf{v}_{1}\\right\\|}=\\frac{1}{2}(1,1,1,-1), \\\\\n& \\mathbf{u}_{2}=\\frac{\\mathbf{v}_{2}}{\\left\\|\\mathbf{v}_{2}\\right\\|}=\\frac{1}{\\sqrt{108}}(9,-3,-3,3)=\\frac{1}{2 \\sqrt{3}}(3,-1,-1,1), \\\\\n& \\mathbf{u}_{3}=\\frac{\\mathbf{v}_{3}}{\\left\\|\\mathbf{v}_{3}\\right\\|}=\\frac{1}{\\sqrt{6}}(0,1,1,2) .\n\\end{aligned}\n\\]\n\n\nThe \\(\\mathbf{u}\\) vectors we calculated before are the \\(\\mathbf{q}\\) vectors in the QR factorization."
  },
  {
    "objectID": "lectures/ch4lecture2.html#section-7",
    "href": "lectures/ch4lecture2.html#section-7",
    "title": "Ch4 Lecture 2",
    "section": "",
    "text": "\\[\n\\begin{gathered}\n\\left\\|\\mathbf{v}_{1}\\right\\|=\\|(1,1,1,-1)\\|=2 \\text { and } \\mathbf{q}_{1}=\\frac{1}{2}(1,1,1,-1) \\\\\n\\left\\|\\mathbf{v}_{2}\\right\\|=\\left\\|\\frac{1}{4}(9,-3,-3,3)\\right\\|=\\frac{3}{2} \\sqrt{3} \\text { and } \\mathbf{q}_{2}=\\frac{1}{2 \\sqrt{3}}(3,-1,-1,1) \\\\\n\\left\\|\\mathbf{v}_{3}\\right\\|=\\|(0,1,1,2)\\|=\\sqrt{6} \\text { and } \\mathbf{q}_{3}=\\frac{1}{\\sqrt{6}}(0,1,1,2)\n\\end{gathered}\n\\]\n\n\\[\n\\begin{aligned}\n\\left\\langle\\mathbf{q}_{1}, \\mathbf{w}_{2}\\right\\rangle & =\\frac{1}{2}(1,1,1,-1) \\cdot(2,-1,-1,1)=-\\frac{1}{2} \\\\\n\\left\\langle\\mathbf{q}_{1}, \\mathbf{w}_{3}\\right\\rangle & =\\frac{1}{2}(1,1,1,-1) \\cdot(-1,2,2,1)=1 \\\\\n\\left\\langle\\mathbf{q}_{2}, \\mathbf{w}_{3}\\right\\rangle & =\\frac{1}{2 \\sqrt{3}}(3,-1,-1,1) \\cdot(-1,2,2,1)=-\\sqrt{3} .\n\\end{aligned}\n\\]\n\n\n\\[\nA=\\left[\\begin{array}{rrr}\n1 / 2 & 3 /(2 \\sqrt{3}) & 0 \\\\\n1 / 2 & -1 /(2 \\sqrt{3}) & 1 / \\sqrt{6} \\\\\n1 / 2 & -1 /(2 \\sqrt{3}) & 1 / \\sqrt{6} \\\\\n-1 / 2 & 1 /(2 \\sqrt{3}) & 2 / \\sqrt{6}\n\\end{array}\\right]\\left[\\begin{array}{rrr}\n2 & -1 / 2 & 1 \\\\\n0 & \\frac{3}{2} \\sqrt{3} & -\\sqrt{3} \\\\\n0 & 0 & \\sqrt{6}\n\\end{array}\\right]=Q R\n\\]"
  },
  {
    "objectID": "lectures/ch4lecture2.html#section-8",
    "href": "lectures/ch4lecture2.html#section-8",
    "title": "Ch4 Lecture 2",
    "section": "",
    "text": "import sympy as sp\nQ = sp.Matrix([[1/2, 3/(2*sp.sqrt(3)), 0], [1/2, -1/(2*sp.sqrt(3)), 1/sp.sqrt(6)], [1/2, -1/(2*sp.sqrt(3)), 1/sp.sqrt(6)], [-1/2, 1/(2*sp.sqrt(3)), 2/sp.sqrt(6)]])\nR = sp.Matrix([[2, -1/2, 1], [0, 3*sp.sqrt(3)/2, -sp.sqrt(3)], [0, 0, sp.sqrt(6)]])\nb = sp.Matrix([1, 1, 1, 1])\nprint('$$\\n Q^T b ='+sp.latex(Q.T*b)+'\\n$$')\nprint('$$\\n '+sp.latex(R)+' \\mathbf{x} ='+sp.latex(Q.T*b)+'\\n$$')\n\\[\nQ^T b =\\left[\\begin{matrix}1.0\\\\\\frac{\\sqrt{3}}{3}\\\\\\frac{2 \\sqrt{6}}{3}\\end{matrix}\\right]\n\\] \\[\n\\left[\\begin{matrix}2 & -0.5 & 1\\\\0 & \\frac{3 \\sqrt{3}}{2} & - \\sqrt{3}\\\\0 & 0 & \\sqrt{6}\\end{matrix}\\right] \\mathbf{x} =\\left[\\begin{matrix}1.0\\\\\\frac{\\sqrt{3}}{3}\\\\\\frac{2 \\sqrt{6}}{3}\\end{matrix}\\right]\n\\]\n\nFind \\[\n\\mathbf{x}=\\left[\\begin{array}{l}\n1 / 3 \\\\\n2 / 3 \\\\\n2 / 3\n\\end{array}\\right]\n\\]"
  },
  {
    "objectID": "lectures/ch4lecture2.html#section-9",
    "href": "lectures/ch4lecture2.html#section-9",
    "title": "Ch4 Lecture 2",
    "section": "",
    "text": "Check:\n\\[\n\\mathbf{r}=\\mathbf{b}-A \\mathbf{x}=\\left[\\begin{array}{l}\n1 \\\\\n1 \\\\\n1 \\\\\n1\n\\end{array}\\right]-\\left[\\begin{array}{rrr}\n1 & 2 & -1 \\\\\n1 & -1 & 2 \\\\\n1 & -1 & 2 \\\\\n-1 & 1 & 1\n\\end{array}\\right]\\left[\\begin{array}{l}\n1 / 3 \\\\\n2 / 3 \\\\\n2 / 3\n\\end{array}\\right]=\\left[\\begin{array}{l}\n0 \\\\\n0 \\\\\n0 \\\\\n0\n\\end{array}\\right] .\n\\]"
  },
  {
    "objectID": "lectures/ch4lecture2.html#section-10",
    "href": "lectures/ch4lecture2.html#section-10",
    "title": "Ch4 Lecture 2",
    "section": "",
    "text": "\\[\n\\begin{array}{rlll}\n\\frac{1}{2}\\left(x_{1}+x_{2}\\right)=y_{2} & \\frac{1}{2}\\left(x_{3}+x_{4}\\right)=y_{4} & \\frac{1}{2}\\left(x_{5}+x_{6}\\right)=y_{6} \\\\\n\\frac{1}{2}\\left(-x_{1}+x_{2}\\right)=z_{2} & \\frac{1}{2}\\left(-x_{3}+x_{4}\\right)=z_{4} & \\frac{1}{2}\\left(-x_{5}+x_{6}\\right)=z_{6}\n\\end{array}\n\\]\n\n\\[\nA_{6} \\mathbf{x} \\equiv \\frac{1}{2}\\left[\\begin{array}{rrrrrr}\n1 & 1 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 1 & 1 \\\\\n-1 & 1 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & -1 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & -1 & 1\n\\end{array}\\right]\\left[\\begin{array}{l}\nx_{1} \\\\\nx_{2} \\\\\nx_{3} \\\\\nx_{4} \\\\\nx_{5} \\\\\nx_{6}\n\\end{array}\\right]=\\left[\\begin{array}{l}\ny_{2} \\\\\ny_{4} \\\\\ny_{6} \\\\\nz_{2} \\\\\nz_{4} \\\\\nz_{6}\n\\end{array}\\right]\n\\]"
  },
  {
    "objectID": "lectures/ch4lecture2.html#section-11",
    "href": "lectures/ch4lecture2.html#section-11",
    "title": "Ch4 Lecture 2",
    "section": "",
    "text": "In general,\n\\(A_{N} A_{N}^{T}=\\frac{1}{2} I_{N}\\)\n\n\nThese matrices are nearly orthogonal."
  },
  {
    "objectID": "lectures/ch5lecture1.html#eigenvalues-and-eigenvectors-1",
    "href": "lectures/ch5lecture1.html#eigenvalues-and-eigenvectors-1",
    "title": "Ch5 Lecture 1",
    "section": "Eigenvalues and Eigenvectors",
    "text": "Eigenvalues and Eigenvectors\n\nAn eigenvector of \\(A\\) is a nonzero vector \\(\\mathbf{x}\\) in \\(\\mathbb{R}^{n}\\) (or \\(\\mathbb{C}^{n}\\)) such that for some scalar \\(\\lambda\\), we have\n\\[\nA \\mathbf{x}=\\lambda \\mathbf{x}\n\\]\n\n\nWhy do we care? One reason: matrix multiplication can become scalar multiplication:\n\\[\n\\begin{aligned}\nA \\mathbf{x} & =\\lambda \\mathbf{x} \\\\\nA^{2} \\mathbf{x} & =A(A \\mathbf{x})=A \\lambda \\mathbf{x}=\\lambda A \\mathbf{x}=\\lambda^{2} \\mathbf{x} \\\\\n& \\vdots \\\\\nA^{k} \\mathbf{x} & =A\\left(A^{k-1} \\mathbf{x}\\right)=\\cdots=\\lambda^{k} \\mathbf{x} .\n\\end{aligned}\n\\]\n\n\nWill see how to use these ideas to understand dynamics even for vectors that aren’t eigenvectors."
  },
  {
    "objectID": "lectures/ch5lecture1.html#finding-eigenvalues-and-eigenvectors",
    "href": "lectures/ch5lecture1.html#finding-eigenvalues-and-eigenvectors",
    "title": "Ch5 Lecture 1",
    "section": "Finding Eigenvalues and Eigenvectors",
    "text": "Finding Eigenvalues and Eigenvectors\n\\(\\lambda\\) is an eigenvalue of \\(A\\) if \\(A \\mathbf{x}=\\lambda \\mathbf{x}\\left(=\\lambda I \\mathbf{x}\\right)\\)\n\n\\[\n\\mathbf{0}=\\lambda I \\mathbf{x}-A \\mathbf{x}=(\\lambda I-A) \\mathbf{x}\n\\]\n\n\npause\n\nRemember a system of equations \\(A \\mathbf{x}=0\\) has a non-trivial solution if and only if \\(A\\) is singular, or in other words \\(\\operatorname{det}(A)=0\\).\n\n\n\nSo, \\(\\lambda\\) is an eigenvalue of \\(A\\) \\(\\iff\\) \\(\\operatorname{det}(\\lambda I-A)=0\\).\n\n\n\nIf \\(A\\) is a square \\(n \\times n\\) matrix, the equation \\(\\operatorname{det}(\\lambda I-A)=0\\) is called the characteristic equation of \\(A\\), and the \\(n\\) th-degree monic polynomial \\(p(\\lambda)=\\operatorname{det}(\\lambda I-A)\\) is called the characteristic polynomial of \\(A\\)."
  },
  {
    "objectID": "lectures/ch5lecture1.html#eigenspaces-and-eigensystems",
    "href": "lectures/ch5lecture1.html#eigenspaces-and-eigensystems",
    "title": "Ch5 Lecture 1",
    "section": "Eigenspaces and Eigensystems",
    "text": "Eigenspaces and Eigensystems\n\nThe eigenspace corresponding to eigenvalue \\(\\lambda\\) is the subspace \\(\\mathcal{N}(\\lambda I-A)\\) of \\(\\mathbb{R}^{n}\\) (or \\(\\left.\\mathbb{C}^{n}\\right)\\).\nWe write \\(\\mathcal{E}_{\\lambda}(A)=\\mathcal{N}(\\lambda I-A)\\).\n\n\nNote: \\(\\mathbf{0}\\) is in every eigenspace, but it is not an eigenvector.\n\n\n\nThe eigensystem of the matrix \\(A\\) is a list of all the eigenvalues of \\(A\\) and, for each eigenvalue \\(\\lambda\\), a complete description of its eigenspace – usually a basis.\n\n\n\n\nEigensystem Algorithm:\nGiven \\(n \\times n\\) matrix \\(A\\), to find an eigensystem for \\(A\\) :\n\nFind the eigenvalues of \\(A\\).\nFor each scalar \\(\\lambda\\) in (1), use the null space algorithm to find a basis of the eigenspace \\(\\mathcal{N}(\\lambda I-A)\\)."
  },
  {
    "objectID": "lectures/ch5lecture1.html#a-reminder-the-null-space-algorithm",
    "href": "lectures/ch5lecture1.html#a-reminder-the-null-space-algorithm",
    "title": "Ch5 Lecture 1",
    "section": "A reminder: the Null Space algorithm",
    "text": "A reminder: the Null Space algorithm\nGiven an \\(m \\times n\\) matrix \\(A\\).\n\nCompute the reduced row echelon form \\(R\\) of \\(A\\).\nUse \\(R\\) to find the general solution to the homogeneous system \\(A \\mathbf{x}=0\\).\n\n\n\nWrite the general solution \\(\\mathbf{x}=\\left(x_{1}, x_{2}, \\ldots, x_{n}\\right)\\) to the homogeneous system in the form\n\n\n\n\\[\n\\mathbf{x}=x_{i_{1}} \\mathbf{w}_{1}+x_{i_{2}} \\mathbf{w}_{2}+\\cdots+x_{i_{n-r}} \\mathbf{w}_{n-r}\n\\]\nwhere \\(x_{i_{1}}, x_{i_{2}}, \\ldots, x_{i_{n-r}}\\) are the free variables.\n\n\n\nList the vectors \\(\\mathbf{w}_{1}, \\mathbf{w}_{2}, \\ldots, \\mathbf{w}_{n-r}\\). These form a basis of \\(\\mathcal{N}(A)\\)."
  },
  {
    "objectID": "lectures/ch5lecture1.html#example-of-the-null-space-algorithm",
    "href": "lectures/ch5lecture1.html#example-of-the-null-space-algorithm",
    "title": "Ch5 Lecture 1",
    "section": "Example of the Null Space algorithm",
    "text": "Example of the Null Space algorithm\nFind a basis for the null space of the matrix \\[A=\\left[\\begin{array}{llll}\n1 & 0 & 1 & 2 \\\\\n1 & 2 & 0 & 1 \\\\\n3 & 2 & 2 & 5 \\\\\n3 & 4 & 1 & 4\n\\end{array}\\right]\n\\]\n\nThe reduced row echelon form of \\(A\\) is\n\\[\nR=\\left[\\begin{array}{rrrr}\n1 & 0 & 1 & 2 \\\\\n0 & 1 & -1 / 2 & -1 / 2 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0\n\\end{array}\\right]\n\\]\n\n\n\n\nThe variables \\(x_{3}\\) and \\(x_{4}\\) are free, while \\(x_{1}\\) and \\(x_{2}\\) are bound.\nHence, the general solution of \\(A \\mathbf{x}=0\\) can be written as\n\\[\n\\begin{aligned}\nx_{1} & =-x_{3}-2 x_{4}, \\\\\nx_{2} & =\\frac{1}{2} x_{3}+\\frac{1}{2} x_{4}, \\\\\nx_{3} & =x_{3}, \\\\\nx_{4} & =x_{4},\n\\end{aligned}\n\\]\n\\[\n\\left[\\begin{array}{l}\nx_{1} \\\\\nx_{2} \\\\\nx_{3} \\\\\nx_{4}\n\\end{array}\\right]=x_{3}\\left[\\begin{array}{r}\n-1 \\\\\n1 / 2 \\\\\n1 \\\\\n0\n\\end{array}\\right]+x_{4}\\left[\\begin{array}{r}\n-2 \\\\\n1 / 2 \\\\\n0 \\\\\n1\n\\end{array}\\right] .\n\\]\n\nwhich becomes, in vector notation,\n\\[\n\\left[\\begin{array}{l}\nx_{1} \\\\\nx_{2} \\\\\nx_{3} \\\\\nx_{4}\n\\end{array}\\right]=x_{3}\\left[\\begin{array}{r}\n-1 \\\\\n1 / 2 \\\\\n1 \\\\\n0\n\\end{array}\\right]+x_{4}\\left[\\begin{array}{r}\n-2 \\\\\n1 / 2 \\\\\n0 \\\\\n1\n\\end{array}\\right] .\n\\]\n:::"
  },
  {
    "objectID": "lectures/ch5lecture1.html#example",
    "href": "lectures/ch5lecture1.html#example",
    "title": "Ch5 Lecture 1",
    "section": "Example",
    "text": "Example\nFind an eigensystem for the matrix \\(A=\\left[\\begin{array}{ll}7 & 4 \\\\ 3 & 6\\end{array}\\right]\\)\n\n\\[\n\\begin{aligned}\n0 & =\\operatorname{det}(\\lambda I-A)=\\operatorname{det}\\left[\\begin{array}{rr}\n\\lambda-7 & -4 \\\\\n-3 & \\lambda-6\n\\end{array}\\right] \\\\\n& =(\\lambda-7)(\\lambda-6)-(-3)(-4) \\\\\n& =\\lambda^{2}-13 \\lambda+42-12 \\\\\n& =\\lambda^{2}-13 \\lambda+30 \\\\\n& =(\\lambda-3)(\\lambda-10) .\n\\end{aligned}\n\\]\n\n\nEigenvalues are \\(\\lambda=3\\) and \\(\\lambda=10\\).\n\n\nFor \\(\\lambda=3\\), we have\n\\[\nA-3 I=\\left[\\begin{array}{rr}\n7-3 & 4 \\\\\n3 & 3-3\n\\end{array}\\right] =  {\\left[\\begin{array}{ll}\n4 & 4 \\\\\n3 & 3\n\\end{array}\\right] \\xrightarrow[E_{21}(-3 / 4)]{E_{1}(1 / 4)}\\left[\\begin{array}{ll}\n1 & 1 \\\\\n0 & 0\n\\end{array}\\right]}\n\\]\n\n\n\n\n\n\nSo \\(x_{2}\\) is free and \\(x_{1}\\) is bound, \\(x_{1}=-x_{2}\\),so\n\\[\n\\left[\\begin{array}{l}\nx_{1} \\\\\nx_{2}\n\\end{array}\\right]=\\left[\\begin{array}{c}\n-x_{2} \\\\\nx_{2}\n\\end{array}\\right]=x_{2}\\left[\\begin{array}{r}\n-1 \\\\\n1\n\\end{array}\\right]\n\\]\nso the eigenspace is spanned by \\(\\left[\\begin{array}{l}1 \\\\ -1\\end{array}\\right]\\).\n\n\n\nThus, a basis of \\(\\mathcal{E}_{3}(A)\\) is \\(\\{(-1,1)\\}\\).\n\n\nSimilarly, for \\(\\lambda=10\\), the basis of \\(\\mathcal{E}_{10}(A)\\) is \\(\\{(4 / 3,1)\\}\\)"
  },
  {
    "objectID": "lectures/ch5lecture1.html#two-examples-with-repeated-eigenvalues",
    "href": "lectures/ch5lecture1.html#two-examples-with-repeated-eigenvalues",
    "title": "Ch5 Lecture 1",
    "section": "Two examples with repeated eigenvalues",
    "text": "Two examples with repeated eigenvalues\nWhat happens when an eigenvalue is repeated?\nExample: find the eigenspace for eigenvalue \\(\\lambda=2\\) of these two matrices:\n\\[\n\\text { (a) }\\left[\\begin{array}{rrr}\n2 & 1 & 2 \\\\\n0 & 1 & -2 \\\\\n0 & 0 & 2\n\\end{array}\\right] \\quad \\text { (b) }\\left[\\begin{array}{lll}\n2 & 1 & 1 \\\\\n0 & 1 & 1 \\\\\n0 & 0 & 2\n\\end{array}\\right]\n\\]\n\nBoth matrices have eigenvalues \\(\\lambda=1,2,2\\), but their eigenspaces for \\(\\lambda=2\\) are different:\n\n\nAfter row reducing the first matrix, we get\n\\[\n\\left[\\begin{array}{rrr}\n0 & 1 & 2 \\\\\n0 & -1 & -2 \\\\\n0 & 0 & 0\n\\end{array}\\right] \\xrightarrow[E_{21}(1)]{\\longrightarrow}\\left[\\begin{array}{lll}\n0 & 1 & 2 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 0\n\\end{array}\\right]\n\\]\n\n\n\\[\n\\left[\\begin{array}{l}\nx_{1} \\\\\nx_{2} \\\\\nx_{3}\n\\end{array}\\right]=\\left[\\begin{array}{c}\nx_{1} \\\\\n-2 x_{3} \\\\\nx_{3}\n\\end{array}\\right]=x_{1}\\left[\\begin{array}{l}\n1 \\\\\n0 \\\\\n0\n\\end{array}\\right]+x_{3}\\left[\\begin{array}{r}\n0 \\\\\n-2 \\\\\n1\n\\end{array}\\right] .\n\\]\n\n\nA basis for \\(\\mathcal{E}_{2}(A)\\) is \\(\\{(1,0,0),(0,-2,1)\\}\\).\nDimension of \\(\\mathcal{E}_{2}(A)=2\\)."
  },
  {
    "objectID": "lectures/ch5lecture1.html#section",
    "href": "lectures/ch5lecture1.html#section",
    "title": "Ch5 Lecture 1",
    "section": "",
    "text": "\\[\n\\text { (a) }\\left[\\begin{array}{rrr}\n2 & 1 & 2 \\\\\n0 & 1 & -2 \\\\\n0 & 0 & 2\n\\end{array}\\right] \\quad \\text { (b) }\\left[\\begin{array}{lll}\n2 & 1 & 1 \\\\\n0 & 1 & 1 \\\\\n0 & 0 & 2\n\\end{array}\\right]\n\\]\nFor the second matrix, we have \\[\n\\left[\\begin{array}{rrr}\n0 & 1 & 1 \\\\\n0 & -1 & 1 \\\\\n0 & 0 & 0\n\\end{array}\\right] \\xrightarrow[E_{21}(1)]{\\longrightarrow}\\left[\\begin{array}{lll}\n0 & 1 & 1 \\\\\n0 & 0 & 2 \\\\\n0 & 0 & 0\n\\end{array}\\right] \\xrightarrow{E_{2}(1 / 2)}\\left[\\begin{array}{lll}\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 0 & 0\n\\end{array}\\right]\n\\]\n\nOnly free variable is \\(x_{1}\\), so\n\\[\n\\left[\\begin{array}{l}\nx_{1} \\\\\nx_{2} \\\\\nx_{3}\n\\end{array}\\right]=\\left[\\begin{array}{c}\nx_{1} \\\\\n0 \\\\\n0\n\\end{array}\\right]=x_{1}\\left[\\begin{array}{l}\n1 \\\\\n0 \\\\\n0\n\\end{array}\\right]\n\\]\n\n\nA basis for \\(\\mathcal{E}_{2}(A)\\) is \\(\\{(1,0,0)\\}\\). Dimension of \\(\\mathcal{E}_{2}(A)=1\\)."
  },
  {
    "objectID": "lectures/ch5lecture1.html#eigenvalue-multiplicity",
    "href": "lectures/ch5lecture1.html#eigenvalue-multiplicity",
    "title": "Ch5 Lecture 1",
    "section": "Eigenvalue Multiplicity",
    "text": "Eigenvalue Multiplicity\n\nThe algebraic multiplicity of \\(\\lambda\\) is the multiplicity of \\(\\lambda\\) as a root of the characteristic equation. The geometric multiplicity of \\(\\lambda\\) is the dimension of the space \\(\\mathcal{E}_{\\lambda}(A)=\\mathcal{N}(\\lambda I-A)\\).\n\n\n\nThe eigenvalue \\(\\lambda\\) of \\(A\\) is said to be simple if its algebraic multiplicity is 1 , that is, the number of times it occurs as a root of the characteristic equation is 1 . Otherwise, the eigenvalue is said to be repeated.\n\n\n\n\nFor an \\(n \\times n\\) matrix \\(A\\), the sum of the algebraic multiplicities of all the eigenvalues of \\(A\\) is \\(n\\).\n\n(In other words, there are \\(n\\) eigenvalues, counting algebraic multiplicities and complex numbers).\n\n\nThe geometric multiplicity of an eigenvalue is always less than or equal to its algebraic multiplicity."
  },
  {
    "objectID": "lectures/ch5lecture1.html#defective-matrices",
    "href": "lectures/ch5lecture1.html#defective-matrices",
    "title": "Ch5 Lecture 1",
    "section": "Defective Matrices",
    "text": "Defective Matrices\n\nA matrix is defective if one of its eigenvalues has geometric multiplicity less than its algebraic multiplicity.\n\n\nThis means that the sum of the geometric multiplicities of all the eigenvalues is less than \\(n\\)."
  },
  {
    "objectID": "lectures/ch5lecture1.html#functions-of-diagonal-matrices",
    "href": "lectures/ch5lecture1.html#functions-of-diagonal-matrices",
    "title": "Ch5 Lecture 1",
    "section": "Functions of Diagonal Matrices",
    "text": "Functions of Diagonal Matrices\nIt’s easy to find functions of matrices if the matrix is diagonal.\n\\[\nD = \\operatorname{diag}\\left\\{\\lambda_{1}, \\lambda_{2}, \\lambda_{3}\\right\\}=\\left[\\begin{array}{ccc}\n\\lambda_{1} & 0 & 0 \\\\\n0 & \\lambda_{2} & 0 \\\\\n0 & 0 & \\lambda_{3}\n\\end{array}\\right]\n\\]\n\n\\(D^{k}=\\operatorname{diag}\\left\\{\\lambda_{1}^{k}, \\lambda_{2}^{k}, \\ldots, \\lambda_{n}^{k}\\right\\}\\)\n\n\n\\[\nf(D)=\\operatorname{diag}\\left\\{f\\left(\\lambda_{1}\\right), f\\left(\\lambda_{2}\\right), \\ldots, f\\left(\\lambda_{n}\\right)\\right\\}\n\\]"
  },
  {
    "objectID": "lectures/ch5lecture1.html#similar-matrices",
    "href": "lectures/ch5lecture1.html#similar-matrices",
    "title": "Ch5 Lecture 1",
    "section": "Similar Matrices",
    "text": "Similar Matrices\n\nA matrix \\(A\\) is said to be similar to matrix \\(B\\) if there exists an invertible matrix \\(P\\) such that\n\\[\nP^{-1} A P=B .\n\\]\n\\(P\\) is called a similarity transformation matrix.\n\n\n\nIf \\(A\\) is similar to \\(M\\), say \\(P^{-1} A P=M\\). Then:\n\nFor every polynomial \\(q(x)\\), \\[\nq(M)=P^{-1} q(A) P .\n\\]\nThe matrices \\(A\\) and \\(M\\) have the same eigenvalues.\n\n\n\n\nSo in particular,\n\\[\n\\begin{equation*}\nA^{k}=P D^{k} P^{-1}\n\\end{equation*}\n\\]"
  },
  {
    "objectID": "lectures/ch5lecture1.html#diagonalization",
    "href": "lectures/ch5lecture1.html#diagonalization",
    "title": "Ch5 Lecture 1",
    "section": "Diagonalization",
    "text": "Diagonalization\n\n\n\n\n\n\n\nSuppose we have a \\(3 \\times 3\\) matrix \\(A\\)\nSuppose we then find three linearly independent eigenvectors \\(\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\mathbf{v}_{3}\\)\n\\(A \\mathbf{v}_{1}=\\lambda_{1} \\mathbf{v}_{1}, A \\mathbf{v}_{2}=\\lambda_{2} \\mathbf{v}_{2}\\)\n\\(A \\mathbf{v}_{1}=\\lambda_{1} \\mathbf{v}_{1}, A \\mathbf{v}_{2}=\\lambda_{2} \\mathbf{v}_{2}\\) etc\nPut this together: \\[\nA\\left[\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\mathbf{v}_{3}\\right]=\\left[\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\mathbf{v}_{3}\\right]\\left[\\begin{array}{ccc}\n\\lambda_{1} & 0 & 0 \\\\\n0 & \\lambda_{2} & 0 \\\\\n0 & 0 & \\lambda_{3}\n\\end{array}\\right]=\\left[\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\mathbf{v}_{3}\\right] \\operatorname{diag}\\left\\{\\lambda_{1}, \\lambda_{2}, \\lambda_{3}\\right\\}\n\\]\nNow define \\(P=\\left[\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\mathbf{v}_{3}\\right]\\) and \\(D=\\operatorname{diag}\\left\\{\\lambda_{1}, \\lambda_{2}, \\lambda_{3}\\right\\}\\)\nMultiply on the right by \\(P^{-1}\\) (can do because columns of \\(P\\) are linearly independent, so it’s invertible):\n\\(A P=P D\\Rightarrow P^{-1} D P=A\\)\n\n\nTo diagonalize an \\(n \\times n\\) matrix \\(A\\), we just need to find \\(n\\) independent eigenvectors and put them into the columns of a matrix \\(P\\). Then \\(P^{-1} A P=D\\)"
  },
  {
    "objectID": "lectures/ch5lecture1.html#example-1",
    "href": "lectures/ch5lecture1.html#example-1",
    "title": "Ch5 Lecture 1",
    "section": "Example",
    "text": "Example\nFind \\(f(A)\\) for \\(A=\\left[\\begin{array}{rrr} 2 & 1 & 2 \\\\0 & 1 & -2 \\\\0 & 0 & 2\\end{array}\\right]\\) with \\(f(x)=\\sin \\left(\\frac{\\pi}{2} x\\right)\\)\n\nFrom before, the basis vectors for the eigenspace of \\(\\lambda=2\\) are \\(\\{(1,0,0),(0,-2,1)\\}\\)\nThe basis for the eigenspace of \\(\\lambda=1\\) is \\(\\{(-1,1,0)\\}\\)\n\n\n\\[\nP=\\left[\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\mathbf{v}_{3}\\right]=\\left[\\begin{array}{rrr}\n1 & 0 & -1 \\\\\n0 & -2 & 1 \\\\\n0 & 1 & 0\n\\end{array}\\right] .\n\\]\n\n\n\\[\nP^{-1}=\\left[\\begin{array}{lll}\n1 & 1 & 2 \\\\\n0 & 0 & 1 \\\\\n0 & 1 & 2\n\\end{array}\\right]\n\\]\n\n\n\\[\nP^{-1} A P=\\left[\\begin{array}{lll}\n2 & 0 & 0 \\\\\n0 & 2 & 0 \\\\\n0 & 0 & 1\n\\end{array}\\right]=D\n\\]"
  },
  {
    "objectID": "lectures/ch5lecture1.html#section-1",
    "href": "lectures/ch5lecture1.html#section-1",
    "title": "Ch5 Lecture 1",
    "section": "",
    "text": "\\[\n\\begin{aligned}\n\\sin \\left(\\frac{\\pi}{2} A\\right)&=P^{-1} \\sin \\left(\\frac{\\pi}{2} D\\right) P \\\\[20pt]\n&= \\left[\\begin{array}{rrr}1 & 0 & -1 \\\\ 0 & -2 & 1 \\\\ 0 & 1 & 0\\end{array}\\right]\\left[\\begin{array}{ccc}\\sin \\left(2 \\frac{\\pi}{2}\\right) & 0 & 0 \\\\ 0 & \\sin \\left(2 \\frac{\\pi}{2}\\right) & 0 \\\\ 0 & 0 & \\sin \\left(1 \\frac{\\pi}{2}\\right)\\end{array}\\right]\\left[\\begin{array}{lll}1 & 1 & 2 \\\\ 0 & 0 & 1 \\\\ 0 & 1 & 2\\end{array}\\right]\\\\[10pt]\n&=\\left[\\begin{array}{ccc}0 & -1 & -2 \\\\ 0 & 1 & 2 \\\\ 0 & 0 & 0\\end{array}\\right]\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lectures/ch5lecture1.html#when-are-matrices-diagonalizable",
    "href": "lectures/ch5lecture1.html#when-are-matrices-diagonalizable",
    "title": "Ch5 Lecture 1",
    "section": "When are matrices diagonalizable?",
    "text": "When are matrices diagonalizable?\n\nThe \\(n \\times n\\) matrix \\(A\\) is diagonalizable if and only if there exists a linearly independent set of eigenvectors \\(\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\ldots, \\mathbf{v}_{n}\\) of \\(A\\) \\(\\dots\\) in which case \\(P=\\left[\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\ldots, \\mathbf{v}_{n}\\right]\\) is a diagonalizing matrix for \\(A\\).\n\n\n\nIf the \\(n \\times n\\) matrix \\(A\\) has \\(n\\) distinct eigenvalues, then \\(A\\) is diagonalizable.\n\nCaution: Just because the \\(n \\times n\\) matrix \\(A\\) has fewer than \\(n\\) distinct eigenvalues, you may not conclude that it is not diagonalizable."
  },
  {
    "objectID": "lectures/ch5lecture1.html#diagonalizable-transition-matrices",
    "href": "lectures/ch5lecture1.html#diagonalizable-transition-matrices",
    "title": "Ch5 Lecture 1",
    "section": "Diagonalizable Transition Matrices",
    "text": "Diagonalizable Transition Matrices\nSuppose you have the discrete dynamical system \\[\n\\mathbf{x}^{(k+1)}=A \\mathbf{x}^{(k)}\n\\]\n\nAssume \\(A\\) is diagonalizable, then we can write complete, linearly independent eigenvectors \\(\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\ldots, \\mathbf{v}_{n}\\), with eigenvalues \\(\\lambda_{1}, \\lambda_{2}, \\ldots, \\lambda_{n}\\).\nSuppose the eigenvalues are ordered, so that \\(|\\lambda_{1}| \\geq|\\lambda_{2}| \\geq \\cdots \\geq|\\lambda_{n}|\\).\n\n\n\n\n\n\n\n\n\nThen the eigenvectors form a basis of \\(\\mathbb{R}^{n}\\) (or \\(\\mathbb{C}^{n}\\)), so we can express our initial state as a linear combination of the eigenvectors: \\[\n\\begin{equation*}\n\\mathbf{x}^{(0)}=c_{1} \\mathbf{v}_{1}+c_{2} \\mathbf{v}_{2}+\\cdots+c_{n} \\mathbf{v}_{n}\n\\end{equation*}\n\\]\nNow we know the effect of multiplying by \\(A\\):\n\\[\n\\begin{aligned}\nA \\mathbf{x}^{(0)} & =A\\left(c_{1} \\mathbf{v}_{1}+c_{2} \\mathbf{v}_{2}+\\cdots+c_{n} \\mathbf{v}_{n}\\right) \\\\\n& =c_{1}\\left(A \\mathbf{v}_{1}\\right)+c_{2}\\left(A \\mathbf{v}_{2}\\right)+\\cdots+c_{n}\\left(A \\mathbf{v}_{n}\\right) \\\\\n& =c_{1} \\lambda_{1} \\mathbf{v}_{1}+c_{2} \\lambda_{2} \\mathbf{v}_{2}+\\cdots+c_{n} \\lambda_{n} \\mathbf{v}_{n}\n\\end{aligned}\n\\]\nCan do it repeatedly: \\[\n\\begin{equation*}\n\\mathbf{x}^{(k)}=c_{1} \\lambda_{1}^{k} \\mathbf{v}_{1}+c_{2} \\lambda_{2}^{k} \\mathbf{v}_{2}+\\cdots+c_{n} \\lambda_{n}^{k} \\mathbf{v}_{n}\n\\end{equation*}\n\\]"
  },
  {
    "objectID": "lectures/ch5lecture1.html#spectral-radius-and-dominant-eigenvalue",
    "href": "lectures/ch5lecture1.html#spectral-radius-and-dominant-eigenvalue",
    "title": "Ch5 Lecture 1",
    "section": "Spectral Radius and Dominant Eigenvalue",
    "text": "Spectral Radius and Dominant Eigenvalue\n\\[\n\\begin{equation*}\n\\mathbf{x}^{(k)}=c_{1} \\lambda_{1}^{k} \\mathbf{v}_{1}+c_{2} \\lambda_{2}^{k} \\mathbf{v}_{2}+\\cdots+c_{n} \\lambda_{n}^{k} \\mathbf{v}_{n}\n\\end{equation*}\n\\]\n\n\n\n\nWhat will happen when \\(\\lambda_{1}\\) is greater than 1? Equal to one? Less than one? What about the others?\n\n\n\nThe spectral radius \\(\\rho(A)\\) of a matrix \\(A\\) with eigenvalues \\(\\lambda_{1}, \\lambda_{2}, \\ldots, \\lambda_{n}\\) is\n\\[\n\\rho(A)=\\max \\left\\{\\left|\\lambda_{1}\\right|,\\left|\\lambda_{2}\\right|, \\ldots,\\left|\\lambda_{n}\\right|\\right\\} .\n\\]\n\n\n\n\nIf \\(\\left|\\lambda_{k}\\right|=\\rho(A)\\) and \\(\\lambda_{k}\\) is the only eigenvalue with this property, then \\(\\lambda_{k}\\) is the dominant eigenvalue of \\(A\\)."
  },
  {
    "objectID": "lectures/ch5lecture1.html#implications-of-the-spectral-radius",
    "href": "lectures/ch5lecture1.html#implications-of-the-spectral-radius",
    "title": "Ch5 Lecture 1",
    "section": "Implications of the Spectral Radius",
    "text": "Implications of the Spectral Radius\n\nFor \\(n \\times n\\) transition matrix \\(A\\), initial state vector \\(\\mathbf{x}^{(0)}\\):\n\n\nIf \\(\\rho(A)&lt;1\\), then \\(\\lim _{k \\rightarrow \\infty} A^{k} \\mathbf{x}^{(0)}=\\mathbf{0}\\).\n\n\n\n\nIf \\(\\rho(A)=1\\), then the sequence of norms \\(\\left\\{\\left\\|\\mathbf{x}^{(k)}\\right\\|\\right\\}_{k=0}^{\\infty}\\) is bounded.\n\n\n\n\nIf \\(\\rho(A)=1\\) and \\(\\lambda=1\\) is the dominant eigenvalue of \\(A\\), then \\(\\lim _{k \\rightarrow \\infty} \\mathbf{x}^{(k)}\\) is an element of \\(\\mathcal{E}_{1}(A)\\) – either an eigenvector or \\(\\mathbf{0}\\).\n\n\n\n\nIf \\(\\rho(A)&gt;1\\), then for some choices of \\(\\mathbf{x}^{(0)}\\) we have \\(\\lim _{k \\rightarrow \\infty}\\|\\mathbf{x}\\|=\\infty\\)"
  },
  {
    "objectID": "lectures/ch5lecture1.html#example-2",
    "href": "lectures/ch5lecture1.html#example-2",
    "title": "Ch5 Lecture 1",
    "section": "Example",
    "text": "Example\nYou have too many frogs.\n\nThere is a species of bird that eats frogs. (Yay!) Can these help you with your frog problem?\n\n\nLet \\(F_{k}\\) and \\(B_{k}\\) be the number of frogs and birds at year \\(k\\).\n\\[\n\\begin{aligned}\nB_{k+1} & =0.6 B_{k}+0.4 F_{k}, \\\\\nF_{k+1} & =-r B_{k}+1.4 F_{k},\n\\end{aligned}\n\\]\nwhere \\(r\\) is the rate at which the birds eat the frogs. (Suppose it’s around 0.35).\n\n\n\n\nThere are some issues with this model. You can end up with negative frogs. And you can have birds appear from nowhere. But let’s ignore that for now.\n\n\n\nIs it a good idea to introduce these birds?"
  },
  {
    "objectID": "lectures/ch5lecture1.html#section-2",
    "href": "lectures/ch5lecture1.html#section-2",
    "title": "Ch5 Lecture 1",
    "section": "",
    "text": "Let the population vector in the \\(k\\) th year be \\(\\mathbf{x}^{(k)}=\\left(B_{k}, F_{k}\\right)\\)\n\\[\n\\left[\\begin{array}{l}\nB_{k+1} \\\\\nF_{k+1}\n\\end{array}\\right]=\\left[\\begin{array}{rr}\n0.6 & 0.4 \\\\\n-0.35 & 1.4\n\\end{array}\\right]\\left[\\begin{array}{l}\nB_{k} \\\\\nF_{k}\n\\end{array}\\right]\n\\]\n\nFind eigenvalues and eigenvectors of the transition matrix.\n\n\nCode\nimport numpy as np\nA = np.array([[0.6, 0.4], [-0.35, 1.4]])\neigvals, eigvecs = np.linalg.eig(A)\neigvals_sorted = np.sort(eigvals)[::-1]\neigvecs_sorted = eigvecs[:, np.argsort(eigvals)[::-1]]\nprint('Eigenvalues: ' + str(eigvals_sorted))\nprint('Eigenvectors: '+ str(eigvecs_sorted))\n\n\nEigenvalues: [1.14142136 0.85857864]\nEigenvectors: [[-0.59421716 -0.83980458]\n [-0.80430465 -0.54288882]]\n\n\n\n\n\nCheck: use Sympy to diagonalize the matrix.\n\n\nD : Matrix([[0.858578643762691, 0], [0, 1.14142135623731]])\nP : Matrix([[0.839804577036026, 0.635068381284203], [0.542888821389189, 0.859598960745815]])\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0.6 & 0.4\\\\-0.35 & 1.4\\end{matrix}\\right]\\)\n\n\n\nWhat this means is that the component of the initial vector which is in the direction of the eigenvector corresponding to the dominant eigenvalue will grow exponentially, while the other component will decay.\nUnless we start only with a vector that’s proportional to the second eigenvector, the frog population will explode."
  },
  {
    "objectID": "lectures/ch5lecture1.html#section-3",
    "href": "lectures/ch5lecture1.html#section-3",
    "title": "Ch5 Lecture 1",
    "section": "",
    "text": "Code\nx0 = np.array([100, 10000]) \n\ndef print_iterations(x0, A, n1=10, n2=1):\n  x = x0\n  for i in range(n1):\n    for j in range(n2):\n      x = A @ x\n    print(x)\n\n\nprint_iterations(x0, A)\n\n\n[ 4060. 13965.]\n[ 8022. 18130.]\n[12065.2 22574.3]\n[16268.84 27381.2 ]\n[20713.784 32639.586]\n[25484.1048 38445.596 ]\n[30668.70128 44904.39772]\n[36362.979856 52132.11136 ]\n[42670.6324576 60257.9129544]\n[49705.54465632 69426.356776  ]"
  },
  {
    "objectID": "lectures/ch5lecture1.html#section-4",
    "href": "lectures/ch5lecture1.html#section-4",
    "title": "Ch5 Lecture 1",
    "section": "",
    "text": "Different starting points…\n\n\nCode\nx0 = eigvecs_sorted[0]*-1000\nprint_iterations(x0, A)\n\n\n[692.45212576 967.75040247]\n[ 802.57143644 1112.49231944]\n[ 926.53978964 1276.58924446]\n[1066.55957157 1462.93601587]\n[1225.11014929 1674.81457217]\n[1404.99191844 1915.95184879]\n[1609.37589058 2190.58541685]\n[1841.85970109 2503.53802188]\n[2106.5310294  2860.30233525]\n[2408.03955174 3267.13740907]"
  },
  {
    "objectID": "lectures/ch5lecture1.html#section-5",
    "href": "lectures/ch5lecture1.html#section-5",
    "title": "Ch5 Lecture 1",
    "section": "",
    "text": "What if we start with the second eigenvector?\n\n\nCode\nx0 = eigvecs_sorted[1]*-1000\nprint_iterations(x0, A)\n\n\n[699.73831812 478.5377227 ]\n[611.25807995 425.04440043]\n[536.77260814 381.12183262]\n[474.51229793 345.70015282]\n[422.98743989 317.90090967]\n[380.9528278  297.01566957]\n[347.37796451 282.48844767]\n[321.42215777 273.90153916]\n[302.41391033 270.96439961]\n[289.83410604 273.50529084]\n\n\n\nLooks good so far, doesn’t blow up. But if we continue (only printing results every 10 years now):\n\n\nCode\nx0 = eigvecs_sorted[1]*-1000\nprint_iterations(x0, A,n1 = 10, n2 = 10)\n\n\n[289.83410604 273.50529084]\n[493.86429346 642.61221892]\n[1724.50444871 2328.57998379]\n[6445.12974686 8722.60198143]\n[24186.97763221 32738.09888286]\n[ 90789.4036837  122888.24713326]\n[340796.1924577  461285.82916599]\n[1279247.72362158 1731530.09096614]\n[4801916.36748821 6499650.17996028]\n[18024969.23527191 24397758.22361141]\n\n\n\n\nEven the roundoff errors are starting to make the frog population explode"
  },
  {
    "objectID": "lectures/ch5lecture1.html#what-about-complex-eigenvalues",
    "href": "lectures/ch5lecture1.html#what-about-complex-eigenvalues",
    "title": "Ch5 Lecture 1",
    "section": "What about complex eigenvalues?",
    "text": "What about complex eigenvalues?\nLet’s repeat the above for a different matrix:\n\\[\n\\left[\\begin{array}{ll}\n0 & 2 \\\\\n-2 & 0\n\\end{array}\\right]\n\\]\n\n\n\nCode\nA = np.array([[0, 2], [-2, 0]])\neigvals, eigvecs = np.linalg.eig(A)\neigvals_sorted = np.sort(eigvals)[::-1]\neigvecs_sorted = eigvecs[:, np.argsort(eigvals)[::-1]]\nprint('Eigenvalues: ' + str(eigvals_sorted))\nprint('Eigenvectors: '+ str(eigvecs_sorted))\nprint_iterations(np.array([1,2]), A,n1 = 10, n2 = 1)\n\n\nEigenvalues: [0.+2.j 0.-2.j]\nEigenvectors: [[0.        -0.70710678j 0.        +0.70710678j]\n [0.70710678+0.j         0.70710678-0.j        ]]\n[ 4 -2]\n[-4 -8]\n[-16   8]\n[16 32]\n[ 64 -32]\n[ -64 -128]\n[-256  128]\n[256 512]\n[1024 -512]\n[-1024 -2048]\n\n\n\n\nLet’s plot that…\n\n\nCode\nimport matplotlib.pyplot as plt\n\ndef plot_iterations(x0, A, n1=10, n2=1):\n  plt.clf()\n  x = x0\n  x_vals = [x]\n  for i in range(n1):\n    for j in range(n2):\n      x = A @ x\n    x_vals.append(x)\n  x_vals = np.array(x_vals)\n  plt.plot(x_vals[:,0], x_vals[:,1], 'o-',)\n  for i, txt in enumerate(range(len(x_vals))):\n    plt.annotate(txt, (x_vals[i,0]+1, x_vals[i,1]+1))\n  #plt.plot(x_vals[:,0], x_vals[:,1], 'o-')\n  plt.xlabel('B')\n  plt.ylabel('F')\n  plt.title('Population dynamics: eigenvalues='+str(np.linalg.eig(A)[0]))\n  plt.show()"
  },
  {
    "objectID": "lectures/ch5lecture1.html#your-turn",
    "href": "lectures/ch5lecture1.html#your-turn",
    "title": "Ch5 Lecture 1",
    "section": "Your turn",
    "text": "Your turn\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/ch5lecture1.html#section-6",
    "href": "lectures/ch5lecture1.html#section-6",
    "title": "Ch5 Lecture 1",
    "section": "",
    "text": "Please enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/ch5lecture1.html#section-7",
    "href": "lectures/ch5lecture1.html#section-7",
    "title": "Ch5 Lecture 1",
    "section": "",
    "text": "Get this into matrix form.\nLet \\[\n\\bar{x}_{k}=\\left[\\begin{array}{c}\nF_{k} \\\\\nF_{k+1}\n\\end{array}\\right]\n\\]\n\nThen\n\\[\n\\bar{x}_{k+1}=\\left[\\begin{array}{c}\nF_{k+1} \\\\\nF_{k+2}\n\\end{array}\\right]=\\left[\\begin{array}{c}\nF_{k+1} \\\\\nF_{k}+F_{k+1}\n\\end{array}\\right]=\\left[\\begin{array}{ll}\n0 & 1 \\\\\n1 & 1\n\\end{array}\\right]\\left[\\begin{array}{c}\nF_{k} \\\\\nF_{k+1}\n\\end{array}\\right]=\\left[\\begin{array}{ll}\n0 & 1 \\\\\n1 & 1\n\\end{array}\\right] \\bar{x}_{k}\n\\]\nfor \\(\\bar{x}_{0}=\\left[\\begin{array}{l}0 \\\\1\\end{array}\\right]\\)\n\n\nTo understand the dynamics, we need to diagonalize the matrix.\n\\[\n\\begin{aligned}\n\\operatorname{det}(A-\\lambda I) & =0 \\\\\n\\operatorname{det}\\left[\\begin{array}{cc}\n-\\lambda & 1 \\\\\n1 & 1-\\lambda\n\\end{array}\\right] & =0 \\\\\n\\lambda^{2}-\\lambda-1 & =0 \\\\\n\\lambda & =\\frac{1 \\pm \\sqrt{5}}{2}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lectures/ch5lecture1.html#another-example",
    "href": "lectures/ch5lecture1.html#another-example",
    "title": "Ch5 Lecture 1",
    "section": "Another example",
    "text": "Another example\n\n\n\nFibonacci Statue\n\n\n\nFibonacci sequence: 0, 1, 1, 2, 3, 5, 8, 13, 21, …\n\\(F_{0}=0\\) and \\(F_{1}=1\\). For \\(n \\geq 2\\), the \\(n\\)-th Fibonacci number is\n\\[\nF_{n}=F_{n-1}+F_{n-2}\n\\]\n\n\nWhat is the \\(n\\)-th Fibonacci number? What is \\(F_{100}\\)?\n(Treatment from here)"
  },
  {
    "objectID": "lectures/ch5lecture1.html#section-8",
    "href": "lectures/ch5lecture1.html#section-8",
    "title": "Ch5 Lecture 1",
    "section": "",
    "text": "Eigenvalues (call them phi and psi): \\[\n\\begin{aligned}\n& \\varphi=\\frac{1+\\sqrt{5}}{2} \\approx 1.618 \\\\\n& \\psi=\\frac{1-\\sqrt{5}}{2} \\approx-0.618\n\\end{aligned}\n\\]\nEigenvectors:\n\\[\n\\bar{v}_{\\varphi}=\\left[\\begin{array}{l}\n1 \\\\\n\\varphi\n\\end{array}\\right] \\quad \\text { and } \\quad \\bar{v}_{\\psi}=\\left[\\begin{array}{l}\n1 \\\\\n\\psi\n\\end{array}\\right]\n\\]\n\n\\[\nA=P D P^{-1}=\\left[\\begin{array}{ll}\n1 & 1 \\\\\n\\varphi & \\psi\n\\end{array}\\right]\\left[\\begin{array}{ll}\n\\varphi & 0 \\\\\n0 & \\psi\n\\end{array}\\right] \\frac{1}{\\sqrt{5}}\\left[\\begin{array}{cc}\n-\\psi & 1 \\\\\n\\varphi & -1\n\\end{array}\\right]\n\\]"
  },
  {
    "objectID": "lectures/ch5lecture1.html#section-9",
    "href": "lectures/ch5lecture1.html#section-9",
    "title": "Ch5 Lecture 1",
    "section": "",
    "text": "\\[\n\\begin{aligned}\n& A^{k}=P D^{k} P^{-1} \\\\\n& =\\left[\\begin{array}{cc}\n1 & 1 \\\\\n\\varphi & \\psi\n\\end{array}\\right]\\left[\\begin{array}{cc}\n\\varphi^{k} & 0 \\\\\n0 & \\psi^{k}\n\\end{array}\\right] \\frac{1}{\\sqrt{5}}\\left[\\begin{array}{cc}\n-\\psi & 1 \\\\\n\\varphi & -1\n\\end{array}\\right] \\\\\n& =\\left[\\begin{array}{cc}\n\\varphi^{k} & \\psi^{k} \\\\\n\\varphi^{k+1} & \\psi^{k+1}\n\\end{array}\\right] \\frac{1}{\\sqrt{5}}\\left[\\begin{array}{cc}\n-\\psi & 1 \\\\\n\\varphi & -1\n\\end{array}\\right] \\\\\n& =\\frac{1}{\\sqrt{5}}\\left[\\begin{array}{cc}\n-\\psi \\varphi^{k}+\\varphi \\psi^{k} & \\varphi^{k}-\\psi^{k} \\\\\n-\\psi \\varphi^{k+1}+\\varphi \\psi^{k+1} & \\varphi^{k+1}-\\psi^{k+1}\n\\end{array}\\right]\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lectures/ch5lecture1.html#section-10",
    "href": "lectures/ch5lecture1.html#section-10",
    "title": "Ch5 Lecture 1",
    "section": "",
    "text": "Finally,\n\\[\n\\left[\\begin{array}{c}\nF_{k} \\\\\nF_{k+1}\n\\end{array}\\right]=A^{k} \\bar{x}_{0}=\\frac{1}{\\sqrt{5}}\\left[\\begin{array}{cc}\n-\\psi \\varphi^{k}+\\varphi \\psi^{k} & \\varphi^{k}-\\psi^{k} \\\\\n-\\psi \\varphi^{k+1}+\\varphi \\psi^{k+1} & \\varphi^{k+1}-\\psi^{k+1}\n\\end{array}\\right]\\left[\\begin{array}{l}\n0 \\\\\n1\n\\end{array}\\right]=\\frac{1}{\\sqrt{5}}\\left[\\begin{array}{c}\n\\varphi^{k}-\\psi^{k} \\\\\n\\varphi^{k+1}-\\psi^{k+1}\n\\end{array}\\right]\n\\]\n\nand so\n\\[\nF_{k}=\\frac{1}{\\sqrt{5}}\\left(\\varphi^{k}-\\psi^{k}\\right)=\\frac{1}{\\sqrt{5}}\\left(\\left(\\frac{1+\\sqrt{5}}{2}\\right)^{k}+\\left(\\frac{1-\\sqrt{5}}{2}\\right)^{k}\\right)\n\\]\n\n\nBut for large \\(k\\), \\(\\psi^{k}\\) is very small, so\n\\[\nF_{k} \\approx \\frac{\\varphi^{k}}{\\sqrt{5}}\n\\]\n\n\n\\[\n\\lim _{k \\rightarrow \\infty} \\frac{F_{k+1}}{F_{k}}=\\varphi\n\\]"
  },
  {
    "objectID": "lectures/ch5lecture1.html#section-11",
    "href": "lectures/ch5lecture1.html#section-11",
    "title": "Ch5 Lecture 1",
    "section": "",
    "text": "This is the golden ratio!\n\n\n\nGolden Ratio spiral"
  },
  {
    "objectID": "lectures/ch5lecture1.html#section-12",
    "href": "lectures/ch5lecture1.html#section-12",
    "title": "Ch5 Lecture 1",
    "section": "",
    "text": "Go back to some of the previous examples we’ve used, or that you’ve used in your projects. Work in pairs to find the eigenvalues and eigenvectors of the transition matrix. What do the eigenvalues tell you about the dynamics of the system?\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n \n\n\n \n\n\n\n\n \n\n\n\nFibonacci Statue\nGolden Ratio spiral"
  },
  {
    "objectID": "HW/HW5.html",
    "href": "HW/HW5.html",
    "title": "Homework 5",
    "section": "",
    "text": "1 \nBy hand, find eigensystems for each of these matrices. Then specify the algebraic and geometric multiplicity of each eigenvalue.\n\n\\(\\left[\\begin{array}{rr}7 & -10 \\\\ 5 & -8\\end{array}\\right]\\)\n\\(\\left[\\begin{array}{rrr}-1 & 0 & 0 \\\\ 1 & -1 & 0 \\\\ 0 & 1 & -1\\end{array}\\right]\\)\n\\(\\left[\\begin{array}{lll}2 & 1 & 1 \\\\ 0 & 3 & 1 \\\\ 0 & 0 & 2\\end{array}\\right]\\)\n\\(\\left[\\begin{array}{ll}0 & 2 \\\\ 2 & 0\\end{array}\\right]\\)\n\\(\\left[\\begin{array}{rr}0 & -2 \\\\ 2 & 0\\end{array}\\right]\\)\n\n\n\n2 \nCompute the eigensystems of these matrices, and identify any defective matrices. (You can do this by hand or on the computer.)\n\n\\(\\left[\\begin{array}{lll}2 & 0 & 1 \\\\ 0 & 0 & 0 \\\\ 1 & 0 & 2\\end{array}\\right]\\)\n\\(\\left[\\begin{array}{lll}2 & 0 & 0 \\\\ 0 & 3 & 1 \\\\ 0 & 6 & 2\\end{array}\\right]\\)\n\\(\\left[\\begin{array}{rr}1+\\mathrm{i} & 3 \\\\ 0 & \\mathrm{i}\\end{array}\\right]\\)\n\\(\\left[\\begin{array}{rrr}1 & -2 & 1 \\\\ -2 & 4 & -2 \\\\ 0 & 0 & 1\\end{array}\\right]\\)\n\\(\\left[\\begin{array}{rrrr}2 & 1 & -1 & -2 \\\\ 0 & 1 & -1 & -2 \\\\ 0 & 0 & 0 & 1 \\\\ 0 & 0 & 1 & 0\\end{array}\\right]\\)\n\n\n\n3 \nBy hand, find a matrix \\(P\\) such that \\(P^{-1} A P\\) is diagonal for each of the following matrices. Then use \\(P\\) to find a formula for \\(A^{k}, k\\) a positive integer.\n\n\\(\\left[\\begin{array}{lll}2 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 3\\end{array}\\right]\\)\n\\(\\left[\\begin{array}{lll}1 & 2 & 2 \\\\ 0 & 0 & 0 \\\\ 0 & 2 & 2\\end{array}\\right]\\)\n\\(\\left[\\begin{array}{ll}1 & 2 \\\\ 3 & 2\\end{array}\\right]\\)\n\\(\\left[\\begin{array}{ll}0 & 2 \\\\ 2 & 0\\end{array}\\right]\\)\n\\(\\left[\\begin{array}{rrrr}2 & 1 & 0 & 0 \\\\ 0 & 0 & -1 & 0 \\\\ 0 & 0 & 3 & 1 \\\\ 0 & 0 & 0 & 1\\end{array}\\right]\\)\n\n\n\n4 \nCompute \\(\\sin \\left(\\frac{\\pi}{6} A\\right)\\) and \\(\\cos \\left(\\frac{\\pi}{6} A\\right)\\), where \\(A=\\left[\\begin{array}{rr}2 & 4 \\\\ 0 & -3\\end{array}\\right]\\).\n\n\n5 \nBy hand, find the spectral radius and dominant eigenvalue, if any, for each of these matrices:\n\n\\(\\left[\\begin{array}{rr}-7 & -6 \\\\ 9 & 8\\end{array}\\right]\\)\n\\(\\frac{1}{3}\\left[\\begin{array}{ll}1 & 3 \\\\ 2 & 0\\end{array}\\right]\\)\n\\(\\left[\\begin{array}{lll}0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 1 & 0 & 0\\end{array}\\right]\\)\n\\(\\frac{1}{2}\\left[\\begin{array}{lll}1 & 0 & 1 \\\\ 1 & 0 & 0 \\\\ 0 & 2 & 1\\end{array}\\right]\\)\n\\(\\left[\\begin{array}{rr}1 & 1 \\\\ -1 & -1\\end{array}\\right]\\)\n\n\n\n6 \nIf the matrices of the previous exercise are transition matrices, for which do all \\(\\mathbf{x}^{(k)}\\) remain bounded as \\(k \\rightarrow \\infty\\) ? Are any of these matrices stable?\n\n\n7 \nThe three-stage insect model of Example 2.21 yields a transition matrix\n\\[\nA=\\left[\\begin{array}{ccc}\n0.2 & 0 & 0.25 \\\\\n0.6 & 0.3 & 0 \\\\\n0 & 0.6 & 0.8\n\\end{array}\\right]\n\\]\nUse a technology tool to calculate the eigenvalues of this matrix. Deduce that \\(A\\) is diagonalizable and determine the approximate growth rate from one state to the next (after much time has passed), given a random initial vector.\n\n\n8 \nThe financial model of Example 2.27 gives rise to a discrete dynamical system \\(x^{(k+1)}=A x^{(k)}\\), where the transition matrix is\n\\[\nA=\\left[\\begin{array}{rrr}\n1 & 0.06 & 0.12 \\\\\n1 & 0 & 0 \\\\\n0 & 1 & 0\n\\end{array}\\right]\n\\]\nUse a technology tool to calculate the eigenvalues of this matrix. Deduce that \\(A\\) is diagonalizable and determine the approximate growth rate from one state to the next, given a random initial vector.\nFind a starting vector \\(x^{(0)}\\) such that growth rate for the first few iterations is quite different from this approximate growth rate, and demonstrate this using a technology tool.\n(If you do this using Sympy, and you set up the matrix A using rational numbers, you can get exact results and find a starting vector which will never reach your approximate growth rage. You can see how this result changes if you use floating point numbers at some point in the calculation, e.g. x = N(x).)\n\nfrom sympy import Matrix, Rational\n\n# Set up the matrix A in symbolic form\nA = Matrix([[1, Rational(6, 100), Rational(12, 100)], [1, 0, 0], [0, 1, 0]])\n\n\n\n9 \nOn this problem,you may use a computer to find eigensystems etc, but don’t solve it using simulations and trial and error.\nA species of bird can be divided into three age groups: age less than 2 years for group 1, age between 2 and 4 years for group 2, and age between 4 and 6 years for the third group. Assume that these birds have at most a 6-year life span. It is estimated that the survival rates for birds in groups 1 and 2 are \\(50 \\%\\) and 75%, respectively. Also, birds in groups 1, 2, and 3 produce 0,1 , and 3 offspring on average in any biennium (period of 2 years).\n\nModel this bird population as a discrete dynamical system and analyze the long-term change in the population – give a percentage by which the population of group 1 changes from year to year.\nNow suppose that the survival rates for group 1 and 2 are not known (so are no longer assumed to be 50% and 75%), but are assumed to be equal to one another. What value for this survival rate would make the population stable in the long run?"
  },
  {
    "objectID": "HW/HW5.sol.html",
    "href": "HW/HW5.sol.html",
    "title": "Homework 5 Solutions",
    "section": "",
    "text": "1 \nBy hand, find eigensystems for each of these matrices. Then specify the algebraic and geometric multiplicity of each eigenvalue.\n\n\\(\\left[\\begin{array}{rr}7 & -10 \\\\ 5 & -8\\end{array}\\right]\\)\n\\(\\left[\\begin{array}{rrr}-1 & 0 & 0 \\\\ 1 & -1 & 0 \\\\ 0 & 1 & -1\\end{array}\\right]\\)\n\\(\\left[\\begin{array}{lll}2 & 1 & 1 \\\\ 0 & 3 & 1 \\\\ 0 & 0 & 2\\end{array}\\right]\\)\n\\(\\left[\\begin{array}{ll}0 & 2 \\\\ 2 & 0\\end{array}\\right]\\)\n\\(\\left[\\begin{array}{rr}0 & -2 \\\\ 2 & 0\\end{array}\\right]\\)\n\n\n5.1.1 and 5.1.3\nEigenvalue, algebraic multiplicity, geometric multiplicity, bases:\n\n\\(\\lambda=-3,1,1,\\{(2,1)\\}, \\lambda=2,1,1,\\{(1,1)\\}\\)\n\\(\\lambda=-1,3,1,\\{(0,0,1)\\}\\),\n\\(\\lambda=2,2,2,\\{(1,0,0),(0,-1,1)\\}, \\lambda=3,1,1,\\{(1,1,0)\\}\\)\n\\(\\lambda=-2,1,1\\), \\(\\{(-1,1)\\}, \\lambda=2,1,1,\\{(1,1)\\}(\\mathrm{e}) \\lambda=-2 \\mathrm{i}, 1,1,\\{(\\mathrm{i},-1)\\}, \\lambda=2 \\mathrm{i}, 1,1,\\{(\\mathrm{i}, 1)\\}\\)\n\n\n\n\n2 \nCompute the eigensystems of these matrices, and identify any defective matrices. (You can do this by hand or on the computer.)\n\n\\(\\left[\\begin{array}{lll}2 & 0 & 1 \\\\ 0 & 0 & 0 \\\\ 1 & 0 & 2\\end{array}\\right]\\)\n\\(\\left[\\begin{array}{lll}2 & 0 & 0 \\\\ 0 & 3 & 1 \\\\ 0 & 6 & 2\\end{array}\\right]\\)\n\\(\\left[\\begin{array}{rr}1+\\mathrm{i} & 3 \\\\ 0 & \\mathrm{i}\\end{array}\\right]\\)\n\\(\\left[\\begin{array}{rrr}1 & -2 & 1 \\\\ -2 & 4 & -2 \\\\ 0 & 0 & 1\\end{array}\\right]\\)\n\\(\\left[\\begin{array}{rrrr}2 & 1 & -1 & -2 \\\\ 0 & 1 & -1 & -2 \\\\ 0 & 0 & 0 & 1 \\\\ 0 & 0 & 1 & 0\\end{array}\\right]\\)\n\n\n5.1.2 and 5.1.4\nEigenvalue, basis, defective (yes/no)\n\n\\(0,\\{(-1,0,1)\\}, 1,\\{(0,1,0)\\}\\), \\(3,\\{(1,0,1)\\}\\), no\n\\(0,\\{(1,0,0)\\}, 2,\\{(0,1,2)\\}, 5,\\{(0,-1,3)\\}\\) no\n\\(1+\\mathrm{i}\\), \\(\\{(1,0)\\}\\), i, \\(\\{(-3,1)\\}\\), no\n\\(0,\\{(2,1,0)\\}, 1,\\{(-1,2,4)\\}, 5,\\{(-1,2,0)\\}\\), no (e) \\(-1,\\{(1,3,-6,6)\\}, 1,\\{(-1,1,0,0)\\}, 2,\\{(1,0,0,0)\\}\\), yes\n\n\n\n\n3 \nBy hand, find a matrix \\(P\\) such that \\(P^{-1} A P\\) is diagonal for each of the following matrices. Then use \\(P\\) to find a formula for \\(A^{k}, k\\) a positive integer.\n\n\\(\\left[\\begin{array}{lll}2 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 3\\end{array}\\right]\\)\n\\(\\left[\\begin{array}{lll}1 & 2 & 2 \\\\ 0 & 0 & 0 \\\\ 0 & 2 & 2\\end{array}\\right]\\)\n\\(\\left[\\begin{array}{ll}1 & 2 \\\\ 3 & 2\\end{array}\\right]\\)\n\\(\\left[\\begin{array}{ll}0 & 2 \\\\ 2 & 0\\end{array}\\right]\\)\n\\(\\left[\\begin{array}{rrrr}2 & 1 & 0 & 0 \\\\ 0 & 0 & -1 & 0 \\\\ 0 & 0 & 3 & 1 \\\\ 0 & 0 & 0 & 1\\end{array}\\right]\\)\n\n\n5.2.3 and 4\nMatrices P:\n\n\\(\\left[\\begin{array}{lll}0 & 1 & 1 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1\\end{array}\\right]\\)\n\\(\\left[\\begin{array}{rll}0 & 1 & 2 \\\\ -1 & 0 & 0 \\\\ 1 & 0 & 1\\end{array}\\right]\\)\n\\(\\left[\\begin{array}{rl}-1 & 2 \\\\ 1 & 3\\end{array}\\right]\\)\n\\(\\left[\\begin{array}{rr}-1 & 1 \\\\ 1 & 1\\end{array}\\right]\\)\n\\(\\left[\\begin{array}{cccc}1 & -1 & 1 & -1 \\\\ -2 & 1 & 0 & -1 \\\\ 0 & -1 & 0 & 3 \\\\ 0 & 2 & 0 & 0\\end{array}\\right]\\)\n\n\\(A^k\\):\n\n\\(\\left[\\begin{array}{rrr}2^{k} & 0 & -2^{k}+3^{k} \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 3^{k}\\end{array}\\right]\\)\n\\(\\left[\\begin{array}{ccr}1 & -2+2^{k+1} & -2 * 2^{k+1} \\\\ 0 & 0 & 0 \\\\ 0 & 2^{k} & 2^{k}\\end{array}\\right]\\)\n\\(\\frac{1}{5}\\left[\\begin{array}{rr}3(-1)^{k}+2 \\cdot 4^{k} & -2(-1)^{k}+2 \\cdot 4^{k} \\\\ -3(-1)^{k}+3 \\cdot 4^{k} & 2(-1)^{k}+3 \\cdot 4^{k}\\end{array}\\right]\\)\n\\(\\frac{1}{2}\\left[\\begin{array}{rr}(-2)^{k}+2^{k} & -(-2)^{k}+2^{k} \\\\ -(-2)^{k}+2^{k} & (-2)^{k}+2^{k}\\end{array}\\right]\\)\n\\(\\left[\\begin{array}{rrrr}2^{k} & 2^{k-1} & 2^{k-1}-3^{k-1}-\\frac{1}{2}+2^{k-1}-\\frac{1}{2} 3^{k-1} \\\\ 0 & 0 & -3^{k-1} & \\frac{1}{2}-\\frac{1}{2} 3^{k-1} \\\\ 0 & 0 & 3^{k} & -\\frac{1}{2}+\\frac{1}{2} 3^{k} \\\\ 0 & 0 & 0 & 1\\end{array}\\right]\\)\n\n\n\n\n4 \nCompute \\(\\sin \\left(\\frac{\\pi}{6} A\\right)\\) and \\(\\cos \\left(\\frac{\\pi}{6} A\\right)\\), where \\(A=\\left[\\begin{array}{rr}2 & 4 \\\\ 0 & -3\\end{array}\\right]\\).\n\n5.2.11\n\\(\\sin \\left(\\frac{\\pi}{6} A\\right)=\\left[\\begin{array}{cc}\\frac{1}{2} \\sqrt{3} & \\frac{4}{5}+\\frac{2}{5} \\sqrt{3} \\\\ 0 & -1\\end{array}\\right], \\cos \\left(\\frac{\\pi}{6} A\\right)=\\left[\\begin{array}{cc}\\frac{1}{2} & \\frac{2}{5} \\\\ 0 & 0\\end{array}\\right]\\)\n\n\n\n5 \nBy hand, find the spectral radius and dominant eigenvalue, if any, for each of these matrices:\n\n\\(\\left[\\begin{array}{rr}-7 & -6 \\\\ 9 & 8\\end{array}\\right]\\)\n\\(\\frac{1}{3}\\left[\\begin{array}{ll}1 & 3 \\\\ 2 & 0\\end{array}\\right]\\)\n\\(\\left[\\begin{array}{lll}0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 1 & 0 & 0\\end{array}\\right]\\)\n\\(\\frac{1}{2}\\left[\\begin{array}{lll}1 & 0 & 1 \\\\ 1 & 0 & 0 \\\\ 0 & 2 & 1\\end{array}\\right]\\)\n\\(\\left[\\begin{array}{rr}1 & 1 \\\\ -1 & -1\\end{array}\\right]\\)\n\n\n5.3.2\n\n2 , dominant eigenvalue 2\n1 , dominant eigenvalue 1\n1 , no dominant eigenvalue\n1 , dominant eígenvalue 1\n0 , no dominant eigenvalue\n\n\n\n\n6 \nIf the matrices of the previous exercise are transition matrices, for which do all \\(\\mathbf{x}^{(k)}\\) remain bounded as \\(k \\rightarrow \\infty\\) ? Are any of these matrices stable?\n\n5.3.6\n\ngives a matrix for which \\(\\mathbf{x}^{(k)} \\rightarrow \\mathbf{0}\\) as \\(k \\rightarrow \\infty\\). (b) and (d) give stable matrices.\n\n\n\n\n7 \nThe three-stage insect model of Example 2.21 yields a transition matrix\n\\[\nA=\\left[\\begin{array}{ccc}\n0.2 & 0 & 0.25 \\\\\n0.6 & 0.3 & 0 \\\\\n0 & 0.6 & 0.8\n\\end{array}\\right]\n\\]\nUse a technology tool to calculate the eigenvalues of this matrix. Deduce that \\(A\\) is diagonalizable and determine the approximate growth rate from one state to the next (after much time has passed), given a random initial vector.\n\nExercise 5.3.11\nEigenvalues are \\(\\lambda \\approx 0.1636 \\pm 0.3393 \\mathrm{i}, 0.973\\) with absolute values 0.3766 and 0.973 . So population will decline at rate of approximately \\(2.7 \\%\\) per time period.\n\n\n\n8 \nThe financial model of Example 2.27 gives rise to a discrete dynamical system \\(x^{(k+1)}=A x^{(k)}\\), where the transition matrix is\n\\[\nA=\\left[\\begin{array}{rrr}\n1 & 0.06 & 0.12 \\\\\n1 & 0 & 0 \\\\\n0 & 1 & 0\n\\end{array}\\right]\n\\]\nUse a technology tool to calculate the eigenvalues of this matrix. Deduce that \\(A\\) is diagonalizable and determine the approximate growth rate from one state to the next, given a random initial vector.\nFind a starting vector \\(x^{(0)}\\) such that growth rate for the first few iterations is quite different from this approximate growth rate, and demonstrate this using a technology tool.\n(If you do this using Sympy, and you set up the matrix A using rational numbers, you can get exact results and find a starting vector which will never reach your approximate growth rage. You can see how this result changes if you use floating point numbers at some point in the calculation, e.g. x = N(x).)\n\nfrom sympy import Matrix, Rational\n\n# Set up the matrix A in symbolic form\nA = Matrix([[1, Rational(6, 100), Rational(12, 100)], [1, 0, 0], [0, 1, 0]])\n\n\nDoing it with numpy:\n\nimport numpy as np\nA = np.array([[1, 0.06, 0.12], [1, 0, 0], [0, 1, 0]])\n# find the eigenvectors and eigenvalues\neigenvalues, eigenvectors = np.linalg.eig(A)\nprint(eigenvalues)\nprint(eigenvectors)\n\n[ 1.14411531+0.j         -0.07205766+0.31574076j -0.07205766-0.31574076j]\n[[ 0.65266846+0.j          0.08945851+0.04307557j  0.08945851-0.04307557j]\n [ 0.57045689+0.j          0.0682135 -0.29889653j  0.0682135 +0.29889653j]\n [ 0.49860087+0.j         -0.94665171+0.j         -0.94665171-0.j        ]]\n\n\n\nx = eigenvectors[2]\n# pick a random initial vector\nx = np.random.rand(3)\nfor i in range(10):\n    xnew = A @ x\n    print(xnew[0] / x[0])\n    x = xnew\n\n1.5369501393593878\n1.2152748467897423\n1.1136176755844605\n1.142547212360476\n1.1468271144564486\n1.1439000530317671\n1.1439256174937906\n1.1441564621544824\n1.1441253281684178\n1.1441107512822049\n\n\nDoing it with sympy:\n\nfrom sympy import Matrix, symbols, randprime, Rational,N\nfrom sympy.abc import x\n\n\nA = Matrix([[1, Rational(6, 100), Rational(12, 100)], [1, 0, 0], [0, 1, 0]])\n\n\n# find the eigenvalues and eigenvectors\neigenvalues = A.eigenvals()\neigenvectors = A.eigenvects()\n\nprint(\"Eigenvalues:\")\nfor value in eigenvalues:\n    print(value)\n\nprint(\"\\nEigenvectors:\")\nfor vector in eigenvectors:\n    print(vector)\n\n# pick a random initial vector\n#x = Matrix([randprime(1, 10) for _ in range(3)])\n\n# use the third eigenvector as the initial\nx = eigenvectors[0][2][0]\n\nfor i in range(100):\n    #xnew = N(A * x)\n    xnew= A * x\n    growth_rate = N(xnew[0] / x[0])\n    print(f\"Growth rate at iteration {i}: {growth_rate}\")\n    x = xnew\n\nEigenvalues:\n59/(450*(sqrt(20707)/1500 + 289/2700)**(1/3)) + 1/3 + (sqrt(20707)/1500 + 289/2700)**(1/3)\n1/3 + 59/(450*(-1/2 + sqrt(3)*I/2)*(sqrt(20707)/1500 + 289/2700)**(1/3)) + (-1/2 + sqrt(3)*I/2)*(sqrt(20707)/1500 + 289/2700)**(1/3)\n1/3 + (-1/2 - sqrt(3)*I/2)*(sqrt(20707)/1500 + 289/2700)**(1/3) + 59/(450*(-1/2 - sqrt(3)*I/2)*(sqrt(20707)/1500 + 289/2700)**(1/3))\n\nEigenvectors:\n(1/3 + (-1/2 - sqrt(3)*I/2)*(sqrt(20707)/1500 + 289/2700)**(1/3) + 59/(450*(-1/2 - sqrt(3)*I/2)*(sqrt(20707)/1500 + 289/2700)**(1/3)), 1, [Matrix([\n[(1/3 + (-1/2 - sqrt(3)*I/2)*(sqrt(20707)/1500 + 289/2700)**(1/3) + 59/(450*(-1/2 - sqrt(3)*I/2)*(sqrt(20707)/1500 + 289/2700)**(1/3)))**2],\n[     1/3 + (-1/2 - sqrt(3)*I/2)*(sqrt(20707)/1500 + 289/2700)**(1/3) + 59/(450*(-1/2 - sqrt(3)*I/2)*(sqrt(20707)/1500 + 289/2700)**(1/3))],\n[                                                                                                                                        1]])])\n(1/3 + 59/(450*(-1/2 + sqrt(3)*I/2)*(sqrt(20707)/1500 + 289/2700)**(1/3)) + (-1/2 + sqrt(3)*I/2)*(sqrt(20707)/1500 + 289/2700)**(1/3), 1, [Matrix([\n[(1/3 + 59/(450*(-1/2 + sqrt(3)*I/2)*(sqrt(20707)/1500 + 289/2700)**(1/3)) + (-1/2 + sqrt(3)*I/2)*(sqrt(20707)/1500 + 289/2700)**(1/3))**2],\n[     1/3 + 59/(450*(-1/2 + sqrt(3)*I/2)*(sqrt(20707)/1500 + 289/2700)**(1/3)) + (-1/2 + sqrt(3)*I/2)*(sqrt(20707)/1500 + 289/2700)**(1/3)],\n[                                                                                                                                        1]])])\n(59/(450*(sqrt(20707)/1500 + 289/2700)**(1/3)) + 1/3 + (sqrt(20707)/1500 + 289/2700)**(1/3), 1, [Matrix([\n[(59/(450*(sqrt(20707)/1500 + 289/2700)**(1/3)) + 1/3 + (sqrt(20707)/1500 + 289/2700)**(1/3))**2],\n[     59/(450*(sqrt(20707)/1500 + 289/2700)**(1/3)) + 1/3 + (sqrt(20707)/1500 + 289/2700)**(1/3)],\n[                                                                                              1]])])\nGrowth rate at iteration 0: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 1: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 2: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 3: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 4: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 5: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 6: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 7: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 8: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 9: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 10: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 11: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 12: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 13: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 14: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 15: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 16: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 17: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 18: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 19: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 20: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 21: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 22: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 23: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 24: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 25: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 26: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 27: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 28: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 29: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 30: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 31: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 32: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 33: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 34: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 35: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 36: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 37: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 38: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 39: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 40: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 41: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 42: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 43: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 44: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 45: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 46: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 47: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 48: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 49: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 50: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 51: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 52: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 53: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 54: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 55: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 56: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 57: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 58: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 59: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 60: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 61: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 62: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 63: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 64: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 65: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 66: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 67: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 68: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 69: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 70: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 71: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 72: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 73: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 74: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 75: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 76: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 77: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 78: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 79: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 80: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 81: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 82: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 83: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 84: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 85: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 86: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 87: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 88: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 89: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 90: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 91: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 92: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 93: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 94: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 95: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 96: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 97: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 98: -0.0720576551574491 - 0.315740759667275*I\nGrowth rate at iteration 99: -0.0720576551574491 - 0.315740759667275*I\n\n\n\n\n\n9 \nOn this problem,you may use a computer to find eigensystems etc, but don’t solve it using simulations and trial and error.\nA species of bird can be divided into three age groups: age less than 2 years for group 1, age between 2 and 4 years for group 2, and age between 4 and 6 years for the third group. Assume that these birds have at most a 6-year life span. It is estimated that the survival rates for birds in groups 1 and 2 are \\(50 \\%\\) and 75%, respectively. Also, birds in groups 1, 2, and 3 produce 0,1 , and 3 offspring on average in any biennium (period of 2 years).\n\nModel this bird population as a discrete dynamical system and analyze the long-term change in the population – give a percentage by which the population of group 1 changes from year to year.\nNow suppose that the survival rates for group 1 and 2 are not known (so are no longer assumed to be 50% and 75%), but are assumed to be equal to one another. What value for this survival rate would make the population stable in the long run?\n\n\n5.3.20\nIf \\(\\mathbf{x}^{(k)}=\\left(x_{1}^{(k)}, x_{2}^{(k)}, x_{3}^{(k)}\\right)\\) is the vector of birds in each age group at the \\(k\\) th biennium, then the model is \\(\\mathbf{x}^{(k+1)}=\\left[\\begin{array}{rrr}0 & 2 & 3 \\\\ 0.5 & 0 \\\\ 0 & 0 \\\\ 0 & 0.75 & 0\\end{array}\\right] \\mathbf{x}^{(k)}\\). Eigenvalues of \\(A\\) are \\(\\lambda \\approx 1.35326,-0.67663 \\pm 0.61114 i\\). Hence, the population will grow at a rate of \\(\\% 35\\) per year in the long run.\nIf survival rates are equal, say \\(s\\), then transition matrix is \\(\\left[\\begin{array}{lll}0 & 2 & 3 \\\\ s & 0 & 0 \\\\ 0 & s & 0\\end{array}\\right]\\), which has a dominant eigenvalue of \\(\\lambda=1\\) when \\(s=\\frac{1}{3}\\)."
  },
  {
    "objectID": "lectures/ch5lecture2.html#positive-definite-matrices",
    "href": "lectures/ch5lecture2.html#positive-definite-matrices",
    "title": "Ch5 Lecture 2",
    "section": "Positive Definite Matrices",
    "text": "Positive Definite Matrices\n\nA matrix \\(A\\) is called positive definite if \\(x^{T} A x&gt;0\\) for all nonzero vectors \\(x\\).\n\n\n\nA symmetric matrix \\(K=K^{T}\\) is positive definite if and only if all of its eigenvalues are strictly positive.\n\n\n\nProof: If \\(\\mathbf{x}=\\mathbf{v} \\neq \\mathbf{0}\\) is an eigenvector with (necessarily real) eigenvalue \\(\\lambda\\), then\n\\[\n\\begin{equation*}\n0&lt;\\mathbf{v}^{T} K \\mathbf{v}=\\mathbf{v}^{T}(\\lambda \\mathbf{v})=\\lambda \\mathbf{v}^{T} \\mathbf{v}=\\lambda\\|\\mathbf{v}\\|^{2}\n\\end{equation*}\n\\]\nSo \\(\\lambda&gt;0\\)\n\n\nConversely, suppose \\(K\\) has all positive eigenvalues.\nLet \\(\\mathbf{u}_{1}, \\ldots, \\mathbf{u}_{n}\\) be the orthonormal eigenvector basis with \\(K \\mathbf{u}_{j}=\\lambda_{j} \\mathbf{u}_{j}\\) with \\(\\lambda_{j}&gt;0\\).\n\\[\n\\mathbf{x}=c_{1} \\mathbf{u}_{1}+\\cdots+c_{n} \\mathbf{u}_{n}, \\quad \\text { we obtain } \\quad K \\mathbf{x}=c_{1} \\lambda_{1} \\mathbf{u}_{1}+\\cdots+c_{n} \\lambda_{n} \\mathbf{u}_{n}\n\\]\n\n\nTherefore,\n\\[\n\\mathbf{x}^{T} K \\mathbf{x}=\\left(c_{1} \\mathbf{u}_{1}^{T}+\\cdots+c_{n} \\mathbf{u}_{n}^{T}\\right)\\left(c_{1} \\lambda_{1} \\mathbf{u}_{1}+\\cdots+c_{n} \\lambda_{n} \\mathbf{u}_{n}\\right)=\\lambda_{1} c_{1}^{2}+\\cdots+\\lambda_{n} c_{n}^{2}&gt;0\n\\]"
  },
  {
    "objectID": "lectures/ch5lecture2.html#section",
    "href": "lectures/ch5lecture2.html#section",
    "title": "Ch5 Lecture 2",
    "section": "",
    "text": "Let \\(A=A^{T}\\) be a real symmetric \\(n \\times n\\) matrix. Then\n\nAll the eigenvalues of \\(A\\) are real.\n\n. . .\n\nEigenvectors corresponding to distinct eigenvalues are orthogonal.\n\n. . .\n\nThere is an orthonormal basis of \\(\\mathbb{R}^{n}\\) consisting of \\(n\\) eigenvectors of \\(A\\). In particular, all real symmetric matrices are non-defective and real diagonalizable."
  },
  {
    "objectID": "lectures/ch5lecture2.html#example",
    "href": "lectures/ch5lecture2.html#example",
    "title": "Ch5 Lecture 2",
    "section": "Example",
    "text": "Example\n\\[\nA=\\left(\\begin{array}{ll}3 & 1 \\\\ 1 & 3\\end{array}\\right)\n\\]\n\nWe compute the determinant in the characteristic equation\n\\[\n\\operatorname{det}(A-\\lambda \\mathrm{I})=\\operatorname{det}\\left(\\begin{array}{cc}\n3-\\lambda & 1 \\\\\n1 & 3-\\lambda\n\\end{array}\\right)=(3-\\lambda)^{2}-1=\\lambda^{2}-6 \\lambda+8\n\\]\n\n\n\\[\n\\lambda^{2}-6 \\lambda+8=(\\lambda-4)(\\lambda-2)=0\n\\]\n\n\nEigenvectors:\nFor the first eigenvalue, the eigenvector equation is\n\\[\n(A-4 \\mathrm{I}) \\mathbf{v}=\\left(\\begin{array}{rr}\n-1 & 1 \\\\\n1 & -1\n\\end{array}\\right)\\binom{x}{y}=\\binom{0}{0}, \\quad \\text { or } \\quad \\begin{array}{r}\n-x+y=0 \\\\\nx-y=0\n\\end{array}\n\\]"
  },
  {
    "objectID": "lectures/ch5lecture2.html#proof-of-part-a",
    "href": "lectures/ch5lecture2.html#proof-of-part-a",
    "title": "Ch5 Lecture 2",
    "section": "Proof of part (a)",
    "text": "Proof of part (a)\nLet \\(A=A^{T}\\) be a real symmetric \\(n \\times n\\) matrix. Then\n\nAll the eigenvalues of \\(A\\) are real.\n\n\nSuppose \\(\\lambda\\) is a complex eigenvalue with complex eigenvector \\(\\mathbf{v} \\in \\mathbb{C}^{n}\\).\n\\[\n(A \\mathbf{v}) \\cdot \\mathbf{v}=(\\lambda \\mathbf{v}) \\cdot \\mathbf{v}=\\lambda\\|\\mathbf{v}\\|^{2}\n\\]\n\n\nNow, if \\(A\\) is real and symmetric,\n\\[\n\\begin{equation*}\n(A \\mathbf{v}) \\cdot \\mathbf{w}=\\left(\\mathbf{v}^T A^T \\right)\\mathbf{w} = \\mathbf{v} \\cdot(A \\mathbf{w}) \\quad \\text { for all } \\quad \\mathbf{v}, \\mathbf{w} \\in \\mathbb{C}^{n}\n\\end{equation*}\n\\]\nTherefore\n\\[\n(A \\mathbf{v}) \\cdot \\mathbf{v}=\\mathbf{v} \\cdot(A \\mathbf{v})=\\mathbf{v} \\cdot(\\lambda \\mathbf{v})=\\mathbf{v}^{T} \\overline{\\lambda \\mathbf{v}}=\\bar{\\lambda}\\|\\mathbf{v}\\|^{2}\n\\]\n\n(We didn’t talk about this, but for complex vectors, we have \\(\\mathbf{v} \\cdot \\mathbf{w}=\\mathbf{w} \\cdot \\mathbf{v}^{*}\\))\n\n\n\n\\(\\Rightarrow\\), \\(\\lambda\\|\\mathbf{v}\\|^{2}=\\bar{\\lambda}\\|\\mathbf{v}\\|^{2}\\) \\(\\Rightarrow\\) \\(\\lambda=\\bar{\\lambda}\\), so \\(\\lambda\\) is real."
  },
  {
    "objectID": "lectures/ch5lecture2.html#proof-of-part-b",
    "href": "lectures/ch5lecture2.html#proof-of-part-b",
    "title": "Ch5 Lecture 2",
    "section": "Proof of part (b)",
    "text": "Proof of part (b)\nPart b: Eigenvectors corresponding to distinct eigenvalues are orthogonal.\nSuppose \\(A \\mathbf{v}=\\lambda \\mathbf{v}, A \\mathbf{w}=\\mu \\mathbf{w}\\), where \\(\\lambda \\neq \\mu\\) are distinct real eigenvalues.\n\n\\(\\lambda \\mathbf{v} \\cdot \\mathbf{w}=(A \\mathbf{v}) \\cdot \\mathbf{w}=\\mathbf{v} \\cdot(A \\mathbf{w})=\\mathbf{v} \\cdot(\\mu \\mathbf{w})=\\mu \\mathbf{v} \\cdot \\mathbf{w}, \\quad\\) and hence \\(\\quad(\\lambda-\\mu) \\mathbf{v} \\cdot \\mathbf{w}=0\\).\n\n\nSince \\(\\lambda \\neq \\mu\\), this implies that \\(\\mathbf{v} \\cdot \\mathbf{w}=0\\), so the eigenvectors \\(\\mathbf{v}, \\mathbf{w}\\) are orthogonal."
  },
  {
    "objectID": "lectures/ch5lecture2.html#proof-of-part-c",
    "href": "lectures/ch5lecture2.html#proof-of-part-c",
    "title": "Ch5 Lecture 2",
    "section": "Proof of part (c)",
    "text": "Proof of part (c)\nPart c: There is an orthonormal basis of \\(\\mathbb{R}^{n}\\) consisting of \\(n\\) eigenvectors of \\(A\\).\n\nIf the eigenvalues are distinct, then the eigenvectors are orthogonal by part (b).\n\n\nIf the eigenvalues are repeated, then we can use the Gram-Schmidt process to orthogonalize the eigenvectors."
  },
  {
    "objectID": "lectures/ch5lecture2.html#diagonalizability-of-symmetric-matrices-the-spectral-theorem",
    "href": "lectures/ch5lecture2.html#diagonalizability-of-symmetric-matrices-the-spectral-theorem",
    "title": "Ch5 Lecture 2",
    "section": "Diagonalizability of Symmetric Matrices: the Spectral Theorem",
    "text": "Diagonalizability of Symmetric Matrices: the Spectral Theorem\n\nEvery real, symmetric matrix admits an eigenvector basis, and hence is diagonalizable.\nMoreover, since we can choose eigenvectors that form an orthonormal basis, the diagonalizing matrix takes a particularly simple form.\nRecall that an \\(n \\times n\\) matrix \\(Q\\) is orthogonal if and only if its columns form an orthonormal basis of \\(\\mathbb{R}^{n}\\).\n\n\nWriting our diagonalization from the previous lecture, specifically for the case of a real symmetric matrix \\(A\\):\n\nIf \\(A=A^{T}\\) is a real symmetric \\(n \\times n\\) matrix, then there exists an orthogonal matrix \\(Q\\) and a real diagonal matrix \\(\\Lambda\\) such that\n\\[\n\\begin{equation*}\nA=Q \\Lambda Q^{-1}=Q \\Lambda Q^{T}\n\\end{equation*}\n\\]\nThe eigenvalues of \\(A\\) appear on the diagonal of \\(\\Lambda\\), while the columns of \\(Q\\) are the corresponding orthonormal eigenvectors."
  },
  {
    "objectID": "lectures/ch5lecture2.html#quadratic-forms",
    "href": "lectures/ch5lecture2.html#quadratic-forms",
    "title": "Ch5 Lecture 2",
    "section": "Quadratic Forms",
    "text": "Quadratic Forms\n\nA quadratic form is a homogeneous polynomial of degree 2 in \\(n\\) variables \\(x_{1}, \\ldots, x_{n}\\). For example, in \\(x, y, z\\): \\(Q(x, y, z)=a x^{2}+b y^{2}+c z^{2}+2 d x y+2 e y z+2 f z x\\).\n\n\nEvery quadratic form can be written in matrix form as \\(Q(\\mathbf{x})=\\mathbf{x}^{T} A \\mathbf{x}\\).\n\n\nExample:\n\\[\nQ(x, y, z)=x^{2}+2 y^{2}+z^{2}+2 x y+y z+3 x z .\n\\]\n\n\n\\[\n\\begin{aligned}\nx(x+2 y+3 z)+y(2 y+z)+z^{2} & =\\left[\\begin{array}{lll}\nx & y & z\n\\end{array}\\right]\\left[\\begin{array}{c}\nx+2 y+3 z \\\\\n2 y+z \\\\\nz\n\\end{array}\\right] \\\\\n& =\\left[\\begin{array}{lll}\nx & y & z\n\\end{array}\\right]\\left[\\begin{array}{lll}\n1 & 2 & 3 \\\\\n0 & 2 & 1 \\\\\n0 & 0 & 1\n\\end{array}\\right]\\left[\\begin{array}{c}\nx \\\\\ny \\\\\nz\n\\end{array}\\right]=\\mathbf{x}^{T} A \\mathbf{x},\n\\end{aligned}\n\\]\nNow, if we have a quadratic form \\(Q(\\mathbf{x})=\\mathbf{x}^{T} A \\mathbf{x}\\), we always write this in terms of an equivalent symmetric matrix \\(B\\) as \\(Q(\\mathbf{x})=\\mathbf{x}^{T} B \\mathbf{x}\\) where \\(B=\\frac{1}{2}(A+A^{T})\\).\n(See Exercise 2.4.34 in your texbook.)\n\n\nSo in this case, we can write \\(Q(\\mathbf{x})=\\mathbf{x}^{T} B \\mathbf{x}\\) where\n\\[\nB=\\frac{1}{2}\\left[\\begin{array}{lll}\n2 & 2 & 3 \\\\\n2 & 4 & 1 \\\\\n3 & 1 & 2\n\\end{array}\\right]\n\\]\nCheck:\n\n\nCode\nimport sympy as sp\nB = sp.Matrix([[2,2,3],[2,4,1],[3,1,2]])/2\nx = sp.Matrix(sp.symbols(['x','y','z']))\n(x.T*B*x).expand()\n\n\n\\(\\displaystyle \\left[\\begin{matrix}x^{2} + 2 x y + 3 x z + 2 y^{2} + y z + z^{2}\\end{matrix}\\right]\\)\n\n\nYes, this is the same as the original quadratic form."
  },
  {
    "objectID": "lectures/ch5lecture2.html#diagonalizing-quadratic-forms",
    "href": "lectures/ch5lecture2.html#diagonalizing-quadratic-forms",
    "title": "Ch5 Lecture 2",
    "section": "Diagonalizing Quadratic Forms",
    "text": "Diagonalizing Quadratic Forms\nSymmetric matrices are diagonalizable \\(\\Rightarrow\\) we can always find a basis in which the quadratic form takes a particularly simple form. Just diagonalize:\n\n\\(Q(\\mathbf{x})=\\mathbf{x}^{T} B \\mathbf{x}=x^{T} P D P^{T} x\\) where \\(P\\) is the matrix of eigenvectors of \\(B\\) and \\(D\\) is the diagonal matrix of eigenvalues of \\(B\\).\n\n\nThen if we define new variables \\(\\mathbf{y}=P^{T} \\mathbf{x}\\), we have \\(Q(\\mathbf{x})=\\mathbf{y}^{T} D \\mathbf{y}\\)\n\n\nwhich just becomes a sum of squares:\n\\[\nq(\\mathbf{x})=\\lambda_{1} y_{1}^{2}+\\cdots+\\lambda_{n} y_{n}^{2}\n\\]"
  },
  {
    "objectID": "lectures/ch5lecture2.html#section-1",
    "href": "lectures/ch5lecture2.html#section-1",
    "title": "Ch5 Lecture 2",
    "section": "",
    "text": "General solution: \\[\nx=y=a, \\quad \\text { so } \\quad \\mathbf{v}=\\binom{a}{a}=a\\binom{1}{1}\n\\]\n. . . \\[\n\\lambda_{1}=4, \\quad \\mathbf{v}_{1}=\\binom{1}{1}, \\quad \\lambda_{2}=2, \\quad \\mathbf{v}_{2}=\\binom{-1}{1}\n\\]\n\nThe eigenvectors are orthogonal: \\(\\mathbf{v}_{1} \\cdot \\mathbf{v}_{2}=0\\)"
  },
  {
    "objectID": "lectures/ch5lecture2.html#section-2",
    "href": "lectures/ch5lecture2.html#section-2",
    "title": "Ch5 Lecture 2",
    "section": "",
    "text": "So in this case, we can write \\(Q(\\mathbf{x})=\\mathbf{x}^{T} B \\mathbf{x}\\) where\n\\[\nB=\\frac{1}{2}\\left[\\begin{array}{lll}\n2 & 2 & 3 \\\\\n2 & 4 & 1 \\\\\n3 & 1 & 2\n\\end{array}\\right]\n\\]\nCheck:\n\n\nCode\nimport sympy as sp\nB = sp.Matrix([[2,2,3],[2,4,1],[3,1,2]])/2\nx = sp.Matrix(sp.symbols(['x','y','z']))\n(x.T*B*x).expand()\n\n\n\\(\\displaystyle \\left[\\begin{matrix}x^{2} + 2 x y + 3 x z + 2 y^{2} + y z + z^{2}\\end{matrix}\\right]\\)\n\n\nYes, this is the same as the original quadratic form."
  },
  {
    "objectID": "lectures/ch5lecture2.html#one-example-of-a-useful-symmetric-matrix-quadratic-forms",
    "href": "lectures/ch5lecture2.html#one-example-of-a-useful-symmetric-matrix-quadratic-forms",
    "title": "Ch5 Lecture 2",
    "section": "One example of a useful symmetric matrix: Quadratic Forms",
    "text": "One example of a useful symmetric matrix: Quadratic Forms\n\nA quadratic form is a homogeneous polynomial of degree 2 in \\(n\\) variables \\(x_{1}, \\ldots, x_{n}\\). For example, in \\(x, y, z\\): \\(Q(x, y, z)=a x^{2}+b y^{2}+c z^{2}+2 d x y+2 e y z+2 f z x\\).\n\n\nEvery quadratic form can be written in matrix form as \\(Q(\\mathbf{x})=\\mathbf{x}^{T} A \\mathbf{x}\\).\n\n\nExample:\n\\[\nQ(x, y, z)=x^{2}+2 y^{2}+z^{2}+2 x y+y z+3 x z .\n\\]\n\n\n\\[\n\\begin{aligned}\nx(x+2 y+3 z)+y(2 y+z)+z^{2} & =\\left[\\begin{array}{lll}\nx & y & z\n\\end{array}\\right]\\left[\\begin{array}{c}\nx+2 y+3 z \\\\\n2 y+z \\\\\nz\n\\end{array}\\right] \\\\\n& =\\left[\\begin{array}{lll}\nx & y & z\n\\end{array}\\right]\\left[\\begin{array}{lll}\n1 & 2 & 3 \\\\\n0 & 2 & 1 \\\\\n0 & 0 & 1\n\\end{array}\\right]\\left[\\begin{array}{c}\nx \\\\\ny \\\\\nz\n\\end{array}\\right]=\\mathbf{x}^{T} A \\mathbf{x},\n\\end{aligned}\n\\]\nNow, if we have a quadratic form \\(Q(\\mathbf{x})=\\mathbf{x}^{T} A \\mathbf{x}\\), we always write this in terms of an equivalent symmetric matrix \\(B\\) as \\(Q(\\mathbf{x})=\\mathbf{x}^{T} B \\mathbf{x}\\) where \\(B=\\frac{1}{2}(A+A^{T})\\).\n(See Exercise 2.4.34 in your texbook.)"
  },
  {
    "objectID": "lectures/ch5lecture2.html#section-3",
    "href": "lectures/ch5lecture2.html#section-3",
    "title": "Ch5 Lecture 2",
    "section": "",
    "text": "Example:\nSuppose we have the quadratic form \\(3x^2+2xy+3y^2\\). We can write this in matrix form as \\(Q(\\mathbf{x})=\\mathbf{x}^{T} B \\mathbf{x}\\) where \\(\\mathbf{x}=\\binom{x_1}{x_2}\\) and\n\\[\nB=\\frac{1}{2}\\left[\\begin{array}{ll}\n3 & 1 \\\\\n1 & 3\n\\end{array}\\right]\n\\]\n\nWe diagonalize \\(B\\):\n\\[\n\\left(\\begin{array}{ll}\n3 & 1 \\\\\n1 & 3\n\\end{array}\\right)=A=P D P^{T}=\\left(\\begin{array}{cc}\n\\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} \\\\\n\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}}\n\\end{array}\\right)\\left(\\begin{array}{ll}\n4 & 0 \\\\\n0 & 2\n\\end{array}\\right)\\left(\\begin{array}{cc}\n\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\\\\n-\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}}\n\\end{array}\\right)\n\\]\n\n\nNow, if we define \\(\\mathbf{y}=P^{T} \\mathbf{x}=\\frac{1}{\\sqrt{2}}\\binom{x_1+x_2}{-x_1+x_2}\\), we have \\(Q(\\mathbf{x})=\\mathbf{y}^{T} D \\mathbf{y}\\), or\n\\[\nq(\\mathbf{x})=3 x_{1}^{2}+2 x_{1} x_{2}+3 x_{2}^{2}=4 y_{1}^{2}+2 y_{2}^{2}\n\\]"
  },
  {
    "objectID": "lectures/ch5lecture2.html#section-4",
    "href": "lectures/ch5lecture2.html#section-4",
    "title": "Ch5 Lecture 2",
    "section": "",
    "text": "The numbers aren’t always clean, though!\n\n\nCode\nQ,Lambda = B.diagonalize()\nQ\n\n\n\\(\\displaystyle \\left[\\begin{matrix}\\frac{- 13248 \\cdot \\left(1 + \\sqrt{3} i\\right) \\left(53 + 9 \\sqrt{1167} i\\right)^{\\frac{2}{3}} - \\sqrt[3]{53 + 9 \\sqrt{1167} i} \\left(184 + \\left(-16 + \\left(1 + \\sqrt{3} i\\right) \\sqrt[3]{53 + 9 \\sqrt{1167} i}\\right) \\left(1 + \\sqrt{3} i\\right) \\sqrt[3]{53 + 9 \\sqrt{1167} i}\\right)^{2} + 72 \\left(1 + \\sqrt{3} i\\right)^{2} \\cdot \\left(11 - \\left(1 + \\sqrt{3} i\\right) \\sqrt[3]{53 + 9 \\sqrt{1167} i}\\right) \\left(53 + 9 \\sqrt{1167} i\\right)}{792 \\left(1 + \\sqrt{3} i\\right)^{2} \\cdot \\left(53 + 9 \\sqrt{1167} i\\right)} & \\frac{72 \\left(1 - \\sqrt{3} i\\right)^{3} \\cdot \\left(11 + \\left(-1 + \\sqrt{3} i\\right) \\sqrt[3]{53 + 9 \\sqrt{1167} i}\\right) \\left(53 + 9 \\sqrt{1167} i\\right) + \\left(-1 + \\sqrt{3} i\\right) \\sqrt[3]{53 + 9 \\sqrt{1167} i} \\left(184 + \\left(-16 + \\left(1 - \\sqrt{3} i\\right) \\sqrt[3]{53 + 9 \\sqrt{1167} i}\\right) \\left(1 - \\sqrt{3} i\\right) \\sqrt[3]{53 + 9 \\sqrt{1167} i}\\right)^{2} - 13248 \\left(1 - \\sqrt{3} i\\right)^{2} \\left(53 + 9 \\sqrt{1167} i\\right)^{\\frac{2}{3}}}{792 \\left(1 - \\sqrt{3} i\\right)^{3} \\cdot \\left(53 + 9 \\sqrt{1167} i\\right)} & \\frac{126 \\sqrt{1167} - 289 i \\left(53 + 9 \\sqrt{1167} i\\right)^{\\frac{2}{3}} - 3 \\sqrt{1167} \\left(53 + 9 \\sqrt{1167} i\\right)^{\\frac{2}{3}} - 742 i + 352 i \\sqrt[3]{53 + 9 \\sqrt{1167} i} + 60 \\sqrt{1167} \\sqrt[3]{53 + 9 \\sqrt{1167} i}}{66 \\cdot \\left(9 \\sqrt{1167} - 53 i\\right)}\\\\\\frac{28 \\left(-22 + \\left(1 + \\sqrt{3} i\\right) \\sqrt[3]{53 + 9 \\sqrt{1167} i}\\right) \\left(1 + \\sqrt{3} i\\right)^{2} \\cdot \\left(53 + 9 \\sqrt{1167} i\\right) + \\sqrt[3]{53 + 9 \\sqrt{1167} i} \\left(184 + \\left(-16 + \\left(1 + \\sqrt{3} i\\right) \\sqrt[3]{53 + 9 \\sqrt{1167} i}\\right) \\left(1 + \\sqrt{3} i\\right) \\sqrt[3]{53 + 9 \\sqrt{1167} i}\\right)^{2} + 5152 \\cdot \\left(1 + \\sqrt{3} i\\right) \\left(53 + 9 \\sqrt{1167} i\\right)^{\\frac{2}{3}}}{264 \\left(1 + \\sqrt{3} i\\right)^{2} \\cdot \\left(53 + 9 \\sqrt{1167} i\\right)} & \\frac{5152 \\cdot \\left(1 - \\sqrt{3} i\\right) \\left(53 + 9 \\sqrt{1167} i\\right)^{\\frac{2}{3}} + \\sqrt[3]{53 + 9 \\sqrt{1167} i} \\left(184 + \\left(-16 + \\left(1 - \\sqrt{3} i\\right) \\sqrt[3]{53 + 9 \\sqrt{1167} i}\\right) \\left(1 - \\sqrt{3} i\\right) \\sqrt[3]{53 + 9 \\sqrt{1167} i}\\right)^{2} + 28 \\left(-22 + \\left(1 - \\sqrt{3} i\\right) \\sqrt[3]{53 + 9 \\sqrt{1167} i}\\right) \\left(1 - \\sqrt{3} i\\right)^{2} \\cdot \\left(53 + 9 \\sqrt{1167} i\\right)}{264 \\left(1 - \\sqrt{3} i\\right)^{2} \\cdot \\left(53 + 9 \\sqrt{1167} i\\right)} & \\frac{18 \\sqrt{1167} - 2222 i \\sqrt[3]{53 + 9 \\sqrt{1167} i} - 145 i \\left(53 + 9 \\sqrt{1167} i\\right)^{\\frac{2}{3}} - 106 i + 18 \\sqrt{1167} \\sqrt[3]{53 + 9 \\sqrt{1167} i} + 9 \\sqrt{1167} \\left(53 + 9 \\sqrt{1167} i\\right)^{\\frac{2}{3}}}{66 \\cdot \\left(9 \\sqrt{1167} - 53 i\\right)}\\\\1 & 1 & 1\\end{matrix}\\right]\\)\n\n\n\n\n\\(\\displaystyle \\left[\\begin{matrix}\\frac{4}{3} + \\left(- \\frac{1}{2} - \\frac{\\sqrt{3} i}{2}\\right) \\sqrt[3]{\\frac{53}{216} + \\frac{\\sqrt{1167} i}{24}} + \\frac{23}{18 \\left(- \\frac{1}{2} - \\frac{\\sqrt{3} i}{2}\\right) \\sqrt[3]{\\frac{53}{216} + \\frac{\\sqrt{1167} i}{24}}} & 0 & 0\\\\0 & \\frac{4}{3} + \\frac{23}{18 \\left(- \\frac{1}{2} + \\frac{\\sqrt{3} i}{2}\\right) \\sqrt[3]{\\frac{53}{216} + \\frac{\\sqrt{1167} i}{24}}} + \\left(- \\frac{1}{2} + \\frac{\\sqrt{3} i}{2}\\right) \\sqrt[3]{\\frac{53}{216} + \\frac{\\sqrt{1167} i}{24}} & 0\\\\0 & 0 & \\frac{4}{3} + \\frac{23}{18 \\sqrt[3]{\\frac{53}{216} + \\frac{\\sqrt{1167} i}{24}}} + \\sqrt[3]{\\frac{53}{216} + \\frac{\\sqrt{1167} i}{24}}\\end{matrix}\\right]\\)\n\n\nYuck!"
  },
  {
    "objectID": "lectures/ch5lecture2.html#section-5",
    "href": "lectures/ch5lecture2.html#section-5",
    "title": "Ch5 Lecture 2",
    "section": "",
    "text": "We can visualize the previous example as a rotation of the axes (a change of basis) to a new coordinate system where the quadratic form is just a sum of squares.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nx = np.linspace(-2,2,100)\ny = np.linspace(-2,2,100)\nX,Y = np.meshgrid(x,y)\nZ = 3*X**2+2*X*Y+3*Y**2\nplt.contour(X,Y,Z,levels=[1,2,3,4,5,6,7,8,9,10])\nplt.xlabel('x1')\nplt.ylabel('x2')\nplt.axis('equal')\n# plot the vector P.T times (1,0) and (0,1)\nP = np.array([[1/np.sqrt(2),1/np.sqrt(2)],[-1/np.sqrt(2),1/np.sqrt(2)]])\nv1 = P.T @ np.array([1,0])\nv2 = P.T @ np.array([0,1])\nplt.quiver(0,0,v1[0],v1[1],angles='xy',scale_units='xy',scale=1,color='r')\nplt.quiver(0,0,v2[0],v2[1],angles='xy',scale_units='xy',scale=1,color='r')\n# label the two quivers (\"x1=1, x2=0\" and \"x1=0, x2=1\")\nplt.text(v1[0],v1[1],'y(x1=1,x2=0)',fontsize=12)\nplt.text(v2[0],v2[1],'y(x1=0,x2=1)',fontsize=12)\nplt.show()"
  },
  {
    "objectID": "lectures/ch5lecture2.html#section-6",
    "href": "lectures/ch5lecture2.html#section-6",
    "title": "Ch5 Lecture 2",
    "section": "",
    "text": "Now make the same plot but in y coordinates:\n\n\nCode\nx = np.linspace(-2,2,100)\ny = np.linspace(-2,2,100)\nX,Y = np.meshgrid(x,y)\nZ = 4*X**2+2*Y**2\nplt.contour(X,Y,Z,levels=[1,2,3,4,5,6,7,8,9,10])\nplt.xlabel('y1')\nplt.ylabel('y2')\nplt.axis('equal')\nv1 = P.T @ np.array([1,0])\nv2 = P.T @ np.array([0,1])\nplt.quiver(0,0,1,0,angles='xy',scale_units='xy',scale=1,color='r')\nplt.quiver(0,0,0,1,angles='xy',scale_units='xy',scale=1,color='r')\n# label the two quivers (\"x1=1, x2=0\" and \"x1=0, x2=1\")\nplt.show()"
  },
  {
    "objectID": "lectures/ch5lecture2.html#section-7",
    "href": "lectures/ch5lecture2.html#section-7",
    "title": "Ch5 Lecture 2",
    "section": "",
    "text": "In general, we can think of the diagonalization as a rotation of the axes followed by a scaling of the axes.\nWe often visualize this by plotting the effects of the transformations on the unit circle."
  },
  {
    "objectID": "lectures/ch5lecture2.html#singular-values",
    "href": "lectures/ch5lecture2.html#singular-values",
    "title": "Ch5 Lecture 2",
    "section": "Singular Values",
    "text": "Singular Values\nWe’ve talked a lot about eigenvalues and eigenvectors, but these only make any sense for square matrices. What can we do for a general \\(m \\times n\\) matrix \\(A\\)?\n\nIt turns out we can learn a lot from the matrix \\(A^{T} A\\) (or \\(A A^{T}\\)), which is always square and symmetric.\n\n\n\nThe singular values \\(\\sigma_{1}, \\ldots, \\sigma_{r}\\) of an \\(m \\times n\\) matrix \\(A\\) are the positive square roots, \\(\\sigma_{i}=\\sqrt{\\lambda_{i}}&gt;0\\), of the nonzero eigenvalues of the associated “Gram matrix” \\(K=A^{T} A\\).\n. . .\nThe corresponding eigenvectors of \\(K\\) are known as the singular vectors of \\(A\\).\n\n\n\nAll of the eigenvalues of \\(K\\) are real and nonnegative – but some may be zero.\n\n\nIf \\(K=A^{T} A\\) has repeated eigenvalues, the singular values of \\(A\\) are repeated with the same multiplicities.\n\n\nThe number \\(r\\) of singular values is equal to the rank of the matrices \\(A\\) and \\(K\\)."
  },
  {
    "objectID": "lectures/ch5lecture2.html#example-1",
    "href": "lectures/ch5lecture2.html#example-1",
    "title": "Ch5 Lecture 2",
    "section": "Example",
    "text": "Example\nLet \\(A=\\left(\\begin{array}{ll}3 & 5 \\\\ 4 & 0\\end{array}\\right)\\).\n\\[\nK=A^{T} A=\\left(\\begin{array}{ll}\n3 & 4 \\\\\n5 & 0\n\\end{array}\\right)\\left(\\begin{array}{ll}\n3 & 5 \\\\\n4 & 0\n\\end{array}\\right)=\\left(\\begin{array}{ll}\n25 & 15 \\\\\n15 & 25\n\\end{array}\\right)\n\\]\n\nThis has eigenvalues \\(\\lambda_{1}=40, \\lambda_{2}=10\\), and corresponding eigenvectors \\(\\mathbf{v}_{1}=\\binom{1}{1}, \\mathbf{v}_{2}=\\binom{1}{-1}\\).\n\n\nTherefore, the singular values of \\(A\\) are \\(\\sigma_{1}=\\sqrt{40}=2 \\sqrt{10}, \\sigma_{2}=\\sqrt{10}\\).\npause\n\nAre the singular values here equal to the eigenvalues of \\(A\\)? No!\nThe eigenvalues of A are \\(\\lambda_{1}=\\frac{1}{2}(3+\\sqrt{89}) \\simeq 6.2170\\) and \\(\\lambda_{2}=\\frac{1}{2}(3-\\sqrt{89}) \\simeq-3.2170\\), nor are the singular vectors eigenvectors of \\(A\\)."
  },
  {
    "objectID": "lectures/ch5lecture2.html#singular-values-of-a-symmetric-matrix",
    "href": "lectures/ch5lecture2.html#singular-values-of-a-symmetric-matrix",
    "title": "Ch5 Lecture 2",
    "section": "Singular Values of a Symmetric Matrix",
    "text": "Singular Values of a Symmetric Matrix\n\nIf \\(A\\) is a symmetric matrix, then \\(K = A^{T} A= A^2\\).\nSo if \\(\\mathbf{v}\\) is an eigenvector of \\(A\\) with eigenvalue \\(\\lambda\\), then \\(A \\mathbf{v}=\\lambda \\mathbf{v}\\), and \\(K \\mathbf{v}=A^{T} A \\mathbf{v}=A \\lambda \\mathbf{v}=\\lambda^{2} \\mathbf{v}\\).\n. . .\nTherefore the eigenvalues of \\(K\\) are the squares of the eigenvalues of \\(A\\).\n. . .\nThus the singular values of \\(A\\), which are the square roots of the non-zero eigenvalues of \\(K\\), are the absolute values of the eigenvalues of \\(A\\)."
  },
  {
    "objectID": "lectures/ch5lecture2.html#singular-value-decomposition",
    "href": "lectures/ch5lecture2.html#singular-value-decomposition",
    "title": "Ch5 Lecture 2",
    "section": "Singular Value Decomposition",
    "text": "Singular Value Decomposition\n\nLet \\(A\\) be an \\(m \\times n\\) real matrix. Then there exist an \\(m \\times m\\) orthogonal matrix \\(U\\), an \\(n \\times n\\) orthogonal matrix \\(V\\), and an \\(m \\times n\\) diagonal matrix \\(\\Sigma\\) with diagonal entries \\(\\sigma_{1} \\geq \\sigma_{2} \\geq \\cdots \\geq \\sigma_{p} \\geq 0\\), with \\(p=\\min \\{m, n\\}\\), such that \\(U^{T} A V=\\Sigma\\). Moreover, the numbers \\(\\sigma_{1}, \\sigma_{2}, \\ldots, \\sigma_{p}\\) are uniquely determined by \\(A\\)."
  },
  {
    "objectID": "lectures/ch5lecture2.html#section-8",
    "href": "lectures/ch5lecture2.html#section-8",
    "title": "Ch5 Lecture 2",
    "section": "",
    "text": "Proof:\n\n\nAssume \\(m \\geq n\\) (the case \\(m&lt;n\\) is similar).\nLet \\(B=A^{T} A\\) and let \\(\\lambda_{1} \\geq \\lambda_{2} \\geq \\cdots \\geq \\lambda_{n}\\) be the eigenvalues of \\(B\\).\nFind the basis of eigenvectoers of \\(B\\): \\(B \\mathbf{v}_{k}=\\sigma_{k}^{2} \\mathbf{v}_{k}, k=1,2, \\ldots, n\\)\nLet \\(V=\\left[\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\ldots, \\mathbf{v}_{n}\\right]\\). Then \\(V\\) is an orthogonal \\(n \\times n\\) matrix.\nWe may assume for some index \\(r\\) that \\(\\sigma_{r+1}, \\sigma_{r+2}, \\ldots, \\sigma_{n}\\) are zero, while \\(\\sigma_{r} \\neq 0\\).\nNext set \\(\\mathbf{u}_{j}=\\frac{1}{\\sigma_{j}} A \\mathbf{v}_{j}, j=1,2, \\ldots, r\\).\nIn other words, \\(\\mathbf{u}_{j}\\) is the vector that you get when you transform \\(\\mathbf{v}_{j}\\) by \\(A\\) and then normalize it by the singular value \\(\\sigma_{j}\\).\nThese are orthonormal vectors in \\(\\mathbb{R}^{m}\\) since\n\n\\[\n\\mathbf{u}_{j}^{T} \\mathbf{u}_{k}=\\frac{1}{\\sigma_{j} \\sigma_{k}} \\mathbf{v}_{j}^{T} A^{T} A \\mathbf{v}_{k}=\\frac{1}{\\sigma_{j} \\sigma_{k}} \\mathbf{v}_{j}^{T} B \\mathbf{v}_{k}=\\frac{\\sigma_{k}^{2}}{\\sigma_{j} \\sigma_{k}} \\mathbf{v}_{j}^{T} \\mathbf{v}_{k}= \\begin{cases}0, & \\text { if } j \\neq k \\\\ 1, & \\text { if } j=k\\end{cases}\n\\]\n\nNow expand this set to an orthonormal basis \\(\\mathbf{u}_{1}, \\mathbf{u}_{2}, \\ldots, \\mathbf{u}_{m}\\) of \\(\\mathbb{R}^{m}\\). This is possible by Theorem 4.7 in Section 4.3. Set \\(U=\\left[\\mathbf{u}_{1}, \\mathbf{u}_{2}, \\ldots, \\mathbf{u}_{m}\\right]\\). This matrix is orthogonal. We calculate that if \\(k&gt;r\\), then \\(\\mathbf{u}_{j}^{T} A \\mathbf{v}_{k}=0\\) since \\(A \\mathbf{v}_{k}=\\mathbf{0}\\), and if \\(k&lt;r\\), then\n\n\\[\n\\mathbf{u}_{j}^{T} A \\mathbf{v}_{k}=\\sigma_{k} \\mathbf{u}_{j}^{T} \\mathbf{u}_{k}=\\left\\{\\begin{array}{l}\n0, \\text { if } j \\neq k, \\\\\n\\sigma_{k}, \\text { if } j=k\n\\end{array}\\right.\n\\]\n\\(U^{T} A V=\\left[\\mathbf{u}_{j}^{T} A \\mathbf{v}_{k}\\right]=\\Sigma\\), which is the desired SVD.\nAnd because \\(U\\) and \\(V\\) are orthonormal, their inverses are their transposes, so \\(A=U \\Sigma V^{T}\\)."
  },
  {
    "objectID": "lectures/ch5lecture2.html#geometric-interpretation-of-the-svd",
    "href": "lectures/ch5lecture2.html#geometric-interpretation-of-the-svd",
    "title": "Ch5 Lecture 2",
    "section": "Geometric interpretation of the SVD",
    "text": "Geometric interpretation of the SVD\n(following closely this blog post)\nGoal: to understand the SVD as finding perpendicular axes that remain perpendicular after a transformation."
  },
  {
    "objectID": "lectures/ch5lecture2.html#section-9",
    "href": "lectures/ch5lecture2.html#section-9",
    "title": "Ch5 Lecture 2",
    "section": "",
    "text": "Take a very simple matrix:\n\\[\nA=\\left(\\begin{array}{cc}\n6 & 2 \\\\\n-7 & 6\n\\end{array}\\right)\n\\]\n\nRepresents a linear map \\(\\mathrm{T}: \\mathrm{R}^{2} \\rightarrow \\mathbf{R}^{2}\\) with respect to the standard basis \\(e_{1}=(1,0)\\) and \\(e_{2}=(0,1)\\).\n\n\nSends the usual basis elements \\(e_{1} \\rightsquigarrow(6,-7)\\) and \\(e_{2} \\rightsquigarrow(2,6)\\)."
  },
  {
    "objectID": "lectures/ch5lecture2.html#section-10",
    "href": "lectures/ch5lecture2.html#section-10",
    "title": "Ch5 Lecture 2",
    "section": "",
    "text": "We can see what T does to any vector:\n\\[\n\\begin{aligned}\nT(2,3) & =T(2,0)+T(0,3)=T\\left(2 e_{1}\\right)+T\\left(3 e_{2}\\right) \\\\\n& =2 T\\left(e_{1}\\right)+3 T\\left(e_{2}\\right)=2(6,-7)+3(2,6) \\\\\n& =(18,4)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lectures/ch5lecture2.html#section-11",
    "href": "lectures/ch5lecture2.html#section-11",
    "title": "Ch5 Lecture 2",
    "section": "",
    "text": "The usual basis elements are no longer perpendicular after the transformation:"
  },
  {
    "objectID": "lectures/ch5lecture2.html#section-12",
    "href": "lectures/ch5lecture2.html#section-12",
    "title": "Ch5 Lecture 2",
    "section": "",
    "text": "Given a linear map T:Rm \\(\\rightarrow R^{n}\\) (which is represented by a \\(n \\times m\\) matrix \\(A\\) with respect to the standard basis),\n\ncan we find an orthonormal basis \\(\\left\\{v_{-} 1, v_{-} 2, \\ldots, v_{-} m\\right\\}\\) of \\(R^{m}\\) such that\n\\(\\left\\{T\\left(v_{-} 1\\right), T\\left(v_{-} 2\\right), \\ldots\\right.\\) \\(\\left.\\mathrm{T}\\left(\\mathrm{v}_{-} \\mathrm{m}\\right)\\right\\}\\) are still mutually perpendicular to each other?\n\n\nThis is what the SVD does!\nLet the SVD of \\(A\\) be \\(A=U \\Sigma V^{T}\\)"
  },
  {
    "objectID": "lectures/ch5lecture2.html#section-13",
    "href": "lectures/ch5lecture2.html#section-13",
    "title": "Ch5 Lecture 2",
    "section": "",
    "text": "How are \\(U\\) and \\(V\\) related?\n\nWhat we want to do is see what happens to two vectors that are orthogonoal to each other – specifically, two of the columns of V.\nRight-multiplying the equation \\(\\mathrm{A}=\\mathrm{U} \\Sigma \\mathrm{V}^{\\mathrm{T}}\\) by \\(\\mathrm{V}\\), gives\n\\(\\mathrm{AV}=\\left(U \\Sigma V^{T}\\right) \\mathrm{V}=U \\Sigma\\left(V^{I} \\mathrm{~V}\\right)=\\mathrm{U} \\Sigma\\)\nAlso, since \\(\\Sigma\\) is an \\(\\mathrm{n} \\times \\mathrm{m}\\) matrix with non-zero entries only along the diagonal entries ( \\(\\Sigma \\mathrm{i}, \\mathrm{i})\\) if at all, the columns of \\(\\mathrm{U} \\Sigma\\) are just the first \\(\\mathrm{m}\\) columns of \\(U\\) scaled by the respective diagonal entries of \\(\\Sigma\\)\nThus T(v_i)=(Σi,i)u_i and they are mutually perpendicular to each other. Since u_i are unit vectors, the length of T(v_i) is precisely the diagonal entry Σi,i."
  },
  {
    "objectID": "lectures/lectures/ch5lecture3.html#thinking-again-about-matrix-diagonalization",
    "href": "lectures/lectures/ch5lecture3.html#thinking-again-about-matrix-diagonalization",
    "title": "Ch 5 Lecture 3",
    "section": "Thinking again about matrix diagonalization",
    "text": "Thinking again about matrix diagonalization\n\nIf \\(A\\) is a (non-defective) \\(n \\times n\\) square matrix,\n\nthen we can write it as \\(A = PDP^{T}\\) where \\(D\\) is a diagonal matrix and \\(P\\) is an orthogonal matrix of eigenvectors.\nWe can find the product of \\(A\\) with a vector \\(\\mathbf{x}\\) where \\(\\mathbf{x}\\) is a column vector of length \\(n\\). This product will also be a column vector of length \\(n\\).\nWhat’s the intuition behind the diagonalization?\nThe matrix \\(P^{T}\\) is a change of basis matrix. It takes a vector \\(\\mathbf{x}\\) expressed in the standard basis and expresses it in terms of the basis of eigenvectors of \\(A\\).\n\nThis works because \\(P\\) is orthogonal.\n\nThe matrix \\(D\\) is a scaling matrix. It scales the eigenvectors by the corresponding eigenvalues. (If \\(A\\) is not full rank, then \\(D\\) will have some zeros on the diagonal.)\nThe matrix \\(P\\) is the inverse of \\(P^{T}\\) (because \\(P\\) is orthogonal). It just takes the scaled eigenvectors and expresses them in terms of the standard basis.\nThis all works because \\(P\\) is orthogonal and there’s a correspondence between the rows of \\(P^{T}\\) and the columns of \\(P\\).\n\nThis inverse projection is what we want because the columns of \\(P\\) are eigenvectors of \\(A\\).\nWe can see this most easily by imagining that \\(\\mathbf{x}\\) is a multiple of one of the eigenvectors.\n\\(P\\mathbf{x}\\) will give the coordinates of \\(\\mathbf{x}\\) in the basis of eigenvectors. For instance, if \\(P\\mathbf{x}=\\begin{bmatrix} 1 \\\\0\\\\ \\vdots \\\\ 0\\end{bmatrix}\\), we know that \\(\\mathbf{x}\\) equals the first eigenvector.\n\nAfter multiplication by the diagonal matrix, we’ll just have \\(\\begin{bmatrix} \\lambda_{1} \\\\0\\\\ \\vdots \\\\ 0\\end{bmatrix}\\).\n\nThen \\(P^{T}\\) will take this back to a multiple of the first eigenvector, back in the standard basis.\nThat’s what we want because we are supposing that \\(\\mathbf{x}\\) is a multiple of the first eigenvector.\nThe same logic will hold when \\(\\mathbf{x}\\) is weighted sum of the eigenvectors.\n\nBecause \\(P\\) is orthonormal, \\(P\\mathbf{x}\\) will give the weights of the eigenvectors in the basis of eigenvectors.\n\n\\(P^{T}\\) will take it back to the weighted sum of the eigenvectors in the standard basis, with the new weights being determined by the original weights and the eigenvalues.\n\nThis will all works when \\(U\\) and \\(V\\) are orthogonal and there’s a correspondence between the rows of \\(V^{T}\\) and the columns of \\(U\\).\n\nWhen we have some zeros on the diagonal, multiplying by \\(D\\) is a projection of \\(\\mathbf{x}\\) onto the row space of \\(A\\). We are removing the components of \\(\\mathbf{x}\\) that are in the null space of \\(A\\)."
  },
  {
    "objectID": "lectures/lectures/ch5lecture3.html#the-same-logic-applied-to-a-general-matrix",
    "href": "lectures/lectures/ch5lecture3.html#the-same-logic-applied-to-a-general-matrix",
    "title": "Ch 5 Lecture 3",
    "section": "The same logic, applied to a general matrix",
    "text": "The same logic, applied to a general matrix\n\nIf \\(A\\) is a \\(m \\times n\\) square matrix,\n\nthen we can write it as \\(A = U\\Sigma V^{T}\\) where \\(\\Sigma\\) is a diagonal matrix and \\(P\\) is an orthogonal matrix of eigenvectors.\nWe can find the product of \\(A\\) with a vector \\(\\mathbf{x}\\) where \\(\\mathbf{x}\\) is a column vector of length \\(n\\). This product will now be a column vector of length \\(m\\).\nThe matrix \\(V^{T}\\) is a change of basis matrix. It takes a vector \\(\\mathbf{x}\\) expressed in the standard basis and expresses it in terms of the basis of right singular vectors of \\(A\\).\nThe matrix \\(\\Sigma\\) is a scaling matrix. It scales the singular vectors by the corresponding singular values. (If \\(A\\) is not square or \\(A\\) is not full rank, then \\(\\Sigma\\) will have some zeros on the diagonal.)\nThe matrix \\(U\\) is not generally the inverse of \\(V^{T}\\). It takes the scaled singular vectors and expresses them in terms of the standard basis of \\(\\mathbb{R}^m\\).\n\nSpecifically, the first column of \\(U\\) must be the correct vector to express the first row of \\(V^{T}\\) in \\(\\mathbbb{R}^m\\)\n\n\nthat is, we need the first column of \\(U\\) to be \\(A\\mathbf{v}_1\\) (divided by the scaling factor).\nThis is because \\(\\Sigma V^{T}\\mathbf{v}_1=\\sigma_1\\begin{bmatrix} 1 \\\\0\\\\ \\vdots \\\\ 0\\end{bmatrix}\\), so that \\(U\\Sigma V^{T}\\mathbf{v}_1=\\sigma_1 U \\begin{bmatrix} 1 \\\\0\\\\ \\vdots \\\\ 0\\end{bmatrix}=\\sigma_1\\mathbf{u_1}\\).\nSo we’d better have \\(\\mathbf{u_1}=A\\mathbf{v}_1/\\sigma_1\\). :::\n\n\n## SVD\n::: notes - Assume \\(m \\geq n\\) (the case \\(m&lt;n\\) is similar). - Let \\(B=A^{T} A\\) and let \\(\\lambda_{1} \\geq \\lambda_{2} \\geq \\cdots \\geq \\lambda_{n}\\) be the eigenvalues of \\(B\\). - Find the basis of eigenvectors of \\(B\\): \\(B \\mathbf{v}_{k}=\\sigma_{k}^{2} \\mathbf{v}_{k}, k=1,2, \\ldots, n\\) - Let \\(V=\\left[\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\ldots, \\mathbf{v}_{n}\\right]\\). Then \\(V\\) is an orthogonal \\(n \\times n\\) matrix. - We may assume for some index \\(r\\) that \\(\\sigma_{r+1}, \\sigma_{r+2}, \\ldots, \\sigma_{n}\\) are zero, while \\(\\sigma_{r} \\neq 0\\). - Next set \\(\\mathbf{u}_{j}=\\frac{1}{\\sigma_{j}} A \\mathbf{v}_{j}, j=1,2, \\ldots, r\\). - In other words, \\(\\mathbf{u}_{j}\\) is the vector that you get when you transform \\(\\mathbf{v}_{j}\\) by \\(A\\) and then normalize it by the singular value \\(\\sigma_{j}\\). - These are orthonormal vectors in \\(\\mathbb{R}^{m}\\) since\n\\[\n\\mathbf{u}_{j}^{T} \\mathbf{u}_{k}=\\frac{1}{\\sigma_{j} \\sigma_{k}} \\mathbf{v}_{j}^{T} A^{T} A \\mathbf{v}_{k}=\\frac{1}{\\sigma_{j} \\sigma_{k}} \\mathbf{v}_{j}^{T} B \\mathbf{v}_{k}=\\frac{\\sigma_{k}^{2}}{\\sigma_{j} \\sigma_{k}} \\mathbf{v}_{j}^{T} \\mathbf{v}_{k}= \\begin{cases}0, & \\text { if } j \\neq k \\\\ 1, & \\text { if } j=k\\end{cases}\n\\]\n\nNow expand this set to an orthonormal basis \\(\\mathbf{u}_{1}, \\mathbf{u}_{2}, \\ldots, \\mathbf{u}_{m}\\) of \\(\\mathbb{R}^{m}\\). This is possible by Theorem 4.7 in Section 4.3. Set \\(U=\\left[\\mathbf{u}_{1}, \\mathbf{u}_{2}, \\ldots, \\mathbf{u}_{m}\\right]\\). This matrix is orthogonal. We calculate that if \\(k&gt;r\\), then \\(\\mathbf{u}_{j}^{T} A \\mathbf{v}_{k}=0\\) since \\(A \\mathbf{v}_{k}=\\mathbf{0}\\), and if \\(k&lt;r\\), then\n\n\\[\n\\mathbf{u}_{j}^{T} A \\mathbf{v}_{k}=\\sigma_{k} \\mathbf{u}_{j}^{T} \\mathbf{u}_{k}=\\left\\{\\begin{array}{l}\n0, \\text { if } j \\neq k, \\\\\n\\sigma_{k}, \\text { if } j=k\n\\end{array}\\right.\n\\]\n\\(U^{T} A V=\\left[\\mathbf{u}_{j}^{T} A \\mathbf{v}_{k}\\right]=\\Sigma\\), which is the desired SVD.\nAnd because \\(U\\) and \\(V\\) are orthonormal, their inverses are their transposes, so \\(A=U \\Sigma V^{T}\\).\nNote also that we can find \\(U\\) as the eigenvectors of \\(A A^{T}\\) and \\(V\\) as the eigenvectors of \\(A^{T} A\\)."
  },
  {
    "objectID": "lectures/lectures/ch5lecture3.html#pseudoinverse",
    "href": "lectures/lectures/ch5lecture3.html#pseudoinverse",
    "title": "Ch 5 Lecture 3",
    "section": "pseudoinverse",
    "text": "pseudoinverse\nWe can simply note that the least squares solution we found earlier can be written in a simple way in terms of the SVD matrices.\nHere is a decent writeup."
  },
  {
    "objectID": "lectures/lectures/ch5lecture3.html#section",
    "href": "lectures/lectures/ch5lecture3.html#section",
    "title": "Ch 5 Lecture 3",
    "section": "",
    "text": "\\[\n\\begin{array}{cc}\nA A^{T}=\\left(\\begin{array}{cc}\n17 & 8 \\\\\n8 & 17\n\\end{array}\\right) & A^{T} A=\\left(\\begin{array}{ccc}\n13 & 12 & 2 \\\\\n12 & 13 & -2 \\\\\n2 & -2 & 8\n\\end{array}\\right) \\\\\n\\begin{array}{c}\n\\text { eigenvalues: } \\lambda_{1}=25, \\lambda_{2}=9 \\\\\n\\text { eigenvectors }\n\\end{array} & \\begin{array}{c}\n\\text { eigenvalues: } \\lambda_{1}=25, \\lambda_{2}=9, \\lambda_{3}=0 \\\\\n\\text { eigenvectors }\n\\end{array} \\\\\nu_{1}=\\binom{1 / \\sqrt{2}}{1 / \\sqrt{2}} \\quad u_{2}=\\binom{1 / \\sqrt{2}}{-1 / \\sqrt{2}} & v_{1}=\\left(\\begin{array}{c}\n1 / \\sqrt{2} \\\\\n1 / \\sqrt{2} \\\\\n0\n\\end{array}\\right) \\quad v_{2}=\\left(\\begin{array}{c}\n1 / \\sqrt{18} \\\\\n-1 / \\sqrt{18} \\\\\n4 / \\sqrt{18}\n\\end{array}\\right) \\quad v_{3}=\\left(\\begin{array}{c}\n2 / 3 \\\\\n-2 / 3 \\\\\n-1 / 3\n\\end{array}\\right)\n\\end{array}\n\\]"
  },
  {
    "objectID": "lectures/lectures/ch5lecture3.html#recap",
    "href": "lectures/lectures/ch5lecture3.html#recap",
    "title": "Ch 5 Lecture 3",
    "section": "Recap",
    "text": "Recap\n\\[\nA=U S V^{T}\n\\]\nwhere\n$$\n\\[\\begin{array}{lccc}\n& & & U & V\n& S \\\\\n& & &\\left(\\begin{array}{ccc}\n\\mid & & \\mid \\\\\n\\mathbf{u}_{1} & \\cdots & \\mathbf{u}_{m} \\\\\n\\mid & & \\mid\n\\end{array}\\right) &\\left(\\begin{array}{ccc}\n\\mid & & \\mid \\\\\n\\mathbf{v}_{1} & \\cdots & \\mathbf{v}_{n} \\\\\n\\mid & & \\mid\n\\end{array}\\right) &\\left(\\begin{array}{cccc}\n\\sigma_1 & & & \\\\\n& \\sigma_2 & \\\\  & & \\ddots & \\\\\n& & &  \\sigma_r & \\\\ & & & & \\ddots \\\\ & & & & & 0\n\n\\end{array}\\right)\\\\\n\\text{eigenvectors of} & & & A A^{T}  & A^{T} A \\\\\n\\end{array}\\]\n$$"
  },
  {
    "objectID": "lectures/lectures/ch5lecture3.html#example",
    "href": "lectures/lectures/ch5lecture3.html#example",
    "title": "Ch 5 Lecture 3",
    "section": "Example",
    "text": "Example\n\\[\nA=\\left(\\begin{array}{ccc}\n3 & 2 & 2 \\\\\n2 & 3 & -2\n\\end{array}\\right)\n\\]\n\n\\[\nA A^{T}=\\left(\\begin{array}{cc}\n17 & 8 \\\\\n8 & 17\n\\end{array}\\right), \\quad A^{T} A=\\left(\\begin{array}{ccc}\n13 & 12 & 2 \\\\\n12 & 13 & -2 \\\\\n2 & -2 & 8\n\\end{array}\\right)\n\\]"
  },
  {
    "objectID": "lectures/lectures/ch5lecture3.html#section-1",
    "href": "lectures/lectures/ch5lecture3.html#section-1",
    "title": "Ch 5 Lecture 3",
    "section": "",
    "text": "SVD decomposition of \\(A\\): \\[\nA=U S V^{T}=\\left(\\begin{array}{cc}\n1 / \\sqrt{2} & 1 / \\sqrt{2} \\\\\n1 / \\sqrt{2} & -1 / \\sqrt{2}\n\\end{array}\\right)\\left(\\begin{array}{ccc}\n5 & 0 & 0 \\\\\n0 & 3 & 0\n\\end{array}\\right)\\left(\\begin{array}{rrr}\n1 / \\sqrt{2} & 1 / \\sqrt{2} & 0 \\\\\n1 / \\sqrt{18} & -1 / \\sqrt{18} & 4 / \\sqrt{18} \\\\\n2 / 3 & -2 / 3 & -1 / 3\n\\end{array}\\right)\n\\]"
  },
  {
    "objectID": "lectures/lectures/ch5lecture3.html#section-2",
    "href": "lectures/lectures/ch5lecture3.html#section-2",
    "title": "Ch 5 Lecture 3",
    "section": "",
    "text": "\\[\n\\begin{aligned}\n& =\\left(\\begin{array}{c}\nu_{11} \\\\\n\\vdots \\\\\nu_{1 m}\n\\end{array}\\right) \\sigma_{1} \\\\\n& A v_{1}=\\sigma_{1} u_{1}\n\\end{aligned}\n\\]\nThis can be generalized as\n\\[\nA v_{i}=\\sigma_{i} u_{i}\n\\]\nRecall,\n\nand\n\\[\n\\left(\\begin{array}{ccc}\n\\begin{array}{c}\n\\mid \\\\\nu_{1} \\\\\n\\mid\n\\end{array} & \\ldots &\n\\begin{array}{c}\n\\mid \\\\\nu_{n} \\\\\n\\mid\n\\end{array}\n\\end{array}\\right)\\left(\\begin{array}{c}\n-v_{l}- \\\\\n\\vdots \\\\\n-v_{n}-\n\\end{array}\\right)=\\left(\\begin{array}{c}\n\\mid \\\\\nu_{1} \\\\\n\\mid\n\\end{array}\\right)\\left(-v_{l} \\textrm{---}\\right)+\\ldots+\\left(\\begin{array}{c}\n\\mid \\\\\nu_{n} \\\\\n\\mid\n\\end{array}\\right)\\left(-v_{n} \\textrm{---}\\right)\n\\]\nThe SVD decomposition can be recognized as a series of outer products of \\(u_{i}\\) and \\(v_{i}\\).\n\\[\nA=\\sigma_{l} u_{l} v_{l}^{T}+\\ldots+\\sigma_{r} u_{r} v_{r}^{T}\n\\]"
  },
  {
    "objectID": "lectures/lectures/ch5lecture3.html#section-3",
    "href": "lectures/lectures/ch5lecture3.html#section-3",
    "title": "Ch 5 Lecture 3",
    "section": "",
    "text": "Start with Figure 5.12.1 from Meyer.\nAwesome explanation of PCA here\nWhere does the covariance matrix come in? This explanation here the one that starts “I can give you an explanation”.\n\nAs for the mean of the projections, let’s observe that \\(v\\) is part of some orthogonal basis of our space, and that if we project our data points on every vector of that basis, their sum will cancel out (it’s like that because projecting on the vectors from the basis is like writing the data points in the new orthogonal basis). So the sum of all the projections on the vector \\(v\\) (let’s call the sum \\(S_v\\)) and the sum of projections on other vectors from the basis (let’s call it \\(S_o\\)) is 0, because it’s the mean of the data points. But \\(S_v\\) is orthogonal to \\(S_o\\)! That means \\(S_o = S_v = 0\\).\nSo the mean of our projections is \\(0\\). Well, that’s convenient, because that means the variance is just the sum of squares of lengths of projections, or in symbols \\[\n\\sum_{i=1}^M (x_i' \\cdot v)^2 = \\sum_{i=1}^M v^T \\cdot x_i'^T \\cdot x_i' \\cdot v =  v^T \\cdot (\\sum_{i=1}^M x_i'^T \\cdot x_i) \\cdot v.\n\\]\nWell well, suddenly the covariance matrix popped out. Let’s denote it simply by \\(X\\). It means we are now looking for a unit vector \\(v\\) that maximizes \\(v^T \\cdot X \\cdot v\\), for some semi-positive definite matrix \\(X\\)."
  },
  {
    "objectID": "lectures/lectures/ch5lecture3.html#reformulating-svd",
    "href": "lectures/lectures/ch5lecture3.html#reformulating-svd",
    "title": "Ch 5 Lecture 3",
    "section": "Reformulating SVD",
    "text": "Reformulating SVD\n\nSince matrix \\(V\\) is orthogonal, \\(V^{I} V\\) equals \\(I\\). We can rewrite the SVD equation as:\n\\[\n\\begin{aligned}\n& A=U S V^{T} \\\\\n& A V=U S\n\\end{aligned}\n\\]\nThis equation establishes an important relationship between \\(u_{i}\\) and \\(v_{i}\\).\nRecall\n\\[\nA B=\\left[\\begin{array}{llll}\nA b_{1} & A b_{2} & \\ldots & A b_{n}\n\\end{array}\\right]\n\\]\nApply \\(A V=U S\\),"
  },
  {
    "objectID": "lectures/lectures/ch5lecture3.html#section-4",
    "href": "lectures/lectures/ch5lecture3.html#section-4",
    "title": "Ch 5 Lecture 3",
    "section": "",
    "text": "\\(A^T=\\left[\\begin{array}{rrrrrrrrrrrr}2.9 & -1.5 & 0.1 & -1.0 & 2.1 & -4.0 & -2.0 & 2.2 & 0.2 & 2.0 & 1.5 & -2.5 \\\\ 4.0 & -0.9 & 0.0 & -1.0 & 3.0 & -5.0 & -3.5 & 2.6 & 1.0 & 3.5 & 1.0 & -4.7\\end{array}\\right]\\)"
  },
  {
    "objectID": "lectures/lectures/ch5lecture3.html#svd-as-a-sum-of-rank-one-matrices",
    "href": "lectures/lectures/ch5lecture3.html#svd-as-a-sum-of-rank-one-matrices",
    "title": "Ch 5 Lecture 3",
    "section": "SVD as a sum of rank-one matrices",
    "text": "SVD as a sum of rank-one matrices\n\\[\nA=\\sigma_{l} u_{l} v_{l}^{T}+\\ldots+\\sigma_{r} u_{r} v_{r}^{T}\n\\]"
  },
  {
    "objectID": "lectures/lectures/ch5lecture3.html#back-to-our-example",
    "href": "lectures/lectures/ch5lecture3.html#back-to-our-example",
    "title": "Ch 5 Lecture 3",
    "section": "Back to our example",
    "text": "Back to our example\n\\[\nA=\\left(\\begin{array}{ccc}\n3 & 2 & 2 \\\\\n2 & 3 & -2\n\\end{array}\\right)\n\\]\n\nCan decompose as\n\n\n\n\\(=\\left(\\begin{array}{ccc}3 & 2 & 2 \\\\ 2 & 3 & -2\\end{array}\\right)\\)"
  },
  {
    "objectID": "lectures/lectures/ch5lecture3.html#moore-penrose-pseudoinverse",
    "href": "lectures/lectures/ch5lecture3.html#moore-penrose-pseudoinverse",
    "title": "Ch 5 Lecture 3",
    "section": "Moore-Penrose pseudoinverse",
    "text": "Moore-Penrose pseudoinverse\nIf we have a linear system\n\\[\n\\begin{aligned}\nA x & =b \\\\\n\\end{aligned}\n\\]\nand \\(A\\) is invertible, then we can solve for \\(x\\):\n\\[\n\\begin{aligned}\nx & =A^{-1} b\n\\end{aligned}\n\\]\n\nIf \\(A\\) is not invertible, we can define instead a pseudoinverse \\(A^{+}\\)\n\n\nDefine \\(A^{+}\\) in order to minimize the least squares error:\n\\[\n\\left\\|\\mathbf{A} \\mathbf{A}^{+}-\\mathbf{I}_{\\mathbf{n}}\\right\\|_{2}\n\\]\n\n\nThen we can estimate \\(x\\) as\n\\[\n\\begin{aligned}\nA x & =b \\\\\nx & \\approx A^{+} b\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lectures/lectures/ch5lecture3.html#finding-the-form-of-the-pseudoinverse",
    "href": "lectures/lectures/ch5lecture3.html#finding-the-form-of-the-pseudoinverse",
    "title": "Ch 5 Lecture 3",
    "section": "Finding the form of the pseudoinverse",
    "text": "Finding the form of the pseudoinverse\n\nRemember the Normal equations:\n\\[\nA^{T} A x=A^{T} b\n\\]\nNow we do the SVD decomposition of \\(A\\):\n\\[\nA=U S V^{T}\n\\]\nThen we can write the Normal equations as\n\\[\nV S^{T} U^{T} U S V^{T} x= V S^2 V^{T} x = V S^{T} U^{T} b\n\\]\nso\n\\[\nS^2 V^{T} x = S U^{T} b\n\\]\nNow we define \\(S^{+}\\) as the matrix with the reciprocals of the non-zero singular values on the diagonal, and zeros elsewhere. Then \\(S S^{+}\\) has ones in place of all the singular values. So\n\\[\n\\begin{aligned}\nV^{T} x &= S^+ S^+ S U^{T} b = S^+ U^{T} b \\\\\nx &= V S^+ U^{T} b\n\\end{aligned}\n\\]\nExample: \\[\n\\left[\\begin{array}{ccc}\n-3 & 0 & 0 \\\\\n0 & 4 & 0 \\\\\n0 & 0 & 0\n\\end{array}\\right]^{+}=\\left[\\begin{array}{ccc}\n-1 / 3 & 0 & 0 \\\\\n0 & 1 / 4 & 0 \\\\\n0 & 0 & 0\n\\end{array}\\right]\n\\]\nTherefore, the pseudoinverse of \\(A\\) is \\(A^{+}=V S^+ U^{T}\\)."
  },
  {
    "objectID": "lectures/lectures/ch5lecture3.html#example-1",
    "href": "lectures/lectures/ch5lecture3.html#example-1",
    "title": "Ch 5 Lecture 3",
    "section": "Example",
    "text": "Example\n\n\\[\n\\begin{aligned}\n& A=\\left[\\begin{array}{ll}\n1 & 1 \\\\\n1 & 1\n\\end{array}\\right] \\text { (singular, inverse does not exist) } \\\\\n& A=U \\Sigma V^{T}=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{rr}\n1 & 1 \\\\\n1 & -1\n\\end{array}\\right]\\left[\\begin{array}{rr}\n2 & 0 \\\\\n0 & 0\n\\end{array}\\right] \\frac{1}{\\sqrt{2}}\\left[\\begin{array}{rr}\n1 & 1 \\\\\n1 & -1\n\\end{array}\\right] \\\\\n& A^{+}=V \\Sigma^{+} U^{T}=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{rr}\n1 & 1 \\\\\n1 & -1\n\\end{array}\\right]\\left[\\begin{array}{cc}\n1 / 2 & 0 \\\\\n0 & 0\n\\end{array}\\right] \\frac{1}{\\sqrt{2}}\\left[\\begin{array}{rr}\n1 & 1 \\\\\n1 & -1\n\\end{array}\\right]=\\frac{1}{4}\\left[\\begin{array}{ll}\n1 & 1 \\\\\n1 & 1\n\\end{array}\\right]\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lectures/lectures/ch5lecture3.html#example-height-and-weight",
    "href": "lectures/lectures/ch5lecture3.html#example-height-and-weight",
    "title": "Ch 5 Lecture 3",
    "section": "Example: Height and weight",
    "text": "Example: Height and weight\n\\(A^T=\\left[\\begin{array}{rrrrrrrrrrrr}2.9 & -1.5 & 0.1 & -1.0 & 2.1 & -4.0 & -2.0 & 2.2 & 0.2 & 2.0 & 1.5 & -2.5 \\\\ 4.0 & -0.9 & 0.0 & -1.0 & 3.0 & -5.0 & -3.5 & 2.6 & 1.0 & 3.5 & 1.0 & -4.7\\end{array}\\right]\\)"
  },
  {
    "objectID": "lectures/lectures/ch5lecture3.html#covariance-matrix",
    "href": "lectures/lectures/ch5lecture3.html#covariance-matrix",
    "title": "Ch 5 Lecture 3",
    "section": "Covariance matrix",
    "text": "Covariance matrix\nCovariance: \\[\n\\begin{aligned}\n\\sigma_{a b}^{2} & =\\operatorname{cov}(a, b)=\\mathrm{E}[(a-\\bar{a})(b-\\bar{b})] \\\\\n\\sigma_{a}^{2} & =\\operatorname{var}(a)=\\operatorname{cov}(a, a)=\\mathrm{E}\\left[(a-\\bar{a})^{2}\\right]\n\\end{aligned}\n\\]\n\nCovariance matrix:\n\\[\n\\mathbf{\\Sigma}=\\left(\\begin{array}{cccc}\nE\\left[\\left(x_{1}-\\mu_{1}\\right)\\left(x_{1}-\\mu_{1}\\right)\\right] & E\\left[\\left(x_{1}-\\mu_{1}\\right)\\left(x_{2}-\\mu_{2}\\right)\\right] & \\ldots & E\\left[\\left(x_{1}-\\mu_{1}\\right)\\left(x_{p}-\\mu_{p}\\right)\\right] \\\\\nE\\left[\\left(x_{2}-\\mu_{2}\\right)\\left(x_{1}-\\mu_{1}\\right)\\right] & E\\left[\\left(x_{2}-\\mu_{2}\\right)\\left(x_{2}-\\mu_{2}\\right)\\right] & \\ldots & E\\left[\\left(x_{2}-\\mu_{2}\\right)\\left(x_{p}-\\mu_{p}\\right)\\right] \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nE\\left[\\left(x_{p}-\\mu_{p}\\right)\\left(x_{1}-\\mu_{1}\\right)\\right] & E\\left[\\left(x_{p}-\\mu_{p}\\right)\\left(x_{2}-\\mu_{2}\\right)\\right] & \\ldots & E\\left[\\left(x_{p}-\\mu_{p}\\right)\\left(x_{p}-\\mu_{p}\\right)\\right]\n\\end{array}\\right)\n\\]\n\n\n\\[\n\\Sigma=\\mathrm{E}\\left[(X-\\bar{X})(X-\\bar{X})^{\\mathrm{T}}\\right]\n\\]\n. . . \\[\n\\Sigma=\\frac{X X^{\\mathrm{T}}}{n} \\quad \\text { (if } X \\text { is already zero centered) }\n\\]\n\n\nFor our dataset,\n\\[\n\\text { Sample covariance } S^{2}=\\frac{A^{T} A}{(\\mathrm{n}-1)}=\\frac{1}{11}\\left[\\begin{array}{rr}\n53.46 & 73.42 \\\\\n73.42 & 107.16\n\\end{array}\\right]\n\\]"
  },
  {
    "objectID": "lectures/lectures/ch5lecture3.html#plotting",
    "href": "lectures/lectures/ch5lecture3.html#plotting",
    "title": "Ch 5 Lecture 3",
    "section": "Plotting",
    "text": "Plotting\n\n\n\n\n\n\n\n\n\nThe columns of the V matrix:"
  },
  {
    "objectID": "lectures/lectures/ch5lecture3.html#section-5",
    "href": "lectures/lectures/ch5lecture3.html#section-5",
    "title": "Ch 5 Lecture 3",
    "section": "",
    "text": "The components of the data matrix"
  },
  {
    "objectID": "lectures/lectures/ch5lecture3.html#the-columns-of-the-u-matrix",
    "href": "lectures/lectures/ch5lecture3.html#the-columns-of-the-u-matrix",
    "title": "Ch 5 Lecture 3",
    "section": "The columns of the U matrix",
    "text": "The columns of the U matrix"
  },
  {
    "objectID": "lectures/lectures/ch5lecture3.html#what-if-we-had-different-orthogonal-matrix-for-v-that-wasnt-eigenvectors-of-the-covariance-matrix",
    "href": "lectures/lectures/ch5lecture3.html#what-if-we-had-different-orthogonal-matrix-for-v-that-wasnt-eigenvectors-of-the-covariance-matrix",
    "title": "Ch 5 Lecture 3",
    "section": "What if we had different orthogonal matrix for V, that wasn’t eigenvectors of the covariance matrix?",
    "text": "What if we had different orthogonal matrix for V, that wasn’t eigenvectors of the covariance matrix?\n\n\n[[4.86       6.67454545]\n [6.67454545 9.74181818]]\n\n\n\n\n\n\n\n\n\nThe columns of the U matrix are no longer orthogonal."
  },
  {
    "objectID": "lectures/ch5lecture3.html#thinking-again-about-matrix-diagonalization",
    "href": "lectures/ch5lecture3.html#thinking-again-about-matrix-diagonalization",
    "title": "Ch 5 Lecture 3",
    "section": "Thinking again about matrix diagonalization",
    "text": "Thinking again about matrix diagonalization\n\n\nIf \\(A\\) is a (non-defective) \\(n \\times n\\) square matrix,\n\nthen we can write it as \\(A = PDP^{T}\\) where \\(D\\) is a diagonal matrix and \\(P\\) is an orthogonal matrix of eigenvectors.\nWe can find the product of \\(A\\) with a vector \\(\\mathbf{x}\\) where \\(\\mathbf{x}\\) is a column vector of length \\(n\\). This product will also be a column vector of length \\(n\\).\nWhat’s the intuition behind the diagonalization?\nThe matrix \\(P^{T}\\) is a change of basis matrix. It takes a vector \\(\\mathbf{x}\\) expressed in the standard basis and expresses it in terms of the basis of eigenvectors of \\(A\\).\n\nThis works because \\(P\\) is orthogonal.\n\nThe matrix \\(D\\) is a scaling matrix. It scales the eigenvectors by the corresponding eigenvalues. (If \\(A\\) is not full rank, then \\(D\\) will have some zeros on the diagonal.)\nThe matrix \\(P\\) is the inverse of \\(P^{T}\\) (because \\(P\\) is orthogonal). It just takes the scaled eigenvectors and expresses them in terms of the standard basis.\nThis all works because \\(P\\) is orthogonal and there’s a correspondence between the rows of \\(P^{T}\\) and the columns of \\(P\\).\n\nThis inverse projection is what we want because the columns of \\(P\\) are eigenvectors of \\(A\\).\nWe can see this most easily by imagining that \\(\\mathbf{x}\\) is a multiple of one of the eigenvectors.\n\\(P\\mathbf{x}\\) will give the coordinates of \\(\\mathbf{x}\\) in the basis of eigenvectors. For instance, if \\(P\\mathbf{x}=\\begin{bmatrix} 1 \\\\0\\\\ \\vdots \\\\ 0\\end{bmatrix}\\), we know that \\(\\mathbf{x}\\) equals the first eigenvector.\n\nAfter multiplication by the diagonal matrix, we’ll just have \\(\\begin{bmatrix} \\lambda_{1} \\\\0\\\\ \\vdots \\\\ 0\\end{bmatrix}\\).\n\nThen \\(P^{T}\\) will take this back to a multiple of the first eigenvector, back in the standard basis.\nThat’s what we want because we are supposing that \\(\\mathbf{x}\\) is a multiple of the first eigenvector.\nThe same logic will hold when \\(\\mathbf{x}\\) is weighted sum of the eigenvectors.\n\nBecause \\(P\\) is orthonormal, \\(P\\mathbf{x}\\) will give the weights of the eigenvectors in the basis of eigenvectors.\n\n\\(P^{T}\\) will take it back to the weighted sum of the eigenvectors in the standard basis, with the new weights being determined by the original weights and the eigenvalues.\n\nThis will all works when \\(U\\) and \\(V\\) are orthogonal and there’s a correspondence between the rows of \\(V^{T}\\) and the columns of \\(U\\).\n\nWhen we have some zeros on the diagonal, multiplying by \\(D\\) is a projection of \\(\\mathbf{x}\\) onto the row space of \\(A\\). We are removing the components of \\(\\mathbf{x}\\) that are in the null space of \\(A\\).\n\n\nMany parts of this inspired by this blog post"
  },
  {
    "objectID": "lectures/ch5lecture3.html#the-same-logic-applied-to-a-general-matrix",
    "href": "lectures/ch5lecture3.html#the-same-logic-applied-to-a-general-matrix",
    "title": "Ch 5 Lecture 3",
    "section": "The same logic, applied to a general matrix",
    "text": "The same logic, applied to a general matrix\n\nIf \\(A\\) is a \\(m \\times n\\) square matrix,\n\nthen we can write it as \\(A = U\\Sigma V^{T}\\) where \\(\\Sigma\\) is a diagonal matrix and \\(P\\) is an orthogonal matrix of eigenvectors.\nWe can find the product of \\(A\\) with a vector \\(\\mathbf{x}\\) where \\(\\mathbf{x}\\) is a column vector of length \\(n\\). This product will now be a column vector of length \\(m\\).\nThe matrix \\(V^{T}\\) is a change of basis matrix. It takes a vector \\(\\mathbf{x}\\) expressed in the standard basis and expresses it in terms of the basis of right singular vectors of \\(A\\).\nThe matrix \\(\\Sigma\\) is a scaling matrix. It scales the singular vectors by the corresponding singular values. (If \\(A\\) is not square or \\(A\\) is not full rank, then \\(\\Sigma\\) will have some zeros on the diagonal.)\nThe matrix \\(U\\) is not generally the inverse of \\(V^{T}\\). It takes the scaled singular vectors and expresses them in terms of the standard basis of \\(\\mathbb{R}^m\\).\n\nSpecifically, the first column of \\(U\\) must be the correct vector to express the first row of \\(V^{T}\\) in \\(\\mathbbb{R}^m\\)\n\n\nthat is, we need the first column of \\(U\\) to be \\(A\\mathbf{v}_1\\) (divided by the scaling factor).\nThis is because \\(\\Sigma V^{T}\\mathbf{v}_1=\\sigma_1\\begin{bmatrix} 1 \\\\0\\\\ \\vdots \\\\ 0\\end{bmatrix}\\), so that \\(U\\Sigma V^{T}\\mathbf{v}_1=\\sigma_1 U \\begin{bmatrix} 1 \\\\0\\\\ \\vdots \\\\ 0\\end{bmatrix}=\\sigma_1\\mathbf{u_1}\\).\nSo we’d better have \\(\\mathbf{u_1}=A\\mathbf{v}_1/\\sigma_1\\). :::\n\n\n## SVD\n::: notes - Assume \\(m \\geq n\\) (the case \\(m&lt;n\\) is similar). - Let \\(B=A^{T} A\\) and let \\(\\lambda_{1} \\geq \\lambda_{2} \\geq \\cdots \\geq \\lambda_{n}\\) be the eigenvalues of \\(B\\). - Find the basis of eigenvectors of \\(B\\): \\(B \\mathbf{v}_{k}=\\sigma_{k}^{2} \\mathbf{v}_{k}, k=1,2, \\ldots, n\\) - Let \\(V=\\left[\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\ldots, \\mathbf{v}_{n}\\right]\\). Then \\(V\\) is an orthogonal \\(n \\times n\\) matrix. - We may assume for some index \\(r\\) that \\(\\sigma_{r+1}, \\sigma_{r+2}, \\ldots, \\sigma_{n}\\) are zero, while \\(\\sigma_{r} \\neq 0\\). - Next set \\(\\mathbf{u}_{j}=\\frac{1}{\\sigma_{j}} A \\mathbf{v}_{j}, j=1,2, \\ldots, r\\). - In other words, \\(\\mathbf{u}_{j}\\) is the vector that you get when you transform \\(\\mathbf{v}_{j}\\) by \\(A\\) and then normalize it by the singular value \\(\\sigma_{j}\\). - These are orthonormal vectors in \\(\\mathbb{R}^{m}\\) since\n\\[\n\\mathbf{u}_{j}^{T} \\mathbf{u}_{k}=\\frac{1}{\\sigma_{j} \\sigma_{k}} \\mathbf{v}_{j}^{T} A^{T} A \\mathbf{v}_{k}=\\frac{1}{\\sigma_{j} \\sigma_{k}} \\mathbf{v}_{j}^{T} B \\mathbf{v}_{k}=\\frac{\\sigma_{k}^{2}}{\\sigma_{j} \\sigma_{k}} \\mathbf{v}_{j}^{T} \\mathbf{v}_{k}= \\begin{cases}0, & \\text { if } j \\neq k \\\\ 1, & \\text { if } j=k\\end{cases}\n\\]\n\nNow expand this set to an orthonormal basis \\(\\mathbf{u}_{1}, \\mathbf{u}_{2}, \\ldots, \\mathbf{u}_{m}\\) of \\(\\mathbb{R}^{m}\\). This is possible by Theorem 4.7 in Section 4.3. Set \\(U=\\left[\\mathbf{u}_{1}, \\mathbf{u}_{2}, \\ldots, \\mathbf{u}_{m}\\right]\\). This matrix is orthogonal. We calculate that if \\(k&gt;r\\), then \\(\\mathbf{u}_{j}^{T} A \\mathbf{v}_{k}=0\\) since \\(A \\mathbf{v}_{k}=\\mathbf{0}\\), and if \\(k&lt;r\\), then\n\n\\[\n\\mathbf{u}_{j}^{T} A \\mathbf{v}_{k}=\\sigma_{k} \\mathbf{u}_{j}^{T} \\mathbf{u}_{k}=\\left\\{\\begin{array}{l}\n0, \\text { if } j \\neq k, \\\\\n\\sigma_{k}, \\text { if } j=k\n\\end{array}\\right.\n\\]\n\\(U^{T} A V=\\left[\\mathbf{u}_{j}^{T} A \\mathbf{v}_{k}\\right]=\\Sigma\\), which is the desired SVD.\nAnd because \\(U\\) and \\(V\\) are orthonormal, their inverses are their transposes, so \\(A=U \\Sigma V^{T}\\).\nNote also that we can find \\(U\\) as the eigenvectors of \\(A A^{T}\\) and \\(V\\) as the eigenvectors of \\(A^{T} A\\)."
  },
  {
    "objectID": "lectures/ch5lecture3.html#recap",
    "href": "lectures/ch5lecture3.html#recap",
    "title": "Ch 5 Lecture 3",
    "section": "Recap",
    "text": "Recap\n\\[\nA=U S V^{T}\n\\]\nwhere\n$$\n\\[\\begin{array}{lccc}\n& & & U & V\n& S \\\\\n& & &\\left(\\begin{array}{ccc}\n\\mid & & \\mid \\\\\n\\mathbf{u}_{1} & \\cdots & \\mathbf{u}_{m} \\\\\n\\mid & & \\mid\n\\end{array}\\right) &\\left(\\begin{array}{ccc}\n\\mid & & \\mid \\\\\n\\mathbf{v}_{1} & \\cdots & \\mathbf{v}_{n} \\\\\n\\mid & & \\mid\n\\end{array}\\right) &\\left(\\begin{array}{cccc}\n\\sigma_1 & & & \\\\\n& \\sigma_2 & \\\\  & & \\ddots & \\\\\n& & &  \\sigma_r & \\\\ & & & & \\ddots \\\\ & & & & & 0\n\n\\end{array}\\right)\\\\\n\\text{eigenvectors of} & & & A A^{T}  & A^{T} A \\\\\n\\end{array}\\]\n$$"
  },
  {
    "objectID": "lectures/ch5lecture3.html#example",
    "href": "lectures/ch5lecture3.html#example",
    "title": "Ch 5 Lecture 3",
    "section": "Example",
    "text": "Example\n\\[\nA=\\left(\\begin{array}{ccc}\n3 & 2 & 2 \\\\\n2 & 3 & -2\n\\end{array}\\right)\n\\]\n\n\\[\nA A^{T}=\\left(\\begin{array}{cc}\n17 & 8 \\\\\n8 & 17\n\\end{array}\\right), \\quad A^{T} A=\\left(\\begin{array}{ccc}\n13 & 12 & 2 \\\\\n12 & 13 & -2 \\\\\n2 & -2 & 8\n\\end{array}\\right)\n\\]"
  },
  {
    "objectID": "lectures/ch5lecture3.html#section",
    "href": "lectures/ch5lecture3.html#section",
    "title": "Ch 5 Lecture 3",
    "section": "",
    "text": "\\[\n\\begin{array}{cc}\nA A^{T}=\\left(\\begin{array}{cc}\n17 & 8 \\\\\n8 & 17\n\\end{array}\\right) & A^{T} A=\\left(\\begin{array}{ccc}\n13 & 12 & 2 \\\\\n12 & 13 & -2 \\\\\n2 & -2 & 8\n\\end{array}\\right) \\\\\n\\begin{array}{c}\n\\text { eigenvalues: } \\lambda_{1}=25, \\lambda_{2}=9 \\\\\n\\text { eigenvectors }\n\\end{array} & \\begin{array}{c}\n\\text { eigenvalues: } \\lambda_{1}=25, \\lambda_{2}=9, \\lambda_{3}=0 \\\\\n\\text { eigenvectors }\n\\end{array} \\\\\nu_{1}=\\binom{1 / \\sqrt{2}}{1 / \\sqrt{2}} \\quad u_{2}=\\binom{1 / \\sqrt{2}}{-1 / \\sqrt{2}} & v_{1}=\\left(\\begin{array}{c}\n1 / \\sqrt{2} \\\\\n1 / \\sqrt{2} \\\\\n0\n\\end{array}\\right) \\quad v_{2}=\\left(\\begin{array}{c}\n1 / \\sqrt{18} \\\\\n-1 / \\sqrt{18} \\\\\n4 / \\sqrt{18}\n\\end{array}\\right) \\quad v_{3}=\\left(\\begin{array}{c}\n2 / 3 \\\\\n-2 / 3 \\\\\n-1 / 3\n\\end{array}\\right)\n\\end{array}\n\\]"
  },
  {
    "objectID": "lectures/ch5lecture3.html#section-1",
    "href": "lectures/ch5lecture3.html#section-1",
    "title": "Ch 5 Lecture 3",
    "section": "",
    "text": "SVD decomposition of \\(A\\): \\[\nA=U S V^{T}=\\left(\\begin{array}{cc}\n1 / \\sqrt{2} & 1 / \\sqrt{2} \\\\\n1 / \\sqrt{2} & -1 / \\sqrt{2}\n\\end{array}\\right)\\left(\\begin{array}{ccc}\n5 & 0 & 0 \\\\\n0 & 3 & 0\n\\end{array}\\right)\\left(\\begin{array}{rrr}\n1 / \\sqrt{2} & 1 / \\sqrt{2} & 0 \\\\\n1 / \\sqrt{18} & -1 / \\sqrt{18} & 4 / \\sqrt{18} \\\\\n2 / 3 & -2 / 3 & -1 / 3\n\\end{array}\\right)\n\\]"
  },
  {
    "objectID": "lectures/ch5lecture3.html#reformulating-svd",
    "href": "lectures/ch5lecture3.html#reformulating-svd",
    "title": "Ch 5 Lecture 3",
    "section": "Reformulating SVD",
    "text": "Reformulating SVD\n\nSince matrix \\(V\\) is orthogonal, \\(V^{I} V\\) equals \\(I\\). We can rewrite the SVD equation as:\n\\[\n\\begin{aligned}\n& A=U S V^{T} \\\\\n& A V=U S\n\\end{aligned}\n\\]\nThis equation establishes an important relationship between \\(u_{i}\\) and \\(v_{i}\\).\nRecall\n\\[\nA B=\\left[\\begin{array}{llll}\nA b_{1} & A b_{2} & \\ldots & A b_{n}\n\\end{array}\\right]\n\\]\nApply \\(A V=U S\\),"
  },
  {
    "objectID": "lectures/ch5lecture3.html#section-2",
    "href": "lectures/ch5lecture3.html#section-2",
    "title": "Ch 5 Lecture 3",
    "section": "",
    "text": "\\[\n\\begin{aligned}\n& =\\left(\\begin{array}{c}\nu_{11} \\\\\n\\vdots \\\\\nu_{1 m}\n\\end{array}\\right) \\sigma_{1} \\\\\n& A v_{1}=\\sigma_{1} u_{1}\n\\end{aligned}\n\\]\nThis can be generalized as\n\\[\nA v_{i}=\\sigma_{i} u_{i}\n\\]\nRecall,\n\nand\n\\[\n\\left(\\begin{array}{ccc}\n\\begin{array}{c}\n\\mid \\\\\nu_{1} \\\\\n\\mid\n\\end{array} & \\ldots &\n\\begin{array}{c}\n\\mid \\\\\nu_{n} \\\\\n\\mid\n\\end{array}\n\\end{array}\\right)\\left(\\begin{array}{c}\n-v_{l}- \\\\\n\\vdots \\\\\n-v_{n}-\n\\end{array}\\right)=\\left(\\begin{array}{c}\n\\mid \\\\\nu_{1} \\\\\n\\mid\n\\end{array}\\right)\\left(-v_{l} \\textrm{---}\\right)+\\ldots+\\left(\\begin{array}{c}\n\\mid \\\\\nu_{n} \\\\\n\\mid\n\\end{array}\\right)\\left(-v_{n} \\textrm{---}\\right)\n\\]\nThe SVD decomposition can be recognized as a series of outer products of \\(u_{i}\\) and \\(v_{i}\\).\n\\[\nA=\\sigma_{l} u_{l} v_{l}^{T}+\\ldots+\\sigma_{r} u_{r} v_{r}^{T}\n\\]"
  },
  {
    "objectID": "lectures/ch5lecture3.html#svd-as-a-sum-of-rank-one-matrices",
    "href": "lectures/ch5lecture3.html#svd-as-a-sum-of-rank-one-matrices",
    "title": "Ch 5 Lecture 3",
    "section": "SVD as a sum of rank-one matrices",
    "text": "SVD as a sum of rank-one matrices\n\\[\nA=\\sigma_{l} u_{l} v_{l}^{T}+\\ldots+\\sigma_{r} u_{r} v_{r}^{T}\n\\]"
  },
  {
    "objectID": "lectures/ch5lecture3.html#back-to-our-example",
    "href": "lectures/ch5lecture3.html#back-to-our-example",
    "title": "Ch 5 Lecture 3",
    "section": "Back to our example",
    "text": "Back to our example\n\\[\nA=\\left(\\begin{array}{ccc}\n3 & 2 & 2 \\\\\n2 & 3 & -2\n\\end{array}\\right)\n\\]\n\nCan decompose as\n\n\n\n\\(=\\left(\\begin{array}{ccc}3 & 2 & 2 \\\\ 2 & 3 & -2\\end{array}\\right)\\)"
  },
  {
    "objectID": "lectures/ch5lecture3.html#moore-penrose-pseudoinverse",
    "href": "lectures/ch5lecture3.html#moore-penrose-pseudoinverse",
    "title": "Ch 5 Lecture 3",
    "section": "Moore-Penrose pseudoinverse",
    "text": "Moore-Penrose pseudoinverse\nIf we have a linear system\n\\[\n\\begin{aligned}\nA x & =b \\\\\n\\end{aligned}\n\\]\nand \\(A\\) is invertible, then we can solve for \\(x\\):\n\\[\n\\begin{aligned}\nx & =A^{-1} b\n\\end{aligned}\n\\]\n\nIf \\(A\\) is not invertible, we can define instead a pseudoinverse \\(A^{+}\\)\n\n\nDefine \\(A^{+}\\) in order to minimize the least squares error:\n\\[\n\\left\\|\\mathbf{A} \\mathbf{A}^{+}-\\mathbf{I}_{\\mathbf{n}}\\right\\|_{2}\n\\]\n\n\nThen we can estimate \\(x\\) as\n\\[\n\\begin{aligned}\nA x & =b \\\\\nx & \\approx A^{+} b\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lectures/ch5lecture3.html#finding-the-form-of-the-pseudoinverse",
    "href": "lectures/ch5lecture3.html#finding-the-form-of-the-pseudoinverse",
    "title": "Ch 5 Lecture 3",
    "section": "Finding the form of the pseudoinverse",
    "text": "Finding the form of the pseudoinverse\n\nRemember the Normal equations:\n\\[\nA^{T} A x=A^{T} b\n\\]\nNow we do the SVD decomposition of \\(A\\):\n\\[\nA=U S V^{T}\n\\]\nThen we can write the Normal equations as\n\\[\nV S^{T} U^{T} U S V^{T} x= V S^2 V^{T} x = V S^{T} U^{T} b\n\\]\nso\n\\[\nS^2 V^{T} x = S U^{T} b\n\\]\nNow we define \\(S^{+}\\) as the matrix with the reciprocals of the non-zero singular values on the diagonal, and zeros elsewhere. Then \\(S S^{+}\\) has ones in place of all the singular values. So\n\\[\n\\begin{aligned}\nV^{T} x &= S^+ S^+ S U^{T} b = S^+ U^{T} b \\\\\nx &= V S^+ U^{T} b\n\\end{aligned}\n\\]\nExample: \\[\n\\left[\\begin{array}{ccc}\n-3 & 0 & 0 \\\\\n0 & 4 & 0 \\\\\n0 & 0 & 0\n\\end{array}\\right]^{+}=\\left[\\begin{array}{ccc}\n-1 / 3 & 0 & 0 \\\\\n0 & 1 / 4 & 0 \\\\\n0 & 0 & 0\n\\end{array}\\right]\n\\]\nTherefore, the pseudoinverse of \\(A\\) is \\(A^{+}=V S^+ U^{T}\\)."
  },
  {
    "objectID": "lectures/ch5lecture3.html#example-1",
    "href": "lectures/ch5lecture3.html#example-1",
    "title": "Ch 5 Lecture 3",
    "section": "Example",
    "text": "Example\n\n\\[\n\\begin{aligned}\n& A=\\left[\\begin{array}{ll}\n1 & 1 \\\\\n1 & 1\n\\end{array}\\right] \\text { (singular, inverse does not exist) } \\\\\n& A=U \\Sigma V^{T}=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{rr}\n1 & 1 \\\\\n1 & -1\n\\end{array}\\right]\\left[\\begin{array}{rr}\n2 & 0 \\\\\n0 & 0\n\\end{array}\\right] \\frac{1}{\\sqrt{2}}\\left[\\begin{array}{rr}\n1 & 1 \\\\\n1 & -1\n\\end{array}\\right] \\\\\n& A^{+}=V \\Sigma^{+} U^{T}=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{rr}\n1 & 1 \\\\\n1 & -1\n\\end{array}\\right]\\left[\\begin{array}{cc}\n1 / 2 & 0 \\\\\n0 & 0\n\\end{array}\\right] \\frac{1}{\\sqrt{2}}\\left[\\begin{array}{rr}\n1 & 1 \\\\\n1 & -1\n\\end{array}\\right]=\\frac{1}{4}\\left[\\begin{array}{ll}\n1 & 1 \\\\\n1 & 1\n\\end{array}\\right]\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lectures/ch5lecture3.html#example-height-and-weight",
    "href": "lectures/ch5lecture3.html#example-height-and-weight",
    "title": "Ch 5 Lecture 3",
    "section": "Example: Height and weight",
    "text": "Example: Height and weight\n\\(A^T=\\left[\\begin{array}{rrrrrrrrrrrr}2.9 & -1.5 & 0.1 & -1.0 & 2.1 & -4.0 & -2.0 & 2.2 & 0.2 & 2.0 & 1.5 & -2.5 \\\\ 4.0 & -0.9 & 0.0 & -1.0 & 3.0 & -5.0 & -3.5 & 2.6 & 1.0 & 3.5 & 1.0 & -4.7\\end{array}\\right]\\)"
  },
  {
    "objectID": "lectures/ch5lecture3.html#covariance-matrix",
    "href": "lectures/ch5lecture3.html#covariance-matrix",
    "title": "Ch 5 Lecture 3",
    "section": "Covariance matrix",
    "text": "Covariance matrix\nCovariance: \\[\n\\begin{aligned}\n\\sigma_{a b}^{2} & =\\operatorname{cov}(a, b)=\\mathrm{E}[(a-\\bar{a})(b-\\bar{b})] \\\\\n\\sigma_{a}^{2} & =\\operatorname{var}(a)=\\operatorname{cov}(a, a)=\\mathrm{E}\\left[(a-\\bar{a})^{2}\\right]\n\\end{aligned}\n\\]\n\nCovariance matrix:\n\\[\n\\mathbf{\\Sigma}=\\left(\\begin{array}{cccc}\nE\\left[\\left(x_{1}-\\mu_{1}\\right)\\left(x_{1}-\\mu_{1}\\right)\\right] & E\\left[\\left(x_{1}-\\mu_{1}\\right)\\left(x_{2}-\\mu_{2}\\right)\\right] & \\ldots & E\\left[\\left(x_{1}-\\mu_{1}\\right)\\left(x_{p}-\\mu_{p}\\right)\\right] \\\\\nE\\left[\\left(x_{2}-\\mu_{2}\\right)\\left(x_{1}-\\mu_{1}\\right)\\right] & E\\left[\\left(x_{2}-\\mu_{2}\\right)\\left(x_{2}-\\mu_{2}\\right)\\right] & \\ldots & E\\left[\\left(x_{2}-\\mu_{2}\\right)\\left(x_{p}-\\mu_{p}\\right)\\right] \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nE\\left[\\left(x_{p}-\\mu_{p}\\right)\\left(x_{1}-\\mu_{1}\\right)\\right] & E\\left[\\left(x_{p}-\\mu_{p}\\right)\\left(x_{2}-\\mu_{2}\\right)\\right] & \\ldots & E\\left[\\left(x_{p}-\\mu_{p}\\right)\\left(x_{p}-\\mu_{p}\\right)\\right]\n\\end{array}\\right)\n\\]\n\n\n\\[\n\\Sigma=\\mathrm{E}\\left[(X-\\bar{X})(X-\\bar{X})^{\\mathrm{T}}\\right]\n\\]\n. . . \\[\n\\Sigma=\\frac{X X^{\\mathrm{T}}}{n} \\quad \\text { (if } X \\text { is already zero centered) }\n\\]\n\n\nFor our dataset,\n\\[\n\\text { Sample covariance } S^{2}=\\frac{A^{T} A}{(\\mathrm{n}-1)}=\\frac{1}{11}\\left[\\begin{array}{rr}\n53.46 & 73.42 \\\\\n73.42 & 107.16\n\\end{array}\\right]\n\\]"
  },
  {
    "objectID": "lectures/ch5lecture3.html#section-4",
    "href": "lectures/ch5lecture3.html#section-4",
    "title": "Ch 5 Lecture 3",
    "section": "",
    "text": "\\(A^T=\\left[\\begin{array}{rrrrrrrrrrrr}2.9 & -1.5 & 0.1 & -1.0 & 2.1 & -4.0 & -2.0 & 2.2 & 0.2 & 2.0 & 1.5 & -2.5 \\\\ 4.0 & -0.9 & 0.0 & -1.0 & 3.0 & -5.0 & -3.5 & 2.6 & 1.0 & 3.5 & 1.0 & -4.7\\end{array}\\right]\\)"
  },
  {
    "objectID": "lectures/ch5lecture3.html#plotting",
    "href": "lectures/ch5lecture3.html#plotting",
    "title": "Ch 5 Lecture 3",
    "section": "Plotting",
    "text": "Plotting\nThe columns of the V matrix:\n\n\n[[-0.57294952 -0.81959066]\n [ 0.81959066 -0.57294952]]\n\n\n\n\n\n\n\n\n\nWalk through why these vectors are actually the eigenvectors of the covariance matrix.\n\nLook at the covariance matrix: \\[\n\\text { Sample covariance } S^{2}=\\frac{A^{T} A}{(\\mathrm{n}-1)}=\\frac{1}{11}\\left[\\begin{array}{rr}\n53.46 & 73.42 \\\\\n73.42 & 107.16\n\\end{array}\\right]\n\\]\nCalculate the covariance matrix times the eigenvector, by hand. See why it comes out the same, because it captures the results of the covariance matrix."
  },
  {
    "objectID": "lectures/ch5lecture3.html#section-5",
    "href": "lectures/ch5lecture3.html#section-5",
    "title": "Ch 5 Lecture 3",
    "section": "",
    "text": "The components of the data matrix"
  },
  {
    "objectID": "lectures/ch5lecture3.html#the-columns-of-the-u-matrix",
    "href": "lectures/ch5lecture3.html#the-columns-of-the-u-matrix",
    "title": "Ch 5 Lecture 3",
    "section": "The columns of the U matrix",
    "text": "The columns of the U matrix"
  },
  {
    "objectID": "lectures/ch5lecture3.html#what-if-we-had-different-orthogonal-matrix-for-v-that-wasnt-eigenvectors-of-the-covariance-matrix",
    "href": "lectures/ch5lecture3.html#what-if-we-had-different-orthogonal-matrix-for-v-that-wasnt-eigenvectors-of-the-covariance-matrix",
    "title": "Ch 5 Lecture 3",
    "section": "What if we had different orthogonal matrix for V, that wasn’t eigenvectors of the covariance matrix?",
    "text": "What if we had different orthogonal matrix for V, that wasn’t eigenvectors of the covariance matrix?\n\n\n[[4.86       6.67454545]\n [6.67454545 9.74181818]]\n\n\n\n\n\n\n\n\n\nThe columns of the U matrix are no longer orthogonal."
  },
  {
    "objectID": "lectures/ch5lecture3.html#section-6",
    "href": "lectures/ch5lecture3.html#section-6",
    "title": "Ch 5 Lecture 3",
    "section": "",
    "text": "The U matrix as a heat plot"
  },
  {
    "objectID": "submitted/projects_1_revised/buyalosbobby_project1-1.html",
    "href": "submitted/projects_1_revised/buyalosbobby_project1-1.html",
    "title": "Project #1 Revisions - Bobby Buyalos",
    "section": "",
    "text": "Three automobile insurance firms compete for a fixed market of customers. We have three companies in our market, company A, B, and C. Let \\(a\\) denote the market share of company A, \\(b\\) represent the market share of company B, and \\(c\\) represent the market share of company C. Let \\(a_n, b_n, c_n\\) represent the market share of the relative companies in year \\(n\\). So, for a given \\(n\\), we know that \\(a_n + b_n + c_n = 1\\), in other words, these three companies disjointly parition the market.\nWe know what preportion of each companies market share moves or stays with the company the following year, in other words, we know how their relative market shares change yearly. So, using the data for say the \\(n-1\\)-th year, we know precicely the market share for the \\(n\\)-th year. We can represent this using Markov Chain. First, view the linear equations that dictate what the \\(n\\)-th year market shares will be based on the \\(n-1\\)-th year:\nConsider our system (\\(n = 1, 2, \\ldots\\)): \\(\\begin{align*}\n    &a_n = .5a_{n-1} + .2b_{n-1} + .3c_{n-1} \\\\\n    &b_n = .2a_{n-1} + .6b_{n-1} + .1c_{n-1} \\\\\n    &c_n = .3a_{n-1} + .2b_{n-1} + .6c_{n-1}\n\\end{align*}\\)\nWe can then put these linear equations into a transition matrix:\n\\[\n\\begin{pmatrix}\n    .5 & .2 & .3 \\\\\n    .2 & .6 & .1 \\\\\n    .3 & .2 & .6\n\\end{pmatrix}\n\\]\nSince this matrix is stochastic, if we initalize it with a distribution vector, then we will have a Markov chain. Lets see what the market shares will be for the next three years if we start off with all three companies having equal market share (\\(a_{0} = b_{0} = c_{0} = 1/3\\)). See the output of the code cell below for the distribution vector that contains the market distribution for each company after each year (first compnent represents share of company A, second component represents that of B, and the last component represents that of C).\n\nimport sympy as sym\n\n#create our transition matrix\nP = sym.Matrix([[.5, .2 , .3 ],\n    [.2 , .6 , .1],\n    [.3 , .2 , .6]])\n\n#inital vector\nx = sym.Matrix([[1/3], [1/3], [1/3]])\n\n#run it for each year and print result\nfor i in range(4):\n    print(f\"\\n Year {i}:\")\n    print((P**i)*x)\n\n\n Year 0:\nMatrix([[0.333333333333333], [0.333333333333333], [0.333333333333333]])\n\n Year 1:\nMatrix([[0.333333333333333], [0.300000000000000], [0.366666666666667]])\n\n Year 2:\nMatrix([[0.336666666666667], [0.283333333333333], [0.380000000000000]])\n\n Year 3:\nMatrix([[0.339000000000000], [0.275333333333333], [0.385666666666667]])\n\n\nNow, assume that you and I are employees of Company A. We are considering three different advertising campaigns to implement, and want run some tests to see which would be better. Creating a transition matrix in the identical fashion as above, each of the campaigns have the following transition matrices:\nCampaign #1 has the following transition matrix:\n\\[\n\\begin{pmatrix}\n    .5 & .32 & .3 \\\\\n    .2 & .48 & .1 \\\\\n    .3 & .2 & .6\n\\end{pmatrix}\n\\]\nCampaign #2 has the following transition matrix:\n\\[\n\\begin{pmatrix}\n    .5 & .2 & .42 \\\\\n    .2 & .6 & .1 \\\\\n    .3 & .2 & .48\n\\end{pmatrix}\n\\]\nThe first way we will test how “good” each campaign is is just by looking at the transition matrix after \\(k\\)-many years.\nRecall that \\(\\mathbf{x^{(k)}} = A^k\\cdot\\mathbf{x^{(0)}}\\).\nIf we want company A to have the greatest market share after \\(k\\) many years, then we want the sum of the top row of our matrix to be as large as possible.\nLets look at the transition matrix for Campaign 1, Campaign 2, and no campaign after 5 years:\n\nimport random\n\nC1 = sym.Matrix([[.5, .32 , .3 ],\n    [.2 , .48 , .1],\n    [.3 , .2 , .6]])\n\nC2 = sym.Matrix([[.5, .2 , .42 ],\n    [.2 , .6 , .1],\n    [.3 , .2 , .48]])\n\n\ndef count_top_row(T):\n    return sum(T[0, :])\n\nprint(\"campaign 1\", count_top_row(C1**5))\nprint(\"campaign 2\", count_top_row(C2**5))\nprint(\"no campaign\", count_top_row(P**5))\n\ncampaign 1 1.14189225920000\ncampaign 2 1.16869517120000\nno campaign 1.02269000000000\n\n\nSince the sum of the coefficents of the top row of a respective transition matrix is the highest in campaign 2, it seems that, on average, campaign 2 will yield the highest market share, at least after 5 years.\nLets run a different type of test to measure how “good” each campaign is to see if we can corroborate this finding.\nFor each test, we will generate 2000 random inital distribution vectors, and feed it into each of our transition matrices for a given amount of years. We will do this for 2 and 5 years, gauging both short and long term interests. We will assign each campaign/no campaign a score out of 2000: the transition matrix that yields the highest market share for a given inital state will win a ‘point’.\n\nimport random\n\nC1 = sym.Matrix([[.5, .32 , .3 ],\n    [.2 , .48 , .1],\n    [.3 , .2 , .6]])\n\nC2 = sym.Matrix([[.5, .2 , .42 ],\n    [.2 , .6 , .1],\n    [.3 , .2 , .48]])\n\ndef generate_input_vector():\n    random1 = random.randint(1, 1000)\n    random2 = random.randint(1, 1000)\n    random3 = random.randint(1, 1000)\n    total = random1 + random2 + random3\n    return sym.Matrix([[random1/total], [random2/total], [random3/total]])\n\ndef test_system(years):\n    P_score = 0\n    C1_score = 0\n    C2_score = 0\n    for i in range(2000):\n        x = generate_input_vector()\n        new_P = P**years\n        new_C1 = C1**years\n        new_C2 = C2**years\n        P_value = (new_P*x)[0]\n        C1_value = (new_C1*x)[0]\n        C2_value = (new_C2*x)[0]\n    \n        if P_value &gt;= max(C1_value, C2_value):\n            P_score += 1\n        if C1_value &gt;= max(P_value, C2_value):\n            C1_score += 1\n        if C2_value &gt;= max(C1_value, P_value):\n            C2_score += 1\n    return (f\"run time of {years} years, P: {P_score}\", f\"C1: {C1_score}\", f\"C2: {C2_score}\")\n\nprint(test_system(2))\nprint(test_system(5))\n\n('run time of 2 years, P: 0', 'C1: 834', 'C2: 1166')\n('run time of 5 years, P: 0', 'C1: 0', 'C2: 2000')\n\n\nBased on these tests, I would reccomend the second campaign. It is clear that the second campaign performs the best in the long term. After just 5 years, it will yield conclusively higher market share than doing nothing or doing campaign 1. This is corroborated by theoritical evidence (observing the transition matrix), and experimental evidence, based on our randomized trials. Even after only 2 years, campaign 2 performs better on average than campaign one, with over half of the inital states resulting with a higher market share after campaign 2. So, when considering both short and long term interests, campaign 2 is the clear winner.\nSome limitations of this study may be that it only looks yearly benchmarks. If the company wants some thing in a shorter time frame, such as the next quarter, then these models would not be useful. In addition, this model assumes that all other conditions are held constant, and that the only factor affecting market share are the campaigns, or other controlled factors used when getting the data to construct the transition matrices. However, it is unlikely that there would be no counter-ad campaign from another company that may change the real transition matrix. In other words, this study does not anticipate other changing factors in the market that may change the transition matrix and thus the results.\n\nNicely done! You’ve addressed the issue with the random vectors, you’ve made it much more clear what you’ve done, and I like the thinking about the meaning of the transition matrix. (Of course, now that we have the tools of the eivenvalue problem, you might choose a different approach – like looking at the market share for company A in the dominant eigenvector!)\nGrade: E"
  },
  {
    "objectID": "submitted/projects_1_revised/buyalosbobby_project1-1.html#markov-chains",
    "href": "submitted/projects_1_revised/buyalosbobby_project1-1.html#markov-chains",
    "title": "Project #1 Revisions - Bobby Buyalos",
    "section": "",
    "text": "Three automobile insurance firms compete for a fixed market of customers. We have three companies in our market, company A, B, and C. Let \\(a\\) denote the market share of company A, \\(b\\) represent the market share of company B, and \\(c\\) represent the market share of company C. Let \\(a_n, b_n, c_n\\) represent the market share of the relative companies in year \\(n\\). So, for a given \\(n\\), we know that \\(a_n + b_n + c_n = 1\\), in other words, these three companies disjointly parition the market.\nWe know what preportion of each companies market share moves or stays with the company the following year, in other words, we know how their relative market shares change yearly. So, using the data for say the \\(n-1\\)-th year, we know precicely the market share for the \\(n\\)-th year. We can represent this using Markov Chain. First, view the linear equations that dictate what the \\(n\\)-th year market shares will be based on the \\(n-1\\)-th year:\nConsider our system (\\(n = 1, 2, \\ldots\\)): \\(\\begin{align*}\n    &a_n = .5a_{n-1} + .2b_{n-1} + .3c_{n-1} \\\\\n    &b_n = .2a_{n-1} + .6b_{n-1} + .1c_{n-1} \\\\\n    &c_n = .3a_{n-1} + .2b_{n-1} + .6c_{n-1}\n\\end{align*}\\)\nWe can then put these linear equations into a transition matrix:\n\\[\n\\begin{pmatrix}\n    .5 & .2 & .3 \\\\\n    .2 & .6 & .1 \\\\\n    .3 & .2 & .6\n\\end{pmatrix}\n\\]\nSince this matrix is stochastic, if we initalize it with a distribution vector, then we will have a Markov chain. Lets see what the market shares will be for the next three years if we start off with all three companies having equal market share (\\(a_{0} = b_{0} = c_{0} = 1/3\\)). See the output of the code cell below for the distribution vector that contains the market distribution for each company after each year (first compnent represents share of company A, second component represents that of B, and the last component represents that of C).\n\nimport sympy as sym\n\n#create our transition matrix\nP = sym.Matrix([[.5, .2 , .3 ],\n    [.2 , .6 , .1],\n    [.3 , .2 , .6]])\n\n#inital vector\nx = sym.Matrix([[1/3], [1/3], [1/3]])\n\n#run it for each year and print result\nfor i in range(4):\n    print(f\"\\n Year {i}:\")\n    print((P**i)*x)\n\n\n Year 0:\nMatrix([[0.333333333333333], [0.333333333333333], [0.333333333333333]])\n\n Year 1:\nMatrix([[0.333333333333333], [0.300000000000000], [0.366666666666667]])\n\n Year 2:\nMatrix([[0.336666666666667], [0.283333333333333], [0.380000000000000]])\n\n Year 3:\nMatrix([[0.339000000000000], [0.275333333333333], [0.385666666666667]])\n\n\nNow, assume that you and I are employees of Company A. We are considering three different advertising campaigns to implement, and want run some tests to see which would be better. Creating a transition matrix in the identical fashion as above, each of the campaigns have the following transition matrices:\nCampaign #1 has the following transition matrix:\n\\[\n\\begin{pmatrix}\n    .5 & .32 & .3 \\\\\n    .2 & .48 & .1 \\\\\n    .3 & .2 & .6\n\\end{pmatrix}\n\\]\nCampaign #2 has the following transition matrix:\n\\[\n\\begin{pmatrix}\n    .5 & .2 & .42 \\\\\n    .2 & .6 & .1 \\\\\n    .3 & .2 & .48\n\\end{pmatrix}\n\\]\nThe first way we will test how “good” each campaign is is just by looking at the transition matrix after \\(k\\)-many years.\nRecall that \\(\\mathbf{x^{(k)}} = A^k\\cdot\\mathbf{x^{(0)}}\\).\nIf we want company A to have the greatest market share after \\(k\\) many years, then we want the sum of the top row of our matrix to be as large as possible.\nLets look at the transition matrix for Campaign 1, Campaign 2, and no campaign after 5 years:\n\nimport random\n\nC1 = sym.Matrix([[.5, .32 , .3 ],\n    [.2 , .48 , .1],\n    [.3 , .2 , .6]])\n\nC2 = sym.Matrix([[.5, .2 , .42 ],\n    [.2 , .6 , .1],\n    [.3 , .2 , .48]])\n\n\ndef count_top_row(T):\n    return sum(T[0, :])\n\nprint(\"campaign 1\", count_top_row(C1**5))\nprint(\"campaign 2\", count_top_row(C2**5))\nprint(\"no campaign\", count_top_row(P**5))\n\ncampaign 1 1.14189225920000\ncampaign 2 1.16869517120000\nno campaign 1.02269000000000\n\n\nSince the sum of the coefficents of the top row of a respective transition matrix is the highest in campaign 2, it seems that, on average, campaign 2 will yield the highest market share, at least after 5 years.\nLets run a different type of test to measure how “good” each campaign is to see if we can corroborate this finding.\nFor each test, we will generate 2000 random inital distribution vectors, and feed it into each of our transition matrices for a given amount of years. We will do this for 2 and 5 years, gauging both short and long term interests. We will assign each campaign/no campaign a score out of 2000: the transition matrix that yields the highest market share for a given inital state will win a ‘point’.\n\nimport random\n\nC1 = sym.Matrix([[.5, .32 , .3 ],\n    [.2 , .48 , .1],\n    [.3 , .2 , .6]])\n\nC2 = sym.Matrix([[.5, .2 , .42 ],\n    [.2 , .6 , .1],\n    [.3 , .2 , .48]])\n\ndef generate_input_vector():\n    random1 = random.randint(1, 1000)\n    random2 = random.randint(1, 1000)\n    random3 = random.randint(1, 1000)\n    total = random1 + random2 + random3\n    return sym.Matrix([[random1/total], [random2/total], [random3/total]])\n\ndef test_system(years):\n    P_score = 0\n    C1_score = 0\n    C2_score = 0\n    for i in range(2000):\n        x = generate_input_vector()\n        new_P = P**years\n        new_C1 = C1**years\n        new_C2 = C2**years\n        P_value = (new_P*x)[0]\n        C1_value = (new_C1*x)[0]\n        C2_value = (new_C2*x)[0]\n    \n        if P_value &gt;= max(C1_value, C2_value):\n            P_score += 1\n        if C1_value &gt;= max(P_value, C2_value):\n            C1_score += 1\n        if C2_value &gt;= max(C1_value, P_value):\n            C2_score += 1\n    return (f\"run time of {years} years, P: {P_score}\", f\"C1: {C1_score}\", f\"C2: {C2_score}\")\n\nprint(test_system(2))\nprint(test_system(5))\n\n('run time of 2 years, P: 0', 'C1: 834', 'C2: 1166')\n('run time of 5 years, P: 0', 'C1: 0', 'C2: 2000')\n\n\nBased on these tests, I would reccomend the second campaign. It is clear that the second campaign performs the best in the long term. After just 5 years, it will yield conclusively higher market share than doing nothing or doing campaign 1. This is corroborated by theoritical evidence (observing the transition matrix), and experimental evidence, based on our randomized trials. Even after only 2 years, campaign 2 performs better on average than campaign one, with over half of the inital states resulting with a higher market share after campaign 2. So, when considering both short and long term interests, campaign 2 is the clear winner.\nSome limitations of this study may be that it only looks yearly benchmarks. If the company wants some thing in a shorter time frame, such as the next quarter, then these models would not be useful. In addition, this model assumes that all other conditions are held constant, and that the only factor affecting market share are the campaigns, or other controlled factors used when getting the data to construct the transition matrices. However, it is unlikely that there would be no counter-ad campaign from another company that may change the real transition matrix. In other words, this study does not anticipate other changing factors in the market that may change the transition matrix and thus the results.\n\nNicely done! You’ve addressed the issue with the random vectors, you’ve made it much more clear what you’ve done, and I like the thinking about the meaning of the transition matrix. (Of course, now that we have the tools of the eivenvalue problem, you might choose a different approach – like looking at the market share for company A in the dominant eigenvector!)\nGrade: E"
  },
  {
    "objectID": "submitted/projects_1_revised/buyalosbobby_project1-1.html#sports-ranking",
    "href": "submitted/projects_1_revised/buyalosbobby_project1-1.html#sports-ranking",
    "title": "Project #1 Revisions - Bobby Buyalos",
    "section": "Sports Ranking:",
    "text": "Sports Ranking:\nWe have a leage of 7 teams who have played 21 games. Now, based off of the play data of these 21 games, we want to create a ranking of these teams. We will explore multiple methods to do this.\nOur first naïve measure of rankings is through the win/loss score of each time. We will calculate this as # wins - # losses (see later below for how to calculate this cleverly):\n\n\n\nTeam\nwin/loss score\n\n\n\n\n7\n4\n\n\n5\n2\n\n\n3\n1\n\n\n6\n0\n\n\n1\n-1\n\n\n2\n-2\n\n\n4\n-4\n\n\n\nNow, we can put their play data into a graph like structure. We can represent each game played as a tuple \\((j, k)\\), that represents a game where team \\(j\\) beat team \\(k\\). This is what the graph looks like:\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nE = [(1, 2), (7, 3), (2, 4), (4, 5), (3, 2), (5, 1), (6, 1), (7, 2), (2, 6), (3, 4), (7, 4), (5, 7), (6, 4), (3, 5), (5, 6), (7, 1), (5, 2), (7, 6), (1, 4), (6, 3)]\nM = [4, 8, 7, 3, 7, 23, 15, 6, 18, 13, 14, 7, 13, 7, 18, 45, 10, 19, 14, 13]\nG = nx.DiGraph()\n\n# Add nodes\nG.add_nodes_from([1, 2, 3, 4, 5, 6, 7])\n\n# Add vertices\nG.add_edges_from(E)\n\n# Draw the graph\nnx.draw_circular(G, with_labels=True)\nplt.show()\n\n\n\n\n\n\n\n\nNow that we have encoded this data into a graph structure, we can create an adjacency matrix from it that represents the graph. The jk-th entry of this matrix counts the number of times that team j beat team k. The matrix is as follows:\n\\[\n\\begin{pmatrix}\n0 & 1 & 0 &  1 & 0 & 0 & 0\\\\\n0 &0 &0 &1& 0 &1 &0\\\\\n0 &1 &0 &1 &1& 0& 0\\\\\n0 &0& 0 &0 &1 &0& 0\\\\\n1 &1& 0& 0& 0 &1 &1\\\\\n1& 0 &1 &1& 0& 0& 0\\\\\n1& 1& 1& 1& 0& 1& 0\n\\end{pmatrix}\n\\]\nSome things we can observe from here: 1) no team played themselves (i-i-th entry is always 0) 2) no team won against another team more than once\nThis matrix provides a clever method to calculate the win-loss ratio: for team j, sum row j and then subtract the sum of column j. This works becase the sum of row j counts the number of wins of team j, and the sum of column j counts the number of losses of team j (recall that spot ij represents an indicator of a win-loss of team i over team j).\n\nNice!\n\nNow that we have created an adjacency matrix, we can now calculate the vertex power for each team. The vertex power is defined formally as “number of directed walks of length 1 or 2 originating at the vertex” (Shores 95). Now, lets decode what this means in our context. A directed walk of length one from vertex j represents a win of team j over some team, say team k. A directe walk of length two, say from j to k to l, then we know that team j beat team k who beat team l. We care about this because if team k beat team l, then it means that team k is somewhat formidable, so thus it means more if team j was able to beat them out! Just any old win means less, because mean their opponent was not very good. But beating someoone who beat someone else indicates that team j is not to be messed with! So, when calculating vertex powers, we are taking into account the ‘quality’ of the wins and summing not only wins, but wins of people who won.\nTo calculate the vertex power for team j, we will sum the j-th row of \\(A^2 + A\\), since the row of \\(A\\) counts the numbers of walks length 1 from j and \\(A^2\\) counts the number of walks length 2 from \\(j\\).\nThis next code cell will contain the work used to express the ideas in the preceding text boxes, and then will compute the vertex powers.\n\nimport scipy\n\n#convert to numpy\nadj_matrix = nx.adjacency_matrix(G)\nadj_matrix = adj_matrix.toarray()\nprint(adj_matrix)\n\n#form transpose to make summing columns easy\ntranspose = adj_matrix.transpose()\n\n#calculate win-loss score\nfor i in range (7):\n    print(f\"Team {i + 1}'s win-loss score: {sum(adj_matrix[i] - transpose[i])}\")\n\nprint(\"\\n\")\n\n#Calculate powers of all verticies:\npower_matrix = adj_matrix + adj_matrix.dot(adj_matrix)\nfor i in range (7):\n    print(f\"Team {i + 1}'s power: {sum(power_matrix[i])}\")\n\n[[0 1 0 1 0 0 0]\n [0 0 0 1 0 1 0]\n [0 1 0 1 1 0 0]\n [0 0 0 0 1 0 0]\n [1 1 0 0 0 1 1]\n [1 0 1 1 0 0 0]\n [1 1 1 1 0 1 0]]\nTeam 1's win-loss score: -1\nTeam 2's win-loss score: -2\nTeam 3's win-loss score: 1\nTeam 4's win-loss score: -4\nTeam 5's win-loss score: 2\nTeam 6's win-loss score: 0\nTeam 7's win-loss score: 4\n\n\nTeam 1's power: 5\nTeam 2's power: 6\nTeam 3's power: 10\nTeam 4's power: 5\nTeam 5's power: 16\nTeam 6's power: 9\nTeam 7's power: 16\n\n\nHere is our vertex power rankings:\n\n\n\nTeam\nPower\n\n\n\n\n5\n16\n\n\n7\n16\n\n\n3\n10\n\n\n6\n9\n\n\n2\n6\n\n\n1\n5\n\n\n4\n5\n\n\n\nNotice that this is somewhat close to our win-los ratio. The one noticable difference between these two rankings is the fact that 5 and 7 are tied in the vertex power rankings, but team 7 has double the win/loss score than team 5.\nOne reason for this is that team 5 has beaten team 7! So, our power ranking for team 5 also counts the runs of length two that stop over at team 7, of which there are four! In other words, while team 7 has beaten a large quantity of teams (high win-loss score), many of these teams have been relatively weak (team 1 and 2 for example), while team 5 has less wins total (lower win-loss score), but has good quality wins, such as their win against team 7. This ‘quality’ of win is therefore reflected in the power rankings!\n\nExcellent, this is exactly the kind of thinking I was looking for!\n\nNow we will do a reverse page rank methodology. This method is useful here because it is a way to measure the importance of a page based on how many outgoing links it has. Since an outgoing link in this context is a win, then this method is useful! To do this method, we will take the transpose of our adjacency matrix and apply the PageRank method.\nThe next code block does the following: 1. Creates a diagonal matrix D where the diag. entry of row i is the inverse of the sum of row i of A^T 2. forms our transition matrix by A*D (A is our adjacency matrix) 3. Finds our stable distribution vector by using equation 2.5 from Shores (page 131)\nThis will find our stationary distribution vector x that satisfies equation 2.4 from Shores (page 131). We will use this distribution vector as our rankings for each respective team / vector component.\n\nfrom sympy import eye\n## Create Transition matrix for our reverse graph (A^T) ##\n\n#Create Diag. Matrix D used to create transition matrix\ndiag_vals = []\nfor i in range(7):\n    total = np.sum(transpose[i])\n    if total != 0:\n        diag_vals.append(1/total)\n    else:\n        diag_vals.append(0)\nD = np.diag(diag_vals)\n\n#form our transition matrix\ntransition = adj_matrix.dot(D)\n\n#create function to solve our system\ndef find_stable(alpha, v, P, Id):\n    to_invert = Id - alpha*P\n    inverse = to_invert.inv()\n    stable = inverse*((1-alpha)*v)\n    return stable\n\n#inital distribution vector\nv = sym.Matrix([[1/7], [1/7], [1/7], [1/7], [1/7], [1/7], [1/7]])\n\nfind_stable(0.85, v, transition, eye(7))\n\n\\(\\displaystyle \\left[\\begin{matrix}0.0601644551488991\\\\0.0802002135998523\\\\0.166343756251851\\\\0.127607872531524\\\\0.249833649654006\\\\0.130864601791489\\\\0.184985451022379\\end{matrix}\\right]\\)\n\n\nSince the i-th row of the output distribution vector represents team i, then we can use the output to form the following ranking. Note that the ‘value’ represents the zero-sum value that each team holds. There is only a total of 1 value/influence to give out (our output is a distribution vector!), and we can rank each team based on how much influence they hoard:\n\n\n\nIndex\nValue\n\n\n\n\n5\n0.250\n\n\n7\n0.185\n\n\n3\n0.166\n\n\n6\n0.131\n\n\n4\n0.128\n\n\n2\n0.080\n\n\n1\n0.060\n\n\n\nWe can interrept these values as the amount of ‘influence’ each team has based on their outgoing links. Observe that these rankings are very close to the power ranks. This is likely because both are measuring the influence of outgoing links (wins), one in the form of runs of length one and two, and one in terms of PageRank.\nThe major difference between the power rankings and the reverse PageRank rankings is team 4, who jumps from last place to third to last. This is likely because team 4 has beaten team 5! So, in the reverse graph, 1/2 of team 5’s influence is going to team 4! Since team 5 is such a high performer, then by extension, team 4 will receive a high number of influence. However, in the power ranking, there is only one path out of vertex 4, so there will be a relatively lower power ranking.\nNow, suppose that we have additional information: the margin by which a given team won. Using this information, we can create a weighted graph, where each directed edge from i to j represents the margin by which i won over j. We can visualize this graph below:\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport scipy\nimport numpy as np\n\nE = [(1, 2), (7, 3), (2, 4), (4, 5), (3, 2), (5, 1), (6, 1), (7, 2), (2, 6), (3, 4), (7, 4), (5, 7), (6, 4), (3, 5), (5, 6), (7, 1), (5, 2), (7, 6), (1, 4), (6, 3)]\n\nM = [4, 8, 7, 3, 7, 23, 15, 6, 18, 13, 14, 7, 13, 7, 18, 45, 10, 19, 14, 13]\n\nF = nx.DiGraph()\nnodes = range(1, 8)\nF.add_nodes_from(nodes)\n\nfor i, tuple in enumerate(E):\n    a, b = tuple\n    F.add_edge(a, b, weight = M[i])\n\npos = nx.circular_layout(F)\nnx.draw(F, pos, with_labels=True, node_size=1000, font_size=12, arrows=True)\n\nlabels = nx.get_edge_attributes(F, 'weight')\nnx.draw_networkx_edge_labels(F, pos, edge_labels=labels)\n\nplt.show()\n\n\n\n\n\n\n\n\nNow, lets create an adjacency matrix for this weighted graph, where instead of an indicator of 0 and 1, we can use 0 to represent a lack of a edge, and an integer to represent that there is an edge and what the weight of that edge is (this works since none of our weights are 0).\nIn the same way as before, we can use this adjacency matrix to construct a power matrix. Like before, we care about runs of length two, because a run from j to i to k means that j beat team i, who in turn beat k, therefore giving more credence to j for beating i.\nBut, now that we have weights, this power matrix gives us even more information. We are not only counting how many runs of length 1 and 2 each team has, the margins of those wins. So, if team j beat team i by 5 points, and team i beat team k by 8 points, then team j will get all 13 points. This works because if team i beat team k by 8 points, and team j beat team i, then it is likely that team j would also beat team k by at least 8 points! By considering these weighted power rankings, we can more information how the ‘scale’ of each win and are able to account for that, giving more credit to big wins versus games won by a hair.\n\n#calculate weighted average:\nweighted_adj = nx.adjacency_matrix(F)\nweighted_adj = weighted_adj.toarray()\nprint(weighted_adj)\n\n#power matrix for weighted graphj\npower_weighted_matrix = weighted_adj + weighted_adj.dot(weighted_adj)\nfor i in range (7):\n    print(f\"Team {i + 1}'s power: {sum(power_weighted_matrix[i])}\")\n\n[[ 0  4  0 14  0  0  0]\n [ 0  0  0  7  0 18  0]\n [ 0  7  0 13  7  0  0]\n [ 0  0  0  0  3  0  0]\n [23 10  0  0  0 18  7]\n [15  0 13 13  0  0  0]\n [45  6  8 14  0 19  0]]\nTeam 1's power: 160\nTeam 2's power: 784\nTeam 3's power: 647\nTeam 4's power: 177\nTeam 5's power: 2104\nTeam 6's power: 701\nTeam 7's power: 2089\n\n\nWe can see the power ranking of the weighted matrix:\n\n\n\nTeam\nPower\n\n\n\n\nTeam 5\n2104\n\n\nTeam 7\n2089\n\n\nTeam 2\n784\n\n\nTeam 6\n701\n\n\nTeam 3\n647\n\n\nTeam 4\n177\n\n\nTeam 1\n160\n\n\n\nWe can observe that this rankings is largely the same as all the previous rankings, except for the fact that team 2 is in third place, whereas it is near the bottom in all the other rankings. Upon inspection, we see that team 2 beat team 6 by a margin of 18 points! Observe that when we are calculating the ‘score’ assigned to a run of length two, we are actually multiplying the margins of both games. So, for all 3 runs of length two that pass through team 6, we are multiplying by the large number 18. Since the second leg of these three runs also have large margins, their product is quite large, giving team 4 a large weighted power ranking here. In fact, I believe that this is a weakness of this ranking:\nsay we have three teams A and B and C, and A beat B by 10 points and B beat C by 5 points. Then, the run from A to C will collect 50 points. But, if team B beat team C by 6 points instead of 5, then that run will collect 60 points! However, I would argue that these two cases should be weighted almost the same, and surely doesn’t merit increasing the value of the run by 20%! Therefore, I would rely more on the first power ranking and the inverse PageRank rankings.\n\nAwesome work.\nMy son recently watched the movie [Next Goal Wins](https://en.wikipedia.org/wiki/Next_Goal_Wins_(2023_film)), about the soccer team from American Samoa who played Australia in a World Cup qualifying match in 2001. Australia won 31-0. By the PageRank metric, any team that ever beat Australia would just zoom to the top of any ranking!\nGrade: E"
  },
  {
    "objectID": "submitted/projects_1_revised/buyalosbobby_project1-1.html#plu-factorization",
    "href": "submitted/projects_1_revised/buyalosbobby_project1-1.html#plu-factorization",
    "title": "Project #1 Revisions - Bobby Buyalos",
    "section": "PLU Factorization",
    "text": "PLU Factorization\nWe will first create the PLU_factorization algo (assuming non-zero diag entries).\nThe pseudo code that summerizes the following code block is as follows: 1. deepcopy A so we do not change our input as we modify it 2. index our rows in an index list (index_list) and call upon row i of matrix P as P[index_list[i]][j] rather than P[i][j] 3. For each column zero out everything below the diagonal and keep track of the multipliers for the places that are zeroed out 4. use our stored multiplier data to form a lower triangular matrix 5. use our index_list data to form permulation matrix and upper triangular matrix\n\nimport copy\ndef PLU_factorization(A):\n    \"\"\"\n    PLU factorization algo. Assuming non-zero diag. entires.\n\n    Inputs:\n    P [numpy array] - input square matrix\n\n    Outputs tuple[numpy array, numpy array, numpy array] - will reutrn P, L \n    and U matrices in that order\n    \"\"\"\n    P = copy.deepcopy(A)\n    numrows = len(P)\n    numcols = len(P[0])\n    index_list = [i for i in range(numrows)]\n    multipliers = {}\n\n    for k in range(numcols - 1):\n        if int(P[index_list[k]][k]) == 0:\n            for i in range(index_list[k], numrows):\n                if P[i][k] != 0:\n                    i_index = index_list.index(i)\n                    k_index = index_list.index(k)\n                    index_list[k_index], index_list[i_index] = index_list[i_index], index_list[k_index]\n                    break\n        #run again in case we dont find a nonzero value\n        if int(P[index_list[k]][k]) != 0:\n            for i in range(k + 1, numrows):\n                m = index_list.index(i)\n                multiplier = - P[m][k] / P[index_list[k]][k]\n                multipliers[(m, k)] = multiplier\n                P[m] = P[m] + multiplier*(P[index_list[k]])\n    \n    #Create Lower Triangular Matrix\n    zero_matrix = np.eye((numrows))\n    for tuple in multipliers:\n        x, y = tuple\n        zero_matrix[index_list[x]][y] = - multipliers[tuple]\n    \n    #Create Permutation Matrix and Upper Triangular Matrix:\n    perm_matrix = []\n    U = []\n    for i in index_list:\n        y = [0]*numrows\n        y[i] = 1\n        perm_matrix.append(y)\n        U.append(list(P[i]))\n    return (np.array(perm_matrix), zero_matrix, np.array(U))\n\nTo show that these work, we will show the LU factorization problems from HW3.\nLet A be the original matrix. Then, we will decompose each into its (P)LU factorization.\nFor each, we will use SymPy’s upper and lower triangular tester (is_upper, is_lower), and test if (P)LU = A by using numpy’s np.array_equal function. View the output of the following code cell:\n(the first two examples we check LU = A since P is the identity, but we check PA = LU for the third problem)\n\nfrom sympy import Matrix\n\n#Problem 11:\ntest1 = np.array([[2, -1, 1], [2, 3, -2], [4, 2, 2]])\nP, L, U = PLU_factorization(test1)\nprint(\"PROBLEM 11\")\nprint(\"Is L lower triangular:\", Matrix(L).is_lower)\nprint(\"Is U upper triangular:\", Matrix(U).is_upper)\nprint(\"Does LU = A:\", np.array_equal(L.dot(U), test1))\nprint(\"\\n\")\n\n#Problem 12:\ntest2 = np.array([[2, 1, 0], [-4, -1, -1], [2, 3, -3]])\nP, L, U = PLU_factorization(test2)\nprint(\"PROBLEM 12\")\nprint(\"Is L lower triangular:\", Matrix(L).is_lower)\nprint(\"Is U upper triangular:\", Matrix(U).is_upper)\nprint(\"Does LU = A:\", np.array_equal(L.dot(U), test2))\nprint(\"\\n\")\n\n#problem 13\ntest3 = np.array([[2, 1, 3], [-4, -2, -1], [2, 3, -3]])\nP, L, U = PLU_factorization(test3)\nprint(\"PROBLEM 13\")\nprint(\"Is L lower triangular:\", Matrix(L).is_lower)\nprint(\"Is U upper triangular:\", Matrix(U).is_upper)\nprint(\"Does LU = PA:\", np.array_equal(L.dot(U), P.dot(test3)))\n\nPROBLEM 11\nIs L lower triangular: True\nIs U upper triangular: True\nDoes LU = A: True\n\n\nPROBLEM 12\nIs L lower triangular: True\nIs U upper triangular: True\nDoes LU = A: True\n\n\nPROBLEM 13\nIs L lower triangular: True\nIs U upper triangular: True\nDoes LU = PA: True\n\n\nNow we will code the PLU solver algo. Given A and b, we want to find vector x such that Ax = b.\nThe pseudo code that summerizes the following code block is the following: 1. decompose A into its P, L, U so we have PA = LU 2. solve y by solving Ly = Pb by forward solving 3. solve x by solving Ux = y by backward solving 4. return x\n\ndef PLU_solver(A, b):\n    \"\"\"\n    Given a square matrix A (non-zero diag entries) and a solution b, solve for the unknowns using\n    PLU factorization (i.e. solve for x in Ax = b)\n    \"\"\"\n    size = len(A)\n    P, L, U = PLU_factorization(A)\n    #find  Ly = Pb\n    Pb = P.dot(b)\n    y = [None]*size\n    y[0] = [(Pb[0]/L[0][0])[0]]\n    for row in range(1, size):\n        sum = 0\n        for col in range(row):\n            sum += (L[row][col])*(y[col][0])\n        y[row] = [((Pb[row] - sum) / L[row][row])[0]]\n    y = np.array(y)\n\n    #solve Ux = y\n    x = [None]*size\n    n = size - 1\n    x[n] = [(y[n] / U[n][n])[0]]\n    row = n - 1\n    while row &gt;=0:\n        sum = 0\n        for col in range(row + 1, size):\n            sum += U[row][col]*(x[col][0])\n        x[row] = [((y[row] - sum) / U[row][row])[0]]\n        row -= 1\n    x = np.array(x)\n    return x\n\nNow we will test this solver by comparing the x obtained through a np solver, and our solver.\n\n#PLU solver test1\nA = np.array([[2, 1, 0], [-2, 0, -1], [2, 3, -3]])\nb = np.array([[1], [0], [1]])\nx = PLU_solver(A, b)\nnps_x = np.linalg.solve(A, b)\n\nprint(\"Numpy solver gives:\", nps_x)\nprint(\"Our solver gives:\", x)\nprint(\"Are these the same:\", np.array_equal(x, nps_x))\n\nNumpy solver gives: [[-1.]\n [ 3.]\n [ 2.]]\nOur solver gives: [[-1.]\n [ 3.]\n [ 2.]]\nAre these the same: True\n\n\n\n#PLU solver test2\nA = np.array([[2, 1, 3], [-4, -2, -1], [2, 3, -3]])\nb = np.array([[4], [3], [5]])\nx = PLU_solver(A, b)\nnps_x = np.linalg.solve(A, b)\n\nprint(\"Numpy solver gives:\", nps_x)\nprint(\"Our solver gives:\", x)\nprint(\"Are these the same:\", np.array_equal(x, nps_x))\n\nNumpy solver gives: [[-4.85]\n [ 7.1 ]\n [ 2.2 ]]\nOur solver gives: [[-4.85]\n [ 7.1 ]\n [ 2.2 ]]\nAre these the same: True\n\n\nNow we will create the inverse solver algo that uses the PLU solver. Here is the pseudo-code that summerizes the following code block: 1. let \\(e_i\\) be a standard basis vector (all zeros and a 1 in the i-th component). Solve for \\(x_i\\) that satisfies \\(Ax_i = e_i\\) 2. Arrange these \\(x_i\\)’s as columns in a matrix and return it\n\n#inverse Solver:\ndef find_inverse(A):\n    size = len(A)\n    output = []\n    for i in range(size):\n        ei = np.zeros((size, 1))\n        ei[i] = [1]\n        xi = PLU_solver(A, ei)\n        output.append(xi)\n    return np.hstack(output)\n\nNow we will show two tests that see that this works. We will have numpy compute the inverse, and then have the solver compute the inverse, and then use the numpy built in function array_equal to test if these two matrices are the same! We find that they both do indeed find the same matrices.\nInterestingly, we can note that for the second test, our two matrices differ by some trivial and small numerical value! If we use np.allclose function, a built-in numpy function that checks if two matrices are approximately equal within a certain tolerance, we get that our two matrices are the same. But, if we use array_equal, which checks for exact equality then we get false. This is due to some small numerical / roundoff error. See the code cell below where we will run both types of equality checkers, and print both matrices to see that they are essentially the same.\nAnd finally, for clarity of mind, we will check that \\(AA^{-1}\\) equals the identity for our inverse.\n\n#test1\nA = np.array([[2, 1, 0], [-2, 0, -1], [2, 3, -3]])\ninvA = find_inverse(A)\nnum_inv = np.linalg.inv(A)\nprint(\"Test #1: Numpy and PLU solver find the same inverse:\", np.array_equal(invA, num_inv))\nprint(\"Test #1: A and its inverse multiply to identity:\", np.array_equal(invA.dot(A), np.eye(3)))\n\nprint(\"\\n\")\n\n#test2\nA = np.array([[1, 2, 3], [0, 1, 4], [5, 6, 0]])\nnum_inv = np.linalg.inv(A)\ninvA = find_inverse(A)\nprint(\"Test #2: Numpy and PLU solver are find exactly the same inverse:\", np.array_equal(invA, num_inv))\nprint(\"Test #2: Numpy and PLU solver are find reasonably close inverses:\", np.allclose(invA, num_inv))\nprint(\"Test #2: A and its inverse multiply to identity:\", np.array_equal(invA.dot(A), np.eye(3)))\nprint(\"My PLU solver returns: \\n\", invA)\nprint(\"Numpy solver returns: \\n\", num_inv)\n\nTest #1: Numpy and PLU solver find the same inverse: True\nTest #1: A and its inverse multiply to identity: True\n\n\nTest #2: Numpy and PLU solver are find exactly the same inverse: False\nTest #2: Numpy and PLU solver are find reasonably close inverses: True\nTest #2: A and its inverse multiply to identity: True\nMy PLU solver returns: \n [[-24.  18.   5.]\n [ 20. -15.  -4.]\n [ -5.   4.   1.]]\nNumpy solver returns: \n [[-24.  18.   5.]\n [ 20. -15.  -4.]\n [ -5.   4.   1.]]\n\n\nFinally, we will compare the run time of our functions compared to their numpy equivalents. We will do this for first the solver, and then the inverse calculator!\nWhile these will yield different results based on the computer, we can see in general that our PLU solver algorithm is not too much worse that numpy’s, and occasionally better! But, the inverse solver is much less efficent than numpy’s. However, as I keep rerunning these results, I tend to get drastically different results from time to time, and also wonder if the results will look much different on a different computer. If you want to play around with this, keep rerunning the below code block and have fun!\n\nimport time\n#Test 1:\nA = np.array([[2, 1, 0], [-2, 0, -1], [2, 3, -3]])\nb = np.array([[1], [0], [1]])\n\nstart_time = time.time()\nPLU_solver(A, b)\nelapsed_time = time.time() - start_time\nprint(f\"Test #1: PLU solver runs at {elapsed_time} seconds\")\n\nstart_time = time.time()\nnp.linalg.solve(A, b)\nelapsed_time = time.time() - start_time\nprint(f\"Test#1: numpy solver takes {elapsed_time} seconds\")\n\nprint(\"\\n\")\n\n#Test 2:\nA = np.array([[2, 1, 3], [-4, -2, -1], [2, 3, -3]])\nb = np.array([[4], [3], [5]])\n\nstart_time = time.time()\nPLU_solver(A, b)\nelapsed_time = time.time() - start_time\nprint(f\"Test #2: PLU solver runs at {elapsed_time} seconds\")\n\nstart_time = time.time()\nnp.linalg.solve(A, b)\nelapsed_time = time.time() - start_time\nprint(f\"Test#2: numpy solver takes {elapsed_time} seconds\")\n\nprint(\"\\n\")\n\n#test 3\nA = np.array([[2, 1, 0], [-2, 0, -1], [2, 3, -3]])\n\nstart_time = time.time()\ninvA = find_inverse(A)\nelapsed_time = time.time() - start_time\nprint(f\"Test #3: PLU inverse finder runs at {elapsed_time} seconds\")\n\nstart_time = time.time()\nnum_inv = np.linalg.inv(A)\nelapsed_time = time.time() - start_time\nprint(f\"Test#3: numpy inverse solver takes {elapsed_time} seconds\")\n\nprint(\"\\n\")\n\n#test 4\nA = np.array([[1, 2, 3], [0, 1, 4], [5, 6, 0]])\n\nstart_time = time.time()\ninvA = find_inverse(A)\nelapsed_time = time.time() - start_time\nprint(f\"Test #4: PLU inverse finder runs at {elapsed_time} seconds\")\n\nstart_time = time.time()\nnum_inv = np.linalg.inv(A)\nelapsed_time = time.time() - start_time\nprint(f\"Test#4: numpy inverse solver takes {elapsed_time} seconds\")\n\nTest #1: PLU solver runs at 0.0002391338348388672 seconds\nTest#1: numpy solver takes 0.00010013580322265625 seconds\n\n\nTest #2: PLU solver runs at 0.00033736228942871094 seconds\nTest#2: numpy solver takes 8.797645568847656e-05 seconds\n\n\nTest #3: PLU inverse finder runs at 0.00060272216796875 seconds\nTest#3: numpy inverse solver takes 9.584426879882812e-05 seconds\n\n\nTest #4: PLU inverse finder runs at 0.00041484832763671875 seconds\nTest#4: numpy inverse solver takes 0.00011873245239257812 seconds\n\n\n\nInteresting, when I run your code on my Macbook Air M2, I am getting very consistent times for the PLU solver. (And the Numpy solver always beats it by about a factor of 10…)\nYou’ve done a great job coding this up, your tests are good, and your explanations are clear. Much improved!\nGrade: E"
  },
  {
    "objectID": "submitted/projects_1_revised/caihanyan_173267_11271607_First_Projects_Revised.html",
    "href": "submitted/projects_1_revised/caihanyan_173267_11271607_First_Projects_Revised.html",
    "title": "Project #1 Revisions - Hanyan Cai",
    "section": "",
    "text": "import networkx as nx\nimport matplotlib.pyplot as plt\nimport scipy\nimport numpy as np\nfrom scipy.stats import norm\n\n#Markov Chain\nThe transition matrix under this context will be\n\\[\n\\begin{bmatrix}\n0.5 & 0.2 & 0.3\\\\\n0.2 & 0.6 & 0.1\\\\\n0.3 & 0.2 & 0.6\\\\\n\\end{bmatrix}\n\\] .\nNow, let us put this transition matrix into code. We will set the initial state of the system to be \\[\n\\begin{bmatrix}\n\\frac{1}{3}\\\\\n\\frac{1}{3}\\\\\n\\frac{1}{3}\n\\end{bmatrix}\n\\]\nto denote all three companies having equal shares of the market.\n\n# Code in this transition matrix and the initial state (Equal shares of customers)\nmarkovTransition = np.array([[0.5, 0.2, 0.3], [0.2, 0.6, 0.1], [0.3, 0.2, 0.6]])\ninitialState = np.array([[1/3], [1/3], [1/3]])\niteration = initialState\n\n# Test three iterations\nfor i in range(3):\n  iteration = markovTransition.dot(iteration)\n  print(f\"{i + 1} Year Later: \\n Percent of customer shares for A: {np.round(iteration[0, 0] * 100, decimals=2)}% \\n Percent of customer shares for B: {np.round(iteration[1, 0] * 100, decimals=2)}% \\n Percent of customer shares for C: {np.round(iteration[2, 0] * 100, decimals=2)}%\")\n\n1 Year Later: \n Percent of customer shares for A: 33.33% \n Percent of customer shares for B: 30.0% \n Percent of customer shares for C: 36.67%\n2 Year Later: \n Percent of customer shares for A: 33.67% \n Percent of customer shares for B: 28.33% \n Percent of customer shares for C: 38.0%\n3 Year Later: \n Percent of customer shares for A: 33.9% \n Percent of customer shares for B: 27.53% \n Percent of customer shares for C: 38.57%\n\n\nWe can see from the results that B begins to lose shares of the market, C gains a considerable amount of shares (around 2%), while A gains small amount of shares (around 0.6%) in three years.\nNow, let us test the effects of the two advertising campaigns.\nAdvertising Campaign 1 convinces 20% of customers who would have to stay in B to switch to A. This can be represented in the modified transition matrix: \\[\n\\begin{bmatrix}\n0.5 & 0.32 & 0.3\\\\\n0.2 & 0.48 & 0.1\\\\\n0.3 & 0.2 & 0.6\\\\\n\\end{bmatrix}\n\\]\nAdvertising Campaign 2 convinces 20% of customers who would have to stay in C to switch to A. This can be represented in the modified transition matrix:\n\\[\n\\begin{bmatrix}\n0.5 & 0.2 & 0.42\\\\\n0.2 & 0.6 & 0.1\\\\\n0.3 & 0.2 & 0.48\\\\\n\\end{bmatrix}\n\\]\nTo view the effectiveness of the campaigns, I will iterate these markov chains over times and create a Gaussian Fit.\n\n# Code in the advertising campaign transition matrices.\ntransition1 = np.array([[0.5, 0.32, 0.3], [0.2, 0.48, 0.1], [0.3, 0.2, 0.6]])\ntransition2 = np.array([[0.5, 0.2, 0.42], [0.2, 0.6, 0.1], [0.3, 0.2, 0.48]])\n\n# Let us iterate over random distribution vectors for the two transition matrices\n# to view the resulting market shares after 3 years, 50 years, and 100 years.\nnp.random.seed(10)\nruns = 10000\n\n# I am defining a function that takes in the two transition matrices and the\n# number of years to iterate and outputs Gaussian fits for the shares for A\n\ndef advertisingGaussian(runs, years, transitionMatrix1, transitionMatrix2):\n  resultForAFrom1 = []\n  resultForAFrom2 = []\n  for run in range(runs):\n    # We do experiment a total of variable \"runs\" times.\n    # Create random initial distribution vector for the intial market share.\n    mold = np.random.random((3,))\n    initialState = mold / mold.sum()\n\n    state = initialState\n    # Now we do the actual iteration for the two matrices and append the results.\n    for year in range(years):\n      state = transition1.dot(state)\n    resultForAFrom1.append(state[0])\n\n    state = initialState\n    for year in range(years):\n      state = transition2.dot(state)\n    resultForAFrom2.append(state[0])\n\n  # Now we can fit the two sets of data for the market share of A\n  fig, axs = plt.subplots(2, 1, figsize=(6, 6))\n\n  # Determine the random color of the histograms\n  randomColorNumber = np.random.rand(3,)\n\n\n  # Fitting the 1 results\n  mu1, sigma1 = norm.fit(resultForAFrom1)\n\n  # Plotting the advertising campaign 1 results\n  axs[0].hist(resultForAFrom1, bins=100, density=True, alpha=0.7, color=randomColorNumber)\n  axs[0].set_title(f'The Distribution of Market Shares of A with Advertising Campaign 1 after {years} years, iterated for {runs} times.\\n(Estimated market share value of A for Campaign 1: {np.round(mu1, decimals=4)})', pad=20)\n  axs[0].set_xlabel('Market Share Distribution for A')\n  axs[0].set_ylabel('Density')\n\n  xmin1, xmax1 = axs[0].get_xlim()\n  x1 = np.linspace(xmin1, xmax1, 100)\n  p1 = norm.pdf(x1, mu1, sigma1)\n  axs[0].plot(x1, p1, 'k', linewidth=2, label='Gaussian Fit')\n\n\n  # Fitting the 2 results\n  mu2, sigma2 = norm.fit(resultForAFrom2)\n\n  # Plotting the advertising campaign 2 results\n  axs[1].hist(resultForAFrom2, bins=100, density=True, alpha=0.7, color=randomColorNumber)\n  axs[1].set_title(f'The Distribution of Market Shares of A with Advertising Campaign 2 after {years} years, iterated for {runs} times.\\n(Estimated market share value of A for Campaign 2: {np.round(mu2, decimals=4)})', pad=20)\n  axs[1].set_xlabel('Market Share Distribution for A')\n  axs[1].set_ylabel('Density')\n\n  xmin2, xmax2 = axs[1].get_xlim()\n  x2 = np.linspace(xmin2, xmax2, 100)\n  p2 = norm.pdf(x2, mu2, sigma2)\n  axs[1].plot(x2, p2, 'k', linewidth=2, label='Gaussian Fit')\n\n  # Showing the plot\n  plt.tight_layout()\n  plt.show()\n\n# Now call the function\n\nadvertisingGaussian(runs, 3, transition1, transition2)\nprint(\"\\n\")\nadvertisingGaussian(runs, 10, transition1, transition2)\nprint(\"\\n\")\nadvertisingGaussian(runs, 50, transition1, transition2)\nprint(\"\\n\")\nadvertisingGaussian(runs, 100, transition1, transition2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/Users/kendra/Library/Python/3.8/lib/python/site-packages/numpy/lib/histograms.py:883: RuntimeWarning: divide by zero encountered in divide\n  return n/db/n.sum(), bin_edges\n/Users/kendra/Library/Python/3.8/lib/python/site-packages/numpy/lib/histograms.py:883: RuntimeWarning: invalid value encountered in divide\n  return n/db/n.sum(), bin_edges\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet us analyze these results. Each pair of histogram denotes the distribution of the market shares for A after some years. The estimated market share values after the set amount of years is denoted under the title. This estimated value is the mu parameter for the Gaussian fit, denoting the value with the greatest probabiltiy distribution.\nFor Advertising Campaign 1, we see that for 1000 random initial distributions of market shares will tend with each iteration towards a stationary state of 0.38 for the market share of A. This suggests that Advertising Campaign 1 will allow A to stabilize at 0.3806 marketshares. This seems to be a very good strategy.\nFor Advertising Campaign 2, we can see that the most probable market share distribution for also increases. With long iterations under numerous random intial states, A seems to always achieve the stationary state of 0.39 for its market share.\nEven after three years, markov simulation suggests that Advertising Campaign 2 performs better than Advertising Campaign 1, as the market share of A under Advertising Campaign 2 is bigger than the market share of A under Advertising Campaign 1. The later iterations also suggests the same thing. Therefore, my model suggests that Advertising Campaign 2 is a more effective choice both for long term and short term increase in market share for A.\nIt is important to note the limitations within this model. First, it simplifies the dynamics of the market. By iterating the static transition matrices over long periods of time, we are assuming that the market does not change. In other words, company B and company C are doing nothing to regain their lost customers. After all, it is quite unreasonable to assume that things will always stay the same in 100 years. The effectiveness of the advertising campaign also can change, which is not reflected in the static transition matrices. Therefore, this model may be effect in approximating the effects of the advertising campaigns for a very short period of iteration before competitors B and C have a chance to react or implement their own strategies.\n\nRevision: In addition, by looking at the initial transition matrices:\n\n\\[\n\\begin{bmatrix}\n0.5 & 0.32 & 0.3\\\\\n0.2 & 0.48 & 0.1\\\\\n0.3 & 0.2 & 0.6\\\\\n\\end{bmatrix}\n\\]\n\\[\n\\begin{bmatrix}\n0.5 & 0.2 & 0.42\\\\\n0.2 & 0.6 & 0.1\\\\\n0.3 & 0.2 & 0.48\\\\\n\\end{bmatrix}\n\\]\nWe can see that every iteration, A is obtaining the same percent of customers from B and C, in the first advertisement matrix, C is obtaining more customers than B. This allows C to stay competitive with A, competing for the shares of B, as opposed to the second advertisement matrix, which makes C less competitive, allowing for A to dominate more equally to A and B.\n\nIt is also worth noting the issue with the histogram. It seems like after 50 years, the stationary state is almost reached despite random initial states, resulting in all possible data contributing to one box of the histogram. I do not know exactly how I should fix this though. Perhaps I should stop after 20 years.\n\n\nI agree that the histogram isn’t so useful when all of the data is concentrated in one bin! But that’s not a problem, per se – your data just gets to the stationary state.\n\nNow, perhaps we wish to go a step further and analyze how much of an effect gaining shares from B or C will have on the long term share of A. Then, we can simply construct functions of A market share with respect to the gaining of the market share of B or C.\n\n# The variable parameter will be a number between 0 and 1, denoting up till\n# 100% of the customer that was suppose to remain with B going to A\n# Iterations will control how many years passes.\ndef functionB(parameter, iterations, initialState):\n  change = parameter * 0.4\n  transitionMatrix = np.array([[0.5, 0.2 + change, 0.3], [0.2, 0.6 - change, 0.1], [0.3, 0.2, 0.6]])\n  # We will start with the assumption for equal shares.\n  for year in range(iterations):\n    initialState = transitionMatrix.dot(initialState)\n  return initialState[0]\n\ndef functionC(parameter, iterations, initialState):\n  change = parameter * 0.4\n  transitionMatrix = np.array([[0.5, 0.2, 0.3 + change], [0.2, 0.6, 0.1], [0.3, 0.2, 0.6 - change]])\n  # We will start with the assumption for equal shares\n  for year in range(iterations):\n    initialState = transitionMatrix.dot(initialState)\n  return initialState[0]\n\n# Setting the initial state to be all companies having equal shares\ninitialState = np.ones((3,1)) / 3\n\n# Readying the plot\nxValues = np.linspace(0,1, num=101)\nBValues = []\nCValues = []\n\nfor x in xValues:\n  BValues.append(functionB(x, 50, initialState))\n  CValues.append(functionC(x, 50, initialState))\n\n# Plotting\nplt.title(\"Market Share of A after 50 years with Respect to Change in Parameter x\")\nplt.plot(xValues, BValues, label=\"Parameter under Advertisement 1 (B to A)\")\nplt.plot(xValues, CValues, label=\"Parameter under Advertisement 2 (C to A)\")\nplt.xlabel(\"Parameter (Percent Change / 100)\")\nplt.ylabel(\"Market Share for A\")\nplt.legend(loc=\"upper left\", frameon=True)\n\n\n\n\n\n\n\n\nThe above graph works is generated by the following method. We introduce the parameter variable that ranges from 0 to 1. For each transition matrix representing the advertisement matrix, the parameter looks at the percentage of customers going to A rather than staying in B/C from the effects of the campaign and multiplies it by the parameter. For example, if the parameter is 1, than for Advertisement 1, 100% of the customers that was originally going to stay with B will go to A, resulting in the following transition matrix: \\(\\begin{bmatrix}\n0.5 & 0.8 & 0.3\\\\\n0.2 & 0.0 & 0.1\\\\\n0.3 & 0.2 & 0.6\\\\\n\\end{bmatrix}\\).\nNow, with the modified transition matrix by the parameter, the initial state of all companies having equal shares\n\\[\\begin{bmatrix}\n\\frac{1}{3}\\\\\n\\frac{1}{3}\\\\\n\\frac{1}{3}\n\\end{bmatrix}\\]\nis initialized. We apply the initial state to the modified transition matrix for 50 times. The resulting market share value of A is then recorded and plotted.\nLooking at the graph, if the resources needed to convert customers from C to A is the same as the resources needed to convert customers from B to A, then it is more effective to convert customers from C since the increase in the market share of A is more for C than B as we increase the parameter.\n#Sports Ranking\nWe will first rank the Teams based on their win-loss ratio.\n\nRevision: It is not the win-loss ratio, but simply the difference between the number of wins and losses for each Team.\n\nThis will be the simplest way to rank them. In fact, we can create a digraph with the given edges before doing so. This allows us to create an adjacency matrix. The sum of the \\(i^{th}\\) row of the matrix gives us the number of wins for Team \\(i\\), while the sum of the \\(i^{th}\\) column of the matrix gives us the number of losses for Team \\(i\\). Therefore, to account for both wins and losses of each Team, we can create a ranking score for each Team determined by their number of wins subtracted by their number of losses. Using the adjacency matrix of the graph we created makes our life a lot easier, so I am introducing the graph before the simple ranking.\n\nsportsGraph = nx.DiGraph()\n\nsportsGraph.add_nodes_from([1, 2, 3, 4, 5, 6, 7])\n\nedges = [\n\n    (1, 2), (7, 3), (2, 4), (4, 5), (3, 2), (5, 1), (6, 1), (7, 2), (2, 6),\n\n    (3, 4), (7, 4), (5, 7), (6, 4), (3, 5), (5, 6), (7, 1), (5, 2), (7, 6),\n\n    (1, 4), (6, 3)\n\n]\n\nsportsGraph.add_edges_from(edges)\n\n# Let us plot the graph representing the wins and losses of the 7 Teams.\nnx.draw_circular(sportsGraph, with_labels=True)\nplt.show()\n\n# To calculate the win-losses for the Teams, we can do it by hand, but it is easier to analyze the adjacency matrix of the graph.\nadj_matrix = nx.adjacency_matrix(sportsGraph)\nadj_matrix = adj_matrix.toarray()\n\n# Define the win and loss arrays, where the index+1 denotes the Team number:\nwins = np.zeros(7)\nlosses = np.zeros(7)\n\n# The ith column of the adjacency matrix is the number of losses Team i experienced. The ith row is the number of wins Team i experienced.\nfor i in range(len(wins)):\n  wins[i] = adj_matrix[i,:].sum()\n\nfor i in range(len(losses)):\n  losses[i] = adj_matrix[:, i].sum()\n\n# We define a very simple score that takes into account the number of wins and the number of losses for each Team. The higher the winLossScore, the better the ranking.\nwinLossScore = wins - losses\n\nsimpleSort = np.argsort(winLossScore)[::-1]\nwLSorted = np.sort(winLossScore)[::-1]\n\nprint(\"\\n\")\nprint(\"Simple ranking of the Teams based on win/loss:\")\n\nfor i in range(len(simpleSort)):\n  print(f\"{i+1}: Team {simpleSort[i] + 1} with a win - loss score of {wLSorted[i]}\")\n\n\n\n\n\n\n\n\n\n\nSimple ranking of the Teams based on win/loss:\n1: Team 7 with a win - loss score of 4.0\n2: Team 5 with a win - loss score of 2.0\n3: Team 3 with a win - loss score of 1.0\n4: Team 6 with a win - loss score of 0.0\n5: Team 1 with a win - loss score of -1.0\n6: Team 2 with a win - loss score of -2.0\n7: Team 4 with a win - loss score of -4.0\n\n\nWe can see that the win-loss score gives us fairly neat ranking. Although we didn’t see the same scores on different Teams, we can account for it by developing a system. If two Teams have the same score, then we look at their total wins. The Team with the most wins will be ranked higher. If they have the same wins, they have to have the same losses (since they have the same win-loss score).\n\nRevision: If two Teams have the same win-loss difference and they have the same number of wins, then they must have the same number of losses. For instance, if Team 1 and Team 2 both have a win-loss difference of 2 and they both won 5 games, then they both must have lost 3 games. Then, there is no value comparing the number of losses of Team 1 and Team 2. Then, if we are under the situation where two Teams has the same win-loss difference and the same number of wins, we cannot really distinguish their ranking. Instead, we should look for a better way to rank.\n\nIn this case, we will have to devise more sophisticated methods for ranking. This is what we do next. We will use the vertex power to rank the Teams. Remember, the vertex power is the number of connections of a vertex with length two of less. This allows us to sort of consider the “weight” of the Team wins. If Team 1 wins Team 2, and Team 2 wins Team 3, then we consider Team 1 to be better than Team 3 and count that as a “win” for Team 1 (lucky!)\nThen, we can rank the Teams based on this power concept. The Team with the highest power will be first place.\n\n# Next, we use the vertex power to rank\n\n# Calculate the power matrix\npowerMatrix = adj_matrix + adj_matrix.dot(adj_matrix)\n\n# This will include all values for the powers of each Team i (index)\npowers = np.zeros(7)\n\nfor i in range(len(powers)):\n  powers[i] = powerMatrix[i, :].sum()\n\n# Now, sort the powers\npowerSort = np.argsort(powers)[::-1]\npowersSorted = np.sort(powers)[::-1]\n\n# Printing the sort\nprint(\"Power Ranking of the Teams\")\nfor i in range(len(powerSort)):\n  print(f\"{i+1}: Team {powerSort[i] + 1} with a power value of {powersSorted[i]}\")\n\nPower Ranking of the Teams\n1: Team 7 with a power value of 16.0\n2: Team 5 with a power value of 16.0\n3: Team 3 with a power value of 10.0\n4: Team 6 with a power value of 9.0\n5: Team 2 with a power value of 6.0\n6: Team 4 with a power value of 5.0\n7: Team 1 with a power value of 5.0\n\n\nWe can see that, compared to the simple ranking based on win-loss scores, the power ranking has all of the places the same except for a switch in the place of Team 1, 2, 4. This is interesting. This means that although Team 2 has a lower win-loss score than Team 1, it has a higher power than Team 1. We also note that the power ranking is not satisfactory, as there are two pairs of Teams (7 and 5, 4 and 1) that has the same power.\n\nRevision: Let us look at the reason why Team 2 has higher power than Team 1, even though Team 1 won more games than Team 2. Team 1 won 2 games. One is against Team 2 and another against Team 4. Team 2 won 2 game. One is against Team 6 and another against Team 4. To calculate the power of the Team in quesiton, we have to also count the number of Teams that the losing Team to the Team in question won. This is what causes the change. Team 2 and Team 4 won a total of 3 games. So the power of Team 1 is 2 (victories of Team 1) + 3 = 5. However, Team 2 won a game against Team 6 (sheer luck?), which won 3 games. Team 4 won 1 games. Therefore, Team 2 will have a score of 2 + 4 = 6, resulting in a higher power than Team 1. Thinking about this intuitively, we are basically incorporating a very simple weighting to the victories. Team 2’s victory against Team 6 is much more valuable than any of the Team 1’s victories because Team 6 is a powerful Team that had won a lot of games. Team 1’s victories against 4 and 2 is not really impressive, since they rarely win.\n\n\nYup, you’ve got it!\n\nTherefore, we should go further and try the Reverse PageRank method. Remember that the Reverse PageRank uses the adjoint of our original adjacency matrix. So, instead of the original PageRank ranking pages based on ingoing links, we now rank based on outgoing links. In our context, this is what we wish, since the outgoing links from Team 1 to Team 2 means that Team 1 won over Team 2. Also, Reverse PageRank involves the inclusion of a teleportation vector. This teleportation vector in this context allows us to encode in some “luck” parameter to all of the Teams. Maybe Team 1 won Team 2 because a member in Team 2 had a stomache. The teleportation vector then represents a global correction to the weighing of all wins.\n\nRevision: Let us further explain this teleportation vector. In the graph context, by including this normalized teleportation vector, we are applying a normalized “path” that allows every node to reach every node. In our context, we are sort of saying that each Team “won” every other Team! With the alpha parameter, we make this “win” weigh much less than the real edges (scores). By adding these “universal” victories, we are giving every Team a bit of slack. By controlling the parameter alpha according, which controls how much influence the teleportation vector has on the transition matrix, we are controlling how the Teams differ in skill! If we make the teleportaion vector weigh more by decreasing the alpha parameter, then we are saying that the Teams are more equal in skill. Under this case, then the actual scores will weigh less, meaning that the actual wins and losses for each Team will be because of factor of luck or other external factors.\n\nNow, let’s do the reverse pagerank.\n\ntransitionMatrix = np.zeros(np.shape(adj_matrix))\n\n# Since we are doing the transpose of the adjacent matrix, we can do a small\n# shortcut and simply normalize the column vectors of the adjacement matrix.\n# This is because the transition matrix is the \"normalized\" transpose of the\n# adjacent matrix. We start with the transpose of the adjacent matrix. So this\n# ends up canceling out.\nfor j in range(np.shape(adj_matrix)[1]):\n  sumBuffer = 0\n  for i in range(np.shape(adj_matrix)[0]):\n    sumBuffer += adj_matrix[i,j]\n  for m in range(np.shape(adj_matrix)[0]):\n    if adj_matrix[m,j] &gt; 0:\n      transitionMatrix[m,j] = adj_matrix[m,j] / sumBuffer\n\n# Now, we solve the PageRank equation with our obtained transitionMatrix\nalpha = 0.85\n\nmatrixToSolve = np.eye(7) - alpha * transitionMatrix\ninverse = np.linalg.inv(matrixToSolve)\noneMinusAlphaV = (1 - alpha) * (1/7 * np.ones((7,1)))\nanswer = inverse.dot(oneMinusAlphaV)\n\n#Now we sort the values based on descending order\nreverseIndex = np.argsort(answer, axis=0).flatten()[::-1]\nreverseSort = np.sort(answer, axis=0).flatten()[::-1]\n\n#Printing the results\nprint(\"The ReversePage Algorithm Ranking: \")\nfor i in range(len(answer)):\n  print(f\"{i+1}: Team {reverseIndex[i] + 1} with the value of {np.round(reverseSort[i], decimals=2)}\")\n\nThe ReversePage Algorithm Ranking: \n1: Team 5 with the value of 0.25\n2: Team 7 with the value of 0.18\n3: Team 3 with the value of 0.17\n4: Team 6 with the value of 0.13\n5: Team 4 with the value of 0.13\n6: Team 2 with the value of 0.08\n7: Team 1 with the value of 0.06\n\n\nLet us think about the results of this ReversePage compared to the power ranking. We can see now that when we consider the “stable state”, which is what we determine to be the ranking, Team 5 has higher “ranking score” than Team 7. This suggests that even though Team 5 and Team 7 have the same power, the influence, or the “quality” of the wins for Team 5 is higher than Team 7.\n\nRevision: Let us look at why the “quality” of the wins for Team 5 is higher than Team 7. Simply put, Team 5 won against Team 7 but Team 7 didn’t win against Team 5. In the reversePageRank context, the outgoing links of the Team 5 nodes is much more valuable than the outgoing links of the Team 7 node. Therefore, solving for the reversePageRank gives us a higher ranking for Team 5 then Team 7. In our context, we are claiming that even though Team 7 won more influential games than Team 5 (Power Ranking), the fact that Team 5 beat Team 7 makes Team 5 better than Team 7, so it definitely makes sence for Team 5 to be above Team 7 in ranking!\n\nAnother interesting result to recognize is the change in the ranking place for Team 4 and Team 6. Even though the “quality” of their wins are the same, the power, or the amount of their wins differ.\nNow, let us add actual weights to the wins with M.\n\nM = [4, 8, 7, 3, 7, 23, 15, 6, 18, 13, 14, 7, 13, 7, 18, 45, 10, 19, 14, 13]\n\n# Administer the weighing\nindex = 0\nnewMatrix = np.zeros(np.shape(adj_matrix))\nfor coordinate in edges:\n  newMatrix[coordinate[0] - 1, coordinate[1] - 1] = adj_matrix[coordinate[0] - 1, coordinate[1] - 1] * M[index]\n  index += 1\n\n# Calculate the new power matrix\nnewPowerMatrix = newMatrix + newMatrix.dot(newMatrix)\n\n# This will include all values for the powers of each Team i (index)\nnewPower = np.zeros(7)\n\nfor i in range(len(newPower)):\n  newPower[i] = newPowerMatrix[i, :].sum()\n\n# Now, sort the powers\nnewPowerSort = np.argsort(newPower)[::-1]\nnewPowersSorted = np.sort(newPower)[::-1]\n\n# Printing the sort\nprint(\"Power ranking with added weighing: \")\nfor i in range(len(newPowerSort)):\n  print(f\"{i+1}: Team {newPowerSort[i] + 1} with a power value of {newPowersSorted[i]}\")\n\nPower ranking with added weighing: \n1: Team 5 with a power value of 2104.0\n2: Team 7 with a power value of 2089.0\n3: Team 2 with a power value of 784.0\n4: Team 6 with a power value of 701.0\n5: Team 3 with a power value of 647.0\n6: Team 4 with a power value of 177.0\n7: Team 1 with a power value of 160.0\n\n\nThe higher the power value for the Teams, we can assume that the significance of their win is larger. This means that we know although Team 5 had less wins than Team 7, Team 5’s wins were just so much more rewarding and meaningful. The same could be said with the other Teams.\n\nYou’ve got nice responses as to the “why” in each situation here – great!\nGrade: E\n\n#LU Factorization\nLet us construct the Gaussian Elimination Algorithm and record all row exchanges with an index array. The overall idea is as follows: For a given matrix A, we first look at the first column of A and find the first nonzero component for this column vector. If this is not the first row of the column vector, we swap it with the first row of the column vector and record this swap with the index array. Then, we find all of the necessary LU factorization factors needed to make the row values of the column vector below 0, recording the multipliers in an identity matrix. Then, we move on to the 2nd column vector of A and do the same thing.\n\ndef PLUSystemSolver(matrix):\n  # Converting to Numpy array\n  matrix = np.array(matrix)\n  matrixSize = np.shape(matrix)\n\n  # Define the row index vector\n  rowIndexArray = np.array(range(0, matrixSize[0]))\n\n  # Defining the mold for the lower triangular matrix L\n  L = np.eye(matrixSize[0])\n\n  # Due to the construction of the algorthm below, we need to temporary save all of the multipliers and its corresponding\n  # coordinate in the L matrix. This is because we have potential row switches, which will mess the row orders up.\n  multiplierSave = []\n  coordinateSave = []\n\n  # Now let the algorithm begin\n  # We need to iterate over all column vectors\n  for columnIndex in range(matrixSize[1] - 1):\n    # For each column vector, iterate over the row components starting from the rowIndex as the columnIndex (indicating the diagonal)\n    for rowIndexSub in range(columnIndex, matrixSize[0]):\n      # Checking for nonzero \"pivots\"\n      if (np.absolute(matrix[rowIndexArray[rowIndexSub], columnIndex]) &gt; 0):\n        # Swapping the first nonzero pivot. DON'T SWITCH THE ACTUAL ROWS\n        rowIndexArray[rowIndexArray[columnIndex]], rowIndexArray[rowIndexSub] = rowIndexArray[rowIndexSub], rowIndexArray[rowIndexArray[columnIndex]]\n        break\n    # Now we do the subtraction algorthm on the column. Defining the value to subtract others.\n    pivotVal = matrix[rowIndexArray[columnIndex], columnIndex]\n    # We first make sure that the column index is not out of bounds (it is not the last column vector)\n    if columnIndex != (matrixSize[1] - 1):\n      for rowIndexSub in range(columnIndex + 1, matrixSize[0]):\n        # If the iterated value is nonzero, we do the subtraction\n        currentVal = matrix[rowIndexArray[rowIndexSub], columnIndex]\n        # Then we do the subtraction and find the multipliers\n        if currentVal != 0:\n          multiplier = (currentVal / pivotVal)\n          # Subtracting\n          matrix[rowIndexArray[rowIndexSub]] = matrix[rowIndexArray[rowIndexSub]] - (matrix[rowIndexArray[columnIndex]] * (multiplier))\n          # Recording the values of the multiplier and coordinates\n          coordinateSave.append([rowIndexSub, columnIndex])\n          multiplierSave.append(multiplier)\n  # Now we input the multipliers to the correct positions\n  for i in range(len(coordinateSave)):\n    L[rowIndexArray[coordinateSave[i][0]], coordinateSave[i][1]] = multiplierSave[i]\n\n  # Construction the Permutation Matrix\n  P = np.zeros(np.shape(matrix))\n  for i in range(len(rowIndexArray)):\n    mold = np.zeros(np.shape(matrix)[1])\n    mold[[rowIndexArray[i]]] = 1\n    P[[i]] = mold\n\n  # Finally, we switch the row position for U\n  matrixCopy = np.copy(matrix)\n  for i in range(matrixSize[0]):\n    matrix[[i]] = matrixCopy[[rowIndexArray[i]]]\n\n  # Returning the output\n  return P, L, matrix\n\n# Testing\nA = np.array([[2, 1, 0], [-4, -1, -1], [2, 3, -3]])\nA2 = np.array([[2, 1, 3],[-4, -2, -1],[2, 3, -3]])\nB1 = PLUSystemSolver(A)\nB2 = PLUSystemSolver(A2)\n\nprint(\"Starting Matrix: \\n\", A)\nprint(\"P Matrix: \\n\", B1[0])\nprint(\"L Matrix: \\n\", B1[1])\nprint(\"U Matrix: \\n\", B1[2])\nprint(\"\\n\")\nprint(\"Starting Matrix: \\n\", A2)\nprint(\"P Matrix: \\n\", B2[0])\nprint(\"L Matrix: \\n\", B2[1])\nprint(\"U Matrix: \\n\", B2[2])\n\nStarting Matrix: \n [[ 2  1  0]\n [-4 -1 -1]\n [ 2  3 -3]]\nP Matrix: \n [[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]]\nL Matrix: \n [[ 1.  0.  0.]\n [-2.  1.  0.]\n [ 1.  2.  1.]]\nU Matrix: \n [[ 2  1  0]\n [ 0  1 -1]\n [ 0  0 -1]]\n\n\nStarting Matrix: \n [[ 2  1  3]\n [-4 -2 -1]\n [ 2  3 -3]]\nP Matrix: \n [[1. 0. 0.]\n [0. 0. 1.]\n [0. 1. 0.]]\nL Matrix: \n [[ 1.  0.  0.]\n [ 1.  1.  0.]\n [-2.  0.  1.]]\nU Matrix: \n [[ 2  1  3]\n [ 0  2 -6]\n [ 0  0  5]]\n\n\nWe have successfully implemented the PLU function. This function takes the matrix in question and returns a tuple of (P matrix, L matrix, U matrix). We can see its success in finding the PLU matrices for HW3 Problem 12 and Problem 13.\nNow, let us implement the module that uses this PLU function to solve general linear systems. We will assume that the diagonals for the upper triangular and lower triangular matrices are nonzero. This systemSolver algorithm will take in the square nonsingular matrix and b in the form of a list with the correct dimension (3 numbers in a list for a 3 by 3 nonsingular square matrix).\n\n# Let us first define an upper triangular system solver.\n# b will be a one dimensional array.\ndef upperSolve(matrix, b):\n  # We are assuming that the matrix input is an upper triangular matrix.\n  # The matrix has to be square, so the size we can just get a single number.\n  matrixSize = np.shape(matrix)[0]\n  # Construct an empty solution vector\n  solution = np.zeros(matrixSize)\n\n  # We iterate in the reverse order on the rows\n  for rowIndex in reversed(range(matrixSize)):\n    # The value pointer is the value of the diagonal.\n    valuePointer = matrix[rowIndex, rowIndex]\n    if rowIndex == matrixSize - 1:\n      solution[matrixSize - 1] = b[matrixSize - 1] / valuePointer\n      continue\n\n    solution[rowIndex] = b[rowIndex] / matrix[rowIndex, rowIndex]\n    for index in range(rowIndex + 1, matrixSize):\n      solution[rowIndex] -= matrix[rowIndex, index] * solution[index] / matrix[rowIndex, rowIndex]\n\n  return solution\n\ndef lowerSolve(matrix, b):\n  # This will be similar to the upperSolve algorithm\n  # We are assuming that the matrix input is an upper triangular matrix.\n  # The matrix has to be square, so the size we can just get a single number.\n  matrixSize = np.shape(matrix)[0]\n  # Construct an empty solution vector\n  solution = np.zeros(matrixSize)\n\n  # We iterate in the reverse order on the rows\n  for rowIndex in range(matrixSize):\n    # The value pointer is the value of the diagonal.\n    valuePointer = matrix[rowIndex, rowIndex]\n    if rowIndex == 0:\n      solution[0] = b[0] / valuePointer\n      continue\n\n    solution[rowIndex] = b[rowIndex] / matrix[rowIndex, rowIndex]\n    for index in (range(0, rowIndex)):\n      solution[rowIndex] -= matrix[rowIndex, index] * solution[index] / matrix[rowIndex, rowIndex]\n\n  return solution\n\n# After defining the triangular solvers, we can carry on solving the entire system.\ndef systemSolver(P, L, U, b):\n  matrixSize = np.shape(U)\n\n  # We have (P^-1)LUx = b, so we should first solve for z, where (P^-1)z = b.\n  z = np.linalg.solve(P, b)\n\n  # Then, we have z = LUx. So let y = Ux. Then Ly = z is a system and we solve for y.\n  y = lowerSolve(L, z)\n\n  # Finally, we have Ux = y.\n  x = upperSolve(U, y)\n\n  return x\n\nThe Linear System solver utilizing PLU factorization has been implemented. To test the accuracy of the algorithm, we can generate random matrices with random solutions.\n\n# From ChatGBT\ndef generate_random_nonsingular_matrix(n):\n    while True:\n        A = np.random.rand(n, n)  # Generate a random n x n matrix\n        if np.linalg.det(A) != 0:  # Check if the determinant is non-zero\n            return A\n\n# Let us time this test. You will see the point soon!\nimport time\n\nstart = time.perf_counter()\nmatrixCreationTime = 0\n\n# Now let us test it\ntrueTimes = 0\nfalseTimes = 0\n\n# Testing for 10000 times\nfor i in range(10000):\n  # Generating random 1 x 10 vector for b and 10 * 10 nonsingular matrices\n  random = np.random.uniform(0, 100000, size=10)\n\n  # I am also timing how long the matrix creation and PLU calculation is\n  start_1 = time.perf_counter()\n  randomSquare = generate_random_nonsingular_matrix(10)\n  end_1 = time.perf_counter()\n\n  P, L, U = PLUSystemSolver(randomSquare)\n\n  matrixCreationTime += end_1 - start_1\n\n  # Test case\n  if systemSolver(P, L, U, random).all() == np.linalg.solve(randomSquare, random).all():\n    trueTimes += 1\n  else:\n    falseTimes = 0\n    print(randomSquare, random)\n\nprint(\"Number of Times the Algorithm is Correct: \", trueTimes)\nprint(\"Number of Times the Algorithm is Incorrect: \", falseTimes)\n\nend = time.perf_counter()\nduration = end - start\nprint(f\"The total test took {duration:.6f} seconds. \\n The matrix creation took a total of {matrixCreationTime:.6f} seconds.\")\nprint(f\"Test time subtracted by matrix creation time is {duration - matrixCreationTime:.6f}\")\n\nNumber of Times the Algorithm is Correct:  10000\nNumber of Times the Algorithm is Incorrect:  0\nThe total test took 5.055142 seconds. \n The matrix creation took a total of 0.119841 seconds.\nTest time subtracted by matrix creation time is 4.935301\n\n\nWe see from the results of the test. After generating 10000 random 1 x 10 vectors for b and 10 * 10 nonsingular matrices, the algorithm always matches the linalg solution. We can conclude with confidence that our algorithm works well.\nLet us try something. Instead of creating a random matrix everytime, let us just use the same matrix. Everything else is the same.\n\ntrueTimes = 0\nfalseTimes = 0\nrandomSquare = generate_random_nonsingular_matrix(10)\nP, L, U = PLUSystemSolver(randomSquare)\n\nstart = time.perf_counter()\n# Testing for 10000 times\nfor i in range(10000):\n  # Generating random 1 x 10 vector for b and 10 * 10 nonsingular matrices\n  random = np.random.uniform(0, 100000, size=10)\n\n  # Keeping the time counters to keep the timing same as last block's test.\n  start_1 = time.perf_counter()\n  end_1 = time.perf_counter()\n  matrixCreationTime += end_1 - start_1\n\n  # Test case\n  if systemSolver(P, L, U, random).all() == np.linalg.solve(randomSquare, random).all():\n    trueTimes += 1\n  else:\n    falseTimes = 0\n    print(randomSquare, random)\n\nprint(\"Number of Times the Algorithm is Correct: \", trueTimes)\nprint(\"Number of Times the Algorithm is Incorrect: \", falseTimes)\n\nend = time.perf_counter()\nduration = end - start\nprint(f\"The test took {duration:.6f} seconds.\")\n\nNumber of Times the Algorithm is Correct:  10000\nNumber of Times the Algorithm is Incorrect:  0\nThe test took 1.396865 seconds.\n\n\nThe difference in the times of these tests is noticeable. The second test, which only calculates P, L, U once is noticeably faster than the first test. My hypothesis was that the second test where we did not create random matrices everytime and used the same matrix could be noticeably shorter. This makes sense, as the advantage of PLU is the fact that once we have PLU, we quickly calculate the solutions corresponding to PLU for any b.\nNow, we can create our final inverse matrix finder module! The inverseFinder takes in nonsingular square matrices and output the inverse :)\n\ndef inverseFinder(matrix):\n  matrixSize = np.shape(matrix)[0]\n  # First, let us generate the standard coordinate basis that will serve as the\n  # b values to the system\n  bSet = []\n  for i in range(matrixSize):\n    e = np.zeros(matrixSize)\n    e[i] = 1\n    bSet.append(e)\n\n  P, L, U = PLUSystemSolver(matrix)\n\n  # Now we can obtain the solutino set\n  solutionSet = []\n  for b in bSet:\n    solutionSet.append(systemSolver(P, L, U, b))\n\n  transpose = np.array(solutionSet)\n  answer = transpose.T\n  return answer\n\n# Simple test\nprint(inverseFinder(A))\nprint(np.linalg.inv(A))\n\n[[-3.  -1.5  0.5]\n [ 7.   3.  -1. ]\n [ 5.   2.  -1. ]]\n[[-3.  -1.5  0.5]\n [ 7.   3.  -1. ]\n [ 5.   2.  -1. ]]\n\n\nYay, the inverse finder is finished. Similar to the systemSolver algorithm, let us test it more extensively by generating random nonsingular square matrices.\n\ntrueTimes = 0\nfalseTimes = 0\n\n# Testing for 10000 times\nfor i in range(10000):\n  # Generating random 1 x 10 vector for b and 10 * 10 nonsingular matrices\n  randomSquare = generate_random_nonsingular_matrix(10)\n  # Test case\n  if np.linalg.inv(randomSquare).all() != None:\n    if inverseFinder(randomSquare).all() == np.linalg.inv(randomSquare).all():\n      trueTimes += 1\n    else:\n      falseTimes = 0\n  else:\n    print(randomSquare)\n\nprint(\"Number of Times the Algorithm is Correct: \", trueTimes)\nprint(\"Number of Times the Algorithm is Incorrect: \", falseTimes)\n\nNumber of Times the Algorithm is Correct:  10000\nNumber of Times the Algorithm is Incorrect:  0\n\n\nThis code will take a little while to run. But after it finishes, we see that the inverseFinder algorithm works very well! There is no time when the algorithm is wrong during this test. It is important to note the limitations of the algorithm systemSolver and inverseFInder as they can only take square nonsingular matrices. In other words, matrices with a nonzero determinant. However, this is expected as the inverse can only be found for matrices with zero determinant. Therefore, the only big limitation will be for systemSolver. We cannot use this algorithm to solve singular matrices or matrix systems with infinite solutions. However, it is also important to note the strength of these algorithms. It uses a relatively clear and straigtforward process to find the system solution and the inverses. Once the PLU matrices are found, the solutions for the linear system corresponding to PLU for any b can be found with small computing power.\n\nGrade: Still E :-)"
  },
  {
    "objectID": "submitted/projects_1_revised/caihanyan_First_Projects_Revised.html",
    "href": "submitted/projects_1_revised/caihanyan_First_Projects_Revised.html",
    "title": "Project #1 Revisions - Hanyan Cai",
    "section": "",
    "text": "import networkx as nx\nimport matplotlib.pyplot as plt\nimport scipy\nimport numpy as np\nfrom scipy.stats import norm\n\n#Markov Chain\nThe transition matrix under this context will be\n\\[\n\\begin{bmatrix}\n0.5 & 0.2 & 0.3\\\\\n0.2 & 0.6 & 0.1\\\\\n0.3 & 0.2 & 0.6\\\\\n\\end{bmatrix}\n\\] .\nNow, let us put this transition matrix into code. We will set the initial state of the system to be \\[\n\\begin{bmatrix}\n\\frac{1}{3}\\\\\n\\frac{1}{3}\\\\\n\\frac{1}{3}\n\\end{bmatrix}\n\\]\nto denote all three companies having equal shares of the market.\n\n# Code in this transition matrix and the initial state (Equal shares of customers)\nmarkovTransition = np.array([[0.5, 0.2, 0.3], [0.2, 0.6, 0.1], [0.3, 0.2, 0.6]])\ninitialState = np.array([[1/3], [1/3], [1/3]])\niteration = initialState\n\n# Test three iterations\nfor i in range(3):\n  iteration = markovTransition.dot(iteration)\n  print(f\"{i + 1} Year Later: \\n Percent of customer shares for A: {np.round(iteration[0, 0] * 100, decimals=2)}% \\n Percent of customer shares for B: {np.round(iteration[1, 0] * 100, decimals=2)}% \\n Percent of customer shares for C: {np.round(iteration[2, 0] * 100, decimals=2)}%\")\n\n1 Year Later: \n Percent of customer shares for A: 33.33% \n Percent of customer shares for B: 30.0% \n Percent of customer shares for C: 36.67%\n2 Year Later: \n Percent of customer shares for A: 33.67% \n Percent of customer shares for B: 28.33% \n Percent of customer shares for C: 38.0%\n3 Year Later: \n Percent of customer shares for A: 33.9% \n Percent of customer shares for B: 27.53% \n Percent of customer shares for C: 38.57%\n\n\nWe can see from the results that B begins to lose shares of the market, C gains a considerable amount of shares (around 2%), while A gains small amount of shares (around 0.6%) in three years.\nNow, let us test the effects of the two advertising campaigns.\nAdvertising Campaign 1 convinces 20% of customers who would have to stay in B to switch to A. This can be represented in the modified transition matrix: \\[\n\\begin{bmatrix}\n0.5 & 0.32 & 0.3\\\\\n0.2 & 0.48 & 0.1\\\\\n0.3 & 0.2 & 0.6\\\\\n\\end{bmatrix}\n\\]\nAdvertising Campaign 2 convinces 20% of customers who would have to stay in C to switch to A. This can be represented in the modified transition matrix:\n\\[\n\\begin{bmatrix}\n0.5 & 0.2 & 0.42\\\\\n0.2 & 0.6 & 0.1\\\\\n0.3 & 0.2 & 0.48\\\\\n\\end{bmatrix}\n\\]\nTo view the effectiveness of the campaigns, I will iterate these markov chains over times and create a Gaussian Fit.\n\n# Code in the advertising campaign transition matrices.\ntransition1 = np.array([[0.5, 0.32, 0.3], [0.2, 0.48, 0.1], [0.3, 0.2, 0.6]])\ntransition2 = np.array([[0.5, 0.2, 0.42], [0.2, 0.6, 0.1], [0.3, 0.2, 0.48]])\n\n# Let us iterate over random distribution vectors for the two transition matrices\n# to view the resulting market shares after 3 years, 50 years, and 100 years.\nnp.random.seed(10)\nruns = 10000\n\n# I am defining a function that takes in the two transition matrices and the\n# number of years to iterate and outputs Gaussian fits for the shares for A\n\ndef advertisingGaussian(runs, years, transitionMatrix1, transitionMatrix2):\n  resultForAFrom1 = []\n  resultForAFrom2 = []\n  for run in range(runs):\n    # We do experiment a total of variable \"runs\" times.\n    # Create random initial distribution vector for the intial market share.\n    mold = np.random.random((3,))\n    initialState = mold / mold.sum()\n\n    state = initialState\n    # Now we do the actual iteration for the two matrices and append the results.\n    for year in range(years):\n      state = transition1.dot(state)\n    resultForAFrom1.append(state[0])\n\n    state = initialState\n    for year in range(years):\n      state = transition2.dot(state)\n    resultForAFrom2.append(state[0])\n\n  # Now we can fit the two sets of data for the market share of A\n  fig, axs = plt.subplots(2, 1, figsize=(6, 6))\n\n  # Determine the random color of the histograms\n  randomColorNumber = np.random.rand(3,)\n\n\n  # Fitting the 1 results\n  mu1, sigma1 = norm.fit(resultForAFrom1)\n\n  # Plotting the advertising campaign 1 results\n  axs[0].hist(resultForAFrom1, bins=100, density=True, alpha=0.7, color=randomColorNumber)\n  axs[0].set_title(f'The Distribution of Market Shares of A with Advertising Campaign 1 after {years} years, iterated for {runs} times.\\n(Estimated market share value of A for Campaign 1: {np.round(mu1, decimals=4)})', pad=20)\n  axs[0].set_xlabel('Market Share Distribution for A')\n  axs[0].set_ylabel('Density')\n\n  xmin1, xmax1 = axs[0].get_xlim()\n  x1 = np.linspace(xmin1, xmax1, 100)\n  p1 = norm.pdf(x1, mu1, sigma1)\n  axs[0].plot(x1, p1, 'k', linewidth=2, label='Gaussian Fit')\n\n\n  # Fitting the 2 results\n  mu2, sigma2 = norm.fit(resultForAFrom2)\n\n  # Plotting the advertising campaign 2 results\n  axs[1].hist(resultForAFrom2, bins=100, density=True, alpha=0.7, color=randomColorNumber)\n  axs[1].set_title(f'The Distribution of Market Shares of A with Advertising Campaign 2 after {years} years, iterated for {runs} times.\\n(Estimated market share value of A for Campaign 2: {np.round(mu2, decimals=4)})', pad=20)\n  axs[1].set_xlabel('Market Share Distribution for A')\n  axs[1].set_ylabel('Density')\n\n  xmin2, xmax2 = axs[1].get_xlim()\n  x2 = np.linspace(xmin2, xmax2, 100)\n  p2 = norm.pdf(x2, mu2, sigma2)\n  axs[1].plot(x2, p2, 'k', linewidth=2, label='Gaussian Fit')\n\n  # Showing the plot\n  plt.tight_layout()\n  plt.show()\n\n# Now call the function\n\nadvertisingGaussian(runs, 3, transition1, transition2)\nprint(\"\\n\")\nadvertisingGaussian(runs, 10, transition1, transition2)\nprint(\"\\n\")\nadvertisingGaussian(runs, 50, transition1, transition2)\nprint(\"\\n\")\nadvertisingGaussian(runs, 100, transition1, transition2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/Users/kendra/Library/Python/3.8/lib/python/site-packages/numpy/lib/histograms.py:883: RuntimeWarning: divide by zero encountered in divide\n  return n/db/n.sum(), bin_edges\n/Users/kendra/Library/Python/3.8/lib/python/site-packages/numpy/lib/histograms.py:883: RuntimeWarning: invalid value encountered in divide\n  return n/db/n.sum(), bin_edges\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet us analyze these results. Each pair of histogram denotes the distribution of the market shares for A after some years. The estimated market share values after the set amount of years is denoted under the title. This estimated value is the mu parameter for the Gaussian fit, denoting the value with the greatest probabiltiy distribution.\nFor Advertising Campaign 1, we see that for 1000 random initial distributions of market shares will tend with each iteration towards a stationary state of 0.38 for the market share of A. This suggests that Advertising Campaign 1 will allow A to stabilize at 0.3806 marketshares. This seems to be a very good strategy.\nFor Advertising Campaign 2, we can see that the most probable market share distribution for also increases. With long iterations under numerous random intial states, A seems to always achieve the stationary state of 0.39 for its market share.\nEven after three years, markov simulation suggests that Advertising Campaign 2 performs better than Advertising Campaign 1, as the market share of A under Advertising Campaign 2 is bigger than the market share of A under Advertising Campaign 1. The later iterations also suggests the same thing. Therefore, my model suggests that Advertising Campaign 2 is a more effective choice both for long term and short term increase in market share for A.\nIt is important to note the limitations within this model. First, it simplifies the dynamics of the market. By iterating the static transition matrices over long periods of time, we are assuming that the market does not change. In other words, company B and company C are doing nothing to regain their lost customers. After all, it is quite unreasonable to assume that things will always stay the same in 100 years. The effectiveness of the advertising campaign also can change, which is not reflected in the static transition matrices. Therefore, this model may be effect in approximating the effects of the advertising campaigns for a very short period of iteration before competitors B and C have a chance to react or implement their own strategies.\n\nRevision: In addition, by looking at the initial transition matrices:\n\n\\[\n\\begin{bmatrix}\n0.5 & 0.32 & 0.3\\\\\n0.2 & 0.48 & 0.1\\\\\n0.3 & 0.2 & 0.6\\\\\n\\end{bmatrix}\n\\]\n\\[\n\\begin{bmatrix}\n0.5 & 0.2 & 0.42\\\\\n0.2 & 0.6 & 0.1\\\\\n0.3 & 0.2 & 0.48\\\\\n\\end{bmatrix}\n\\]\nWe can see that every iteration, A is obtaining the same percent of customers from B and C, in the first advertisement matrix, C is obtaining more customers than B. This allows C to stay competitive with A, competing for the shares of B, as opposed to the second advertisement matrix, which makes C less competitive, allowing for A to dominate more equally to A and B.\n\nIt is also worth noting the issue with the histogram. It seems like after 50 years, the stationary state is almost reached despite random initial states, resulting in all possible data contributing to one box of the histogram. I do not know exactly how I should fix this though. Perhaps I should stop after 20 years.\n\n\nI agree that the histogram isn’t so useful when all of the data is concentrated in one bin! But that’s not a problem, per se – your data just gets to the stationary state.\n\nNow, perhaps we wish to go a step further and analyze how much of an effect gaining shares from B or C will have on the long term share of A. Then, we can simply construct functions of A market share with respect to the gaining of the market share of B or C.\n\n# The variable parameter will be a number between 0 and 1, denoting up till\n# 100% of the customer that was suppose to remain with B going to A\n# Iterations will control how many years passes.\ndef functionB(parameter, iterations, initialState):\n  change = parameter * 0.4\n  transitionMatrix = np.array([[0.5, 0.2 + change, 0.3], [0.2, 0.6 - change, 0.1], [0.3, 0.2, 0.6]])\n  # We will start with the assumption for equal shares.\n  for year in range(iterations):\n    initialState = transitionMatrix.dot(initialState)\n  return initialState[0]\n\ndef functionC(parameter, iterations, initialState):\n  change = parameter * 0.4\n  transitionMatrix = np.array([[0.5, 0.2, 0.3 + change], [0.2, 0.6, 0.1], [0.3, 0.2, 0.6 - change]])\n  # We will start with the assumption for equal shares\n  for year in range(iterations):\n    initialState = transitionMatrix.dot(initialState)\n  return initialState[0]\n\n# Setting the initial state to be all companies having equal shares\ninitialState = np.ones((3,1)) / 3\n\n# Readying the plot\nxValues = np.linspace(0,1, num=101)\nBValues = []\nCValues = []\n\nfor x in xValues:\n  BValues.append(functionB(x, 50, initialState))\n  CValues.append(functionC(x, 50, initialState))\n\n# Plotting\nplt.title(\"Market Share of A after 50 years with Respect to Change in Parameter x\")\nplt.plot(xValues, BValues, label=\"Parameter under Advertisement 1 (B to A)\")\nplt.plot(xValues, CValues, label=\"Parameter under Advertisement 2 (C to A)\")\nplt.xlabel(\"Parameter (Percent Change / 100)\")\nplt.ylabel(\"Market Share for A\")\nplt.legend(loc=\"upper left\", frameon=True)\n\n\n\n\n\n\n\n\nThe above graph works is generated by the following method. We introduce the parameter variable that ranges from 0 to 1. For each transition matrix representing the advertisement matrix, the parameter looks at the percentage of customers going to A rather than staying in B/C from the effects of the campaign and multiplies it by the parameter. For example, if the parameter is 1, than for Advertisement 1, 100% of the customers that was originally going to stay with B will go to A, resulting in the following transition matrix: \\(\\begin{bmatrix}\n0.5 & 0.8 & 0.3\\\\\n0.2 & 0.0 & 0.1\\\\\n0.3 & 0.2 & 0.6\\\\\n\\end{bmatrix}\\).\nNow, with the modified transition matrix by the parameter, the initial state of all companies having equal shares\n\\[\\begin{bmatrix}\n\\frac{1}{3}\\\\\n\\frac{1}{3}\\\\\n\\frac{1}{3}\n\\end{bmatrix}\\]\nis initialized. We apply the initial state to the modified transition matrix for 50 times. The resulting market share value of A is then recorded and plotted.\nLooking at the graph, if the resources needed to convert customers from C to A is the same as the resources needed to convert customers from B to A, then it is more effective to convert customers from C since the increase in the market share of A is more for C than B as we increase the parameter.\n#Sports Ranking\nWe will first rank the Teams based on their win-loss ratio.\n\nRevision: It is not the win-loss ratio, but simply the difference between the number of wins and losses for each Team.\n\nThis will be the simplest way to rank them. In fact, we can create a digraph with the given edges before doing so. This allows us to create an adjacency matrix. The sum of the \\(i^{th}\\) row of the matrix gives us the number of wins for Team \\(i\\), while the sum of the \\(i^{th}\\) column of the matrix gives us the number of losses for Team \\(i\\). Therefore, to account for both wins and losses of each Team, we can create a ranking score for each Team determined by their number of wins subtracted by their number of losses. Using the adjacency matrix of the graph we created makes our life a lot easier, so I am introducing the graph before the simple ranking.\n\nsportsGraph = nx.DiGraph()\n\nsportsGraph.add_nodes_from([1, 2, 3, 4, 5, 6, 7])\n\nedges = [\n\n    (1, 2), (7, 3), (2, 4), (4, 5), (3, 2), (5, 1), (6, 1), (7, 2), (2, 6),\n\n    (3, 4), (7, 4), (5, 7), (6, 4), (3, 5), (5, 6), (7, 1), (5, 2), (7, 6),\n\n    (1, 4), (6, 3)\n\n]\n\nsportsGraph.add_edges_from(edges)\n\n# Let us plot the graph representing the wins and losses of the 7 Teams.\nnx.draw_circular(sportsGraph, with_labels=True)\nplt.show()\n\n# To calculate the win-losses for the Teams, we can do it by hand, but it is easier to analyze the adjacency matrix of the graph.\nadj_matrix = nx.adjacency_matrix(sportsGraph)\nadj_matrix = adj_matrix.toarray()\n\n# Define the win and loss arrays, where the index+1 denotes the Team number:\nwins = np.zeros(7)\nlosses = np.zeros(7)\n\n# The ith column of the adjacency matrix is the number of losses Team i experienced. The ith row is the number of wins Team i experienced.\nfor i in range(len(wins)):\n  wins[i] = adj_matrix[i,:].sum()\n\nfor i in range(len(losses)):\n  losses[i] = adj_matrix[:, i].sum()\n\n# We define a very simple score that takes into account the number of wins and the number of losses for each Team. The higher the winLossScore, the better the ranking.\nwinLossScore = wins - losses\n\nsimpleSort = np.argsort(winLossScore)[::-1]\nwLSorted = np.sort(winLossScore)[::-1]\n\nprint(\"\\n\")\nprint(\"Simple ranking of the Teams based on win/loss:\")\n\nfor i in range(len(simpleSort)):\n  print(f\"{i+1}: Team {simpleSort[i] + 1} with a win - loss score of {wLSorted[i]}\")\n\n\n\n\n\n\n\n\n\n\nSimple ranking of the Teams based on win/loss:\n1: Team 7 with a win - loss score of 4.0\n2: Team 5 with a win - loss score of 2.0\n3: Team 3 with a win - loss score of 1.0\n4: Team 6 with a win - loss score of 0.0\n5: Team 1 with a win - loss score of -1.0\n6: Team 2 with a win - loss score of -2.0\n7: Team 4 with a win - loss score of -4.0\n\n\nWe can see that the win-loss score gives us fairly neat ranking. Although we didn’t see the same scores on different Teams, we can account for it by developing a system. If two Teams have the same score, then we look at their total wins. The Team with the most wins will be ranked higher. If they have the same wins, they have to have the same losses (since they have the same win-loss score).\n\nRevision: If two Teams have the same win-loss difference and they have the same number of wins, then they must have the same number of losses. For instance, if Team 1 and Team 2 both have a win-loss difference of 2 and they both won 5 games, then they both must have lost 3 games. Then, there is no value comparing the number of losses of Team 1 and Team 2. Then, if we are under the situation where two Teams has the same win-loss difference and the same number of wins, we cannot really distinguish their ranking. Instead, we should look for a better way to rank.\n\nIn this case, we will have to devise more sophisticated methods for ranking. This is what we do next. We will use the vertex power to rank the Teams. Remember, the vertex power is the number of connections of a vertex with length two of less. This allows us to sort of consider the “weight” of the Team wins. If Team 1 wins Team 2, and Team 2 wins Team 3, then we consider Team 1 to be better than Team 3 and count that as a “win” for Team 1 (lucky!)\nThen, we can rank the Teams based on this power concept. The Team with the highest power will be first place.\n\n# Next, we use the vertex power to rank\n\n# Calculate the power matrix\npowerMatrix = adj_matrix + adj_matrix.dot(adj_matrix)\n\n# This will include all values for the powers of each Team i (index)\npowers = np.zeros(7)\n\nfor i in range(len(powers)):\n  powers[i] = powerMatrix[i, :].sum()\n\n# Now, sort the powers\npowerSort = np.argsort(powers)[::-1]\npowersSorted = np.sort(powers)[::-1]\n\n# Printing the sort\nprint(\"Power Ranking of the Teams\")\nfor i in range(len(powerSort)):\n  print(f\"{i+1}: Team {powerSort[i] + 1} with a power value of {powersSorted[i]}\")\n\nPower Ranking of the Teams\n1: Team 7 with a power value of 16.0\n2: Team 5 with a power value of 16.0\n3: Team 3 with a power value of 10.0\n4: Team 6 with a power value of 9.0\n5: Team 2 with a power value of 6.0\n6: Team 4 with a power value of 5.0\n7: Team 1 with a power value of 5.0\n\n\nWe can see that, compared to the simple ranking based on win-loss scores, the power ranking has all of the places the same except for a switch in the place of Team 1, 2, 4. This is interesting. This means that although Team 2 has a lower win-loss score than Team 1, it has a higher power than Team 1. We also note that the power ranking is not satisfactory, as there are two pairs of Teams (7 and 5, 4 and 1) that has the same power.\n\nRevision: Let us look at the reason why Team 2 has higher power than Team 1, even though Team 1 won more games than Team 2. Team 1 won 2 games. One is against Team 2 and another against Team 4. Team 2 won 2 game. One is against Team 6 and another against Team 4. To calculate the power of the Team in quesiton, we have to also count the number of Teams that the losing Team to the Team in question won. This is what causes the change. Team 2 and Team 4 won a total of 3 games. So the power of Team 1 is 2 (victories of Team 1) + 3 = 5. However, Team 2 won a game against Team 6 (sheer luck?), which won 3 games. Team 4 won 1 games. Therefore, Team 2 will have a score of 2 + 4 = 6, resulting in a higher power than Team 1. Thinking about this intuitively, we are basically incorporating a very simple weighting to the victories. Team 2’s victory against Team 6 is much more valuable than any of the Team 1’s victories because Team 6 is a powerful Team that had won a lot of games. Team 1’s victories against 4 and 2 is not really impressive, since they rarely win.\n\n\nYup, you’ve got it!\n\nTherefore, we should go further and try the Reverse PageRank method. Remember that the Reverse PageRank uses the adjoint of our original adjacency matrix. So, instead of the original PageRank ranking pages based on ingoing links, we now rank based on outgoing links. In our context, this is what we wish, since the outgoing links from Team 1 to Team 2 means that Team 1 won over Team 2. Also, Reverse PageRank involves the inclusion of a teleportation vector. This teleportation vector in this context allows us to encode in some “luck” parameter to all of the Teams. Maybe Team 1 won Team 2 because a member in Team 2 had a stomache. The teleportation vector then represents a global correction to the weighing of all wins.\n\nRevision: Let us further explain this teleportation vector. In the graph context, by including this normalized teleportation vector, we are applying a normalized “path” that allows every node to reach every node. In our context, we are sort of saying that each Team “won” every other Team! With the alpha parameter, we make this “win” weigh much less than the real edges (scores). By adding these “universal” victories, we are giving every Team a bit of slack. By controlling the parameter alpha according, which controls how much influence the teleportation vector has on the transition matrix, we are controlling how the Teams differ in skill! If we make the teleportaion vector weigh more by decreasing the alpha parameter, then we are saying that the Teams are more equal in skill. Under this case, then the actual scores will weigh less, meaning that the actual wins and losses for each Team will be because of factor of luck or other external factors.\n\nNow, let’s do the reverse pagerank.\n\ntransitionMatrix = np.zeros(np.shape(adj_matrix))\n\n# Since we are doing the transpose of the adjacent matrix, we can do a small\n# shortcut and simply normalize the column vectors of the adjacement matrix.\n# This is because the transition matrix is the \"normalized\" transpose of the\n# adjacent matrix. We start with the transpose of the adjacent matrix. So this\n# ends up canceling out.\nfor j in range(np.shape(adj_matrix)[1]):\n  sumBuffer = 0\n  for i in range(np.shape(adj_matrix)[0]):\n    sumBuffer += adj_matrix[i,j]\n  for m in range(np.shape(adj_matrix)[0]):\n    if adj_matrix[m,j] &gt; 0:\n      transitionMatrix[m,j] = adj_matrix[m,j] / sumBuffer\n\n# Now, we solve the PageRank equation with our obtained transitionMatrix\nalpha = 0.85\n\nmatrixToSolve = np.eye(7) - alpha * transitionMatrix\ninverse = np.linalg.inv(matrixToSolve)\noneMinusAlphaV = (1 - alpha) * (1/7 * np.ones((7,1)))\nanswer = inverse.dot(oneMinusAlphaV)\n\n#Now we sort the values based on descending order\nreverseIndex = np.argsort(answer, axis=0).flatten()[::-1]\nreverseSort = np.sort(answer, axis=0).flatten()[::-1]\n\n#Printing the results\nprint(\"The ReversePage Algorithm Ranking: \")\nfor i in range(len(answer)):\n  print(f\"{i+1}: Team {reverseIndex[i] + 1} with the value of {np.round(reverseSort[i], decimals=2)}\")\n\nThe ReversePage Algorithm Ranking: \n1: Team 5 with the value of 0.25\n2: Team 7 with the value of 0.18\n3: Team 3 with the value of 0.17\n4: Team 6 with the value of 0.13\n5: Team 4 with the value of 0.13\n6: Team 2 with the value of 0.08\n7: Team 1 with the value of 0.06\n\n\nLet us think about the results of this ReversePage compared to the power ranking. We can see now that when we consider the “stable state”, which is what we determine to be the ranking, Team 5 has higher “ranking score” than Team 7. This suggests that even though Team 5 and Team 7 have the same power, the influence, or the “quality” of the wins for Team 5 is higher than Team 7.\n\nRevision: Let us look at why the “quality” of the wins for Team 5 is higher than Team 7. Simply put, Team 5 won against Team 7 but Team 7 didn’t win against Team 5. In the reversePageRank context, the outgoing links of the Team 5 nodes is much more valuable than the outgoing links of the Team 7 node. Therefore, solving for the reversePageRank gives us a higher ranking for Team 5 then Team 7. In our context, we are claiming that even though Team 7 won more influential games than Team 5 (Power Ranking), the fact that Team 5 beat Team 7 makes Team 5 better than Team 7, so it definitely makes sence for Team 5 to be above Team 7 in ranking!\n\nAnother interesting result to recognize is the change in the ranking place for Team 4 and Team 6. Even though the “quality” of their wins are the same, the power, or the amount of their wins differ.\nNow, let us add actual weights to the wins with M.\n\nM = [4, 8, 7, 3, 7, 23, 15, 6, 18, 13, 14, 7, 13, 7, 18, 45, 10, 19, 14, 13]\n\n# Administer the weighing\nindex = 0\nnewMatrix = np.zeros(np.shape(adj_matrix))\nfor coordinate in edges:\n  newMatrix[coordinate[0] - 1, coordinate[1] - 1] = adj_matrix[coordinate[0] - 1, coordinate[1] - 1] * M[index]\n  index += 1\n\n# Calculate the new power matrix\nnewPowerMatrix = newMatrix + newMatrix.dot(newMatrix)\n\n# This will include all values for the powers of each Team i (index)\nnewPower = np.zeros(7)\n\nfor i in range(len(newPower)):\n  newPower[i] = newPowerMatrix[i, :].sum()\n\n# Now, sort the powers\nnewPowerSort = np.argsort(newPower)[::-1]\nnewPowersSorted = np.sort(newPower)[::-1]\n\n# Printing the sort\nprint(\"Power ranking with added weighing: \")\nfor i in range(len(newPowerSort)):\n  print(f\"{i+1}: Team {newPowerSort[i] + 1} with a power value of {newPowersSorted[i]}\")\n\nPower ranking with added weighing: \n1: Team 5 with a power value of 2104.0\n2: Team 7 with a power value of 2089.0\n3: Team 2 with a power value of 784.0\n4: Team 6 with a power value of 701.0\n5: Team 3 with a power value of 647.0\n6: Team 4 with a power value of 177.0\n7: Team 1 with a power value of 160.0\n\n\nThe higher the power value for the Teams, we can assume that the significance of their win is larger. This means that we know although Team 5 had less wins than Team 7, Team 5’s wins were just so much more rewarding and meaningful. The same could be said with the other Teams.\n\nYou’ve got nice responses as to the “why” in each situation here – great!\nGrade: E\n\n#LU Factorization\nLet us construct the Gaussian Elimination Algorithm and record all row exchanges with an index array. The overall idea is as follows: For a given matrix A, we first look at the first column of A and find the first nonzero component for this column vector. If this is not the first row of the column vector, we swap it with the first row of the column vector and record this swap with the index array. Then, we find all of the necessary LU factorization factors needed to make the row values of the column vector below 0, recording the multipliers in an identity matrix. Then, we move on to the 2nd column vector of A and do the same thing.\n\ndef PLUSystemSolver(matrix):\n  # Converting to Numpy array\n  matrix = np.array(matrix)\n  matrixSize = np.shape(matrix)\n\n  # Define the row index vector\n  rowIndexArray = np.array(range(0, matrixSize[0]))\n\n  # Defining the mold for the lower triangular matrix L\n  L = np.eye(matrixSize[0])\n\n  # Due to the construction of the algorthm below, we need to temporary save all of the multipliers and its corresponding\n  # coordinate in the L matrix. This is because we have potential row switches, which will mess the row orders up.\n  multiplierSave = []\n  coordinateSave = []\n\n  # Now let the algorithm begin\n  # We need to iterate over all column vectors\n  for columnIndex in range(matrixSize[1] - 1):\n    # For each column vector, iterate over the row components starting from the rowIndex as the columnIndex (indicating the diagonal)\n    for rowIndexSub in range(columnIndex, matrixSize[0]):\n      # Checking for nonzero \"pivots\"\n      if (np.absolute(matrix[rowIndexArray[rowIndexSub], columnIndex]) &gt; 0):\n        # Swapping the first nonzero pivot. DON'T SWITCH THE ACTUAL ROWS\n        rowIndexArray[rowIndexArray[columnIndex]], rowIndexArray[rowIndexSub] = rowIndexArray[rowIndexSub], rowIndexArray[rowIndexArray[columnIndex]]\n        break\n    # Now we do the subtraction algorthm on the column. Defining the value to subtract others.\n    pivotVal = matrix[rowIndexArray[columnIndex], columnIndex]\n    # We first make sure that the column index is not out of bounds (it is not the last column vector)\n    if columnIndex != (matrixSize[1] - 1):\n      for rowIndexSub in range(columnIndex + 1, matrixSize[0]):\n        # If the iterated value is nonzero, we do the subtraction\n        currentVal = matrix[rowIndexArray[rowIndexSub], columnIndex]\n        # Then we do the subtraction and find the multipliers\n        if currentVal != 0:\n          multiplier = (currentVal / pivotVal)\n          # Subtracting\n          matrix[rowIndexArray[rowIndexSub]] = matrix[rowIndexArray[rowIndexSub]] - (matrix[rowIndexArray[columnIndex]] * (multiplier))\n          # Recording the values of the multiplier and coordinates\n          coordinateSave.append([rowIndexSub, columnIndex])\n          multiplierSave.append(multiplier)\n  # Now we input the multipliers to the correct positions\n  for i in range(len(coordinateSave)):\n    L[rowIndexArray[coordinateSave[i][0]], coordinateSave[i][1]] = multiplierSave[i]\n\n  # Construction the Permutation Matrix\n  P = np.zeros(np.shape(matrix))\n  for i in range(len(rowIndexArray)):\n    mold = np.zeros(np.shape(matrix)[1])\n    mold[[rowIndexArray[i]]] = 1\n    P[[i]] = mold\n\n  # Finally, we switch the row position for U\n  matrixCopy = np.copy(matrix)\n  for i in range(matrixSize[0]):\n    matrix[[i]] = matrixCopy[[rowIndexArray[i]]]\n\n  # Returning the output\n  return P, L, matrix\n\n# Testing\nA = np.array([[2, 1, 0], [-4, -1, -1], [2, 3, -3]])\nA2 = np.array([[2, 1, 3],[-4, -2, -1],[2, 3, -3]])\nB1 = PLUSystemSolver(A)\nB2 = PLUSystemSolver(A2)\n\nprint(\"Starting Matrix: \\n\", A)\nprint(\"P Matrix: \\n\", B1[0])\nprint(\"L Matrix: \\n\", B1[1])\nprint(\"U Matrix: \\n\", B1[2])\nprint(\"\\n\")\nprint(\"Starting Matrix: \\n\", A2)\nprint(\"P Matrix: \\n\", B2[0])\nprint(\"L Matrix: \\n\", B2[1])\nprint(\"U Matrix: \\n\", B2[2])\n\nStarting Matrix: \n [[ 2  1  0]\n [-4 -1 -1]\n [ 2  3 -3]]\nP Matrix: \n [[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]]\nL Matrix: \n [[ 1.  0.  0.]\n [-2.  1.  0.]\n [ 1.  2.  1.]]\nU Matrix: \n [[ 2  1  0]\n [ 0  1 -1]\n [ 0  0 -1]]\n\n\nStarting Matrix: \n [[ 2  1  3]\n [-4 -2 -1]\n [ 2  3 -3]]\nP Matrix: \n [[1. 0. 0.]\n [0. 0. 1.]\n [0. 1. 0.]]\nL Matrix: \n [[ 1.  0.  0.]\n [ 1.  1.  0.]\n [-2.  0.  1.]]\nU Matrix: \n [[ 2  1  3]\n [ 0  2 -6]\n [ 0  0  5]]\n\n\nWe have successfully implemented the PLU function. This function takes the matrix in question and returns a tuple of (P matrix, L matrix, U matrix). We can see its success in finding the PLU matrices for HW3 Problem 12 and Problem 13.\nNow, let us implement the module that uses this PLU function to solve general linear systems. We will assume that the diagonals for the upper triangular and lower triangular matrices are nonzero. This systemSolver algorithm will take in the square nonsingular matrix and b in the form of a list with the correct dimension (3 numbers in a list for a 3 by 3 nonsingular square matrix).\n\n# Let us first define an upper triangular system solver.\n# b will be a one dimensional array.\ndef upperSolve(matrix, b):\n  # We are assuming that the matrix input is an upper triangular matrix.\n  # The matrix has to be square, so the size we can just get a single number.\n  matrixSize = np.shape(matrix)[0]\n  # Construct an empty solution vector\n  solution = np.zeros(matrixSize)\n\n  # We iterate in the reverse order on the rows\n  for rowIndex in reversed(range(matrixSize)):\n    # The value pointer is the value of the diagonal.\n    valuePointer = matrix[rowIndex, rowIndex]\n    if rowIndex == matrixSize - 1:\n      solution[matrixSize - 1] = b[matrixSize - 1] / valuePointer\n      continue\n\n    solution[rowIndex] = b[rowIndex] / matrix[rowIndex, rowIndex]\n    for index in range(rowIndex + 1, matrixSize):\n      solution[rowIndex] -= matrix[rowIndex, index] * solution[index] / matrix[rowIndex, rowIndex]\n\n  return solution\n\ndef lowerSolve(matrix, b):\n  # This will be similar to the upperSolve algorithm\n  # We are assuming that the matrix input is an upper triangular matrix.\n  # The matrix has to be square, so the size we can just get a single number.\n  matrixSize = np.shape(matrix)[0]\n  # Construct an empty solution vector\n  solution = np.zeros(matrixSize)\n\n  # We iterate in the reverse order on the rows\n  for rowIndex in range(matrixSize):\n    # The value pointer is the value of the diagonal.\n    valuePointer = matrix[rowIndex, rowIndex]\n    if rowIndex == 0:\n      solution[0] = b[0] / valuePointer\n      continue\n\n    solution[rowIndex] = b[rowIndex] / matrix[rowIndex, rowIndex]\n    for index in (range(0, rowIndex)):\n      solution[rowIndex] -= matrix[rowIndex, index] * solution[index] / matrix[rowIndex, rowIndex]\n\n  return solution\n\n# After defining the triangular solvers, we can carry on solving the entire system.\ndef systemSolver(P, L, U, b):\n  matrixSize = np.shape(U)\n\n  # We have (P^-1)LUx = b, so we should first solve for z, where (P^-1)z = b.\n  z = np.linalg.solve(P, b)\n\n  # Then, we have z = LUx. So let y = Ux. Then Ly = z is a system and we solve for y.\n  y = lowerSolve(L, z)\n\n  # Finally, we have Ux = y.\n  x = upperSolve(U, y)\n\n  return x\n\nThe Linear System solver utilizing PLU factorization has been implemented. To test the accuracy of the algorithm, we can generate random matrices with random solutions.\n\n# From ChatGBT\ndef generate_random_nonsingular_matrix(n):\n    while True:\n        A = np.random.rand(n, n)  # Generate a random n x n matrix\n        if np.linalg.det(A) != 0:  # Check if the determinant is non-zero\n            return A\n\n# Let us time this test. You will see the point soon!\nimport time\n\nstart = time.perf_counter()\nmatrixCreationTime = 0\n\n# Now let us test it\ntrueTimes = 0\nfalseTimes = 0\n\n# Testing for 10000 times\nfor i in range(10000):\n  # Generating random 1 x 10 vector for b and 10 * 10 nonsingular matrices\n  random = np.random.uniform(0, 100000, size=10)\n\n  # I am also timing how long the matrix creation and PLU calculation is\n  start_1 = time.perf_counter()\n  randomSquare = generate_random_nonsingular_matrix(10)\n  end_1 = time.perf_counter()\n\n  P, L, U = PLUSystemSolver(randomSquare)\n\n  matrixCreationTime += end_1 - start_1\n\n  # Test case\n  if systemSolver(P, L, U, random).all() == np.linalg.solve(randomSquare, random).all():\n    trueTimes += 1\n  else:\n    falseTimes = 0\n    print(randomSquare, random)\n\nprint(\"Number of Times the Algorithm is Correct: \", trueTimes)\nprint(\"Number of Times the Algorithm is Incorrect: \", falseTimes)\n\nend = time.perf_counter()\nduration = end - start\nprint(f\"The total test took {duration:.6f} seconds. \\n The matrix creation took a total of {matrixCreationTime:.6f} seconds.\")\nprint(f\"Test time subtracted by matrix creation time is {duration - matrixCreationTime:.6f}\")\n\nNumber of Times the Algorithm is Correct:  10000\nNumber of Times the Algorithm is Incorrect:  0\nThe total test took 5.055142 seconds. \n The matrix creation took a total of 0.119841 seconds.\nTest time subtracted by matrix creation time is 4.935301\n\n\nWe see from the results of the test. After generating 10000 random 1 x 10 vectors for b and 10 * 10 nonsingular matrices, the algorithm always matches the linalg solution. We can conclude with confidence that our algorithm works well.\nLet us try something. Instead of creating a random matrix everytime, let us just use the same matrix. Everything else is the same.\n\ntrueTimes = 0\nfalseTimes = 0\nrandomSquare = generate_random_nonsingular_matrix(10)\nP, L, U = PLUSystemSolver(randomSquare)\n\nstart = time.perf_counter()\n# Testing for 10000 times\nfor i in range(10000):\n  # Generating random 1 x 10 vector for b and 10 * 10 nonsingular matrices\n  random = np.random.uniform(0, 100000, size=10)\n\n  # Keeping the time counters to keep the timing same as last block's test.\n  start_1 = time.perf_counter()\n  end_1 = time.perf_counter()\n  matrixCreationTime += end_1 - start_1\n\n  # Test case\n  if systemSolver(P, L, U, random).all() == np.linalg.solve(randomSquare, random).all():\n    trueTimes += 1\n  else:\n    falseTimes = 0\n    print(randomSquare, random)\n\nprint(\"Number of Times the Algorithm is Correct: \", trueTimes)\nprint(\"Number of Times the Algorithm is Incorrect: \", falseTimes)\n\nend = time.perf_counter()\nduration = end - start\nprint(f\"The test took {duration:.6f} seconds.\")\n\nNumber of Times the Algorithm is Correct:  10000\nNumber of Times the Algorithm is Incorrect:  0\nThe test took 1.396865 seconds.\n\n\nThe difference in the times of these tests is noticeable. The second test, which only calculates P, L, U once is noticeably faster than the first test. My hypothesis was that the second test where we did not create random matrices everytime and used the same matrix could be noticeably shorter. This makes sense, as the advantage of PLU is the fact that once we have PLU, we quickly calculate the solutions corresponding to PLU for any b.\nNow, we can create our final inverse matrix finder module! The inverseFinder takes in nonsingular square matrices and output the inverse :)\n\ndef inverseFinder(matrix):\n  matrixSize = np.shape(matrix)[0]\n  # First, let us generate the standard coordinate basis that will serve as the\n  # b values to the system\n  bSet = []\n  for i in range(matrixSize):\n    e = np.zeros(matrixSize)\n    e[i] = 1\n    bSet.append(e)\n\n  P, L, U = PLUSystemSolver(matrix)\n\n  # Now we can obtain the solutino set\n  solutionSet = []\n  for b in bSet:\n    solutionSet.append(systemSolver(P, L, U, b))\n\n  transpose = np.array(solutionSet)\n  answer = transpose.T\n  return answer\n\n# Simple test\nprint(inverseFinder(A))\nprint(np.linalg.inv(A))\n\n[[-3.  -1.5  0.5]\n [ 7.   3.  -1. ]\n [ 5.   2.  -1. ]]\n[[-3.  -1.5  0.5]\n [ 7.   3.  -1. ]\n [ 5.   2.  -1. ]]\n\n\nYay, the inverse finder is finished. Similar to the systemSolver algorithm, let us test it more extensively by generating random nonsingular square matrices.\n\ntrueTimes = 0\nfalseTimes = 0\n\n# Testing for 10000 times\nfor i in range(10000):\n  # Generating random 1 x 10 vector for b and 10 * 10 nonsingular matrices\n  randomSquare = generate_random_nonsingular_matrix(10)\n  # Test case\n  if np.linalg.inv(randomSquare).all() != None:\n    if inverseFinder(randomSquare).all() == np.linalg.inv(randomSquare).all():\n      trueTimes += 1\n    else:\n      falseTimes = 0\n  else:\n    print(randomSquare)\n\nprint(\"Number of Times the Algorithm is Correct: \", trueTimes)\nprint(\"Number of Times the Algorithm is Incorrect: \", falseTimes)\n\nNumber of Times the Algorithm is Correct:  10000\nNumber of Times the Algorithm is Incorrect:  0\n\n\nThis code will take a little while to run. But after it finishes, we see that the inverseFinder algorithm works very well! There is no time when the algorithm is wrong during this test. It is important to note the limitations of the algorithm systemSolver and inverseFInder as they can only take square nonsingular matrices. In other words, matrices with a nonzero determinant. However, this is expected as the inverse can only be found for matrices with zero determinant. Therefore, the only big limitation will be for systemSolver. We cannot use this algorithm to solve singular matrices or matrix systems with infinite solutions. However, it is also important to note the strength of these algorithms. It uses a relatively clear and straigtforward process to find the system solution and the inverses. Once the PLU matrices are found, the solutions for the linear system corresponding to PLU for any b can be found with small computing power.\n\nGrade: Still E :-)"
  },
  {
    "objectID": "submitted/projects_1_revised/tandocalec_project1_revision.html",
    "href": "submitted/projects_1_revised/tandocalec_project1_revision.html",
    "title": "Project #1 Revisions - Alec Tandoc",
    "section": "",
    "text": "import sympy as sp\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nIn this problem, we are calculating the effectivness of different marketing strategies on distribution vectors representing a companies market share. We will be trying to optimize the plan so that we provide Company A with the most market share.\nFirst, I will construct the original market conclusions as a Markov chain and demonstrate its effects after 3 years.\n\n# Markov chain\nM = sp.Matrix([\n    [0.5, 0.2, 0.3],\n    [0.2, 0.6, 0.1],\n    [0.3, 0.2, 0.6]\n])\n\n\norig = sp.Matrix([sp.Rational(1,3), sp.Rational(1,3), sp.Rational(1,3)])\n(M**3)*orig\n\n\\(\\displaystyle \\left[\\begin{matrix}0.339\\\\0.275333333333333\\\\0.385666666666667\\end{matrix}\\right]\\)\n\n\n\nIf you find yourself using Sympy in the future, here is a trick I have learned: if you have a matrix, you can get it into exact rational form with the command P=sp.nsimplify(P,rational=True).\nI was trying to understand why (M**3)*orig was returning a floating point form. This is because M itself wasn’t in rational form. Just for kicks, I have fixed this in the code below…\n\nMr = sp.nsimplify(M, rational=True)\n(Mr**3)*orig\n\n\\(\\displaystyle \\left[\\begin{matrix}\\frac{339}{1000}\\\\\\frac{413}{1500}\\\\\\frac{1157}{3000}\\end{matrix}\\right]\\)\n\n\n\nAs demonstrated, after 3 years, the Markov chain slightly favors C, with B being the least favored\nNow, we consider Plan 1 and Plan 2, as denoted by variables plan1 and plan2 respectively.\n\nplan1 = sp.Matrix([\n    [0.5, 0.2 + .6*0.2, 0.3],\n    [0.2, 0.6*.8, 0.1],\n    [0.3, 0.2, 0.6]\n])\nplan2 = sp.Matrix([\n    [0.5, 0.2, 0.3+0.6*.2],\n    [0.2, 0.6, 0.1],\n    [0.3, 0.2, 0.6*0.8]\n])\n\nWe will create 4 vectors, representing 4 different distribution types. The first vector will have an equal distribution, with variable equal. The next three vectors will have 50% of the population weighted on company A, B, C respectively with the other two sharing an equal 25% population distribution.\n\nequal = sp.Matrix([sp.Rational(1,3), sp.Rational(1,3), sp.Rational(1,3)])\n\nThis function creates n vectors, where n is equal to the count of companies. For a given company i, i in [1,n], each (n-i)th item in the resulting list is the distribution that favors i and equally distributes the other n-1 companies.\n\ndef weight_distributor(distribution, company_count = 3):\n  result = []\n  for i in range(company_count):\n    cur_list = []\n    for j in range(company_count):\n      cur_list.append(distribution[(j + i)%company_count])\n    cur = sp.Matrix(cur_list)\n    result.append(cur)\n  return result[::-1]\n\nThe function returns the desired weight vectors. From now on, weights[0] represents the weighted distribution for Company A, weights[1] for Company B, and weights[2] for Company C.\n\ndistribution = [sp.Rational(25, 100), sp.Rational(25,100), sp.Rational(50,100)]\nweights = [equal] + weight_distributor([sp.Rational(25, 100), sp.Rational(25,100), sp.Rational(50,100)])\n\nThis function returns the states for all weights, with the ith component representing the ith weight under the given plan after 0 years, 1 year, 5 years, and 10 years.\n\ndef state_calculator(weights, plan):\n  states = []\n  for weight in weights:\n    states.append([weight, (plan ** 1)*weight, (plan ** 5)*weight, (plan ** 10)*weight])\n  return states\n\nCalculating states for 0, 1, 5, and 10 years for equal distributions and then 50, 25, 25 distributions for all companies.\n\n# Varying states for plan 1\nplan1_states = state_calculator(weights, plan1)\n# Varying states for plan 2\nplan2_states = state_calculator(weights, plan2)\n\n\ntime = [0,1,5,10]\ncompanies = ['A', 'B', 'C']\ncompanies_labels = {'A': 0, 'B': 1, 'C': 3}\n\nThis function finds the maximum distribution of a given company under a given plan after 10 years and which distribution was best. The next one finds the minimum.\n\n# Find company max distribution\ndef plan_max(company, plan):\n  result = plan[0][3][company]\n  dis = 0\n  if result &lt; plan[1][3][company]:\n    result = plan[1][3][company]\n    dis = 1\n  if result &lt; plan[2][3][company]:\n    result = plan[2][3][company]\n    dis = 2\n  if result &lt; plan[3][3][company]:\n    result = plan[3][3][company]\n    dis = 3\n  return result, dis\n\n\nAs you learn to code, this can be a useful trick. I selected the above code and asked CoPilot “simplify this”. Here is what it gave me:\n\ndef plan_max(company, plan):\n  result = plan[0][3][company]\n  dis = 0\n  for i in range(1, 4):\n    if result &lt; plan[i][3][company]:\n      result = plan[i][3][company]\n      dis = i\n  return result, dis\n\nThis put the repeated code into a loop. I find it a little bit harder to understand, but it’s also harder to make a mistake when you write the code only once. But we can do better:\nI again asked CoPilot to simplify. This time I said “simplify this using np.min”. Here is what I got:\n\ndef plan_max(company, plan):\n  result = np.min(plan[0][3][company])\n  dis = np.argmin(plan[0][3][company])\n  return result, dis\n\nWell, here is we need to be careful! This result looks good, but it won’t actually work. Can you see why? (One reason is that I asked it to use “min”, where “max” would have been better. I was confused by the “&lt;” signs in your code; I had to read it again to understand that you were indeed finding a max. But there’s even a much bigger problem..)\nI tried again. This time I said “simplify this using min instead of the loop”. I was given:\n\ndef plan_max(company, plan):\n  result = np.max(plan[:, 3, company])\n  dis = np.argmax(plan[:, 3, company])\n  return result, dis\n\nThis one I believe should work. I’m going to test it…\nAnd, no, I got the error TypeError: list indices must be integers or slices, not tuple\"So now I'm going to ask CoPilot to fix that error...\nI asked CoPilot to fix this, and it got snarky with me. “To fix the”TypeError: list indices must be integers or slices, not tuple” error, replace the code inside the selection with the correct indexing syntax using integers instead of tuples.”\nOK. I tried one more time, asking CoPilot to fix it. Finally, it even explained, and gave me code that actually worked:\n\n“The error message suggests that the plan variable is a list, not a NumPy array. The slicing operation plan[:, 3, company] is specific to NumPy arrays and doesn’t work with Python lists.\nTo fix this, you can convert the plan list to a NumPy array inside the function. Here’s how you can do it:”\n\n\ndef plan_max(company, plan):\n  plan = np.array(plan)  # convert list to numpy array\n  result = np.max(plan[:, 3, company])\n  dis = np.argmax(plan[:, 3, company])\n  return result, dis\n\nAnyways. I will leave it to you to decide how much you want to use these AI tools in your learning! As you can see, it can definitely be a frustrating exercise.\n\n\n# Find company max distribution\ndef plan_min(company, plan):\n  result = plan[0][3][company]\n  dis = 0\n  if result &gt; plan[1][3][company]:\n    result = plan[1][3][company]\n    dis = 1\n  if result &gt; plan[2][3][company]:\n    result = plan[2][3][company]\n    dis = 2\n  if result &gt; plan[3][3][company]:\n    result = plan[3][3][company]\n    dis = 3\n  return result, dis\n\nWe now calculate Company A’s performance under the original plan based on our 4 different distributions.\n\nbase = sp.Matrix([((M**10)*weights[0])[0], ((M**10)*weights[1])[0], ((M**10)*weights[2])[0], ((M**10)*weights[3])[0]])\n\n\nI’m guessing that calculating M**10 is a bit slow for Sympy! I’d calculate it once and store it…\n\nFor comparison, we calculate the base states using the original plan.\n\nbase_states = state_calculator(weights, M)\n\nWe now look at Company A’s distribution under different distributions after 10 years.\n\nplan1_dis = sp.Matrix([plan1_states[0][3][companies_labels['A']], plan1_states[1][3][companies_labels['A']], plan1_states[2][3][companies_labels['A']], plan1_states[3][3][companies_labels['A']]])\nplan2_dis = sp.Matrix([plan2_states[0][3][companies_labels['A']], plan2_states[1][3][companies_labels['A']], plan2_states[2][3][companies_labels['A']], plan2_states[3][3][companies_labels['A']]])\n\nThese next two blocks compute the sum of the difference of plan 1 or plan 2’s distribution for A after 10 years compared to the A’s performance under the base plan.\n\nsum(plan1_dis - base)\n\n\\(\\displaystyle 0.156465181152655\\)\n\n\n\nplan1_dis - base\n\n\\(\\displaystyle \\left[\\begin{matrix}0.0391162952881637\\\\0.0391123655075863\\\\0.0391439122722667\\\\0.0390926080846383\\end{matrix}\\right]\\)\n\n\n\nsum(plan2_dis - base)\n\n\\(\\displaystyle 0.194330292122828\\)\n\n\n\nplan2_dis - base\n\n\\(\\displaystyle \\left[\\begin{matrix}0.048582573030707\\\\0.0485857956136774\\\\0.0485811891277593\\\\0.0485807343506842\\end{matrix}\\right]\\)\n\n\n\nplan2_dis - plan1_dis\n\n\\(\\displaystyle \\left[\\begin{matrix}0.00946627774254322\\\\0.00947343010609109\\\\0.00943727685549262\\\\0.00948812626604584\\end{matrix}\\right]\\)\n\n\nAs demonstrated, both plans provide positive results for A after 10 years. As a long term strategy both are optimal. However, as we can see, plan 2 provides slightly better performance than plan 1. Thus, we choose plan 2.\nWe now find the max of each plan\n\nplan_max(0, plan2_states)\n\n(0.390054582225684, 3)\n\n\n\nplan_min(0, plan2_states)\n\n(0.390005782377759, 2)\n\n\n\nplan2_dis\n\n\\(\\displaystyle \\left[\\begin{matrix}0.390033780497374\\\\0.390040976888677\\\\0.390005782377759\\\\0.390054582225684\\end{matrix}\\right]\\)\n\n\nAs demonstrated, we can see that overall, Plan 2 performs the best in terms of Company A’s long term market-share. This is based 4 distributions: equal distribution, and 50%, 25%, 25% market shares for each company. Interestingly, plan 2 performs. We will now overlay Plan 2’s best performance distribution compared to its worst performance distribution: Weight C v. Weight B\n\n# Plan 1: Equal distribution\nmarket = plan2_states[3]\nfor i in range(3):\n  plt.plot(time, [market[0][i], market[1][i], market[2][i], market[3][i]], label=f\"Company {companies[i]}: Weight C\")\nmarket = plan2_states[2]\nfor i in range(3):\n  plt.plot(time, [market[0][i], market[1][i], market[2][i], market[3][i]], label=f\"Company {companies[i]}: Weight B\", linestyle=\":\")\nplt.ylabel('Customer Market Share: Weight C v. Weight B')\nplt.xlabel('Years')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nAs demonstrated, Company A’s performance evens out through all distributions. We will now overlay a few distributions comparing the original plan to Plan 2.\n\nmarket = plan2_states[0]\nfor i in range(3):\n  plt.plot(time, [market[0][i], market[1][i], market[2][i], market[3][i]], label=f\"Company {companies[i]}: Equal 2\")\nmarket = base_states[0]\nfor i in range(3):\n  plt.plot(time, [market[0][i], market[1][i], market[2][i], market[3][i]], label=f\"Company {companies[i]}: Base\", linestyle=\":\")\nplt.ylabel('Customer Market Share: Weight A Distribution')\nplt.xlabel('Years')\nplt.legend(loc=3)\nplt.show()\n\n\n\n\n\n\n\n\n\nmarket = plan2_states[1]\nfor i in range(3):\n  plt.plot(time, [market[0][i], market[1][i], market[2][i], market[3][i]], label=f\"Company {companies[i]}: Weight A 2\")\nmarket = base_states[1]\nfor i in range(3):\n  plt.plot(time, [market[0][i], market[1][i], market[2][i], market[3][i]], label=f\"Company {companies[i]}: Base\", linestyle=\":\")\nplt.ylabel('Customer Market Share: Weight B Distribution')\nplt.xlabel('Years')\nplt.legend(loc=3)\nplt.show()\n\n\n\n\n\n\n\n\n\nmarket = plan2_states[2]\nfor i in range(3):\n  plt.plot(time, [market[0][i], market[1][i], market[2][i], market[3][i]], label=f\"Company {companies[i]}: Weight B 2\")\nmarket = base_states[2]\nfor i in range(3):\n  plt.plot(time, [market[0][i], market[1][i], market[2][i], market[3][i]], label=f\"Company {companies[i]}: Base\", linestyle=\":\")\nplt.ylabel('Customer Market Share: Weight C Distribution')\nplt.xlabel('Years')\nplt.legend(loc=3)\nplt.show()\n\n\n\n\n\n\n\n\n\nmarket = plan2_states[3]\nfor i in range(3):\n  plt.plot(time, [market[0][i], market[1][i], market[2][i], market[3][i]], label=f\"Company {companies[i]}: Weight C 2\")\nmarket = base_states[3]\nfor i in range(3):\n  plt.plot(time, [market[0][i], market[1][i], market[2][i], market[3][i]], label=f\"Company {companies[i]}: Base\", linestyle=\":\")\nplt.ylabel('Customer Market Share: Weight C')\nplt.xlabel('Years')\nplt.legend(loc=3)\nplt.show()\n\n\n\n\n\n\n\n\nAs demonstrated, A performs consistently better under Plan 2 than the original plan. Thus, we can see that it would be ideal for the company to undergo marketing strategies to implement plan 2. For exceptional circumstances, we consider a market in which we start with 0% of the user base. However, as displayed below, even if we start with 0 market share, our new marketing strategy will propel us to the largest market holder within a 10 year period.\n\n(plan2 ** 10) * sp.Matrix([0.0, 0.99, .01])\n\n\\(\\displaystyle \\left[\\begin{matrix}0.389923740012833\\\\0.278200270113832\\\\0.331875989873334\\end{matrix}\\right]\\)\n\n\n::: comments This is much clearer now – I can understand exactly what you are doing in each bit of code, and why. It definitely meets expectations, and is appropriate for projects in this class. Nice!\nThe next level, now that you are clearly communicating what you’ve done, would be to think about how to separate your math and analytical reasoning from your explanations about implementation. This is important when you want to be able to communicate with readers who may be more interested in one part rather than the other. For example, you might start off explaining the problem you are trying to solve and giving a high-level overview of how you went about solving it: “I evaluated the predicted performance under the different marketing plans using a Markov Chain approach. Specifically, for each plan I calculated a transition matrix to determine year-over-year change in market share. I examined a number of possible startng distributions and found their long-term performance under each plan by exponentiating the transition matrix. For each scenario, I determined which of the three companies had the highest market share in the long term.”\nOne option would be to put text like this at the top of the document, and then use headers to match sections of code with parts of your high-level plan: “Determining the transition matrix”, “Creating initial distributions”, etc. Another option would be to break it apart and put this kind of text in different Markdown cells throughout the document, and then have the implementation details in comments within the code cells.\nGrade: M ::\n\nSports Ranking\nIn this problem, we are attempting to rank teams based on a win graph. We will iterate through the PowerRank, reverse PageRank, and weighted win techniques to determine the best team.\n\n# Do imports\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport scipy\nimport numpy as np\n\nHere we just set-up a graph using the given nodes and edges\n\n# Create a directed graph\nG = nx.DiGraph()\n\n# Add nodes\nG.add_nodes_from([1, 2, 3, 4, 5, 6, 7])\n\n# Add vertices\nedges = [(1,2),(7,3),(2,4),(4,5),(3,2),(5,1),(6,1),(3,1),(7,2),(2,6), (3, 4), (7,4),(5,7),(6,4),(3,5),(5,6),(7,1),(5,2),(7,6),(1,4),(6,3)]\nG.add_edges_from(edges)\n\n# Draw the graph\nnx.draw_circular(G, with_labels=True)\nplt.show()\n\n\n\n\n\n\n\n\nWe calculate the adjacency matrix of this matrix. Rows represents where the edge originates. Columns represent where they end up\n\nadj_matrix = nx.adjacency_matrix(G).toarray()\nadj_matrix\n\narray([[0, 1, 0, 1, 0, 0, 0],\n       [0, 0, 0, 1, 0, 1, 0],\n       [1, 1, 0, 1, 1, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0],\n       [1, 1, 0, 0, 0, 1, 1],\n       [1, 0, 1, 1, 0, 0, 0],\n       [1, 1, 1, 1, 0, 1, 0]])\n\n\nThe power matrix is one method of calculating the most “winning” team. An entry a_(ij) in the adjacency matrix indicates a win from team j on team i. An entry b_(ij) in the adjacency matrix squared represents how many teams that team j has beaten that have beaten another team. As we can see, when we take the sum of those two matrices, we are left with our power matrix. What we can infer from the power matrix is that the sum of each row represents how many teams a team has beaten plus how many “winning” teams they have beaten.\n\n# Power Matrix\npower_matrix = adj_matrix + adj_matrix ** 2\npower_matrix\n\narray([[0, 2, 0, 2, 0, 0, 0],\n       [0, 0, 0, 2, 0, 2, 0],\n       [2, 2, 0, 2, 2, 0, 0],\n       [0, 0, 0, 0, 2, 0, 0],\n       [2, 2, 0, 0, 0, 2, 2],\n       [2, 0, 2, 2, 0, 0, 0],\n       [2, 2, 2, 2, 0, 2, 0]])\n\n\nThus, when we calculate the sum of each row of the power matrix, we can rank the teams, sorting by highest sum to lowest.\n\n# Calculating team ranking with the power matrix\npower_sum = [sum(power_matrix[i]) for i in range(7)]\nrrank = np.argsort(power_sum)\nrank = np.flip(rrank + 1)\nrank\n\narray([7, 5, 3, 6, 2, 1, 4])\n\n\nAs can be seen, using the power matrix, team 7 seems to be the most “winning” team, which seems to be the case when analyzing the power matrix visually. Team 7 seems to have the most teams beaten + winning teams beaten\nTo perform reverse pagerank, we must first find the adjacency matrix of the reverse graph, which is just the transpose of the adjacency matrix. We then want to multiply it by a diagonal matrix D which turns the columns of our reverse adjacency matrix into distribution vectors. This will allow us to perform the pagerank algorithm to solve for x. We know that PageRank allows us to determine the importance of a node. However, if we reverse PageRank, we are able to calculate the “sphere of influence” that a node has, i.e. how many other teams a single team has affected. In this case, “affecting” a team indicates beating them.\n\n# Calculating team ranking with reverse pagerank: calculate diagonal matrix D\nadjT = adj_matrix.T\nD = np.zeros((7, 7))\nfor i in range(7):\n  D[i][i] = 1 / sum(adjT[i])\n\nHere we follow the steps of the pagerank matrix, using the given alpha and teleporation vectors to ensure that our multiplication does not end prematurely due to a row/column of 0s.\n\n# Determine P = ATD -&gt; (AT)TD = AD with alpha a = 0.85, teleporation vector 1/7 e_7)\nP = sp.Matrix(sp.Matrix(adj_matrix) * sp.Matrix(D))\na = 0.85\nv = sp.Matrix(np.ones(7) * 1/7)\n\nWe solve for x and are left with the influence of each team. We then sort by lowest to highest and flip to find the team with the most influence.\n\n# Solve for x\nM = (sp.eye(7) - a * P)\nb = (1 - a) * v\nsol = M.solve(b)\nx = np.array([sol[i] for i in range(7)], dtype='float')\nx\n\narray([0.05966732, 0.07966581, 0.17626964, 0.12535158, 0.24452472,\n       0.13033224, 0.18418869])\n\n\n\n# Find ranking\npr_rank = np.flip(np.argsort(x))+1\npr_rank\n\narray([5, 7, 3, 6, 4, 2, 1])\n\n\nWe see now that 5 is at top. This may be explained by the fact that 5 has beaten 7, such that 5 has beaten the team that has won the most. Therefore, it should exert the most “influence” amongst teams since has beaten the most winning team.\nWe will now weight our graph so that we can perform a weighted power ranking.\n\n# Create a weighted directed graph\nG = nx.DiGraph()\n\n# Add nodes\nG.add_nodes_from([1, 2, 3, 4, 5, 6, 7])\n\n# Add vertices\nedges = [(1,2),(7,3),(2,4),(4,5),(3,2),(5,1),(6,1),(3,1),(7,2),(2,6), (3, 4), (7,4),(5,7),(6,4),(3,5),(5,6),(7,1),(5,2),(7,6),(1,4),(6,3)]\nlength = len(edges)\nweight = [4,8,7,3,7,7,23,15,6,18,13,14,7,13,7,18,45,10,19,14,13]\nfor i in range(length):\n  G.add_edge(edges[i][0], edges[i][1], weight=weight[i])\n\npos = nx.spring_layout(G, seed=7)  # positions for all nodes - seed for reproducibility\n\n# nodes\nnx.draw_networkx_nodes(G, pos, node_size=700)\n\n# edges\nnx.draw_networkx_edges(G, pos, edgelist=edges, width=1)\n\n# node labels\nnx.draw_networkx_labels(G, pos, font_size=20, font_family=\"sans-serif\")\n# edge weight labels\nedge_labels = nx.get_edge_attributes(G, \"weight\")\nnx.draw_networkx_edge_labels(G, pos, edge_labels)\n\nplt.show()\n\n\n\n\n\n\n\n\nWe now find take adjacency matrix of the graph and then replace the edge indicators with the weight of the each edge.\n\n# Replace adjacency booleans with weights\nweight_adj = adj_matrix\nfor i in range(length):\n  weight_adj[edges[i][0]-1, edges[i][1]-1] = weight[i]\nweight_adj\n\narray([[ 0,  4,  0, 14,  0,  0,  0],\n       [ 0,  0,  0,  7,  0, 18,  0],\n       [15,  7,  0, 13,  7,  0,  0],\n       [ 0,  0,  0,  0,  3,  0,  0],\n       [ 7, 10,  0,  0,  0, 18,  7],\n       [23,  0, 13, 13,  0,  0,  0],\n       [45,  6,  8, 14,  0, 19,  0]])\n\n\nWe now follow the same steps to rank the teams, first using the power matrix technique\n\nweight_power = weight_adj + weight_adj**2\nweight_power_sum = [sum(weight_power[i]) for i in range(7)]\nrrank_weight = np.argsort(weight_power_sum)\nrank_weight = np.flip(rrank_weight + 1)\nrank_weight\n\narray([7, 6, 5, 3, 2, 1, 4])\n\n\nHere we see that 7 is ranked at the top once again, most likely due to the fact that it has the most wins and the mots dominant win, with 45 points against team 1.\nNow, we use reverse pagerank again.\n\n# Calculating team ranking with reverse pagerank: calculate diagonal matrix D\nadjT_weight = weight_adj.T\nD = np.zeros((7, 7))\nfor i in range(7):\n  D[i][i] = 1 / sum(adjT_weight[i])\n# Determine P = ATD -&gt; (AT)TD = AD with alpha a = 0.85, teleporation vector 1/7 e_7)\nP = sp.Matrix(sp.Matrix(weight_adj) * sp.Matrix(D))\na = 0.85\nv = sp.Matrix(np.ones(7) * 1/7)\n# Solve for x\nM = (sp.eye(7) - a * P)\nb = (1 - a) * v\nsol = M.solve(b)\nx = np.array([sol[i] for i in range(7)], dtype='float')\n# Find ranking\npr_rank = np.flip(np.argsort(x))+1\npr_rank\n\narray([5, 3, 7, 6, 4, 2, 1])\n\n\nWe can see that weighting the edges have changed the order of our rankings. Although 7 and 5 both remained high up on their performance, we can see that 6 rose in rankings during the weighted Reverse PageRank. This is likely due to the fact that team 6 had a strong performance against team 3, who has had many strong performances against other teams.\n\nThis is exactly what I was looking for! You’ve explained your thinking; it’s nicely separated between reasoning (in Markdown) and implementation (in code with comments). Your analysis is also solid.\nGrade: E\n\n\n\nIso Rank\nIn this problem, we attempt to imperfectly map graphs onto other graphs. Specifically, mappings that are not bijections, such that there is no perfect 1-1 mapping of the nodes.\nWe first create our graphs as adjaceny matrices.\n\n# Construct adjacency matrices for A,B,C,D,E and 1,2,3,4,5\n# G1 = A,B,C,D,E\n# G2 = 1,2,3,4,5\nnp_G1 = np.matrix([\n    [0,1,1,0,1],\n    [1,0,0,1,0],\n    [1,0,0,0,0],\n    [0,1,0,0,0],\n    [1,0,0,0,0]\n], dtype='float')\nnp_G2 = np.matrix([\n    [0,0,1,0,0],\n    [0,0,1,0,0],\n    [1,1,0,1,0],\n    [0,0,1,0,1],\n    [0,0,0,1,0]\n], dtype='float')\nG1T = np_G1.transpose()\nG2T = np_G2.transpose()\nfor i in range(5):\n  G1T[i] /= np.sum(G1T[i])\n  G2T[i] /= np.sum(G2T[i])\nG1 = sp.Matrix(np.transpose(G1T))\nG2 = sp.Matrix(np.transpose(G2T))\nG2\n\n\\(\\displaystyle \\left[\\begin{matrix}0 & 0 & 0.333333333333333 & 0 & 0\\\\0 & 0 & 0.333333333333333 & 0 & 0\\\\1.0 & 1.0 & 0 & 0.5 & 0\\\\0 & 0 & 0.333333333333333 & 0 & 1.0\\\\0 & 0 & 0 & 0.5 & 0\\end{matrix}\\right]\\)\n\n\nWe then create a surfing matrix which is represented as a multiplication of each element of G2 with the entire graph of G1 (e.g. M[0:2][0:2] = G2[0][0] * G1).\n\nIt would be useful here to have a description of what a surfing matrix is, and why it’s calculated this way, etc.\n\n\n# Construct G2 x G1 (Outer x Inner) surfing matrix\nM = sp.zeros(25)\nfor h in range(5):\n  for k in range(5):\n    for i in range(5):\n      for j in range(5):\n        M[i+5*(h), j+5*(k)] = G2[h, k] * G1[i, j]\nM\n\n\\(\\displaystyle \\left[\\begin{array}{ccccccccccccccccccccccccc}0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.166666666666667 & 0.333333333333333 & 0 & 0.333333333333333 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.111111111111111 & 0 & 0 & 0.333333333333333 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.111111111111111 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.166666666666667 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.111111111111111 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.166666666666667 & 0.333333333333333 & 0 & 0.333333333333333 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.111111111111111 & 0 & 0 & 0.333333333333333 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.111111111111111 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.166666666666667 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.111111111111111 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & 0.5 & 1.0 & 0 & 1.0 & 0 & 0.5 & 1.0 & 0 & 1.0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.25 & 0.5 & 0 & 0.5 & 0 & 0 & 0 & 0 & 0\\\\0.333333333333333 & 0 & 0 & 1.0 & 0 & 0.333333333333333 & 0 & 0 & 1.0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.166666666666667 & 0 & 0 & 0.5 & 0 & 0 & 0 & 0 & 0 & 0\\\\0.333333333333333 & 0 & 0 & 0 & 0 & 0.333333333333333 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.166666666666667 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & 0.5 & 0 & 0 & 0 & 0 & 0.5 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.25 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\0.333333333333333 & 0 & 0 & 0 & 0 & 0.333333333333333 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.166666666666667 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.166666666666667 & 0.333333333333333 & 0 & 0.333333333333333 & 0 & 0 & 0 & 0 & 0 & 0 & 0.5 & 1.0 & 0 & 1.0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.111111111111111 & 0 & 0 & 0.333333333333333 & 0 & 0 & 0 & 0 & 0 & 0 & 0.333333333333333 & 0 & 0 & 1.0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.111111111111111 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.333333333333333 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.166666666666667 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.5 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.111111111111111 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.333333333333333 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.25 & 0.5 & 0 & 0.5 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.166666666666667 & 0 & 0 & 0.5 & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.166666666666667 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.25 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.166666666666667 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\end{array}\\right]\\)\n\n\nUsing this surfing matrix, we then solve for x using the PageRank formula. From here, we are then giving a list of node pairings from G1 to G2. Every 5 values represents all the nodes from G1 (A, B, C, D, E) and how closely related they are to node i in G2 (e.g. values 6-10 are the closeness of nodes A, B, C, D, and E to node 2)\n\nWhat is x? What is the PageRank formula? Please include formulas if you are going to solve them! And, why should it work to show how closely related the nodes are?\n\n\n# Calculate x given alpha = 0.85, v = 1/25 * e25\na = 0.85\nv = 1/25 * sp.Matrix(np.ones(25))\nmat = (sp.eye(25) - a * M)\nb = (1-a)*v\nx = mat.solve(b)\nx\n\n\\(\\displaystyle \\left[\\begin{matrix}0.0422740152656166\\\\0.032389985878358\\\\0.0191626479069361\\\\0.0183187399484339\\\\0.0191626479069361\\\\0.0422740152656166\\\\0.032389985878358\\\\0.0191626479069361\\\\0.0183187399484339\\\\0.0191626479069361\\\\0.139369213132264\\\\0.0869558114007097\\\\0.0422740152656167\\\\0.0466847222520773\\\\0.0422740152656166\\\\0.0869558114007097\\\\0.0618975729669319\\\\0.0323899858783579\\\\0.0318586781711893\\\\0.0323899858783579\\\\0.0466847222520773\\\\0.0318586781711893\\\\0.0183187399484339\\\\0.019153234255473\\\\0.0183187399484339\\end{matrix}\\right]\\)\n\n\nWe then tabulate this data, and attempt to match nodes by removing the element with the highest value. This corresponds to the nodes from G1 and G2 that have the highest correspondence. We then remove that elements respective row and column as to not match the next nodes with an already matched pair. While we could perform this as a function, we will notice in further examples that some matchings the exact same correspondence value, and thus the function may return arbitrary results. As such, we choose to do this by hand.\n\nI actually like that you’ve gone through this step by step by hand, because it makes it very easy to follow what you’ve done. If I were lecturing on this topic, I might try to create a graphic to illustrate this process in cartoon form.\n\n\nimport math\niso = sp.zeros(5,5)\nfor i in range(5):\n  for j in range(5):\n    iso[i, j] = x[i + 5 * j]\ntemp = iso\niso\n\n\\(\\displaystyle \\left[\\begin{matrix}0.0422740152656166 & 0.0422740152656166 & 0.139369213132264 & 0.0869558114007097 & 0.0466847222520773\\\\0.032389985878358 & 0.032389985878358 & 0.0869558114007097 & 0.0618975729669319 & 0.0318586781711893\\\\0.0191626479069361 & 0.0191626479069361 & 0.0422740152656167 & 0.0323899858783579 & 0.0183187399484339\\\\0.0183187399484339 & 0.0183187399484339 & 0.0466847222520773 & 0.0318586781711893 & 0.019153234255473\\\\0.0191626479069361 & 0.0191626479069361 & 0.0422740152656166 & 0.0323899858783579 & 0.0183187399484339\\end{matrix}\\right]\\)\n\n\nRow Labels: A,B,C,D,E. Column Labels: 1,2,3,4,5. Thus, the best match is immediately A:3\n\ntemp.row_del(0)\ntemp.col_del(2)\ntemp\n\n\\(\\displaystyle \\left[\\begin{matrix}0.032389985878358 & 0.032389985878358 & 0.0618975729669319 & 0.0318586781711893\\\\0.0191626479069361 & 0.0191626479069361 & 0.0323899858783579 & 0.0183187399484339\\\\0.0183187399484339 & 0.0183187399484339 & 0.0318586781711893 & 0.019153234255473\\\\0.0191626479069361 & 0.0191626479069361 & 0.0323899858783579 & 0.0183187399484339\\end{matrix}\\right]\\)\n\n\nNext match is clearly B:4\n\ntemp.row_del(0)\ntemp.col_del(2)\ntemp\n\n\\(\\displaystyle \\left[\\begin{matrix}0.0191626479069361 & 0.0191626479069361 & 0.0183187399484339\\\\0.0183187399484339 & 0.0183187399484339 & 0.019153234255473\\\\0.0191626479069361 & 0.0191626479069361 & 0.0183187399484339\\end{matrix}\\right]\\)\n\n\nNext choice C:1, C:2, E:1, E:2. We choose E:1\n\ntemp.row_del(2)\ntemp.col_del(0)\ntemp\n\n\\(\\displaystyle \\left[\\begin{matrix}0.0191626479069361 & 0.0183187399484339\\\\0.0183187399484339 & 0.019153234255473\\end{matrix}\\right]\\)\n\n\nWe round-off with C:2, D:5. Thus, our final matching is A:3, B:4, C:2, D:5, E:1. This is 100% accurate with our original graph, thus we have found a bijective mapping.\nWe now consider Figure 2.12 with e = {B,C} removed. We will perform the same steps of constructing a surfing matrix and matching.\n\n# G1 = P, G2 = Q\nnp_P = np.matrix([\n    [0,1,1],\n    [1,0,1],\n    [1,1,0]\n], dtype='float')\nnp_Q = np.matrix([\n    [0,1,0,1,0],\n    [1,0,0,1,0],\n    [0,0,0,1,0],\n    [1,1,1,0,1],\n    [0,0,0,1,0]\n], dtype='float')\nPT = np_P.transpose()\nQT = np_Q.transpose()\nfor i in range(5):\n  QT[i] /= np.sum(QT[i])\nfor i in range(3):\n  PT[i] /= np.sum(PT[i])\nP = sp.Matrix(np.transpose(PT))\nQ = sp.Matrix(np.transpose(QT))\nP\n\n\\(\\displaystyle \\left[\\begin{matrix}0 & 0.5 & 0.5\\\\0.5 & 0 & 0.5\\\\0.5 & 0.5 & 0\\end{matrix}\\right]\\)\n\n\n\n# Contruct N = Q x P\nN = sp.zeros(15)\nfor h in range(5):\n  for k in range(5):\n    for i in range(3):\n      for j in range(3):\n        N[i+3*(h), j+3*(k)] = Q[h, k] * P[i, j]\nN\n\n\\(\\displaystyle \\left[\\begin{array}{ccccccccccccccc}0 & 0 & 0 & 0 & 0.25 & 0.25 & 0 & 0 & 0 & 0 & 0.125 & 0.125 & 0 & 0 & 0\\\\0 & 0 & 0 & 0.25 & 0 & 0.25 & 0 & 0 & 0 & 0.125 & 0 & 0.125 & 0 & 0 & 0\\\\0 & 0 & 0 & 0.25 & 0.25 & 0 & 0 & 0 & 0 & 0.125 & 0.125 & 0 & 0 & 0 & 0\\\\0 & 0.25 & 0.25 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.125 & 0.125 & 0 & 0 & 0\\\\0.25 & 0 & 0.25 & 0 & 0 & 0 & 0 & 0 & 0 & 0.125 & 0 & 0.125 & 0 & 0 & 0\\\\0.25 & 0.25 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.125 & 0.125 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.125 & 0.125 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.125 & 0 & 0.125 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.125 & 0.125 & 0 & 0 & 0 & 0\\\\0 & 0.25 & 0.25 & 0 & 0.25 & 0.25 & 0 & 0.5 & 0.5 & 0 & 0 & 0 & 0 & 0.5 & 0.5\\\\0.25 & 0 & 0.25 & 0.25 & 0 & 0.25 & 0.5 & 0 & 0.5 & 0 & 0 & 0 & 0.5 & 0 & 0.5\\\\0.25 & 0.25 & 0 & 0.25 & 0.25 & 0 & 0.5 & 0.5 & 0 & 0 & 0 & 0 & 0.5 & 0.5 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.125 & 0.125 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.125 & 0 & 0.125 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.125 & 0.125 & 0 & 0 & 0 & 0\\end{array}\\right]\\)\n\n\n\n# Solve for X\na = 0.85\nv = 1/15 * sp.Matrix(np.ones(15))\nmat = (sp.eye(15) - a * N)\nb = (1-a)*v\nx = mat.solve(b)\nx\n\n\\(\\displaystyle \\left[\\begin{matrix}0.0649589820860539\\\\0.0649589820860539\\\\0.0649589820860539\\\\0.0649589820860539\\\\0.0649589820860539\\\\0.0649589820860539\\\\0.037351414699481\\\\0.037351414699481\\\\0.037351414699481\\\\0.128712539762264\\\\0.128712539762264\\\\0.128712539762264\\\\0.037351414699481\\\\0.037351414699481\\\\0.037351414699481\\end{matrix}\\right]\\)\n\n\nAs evident from this table, some matchings are equally likely as others. This is likely due to the fact that visually, we can see that P is a subset of, where the subset of Q and P represent triangles with the exact same amount of edges. Thus, flipping 1, 2, 3 with A, B, and C, can result in the exact same matchings, as we demonstrate below.\n\nI appreciate that you’ve explained the issue both in terms of the implementation and its meaning with respect to the graphs.\n\n\niso = sp.zeros(3,5)\nfor i in range(3):\n  for j in range(5):\n    iso[i, j] = x[i + 3 * j]\ntemp = iso\niso\n\n\\(\\displaystyle \\left[\\begin{matrix}0.0649589820860539 & 0.0649589820860539 & 0.037351414699481 & 0.128712539762264 & 0.037351414699481\\\\0.0649589820860539 & 0.0649589820860539 & 0.037351414699481 & 0.128712539762264 & 0.037351414699481\\\\0.0649589820860539 & 0.0649589820860539 & 0.037351414699481 & 0.128712539762264 & 0.037351414699481\\end{matrix}\\right]\\)\n\n\nColumn (Top) Labels: A, B, C, D, E. Row (Side) Labels: 1, 2, 3. Best matchings: [(D:1 or D:2 or D:3), ((A:2 or A:3 or B:2 or B:3) or (A:1 or A:3 or B:1 or B:3) or (A:1 or A:2 or B:1 or B:2)), choose the remaining A or B)]\nWe now will perform IsoRank on a directed graph. We start with the same steps, but adding a correction vector of {1} since the node D in G2 does not have any outgoing edges.\n\n# Solve for A, B\nA_adj = np.matrix([\n    [0,1,0],\n    [0,0,1],\n    [1,0,0]\n], dtype='float')\n# The fourth row of Q should be the zero vector, we elect to replace with the one\n# vector to find our correction vector\nB_adj = np.matrix([\n    [0,0,1,0,0],\n    [1,0,0,0,0],\n    [0,1,0,1,0],\n    [1,1,1,1,1],\n    [0,1,0,0,0]\n], dtype='float')\nfor i in range(5):\n  B_adj[i] /= np.sum(B_adj[i])\nfor i in range(3):\n  A_adj[i] /= np.sum(A_adj[i])\nA = A_adj.transpose()\nB = B_adj.transpose()\n\nWe now construct the surfing matrix, following the same steps.\n\n# Construct C = B x A\n# Contruct N = Q x P\nC = sp.zeros(15)\nfor h in range(5):\n  for k in range(5):\n    for i in range(3):\n      for j in range(3):\n        C[i+3*(h), j+3*(k)] = B[h, k] * A[i, j]\nC\n\n\\(\\displaystyle \\left[\\begin{array}{ccccccccccccccc}0 & 0 & 0 & 0 & 0 & 1.0 & 0 & 0 & 0 & 0 & 0 & 0.2 & 0 & 0 & 0\\\\0 & 0 & 0 & 1.0 & 0 & 0 & 0 & 0 & 0 & 0.2 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 1.0 & 0 & 0 & 0 & 0 & 0 & 0.2 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.5 & 0 & 0 & 0.2 & 0 & 0 & 1.0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0.5 & 0 & 0 & 0.2 & 0 & 0 & 1.0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.5 & 0 & 0 & 0.2 & 0 & 0 & 1.0 & 0\\\\0 & 0 & 1.0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.2 & 0 & 0 & 0\\\\1.0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.2 & 0 & 0 & 0 & 0 & 0\\\\0 & 1.0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.2 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.5 & 0 & 0 & 0.2 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0.5 & 0 & 0 & 0.2 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.5 & 0 & 0 & 0.2 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.2 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.2 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.2 & 0 & 0 & 0 & 0\\end{array}\\right]\\)\n\n\nWe solve for x again and tabulate our data.\n\n# Solve for X\na = 0.85\nv = 1/15 * sp.Matrix(np.ones(15))\nmat = (sp.eye(15) - a * C)\nb = (1-a)*v\nx = mat.solve(b)\nx\n\n\\(\\displaystyle \\left[\\begin{matrix}0.0850597851234713\\\\0.0850597851234713\\\\0.0850597851234713\\\\0.076434208276694\\\\0.076434208276694\\\\0.076434208276694\\\\0.0923915254432319\\\\0.0923915254432319\\\\0.0923915254432319\\\\0.0593571064016549\\\\0.0593571064016549\\\\0.0593571064016549\\\\0.0200907080882813\\\\0.0200907080882813\\\\0.0200907080882813\\end{matrix}\\right]\\)\n\n\nAs demonstrated by the previous problem, we can see that because we have a triangle in G1 and a triangle subset in G2, our best match will be fixing node 1, 2, or 3 to A, C, or B, and subsequently choosing the next two based off that.\n\niso = sp.zeros(3,5)\nfor i in range(3):\n  for j in range(5):\n    iso[i, j] = x[i + 3 * j]\ntemp = iso\niso\n\n\\(\\displaystyle \\left[\\begin{matrix}0.0850597851234713 & 0.076434208276694 & 0.0923915254432319 & 0.0593571064016549 & 0.0200907080882813\\\\0.0850597851234713 & 0.076434208276694 & 0.0923915254432319 & 0.0593571064016549 & 0.0200907080882813\\\\0.0850597851234713 & 0.076434208276694 & 0.0923915254432319 & 0.0593571064016549 & 0.0200907080882813\\end{matrix}\\right]\\)\n\n\nColumn Labels: A,B,C,D,E Row Labels: 1,2,3 Possible matchings: [(C:1 or C:2 or C:3), ((A:2 or A:3), (A:1 or A:3), (A:1 or A:2), choose B:k where k is the remaining row. The best matching is [(C:1 or C:2 or C:3), (A:3 or A:1 or A:2), (B:2 or B:3 or B:1)] choosing each one respectively (i.e. choosing C:1 (index 0) chooses A:3 and B:2)\nWe can see that our correspondence values in our digraph table are noticeably higher than the correspondence values in the previous example. This is likely due to the fact that we now have edge directions in our graph. Thus, we cannot arbitrarily place our nodes in our triangle now, they have a specific order they belong to. Thus, fixing a node now determines the position of the other two, instead of before. Therefore, there is more weight in the first node affixation and is thus why correspendonce values are higher.\n\nYou’ve explained your steps clearly here, and I like how you thoughtfully explored the issue of equal correspondences.\nIn future, I’d like to see a bit more explanation of the math & formulas; what is a surfing matrix? Why does it give us the result we want? etc.\nGrade: M"
  },
  {
    "objectID": "submitted/projects_1_revised/yangsally_Revised.html",
    "href": "submitted/projects_1_revised/yangsally_Revised.html",
    "title": "Project #1 Revisions - Sally Yang",
    "section": "",
    "text": "import numpy as np\n\nPart 1. Perform PLU Factorization on Matrices\n\ndef PLU_factorization(A):\n    n = len(A)\n    L = np.eye(n)  # Initialize L as identity matrix\n    U = np.copy(A)  # To derive U from A, begin with partial pivoting then Gaussian elimination...\n    indx = np.arange(1, n + 1)  # Initialize indx array with 1, 2, ..., n \n    \n    for k in range(n - 1): \n        # Partial pivoting\n        max_index = np.argmax(np.abs(U[k:, k])) + k # Search for the largest entry in the kth column\n        \n        if max_index != k: # Check if the (k,k)th entry is the largest entry or not\n            # If the (k,k)th entry is the largest entry in the column, can just select it as pivot\n            # If not, then exchange the kth row with the row with kth row's largest entry in both U and L\n            # So that the new (k,k)th entry, which is the largest in the column, is a more proper pivot \n            U[[k, max_index]] = U[[max_index, k]]\n            L[[k, max_index], :k] = L[[max_index, k], :k]\n            \n            # Update indx array to reflect the row exchange \n            indx[k], indx[max_index] = indx[max_index], indx[k] \n        \n        for i in range(k + 1, n):\n            # Compute multipliers and perform Gaussian elimination\n            L[i, k] = U[i, k] / U[k, k]\n            U[i, k:] -= L[i, k] * U[k, k:]\n            \n    P = np.eye(n)[indx - 1] # Derive permutation matrix P from indx array\n    return L, U, indx, P\n\nNow test this program with nonsingular matrices. Also, verify that L is indeed lower triangular with U being upper triangular, while PLU does equal the original matrix we inputted into the program.\n\nA = np.array([[2, 1, -1],\n              [2, 3, 3],\n              [4, 1, -3]], dtype=float)\nL, U, indx, P = PLU_factorization(A)\n\nprint(\"Lower triangular matrix L:\")\nprint(L)\nprint(\"\\nUpper triangular matrix U:\")\nprint(U)\nprint(\"\\nPermuted index array indx:\")\nprint(indx)\nprint(\"\\nPermutatation matrix P: \")\nprint(P)\n\nupper_check = np.allclose(U, np.triu(U)) # Check if U is upper triangular\ndef check_condition_upper(x):\n    if x:\n        print(\"U is indeed upper triangular\")\n    else:\n        print(\"U is not upper triangular as desired\")\nprint(check_condition_upper(upper_check))\n\nlower_check = np.allclose(L, np.tril(L)) # Check if L is lower triangular\ndef check_condition_lower(x):\n    if x:\n        print(\"L is indeed lower triangular\")\n    else:\n        print(\"L is not lower triangular as desired\")\nprint(check_condition_lower(lower_check))\n\nimport numpy as np\n\ndef check_product_A(A, P, L, U):\n    product = np.dot(P, np.dot(L, U))\n    return np.array_equal(A, product)\n\nresult = check_product_A(A, P, L, U)\nif result:\n    print(\"A = PLU verified\")\nelse:\n    print(\"A is not equal to PLU. Something's wrong with the program.\")\n\nLower triangular matrix L:\n[[1.  0.  0. ]\n [0.5 1.  0. ]\n [0.5 0.2 1. ]]\n\nUpper triangular matrix U:\n[[ 4.   1.  -3. ]\n [ 0.   2.5  4.5]\n [ 0.   0.  -0.4]]\n\nPermuted index array indx:\n[3 2 1]\n\nPermutatation matrix P: \n[[0. 0. 1.]\n [0. 1. 0.]\n [1. 0. 0.]]\nU is indeed upper triangular\nNone\nL is indeed lower triangular\nNone\nA = PLU verified\n\n\n\nB = np.array([[3, 2, 1],\n              [-4, 1, 1],\n              [0, 1, 1]], dtype=float)\nL, U, indx, P = PLU_factorization(B)\n\nprint(\"Lower triangular matrix L:\")\nprint(L)\nprint(\"\\nUpper triangular matrix U:\")\nprint(U)\nprint(\"\\nPermuted index array indx:\")\nprint(indx)\nprint(\"\\nPermutatation matrix P: \")\nprint(P)\n\nupper_check = np.allclose(U, np.triu(U)) # Check if U is upper triangular\ndef check_condition_upper(x):\n    if x:\n        print(\"U is indeed upper triangular\")\n    else:\n        print(\"U is not upper triangular as desired\")\nprint(check_condition_upper(upper_check))\n\nlower_check = np.allclose(L, np.tril(L)) # Check if L is lower triangular\ndef check_condition_lower(x):\n    if x:\n        print(\"L is indeed lower triangular\")\n    else:\n        print(\"L is not lower triangular as desired\")\nprint(check_condition_lower(lower_check))\n\ndef check_product_B(B, P, L, U):\n    product = np.dot(P, np.dot(L, U))\n    return np.array_equal(B, product)\n\nresult = check_product_B(B, P, L, U)\nif result:\n    print(\"B = PLU verified\")\nelse:\n    print(\"B is not equal to PLU. Something's wrong with the program.\")\n\nLower triangular matrix L:\n[[ 1.          0.          0.        ]\n [-0.75        1.          0.        ]\n [-0.          0.36363636  1.        ]]\n\nUpper triangular matrix U:\n[[-4.          1.          1.        ]\n [ 0.          2.75        1.75      ]\n [ 0.          0.          0.36363636]]\n\nPermuted index array indx:\n[2 1 3]\n\nPermutatation matrix P: \n[[0. 1. 0.]\n [1. 0. 0.]\n [0. 0. 1.]]\nU is indeed upper triangular\nNone\nL is indeed lower triangular\nNone\nB = PLU verified\n\n\n\nC = np.array([[2, 5, 1, 2],\n              [1, 4, 2, -1],\n              [2, -1, 7, 3],\n              [1, 4, -2, 1]], dtype=float)\nL, U, indx, P = PLU_factorization(C)\n\nprint(\"Lower triangular matrix L:\")\nprint(L)\nprint(\"\\nUpper triangular matrix U:\")\nprint(U)\nprint(\"\\nPermuted index array indx:\")\nprint(indx)\nprint(\"\\nPermutatation matrix P: \")\nprint(P)\n\nupper_check = np.allclose(U, np.triu(U)) # Check if U is upper triangular\ndef check_condition_upper(x):\n    if x:\n        print(\"U is indeed upper triangular\")\n    else:\n        print(\"U is not upper triangular as desired\")\nprint(check_condition_upper(upper_check))\n\nlower_check = np.allclose(L, np.tril(L)) # Check if L is lower triangular\ndef check_condition_lower(x):\n    if x:\n        print(\"L is indeed lower triangular\")\n    else:\n        print(\"L is not lower triangular as desired\")\nprint(check_condition_lower(lower_check))\n\ndef check_product_C(C, P, L, U):\n    product = np.dot(P, np.dot(L, U))\n    return np.array_equal(C, product)\n\nresult = check_product_C(C, P, L, U)\nif result:\n    print(\"C = PLU verified\")\nelse:\n    print(\"C is not equal to PLU. Something's wrong with the program.\")\n\nLower triangular matrix L:\n[[ 1.          0.          0.          0.        ]\n [ 1.          1.          0.          0.        ]\n [ 0.5        -0.25        1.          0.        ]\n [ 0.5        -0.25       -0.33333333  1.        ]]\n\nUpper triangular matrix U:\n[[ 2.          5.          1.          2.        ]\n [ 0.         -6.          6.          1.        ]\n [ 0.          0.          3.         -1.75      ]\n [ 0.          0.          0.         -0.33333333]]\n\nPermuted index array indx:\n[1 3 2 4]\n\nPermutatation matrix P: \n[[1. 0. 0. 0.]\n [0. 0. 1. 0.]\n [0. 1. 0. 0.]\n [0. 0. 0. 1.]]\nU is indeed upper triangular\nNone\nL is indeed lower triangular\nNone\nC = PLU verified\n\n\nNow we test the PLU factorization program one last time to see what it produces for a singular matrix. As a matter of fact, LU factorization is literally meaningless and useless for singular matrices, since not all leading minors are nonzero.\n\nThis isn’t quite true. There are some singular matrices which do have LU factorizations – although they are not necessarily easy to find; your algorithm above fails to find the factorization for the matrix below, because there are steps which require division by zero.:\n\ndef check_product_D(D, P, L, U):\n    product = np.dot(P, np.dot(L, U))\n    return np.array_equal(D, product)\nD2 = np.array([[1, 1, 1],\n              [1, 1, 2],\n              [1, 1, 3]], dtype=float)\n\nprint(\"Rank: \" + str(np.linalg.matrix_rank(D2)))\nprint(\"Determinant: \"+str(np.linalg.det(D2)))\n\nUa = np.array([[1, 1, 1],\n              [0, 0, 1],\n              [0, 0, 1]], dtype=float)\nLa = np.array([[1, 0, 0],\n              [1, 1, 0],\n              [1, 1, 1]], dtype=float)\n\nPa = np.eye(3)\n\ncheck_condition_upper(np.allclose(Ua,np.triu(Ua)))\ncheck_condition_lower(np.allclose(La,np.tril(La)))\ncheck_product_D(D2, Pa, La, Ua)\n\nRank: 2\nDeterminant: 0.0\nU is indeed upper triangular\nL is indeed lower triangular\n\n\nTrue\n\n\nThis works because the matrix is rank 2 and two of the principal minors are nonzero. In general, a singular matrix will admit an LU factorization if it is of rank \\(k\\) and the first \\(k\\) principal minors are nonzero; in this example, we need to permute the matrix first to get it into this configuration.\nAnd indeed, if we do that permutation first, then even your algorithm works! Here D3 is a permuted version of D2 above:\n\nD3 = np.array([[1, 1, 1],\n              [2, 1, 1],\n              [3, 1, 1]], dtype=float).T\n\nprint(\"Rank: \" + str(np.linalg.matrix_rank(D3)))\nprint(\"Determinant: \"+str(np.linalg.det(D3)))\n\nLa, Ua, indx, Pa = PLU_factorization(D3)\n\ncheck_condition_upper(np.allclose(Ua,np.triu(Ua)))\ncheck_condition_lower(np.allclose(La,np.tril(La)))\n\ncheck_product_D(D3, Pa, La, Ua)\n\nRank: 2\nDeterminant: 0.0\nU is indeed upper triangular\nL is indeed lower triangular\n\n\nTrue\n\n\n\n\n# find the first two leading minors\nprint(np.linalg.det(D2[2:,2:]))\nprint(np.linalg.det(D2[1:,1:]))\n\n3.0000000000000004\n1.0\n\n\n\nD = np.array([[1, 2, 3],\n              [2, 4, 5],\n              [1, 3, 4]], dtype=float)\nL, U, indx, P = PLU_factorization(D)\n\nprint(\"Lower triangular matrix L:\")\nprint(L)\nprint(\"\\nUpper triangular matrix U:\")\nprint(U)\nprint(\"\\nPermuted index array indx:\")\nprint(indx)\nprint(\"\\nPermutatation matrix P: \")\nprint(P)\n\nupper_check = np.allclose(U, np.triu(U)) # Check if U is upper triangular\ndef check_condition_upper(x):\n    if x:\n        print(\"U is indeed upper triangular\")\n    else:\n        print(\"U is not upper triangular as desired\")\nprint(check_condition_upper(upper_check))\n\nlower_check = np.allclose(L, np.tril(L)) # Check if L is lower triangular\ndef check_condition_lower(x):\n    if x:\n        print(\"L is indeed lower triangular\")\n    else:\n        print(\"L is not lower triangular as desired\")\nprint(check_condition_lower(lower_check))\n\ndef check_product_D(D, P, L, U):\n    product = np.dot(P, np.dot(L, U))\n    return np.array_equal(D, product)\n\nresult = check_product_D(D, P, L, U)\nif result:\n    print(\"D = PLU verified\")\nelse:\n    print(\"D is not equal to PLU. Something's wrong with the program.\")\n\nLower triangular matrix L:\n[[1.  0.  0. ]\n [0.5 1.  0. ]\n [0.5 0.  1. ]]\n\nUpper triangular matrix U:\n[[2.  4.  5. ]\n [0.  1.  1.5]\n [0.  0.  0.5]]\n\nPermuted index array indx:\n[2 3 1]\n\nPermutatation matrix P: \n[[0. 1. 0.]\n [0. 0. 1.]\n [1. 0. 0.]]\nU is indeed upper triangular\nNone\nL is indeed lower triangular\nNone\nD is not equal to PLU. Something's wrong with the program.\n\n\n\nI love that you’ve tested this on a singular matrix to see what happens! It’s good thinking to test the limits of your code.\nOne comment: you have redefined the check_condition_upper and check_condition_lower and other functions multiple times. It would be better to define them once at the beginning of the code and then reuse them as needed.\n\nPart 2. Solving Linear Systems Using LU Factorization\n\ndef solve_linear_system(A, b):\n    n = A.shape[0]\n    \n    # First perform PLU factorization \n    L, U, indx, P = PLU_factorization(A)\n    \n    # Permute vector b using P \n    b_permuted = P.dot(b)\n    \n    # Solve Ly = Pb for y using forward substitution\n    y = np.zeros(n)\n    for i in range(n):\n        y[i] = b_permuted[i] - L[i, :i].dot(y[:i])\n    \n    # Solve Ux = y for x using backward substitution\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = (y[i] - U[i, i+1:].dot(x[i+1:])) / U[i, i]\n    return x\n\nTest this program using different nonsingular matrices with different b vectors.\n\nA = np.array([[2, 1, -1],\n              [2, 3, 3],\n              [4, 1, -3]], dtype=float)\nb = np.array([1, 2, 3])\nx = solve_linear_system(A, b)\n\nprint(\"Solution to the linear system is :\")\nprint(x)\nprint(\"Plug this x into Ax - b, then we get\")\nprint(A@x - b)\n\nSolution to the linear system is :\n[ 2.5 -2.5  1.5]\nPlug this x into Ax - b, then we get\n[4.4408921e-16 0.0000000e+00 4.4408921e-16]\n\n\nAs shown above, the difference/error is considerably small, which may arise from roundoff issues when this program is computing for the solution x.\n\nA = np.array([[2, 1, -1],\n              [2, 3, 3],\n              [4, 1, -3]], dtype=float)\nb = np.array([8, 7, 0])\nx = solve_linear_system(A, b)\n\nprint(\"Solution to the linear system is :\")\nprint(x)\nprint(\"Plug this x into Ax - b, then we get\")\nprint(A@x - b)\n\nSolution to the linear system is :\n[-20.5  32.5 -16.5]\nPlug this x into Ax - b, then we get\n[3.55271368e-15 0.00000000e+00 7.10542736e-15]\n\n\n\nA = np.array([[2, 1, -1],\n              [2, 3, 3],\n              [4, 1, -3]], dtype=float)\nb = np.array([0, 0, 1])\nx = solve_linear_system(A, b)\n\nprint(\"Solution to the linear system is :\")\nprint(x)\nprint(\"Plug this x into Ax - b, then we get\")\nprint(A@x - b)\n\nSolution to the linear system is :\n[ 1.5 -2.   1. ]\nPlug this x into Ax - b, then we get\n[0. 0. 0.]\n\n\n\nB = np.array([[3, 2, 1],\n              [-4, 1, 1],\n              [0, 1, 1]], dtype=float)\nb = np.array([1, 2, 3])\nx = solve_linear_system(B, b)\n\nprint(\"Solution to the linear system is :\")\nprint(x)\nprint(\"Plug this x into Bx - b, then we get\")\nprint(B@x - b)\n\nSolution to the linear system is :\n[ 0.25 -2.75  5.75]\nPlug this x into Bx - b, then we get\n[0. 0. 0.]\n\n\n\nB = np.array([[3, 2, 1],\n              [-4, 1, 1],\n              [0, 1, 1]], dtype=float)\nb = np.array([-2, 0, 8])\nx = solve_linear_system(B, b)\n\nprint(\"Solution to the linear system is :\")\nprint(x)\nprint(\"Plug this x into Bx - b, then we get\")\nprint(B@x - b)\n\nSolution to the linear system is :\n[  2. -16.  24.]\nPlug this x into Bx - b, then we get\n[-3.55271368e-15  0.00000000e+00 -1.77635684e-15]\n\n\n\nB = np.array([[3, 2, 1],\n              [-4, 1, 1],\n              [0, 1, 1]], dtype=float)\nb = np.array([0, 0, 1])\nx = solve_linear_system(B, b)\n\nprint(\"Solution to the linear system is :\")\nprint(x)\nprint(\"Plug this x into Bx - b, then we get\")\nprint(B@x - b)\n\nSolution to the linear system is :\n[ 0.25 -1.75  2.75]\nPlug this x into Bx - b, then we get\n[0. 0. 0.]\n\n\n\nC = np.array([[2, 5, 1, 2],\n              [1, 4, 2, -1],\n              [2, -1, 7, 3],\n              [1, 4, -2, 1]], dtype=float)\nb = np.array([1, 2, 3, 4])\nx = solve_linear_system(C, b)\n\nprint(\"Solution to the linear system is :\")\nprint(x)\nprint(\"Plug this x into Cx - b, then we get\")\nprint(C@x - b)\n\nSolution to the linear system is :\n[ 43.66666667 -10.16666667  -7.5        -14.        ]\nPlug this x into Cx - b, then we get\n[ 0.00000000e+00 -3.55271368e-15 -7.10542736e-15  0.00000000e+00]\n\n\n\nC = np.array([[2, 5, 1, 2],\n              [1, 4, 2, -1],\n              [2, -1, 7, 3],\n              [1, 4, -2, 1]], dtype=float)\nb = np.array([0, 0, 0, 1])\nx = solve_linear_system(C, b)\n\nprint(\"Solution to the linear system is :\")\nprint(x)\nprint(\"Plug this x into Cx - b, then we get\")\nprint(C@x - b)\n\nSolution to the linear system is :\n[ 9.5  -2.25 -1.75 -3.  ]\nPlug this x into Cx - b, then we get\n[ 0.00000000e+00 -8.88178420e-16 -5.32907052e-15  0.00000000e+00]\n\n\nPart 3. Using LU factorization to solve for inverse of (nonsingular) matrix\n\ndef matrix_inverse(A): \n    n = A.shape[0] \n    \n    # Initialize the matrix inverse as empty to update with x_i's later \n    A_inv = np.zeros_like(A, dtype=float)\n    \n    # Perform PLU factorization\n    L, U, indx, P = PLU_factorization(A)\n    \n    # Solve Ax_i = e_i for i =1, 2, ..., n just as described in the project description\n    for i in range(n):\n        e_i = np.eye(n)[:, i]  # e_i is the ith column of n by n identity matrix\n        \n        # Permute e_i according to the permutation matrix P\n        e_i_permuted = P.dot(e_i)\n        \n        # Solve linear system Ax_i = e_i for x_i\n        y = np.zeros(n)\n        for j in range(n):\n            y[j] = e_i_permuted[j] - L[j, :j].dot(y[:j])\n        \n        x_i = np.zeros(n)\n        for j in range(n - 1, -1, -1):\n            x_i[j] = (y[j] - U[j, j+1:].dot(x_i[j+1:])) / U[j, j]\n        \n        # Store x_i as the ith column vector (from left to right) of the matrix inverse\n        A_inv[:, i] = x_i \n    \n    return A_inv # Now the matrix inverse is updated with x_i's obtained from the linear systems solver\n\nNow we test for a few different choices of nonsingular matrix and verify if the result yielded equals the actual inverse (which we compute using the given matrix inverse function embedded in numpy)\n\nA = np.array([[2, 1, -1],\n              [2, 3, 3],\n              [4, 1, -3]], dtype=float)\nA_inv = matrix_inverse(A)\n\nprint(\"Inverse of matrix A:\")\nprint(A_inv)\n\nprint(\"Subtracting the actual inverse of matrix from the inverse we get will then yield:\")\nprint(A_inv - np.linalg.inv(A))\n\nInverse of matrix A:\n[[-3.   0.5  1.5]\n [ 4.5 -0.5 -2. ]\n [-2.5  0.5  1. ]]\nSubtracting the actual inverse of matrix from the inverse we get will then yield:\n[[0. 0. 0.]\n [0. 0. 0.]\n [0. 0. 0.]]\n\n\n\nB = np.array([[3, 2, 1],\n              [-4, 1, 1],\n              [0, 1, 1]], dtype=float)\nB_inv = matrix_inverse(B)\n\nprint(\"Inverse of matrix B:\")\nprint(B_inv)\n\nprint(\"Subtracting the actual inverse of matrix from the inverse we get will then yield:\")\nprint(B_inv - np.linalg.inv(B))\n\nInverse of matrix B:\n[[-0.   -0.25  0.25]\n [ 1.    0.75 -1.75]\n [-1.   -0.75  2.75]]\nSubtracting the actual inverse of matrix from the inverse we get will then yield:\n[[0.00000000e+00 2.77555756e-17 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.11022302e-16 0.00000000e+00]]\n\n\nStill, casting aside the slight errors that most likely arise from roundoff issues, the resulted inverses we get from LU factorization match the actual inverses.\n\nThis is really nice now!\nGrade: E"
  },
  {
    "objectID": "submitted/projects_1_revised/zhengvincent.html",
    "href": "submitted/projects_1_revised/zhengvincent.html",
    "title": "Project 1",
    "section": "",
    "text": "The gas in the tube is excellent now – I love that you included an animation. I’m still a little curious about why you were getting negative numbers.\nI assume that what the nsimplify with the tolerance was doing, in your case, was to set to zero any values that were less than your threshold of 0.01, and any negative numbers you were getting were small enough to be rounded down to zero?\nOf course you are right that negative numbers don’t make physical sense. If you were modelling this system in a different context, you might want to try making either your timesteps or your spatial discritization smaller to see if you can get a more accurate result without having to round down to zero; if that didn’t work, you might have some issue with the code or with the model itself that you would need to address. However, in this assignment, you don’t really have the freedom to change the discretization or the timestep, since you are trying to match the given values. So, I think you did the best you could in this case.\nThe sports ranking section is now well explained, thanks.\nFinal grades: E E M."
  },
  {
    "objectID": "submitted/projects_1_revised/zhengvincent.html#introduction",
    "href": "submitted/projects_1_revised/zhengvincent.html#introduction",
    "title": "Project 1",
    "section": "Introduction",
    "text": "Introduction\nHere is the system of equations that represents the scenario.\n\n\n\nSystems of equations\n\n\nHere is the transition matrix. The market shares after 3 years are shown below given that the initial market shares are equal, which meant that the initial vector is [1/3, 1/3, 1/3].\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0.5 & 0.2 & 0.3\\\\0.2 & 0.6 & 0.1\\\\0.3 & 0.2 & 0.6\\end{matrix}\\right]\\)\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0.339\\\\0.275333333333333\\\\0.385666666666667\\end{matrix}\\right]\\)"
  },
  {
    "objectID": "submitted/projects_1_revised/zhengvincent.html#recommendation",
    "href": "submitted/projects_1_revised/zhengvincent.html#recommendation",
    "title": "Project 1",
    "section": "Recommendation",
    "text": "Recommendation\nAs seen in the below graphs, the effect of launching campaign 2 is better than campaign 1. Note that campaign 0 shows what would happen if no campiagn is launched. In that scenario, company C would have the highest market share whereas we would be stuck in the middle. Launching campaign 1 would not change this situation but we would be closer to C. However, launching campaign 2 would allow us to surpass C and have the highest market share, bringing us to 0.4.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou might have noticed that we’re assuming that the market shares are equal at the beginning. However, the transition matrix is a stable discrete dynamical system which would have the same equilibrium state regardless of the initial vector. To identify a stable system, we need the dominant eigenvalue to be equal to 1 which is the case here. An example is seen below where the initial market shares are [0, 0, 1].\n\n\n{1.00000000000000: 1, 0.461803398874989: 1, 0.238196601125011: 1}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFurther analysis shows that there is a diminishing marginal return if more money is spent on a campaign to take more customers away from B or C. The is shown as the curves are concave down. Additionally, these graphs also show that spending money on taking C’s customers is more valuable than taking B’s because A increases around 0.15 for the former and 0.1 for the latter. Taking C’s customers also makes A the market leader by a significant margin. It should be noted that this also assumes the amount of money it takes to take B’s customers is the same as taking C’s customers.\nAdditionally, although the initial market share doesn’t matter, it is important to note that we’re assuming these events are independent of each other. For example, perhaps customers are more likely to follow the pack if a majority of the population are using companies which means that the market would be more volatile than my simulations suggest. This would also mean that it’s more important for us to have the highest market share at the beginning, making initial market shares more important."
  },
  {
    "objectID": "submitted/projects_1_revised/zhengvincent.html#win-loss-ratio",
    "href": "submitted/projects_1_revised/zhengvincent.html#win-loss-ratio",
    "title": "Project 1",
    "section": "Win Loss Ratio",
    "text": "Win Loss Ratio\nWhen first ranking the teams based on win loss ratio, we see that team 7 is the best team with a ratio of 5 and Team 4 is the worst team with a ratio of 0.2. This is done by summing the ith row to get the number of wins and the ith column to get the number of losses and then dividing to get the win loss ratio of the ith team.\n\n\narray([[0., 1., 0., 1., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 1., 0.],\n       [1., 1., 0., 1., 1., 0., 0.],\n       [0., 0., 0., 0., 1., 0., 0.],\n       [1., 1., 0., 0., 0., 1., 1.],\n       [1., 0., 1., 1., 0., 0., 0.],\n       [1., 1., 1., 1., 0., 1., 0.]])\n\n\n\n\n\n\n\n\n\n\nWins\nLosses\nRatio\n\n\nTeam\n\n\n\n\n\n\n\n7\n5.0\n1.0\n5.0\n\n\n3\n4.0\n2.0\n2.0\n\n\n5\n4.0\n2.0\n2.0\n\n\n6\n3.0\n3.0\n1.0\n\n\n1\n2.0\n4.0\n0.5\n\n\n2\n2.0\n4.0\n0.5\n\n\n4\n1.0\n5.0\n0.2"
  },
  {
    "objectID": "submitted/projects_1_revised/zhengvincent.html#vertex-power",
    "href": "submitted/projects_1_revised/zhengvincent.html#vertex-power",
    "title": "Project 1",
    "section": "Vertex Power",
    "text": "Vertex Power\nTo rank the teams based on vertex power, I took the square of the adjacency matrix and added the adjacency matrix to get the vertex power matrix. The square of the matrix takes into account the wins of the team that the team beat, which thus takes into account the strength the opponent. The rankings are similar but this ranking broke ties that were seen in the win loss ranking.\n\n\nCode\n# directed graph \nG = nx.DiGraph() \nrows, columns = M.shape\nfor i in range(rows): \n for j in range(columns): \n   if M[i][j] == 1: \n      G.add_edge(i+1,j+1) \nnx.draw(G, with_labels=True, node_size = 5000, node_color = 'lightblue', font_size = 20, font_color = 'black')\n\nMsquared = np.dot(M,M)\n\nvertex_power_matrix = Msquared + M\nvertex_power = np.sum(vertex_power_matrix, axis=1)\ndf = pd.DataFrame({'Team': team, 'Vertex Power': vertex_power}).sort_values(by='Vertex Power', ascending=False).set_index('Team')\ndf\n\n\n\n\n\n\n\n\n\n\nVertex Power\n\n\nTeam\n\n\n\n\n\n7\n17.0\n\n\n5\n16.0\n\n\n3\n13.0\n\n\n6\n10.0\n\n\n2\n6.0\n\n\n1\n5.0\n\n\n4\n5.0"
  },
  {
    "objectID": "submitted/projects_1_revised/zhengvincent.html#reverse-page-rank",
    "href": "submitted/projects_1_revised/zhengvincent.html#reverse-page-rank",
    "title": "Project 1",
    "section": "Reverse Page Rank",
    "text": "Reverse Page Rank\nNote that I used alpha = 0.85 and the teleportation vector is [1/7, 1/7, 1/7, 1/7, 1/7, 1/7, 1/7].\nThis ranking is done by taking the transpose of the matrix and applying the equation (I - aP)x = (1-a)v where P is the transition matrix, v is the teleportation vector, and a is the damping factor. The general intuition behind this method is that it tells us the amount of different teams that one specific team would lead to. Applied to this case, we’re ranking by the strength of each team.\nThe most notable change is that 5 first rather than 7 and this difference could be due to the teleportation vector. The intuition behind the teleportation vector in this scenario is that we’re making it equally likely that a team beats another team, thus reducing the impact of the other wins. Additionally, we can note that 5 did beat 7 which beat a lot of other teams and while this is taken into account in the vertex power ranking, it perhaps is emphasized more here.\n\n\nCode\n# reverse graph\n\nM = np.zeros((7,7))\nM[0,1] = 1\nM[6,2] = 1\nM[1,3] = 1\nM[3,4] = 1\nM[2,1] = 1\nM[4,0] = 1\nM[5,0] = 1\nM[2,0] = 1\nM[6,1] = 1\nM[1,5] = 1\nM[2,3] = 1\nM[6,3] = 1\nM[4,6] = 1\nM[5,3] = 1\nM[2,4] = 1\nM[4,5] = 1\nM[6,0] = 1\nM[4,1] = 1\nM[6,5] = 1\nM[0,3] = 1\nM[5,2] = 1\n\nT = M.transpose()\nD = np.identity(7)\nfor i in range(7):\n    D[i,i] = 1/(np.sum(T[i],axis = 0))\n\n# transition matrix\nP = np.dot(T.transpose(),D)\n\n\n# teleportation vector\nv = np.array([1/7, 1/7, 1/7, 1/7, 1/7, 1/7, 1/7])\na = 0.85\n\nlhs = (np.identity(7) - a * P)\nrhs = (1-a) * v\n\nx = np.linalg.solve(lhs, rhs)\ndf = pd.DataFrame({'Team': team, 'Reverse Page Rank': x}).sort_values(by='Reverse Page Rank', ascending=False).set_index('Team')\ndf\n\n\n\n\n\n\n\n\n\n\nReverse Page Rank\n\n\nTeam\n\n\n\n\n\n5\n0.244525\n\n\n7\n0.184189\n\n\n3\n0.176270\n\n\n6\n0.130332\n\n\n4\n0.125352\n\n\n2\n0.079666\n\n\n1\n0.059667"
  },
  {
    "objectID": "submitted/projects_1_revised/zhengvincent.html#weighted-vertex-power",
    "href": "submitted/projects_1_revised/zhengvincent.html#weighted-vertex-power",
    "title": "Project 1",
    "section": "Weighted Vertex Power",
    "text": "Weighted Vertex Power\nWith the added weights to the adjacency matrix, the ranking becomes more accurate. Weights help take into consideration how convincingly the team won while punishing those that were a closer game. Rankings show again, and more convincingly, that 7 is at the top with 5 not too close behind but the rest of the teams are more spread out. This helps show that more nuance in the skill disparity between the teams.\nNote I couldn’t figure out how to rename the nodes to 1-7 instead of 0-6.\n\n\nCode\n# weighted adjacency matrix\nM = np.zeros((7,7))\nM[0,1] = 4\nM[6,2] = 8\nM[1,3] = 7\nM[3,4] = 3\nM[2,1] = 7\nM[4,0] = 7\nM[5,0] = 23\nM[2,0] = 15\nM[6,1] = 6\nM[1,5] = 18\nM[2,3] = 13\nM[6,3] = 14\nM[4,6] = 7\nM[5,3] = 13\nM[2,4] = 7\nM[4,5] = 18\nM[6,0] = 45\nM[4,1] = 10\nM[6,5] = 19\nM[0,3] = 14\nM[5,2] = 13\n\ndisplay(M)\n\nG = nx.from_numpy_array(np.matrix(M), create_using=nx.DiGraph)\n\n\nlayout = nx.spring_layout(G)\n\nnx.draw(G, layout, with_labels=True, node_size = 1000, node_color = 'lightblue', font_size = 20, font_color = 'black')\nlabels = nx.get_edge_attributes(G, \"weight\")\nnx.draw_networkx_edge_labels(G, pos=layout, edge_labels=labels)\nplt.show()\n\nMsquared = np.dot(M,M)\n\nvertex_power_matrix = Msquared + M\nvertex_power = np.sum(vertex_power_matrix, axis=1)\ndf = pd.DataFrame({'Team': team, 'Vertex Power': vertex_power}).sort_values(by='Vertex Power', ascending=False).set_index('Team')\ndf\n\n\narray([[ 0.,  4.,  0., 14.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  7.,  0., 18.,  0.],\n       [15.,  7.,  0., 13.,  7.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  3.,  0.,  0.],\n       [ 7., 10.,  0.,  0.,  0., 18.,  7.],\n       [23.,  0., 13., 13.,  0.,  0.,  0.],\n       [45.,  6.,  8., 14.,  0., 19.,  0.]])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVertex Power\n\n\nTeam\n\n\n\n\n\n7\n2361.0\n\n\n5\n1944.0\n\n\n6\n1048.0\n\n\n2\n928.0\n\n\n3\n820.0\n\n\n1\n160.0\n\n\n4\n129.0"
  },
  {
    "objectID": "submitted/projects_1_revised/proj 1 XW/proj1XW.html",
    "href": "submitted/projects_1_revised/proj 1 XW/proj1XW.html",
    "title": "Xiao Wei",
    "section": "",
    "text": "Markov Chains\nFor this problem, we are given the transition stochastic matrices of market shares for a market composed of three players. The exercise asks us to find steady state of market shares under a hypothetical scenario where we have to choose between two different transition matrices.\n\nimport sympy as sym\nsb = sym.Symbol\nimport pandas as pd\n\nThe transition matrix\n\nM = sym.Matrix([[.5, .2, .3],[.2, .6, .1], [.3, .2, .6]])\nM\n\n\\(\\displaystyle \\left[\\begin{matrix}0.5 & 0.2 & 0.3\\\\0.2 & 0.6 & 0.1\\\\0.3 & 0.2 & 0.6\\end{matrix}\\right]\\)\n\n\nThe market shares 3 years later if the starting state is the market split evenly between the 3 players. I was a bit surprised that company 2 got the least amount of share instead of company 1 since company 1 has the smallest value in the diagonal meaning they keep less of their own customers in each subsequent state.\n\nM**3 * sym.Matrix([0.33333, 0.33333, 0.33333])\n\n\\(\\displaystyle \\left[\\begin{matrix}0.33899661\\\\0.27533058\\\\0.38566281\\end{matrix}\\right]\\)\n\n\nFirst campaign transition matrix\n\nMc1 = sym.Matrix([[.5, .32, .3],[.2, .48, .1], [.3, .2, .6]])\nMc1\n\n\\(\\displaystyle \\left[\\begin{matrix}0.5 & 0.32 & 0.3\\\\0.2 & 0.48 & 0.1\\\\0.3 & 0.2 & 0.6\\end{matrix}\\right]\\)\n\n\nTo calculate the steady state of the market shares, we multiply the transition matrix large number of times\n\nMc1**50 * sym.Matrix([0.33333, 0.33333, 0.33333])\n\n\\(\\displaystyle \\left[\\begin{matrix}0.380562995951417\\\\0.22266983805668\\\\0.396757165991903\\end{matrix}\\right]\\)\n\n\nAs a check we can use the transition matrix one more time to see that the market share stays the same\n\nMc1**51 * sym.Matrix([0.33333, 0.33333, 0.33333])\n\n\\(\\displaystyle \\left[\\begin{matrix}0.380562995951417\\\\0.22266983805668\\\\0.396757165991903\\end{matrix}\\right]\\)\n\n\nI’m not sure why this solution doesn’t work, as it would serve as a good proof of the steady state. The steady state is the vector in which the transition matrix applied to the vector results in the same vector.\nformula:\nAx = x\nA(x - I) = 0\n\nI’ve just spent a while playing around with this! The matrix is ill-conditioned, so the solver is having trouble. It ends up deciding that the matrix is singular and giving you the trivial solution. But it’s not singular, it’s just very close to it.\nIn any case, you do get the right result just by finding the eigenvectors of Mc1; the eigenvector corresponding to the eigenvalue 1 is the steady state you found. So odd! But it’s a good reminder that numerical linear algebra can be a bit of a minefield.\n\n\n(sym.eye(3) - Mc1).gauss_jordan_solve(sym.zeros(3, 1))[0]\n\n\\(\\displaystyle \\left[\\begin{matrix}0\\\\0\\\\0\\end{matrix}\\right]\\)\n\n\nWe can also see that the intial state doesn’t matter when calculating the steady state\n\nMc1**50 * sym.Matrix([0, 1, 0])\n\n\\(\\displaystyle \\left[\\begin{matrix}0.380566801619433\\\\0.222672064777328\\\\0.396761133603239\\end{matrix}\\right]\\)\n\n\nsecond campaign transition matrix\n\nMc2 = sym.Matrix([[.5, .32, .3+.6*.2],[.2, .48, .1], [.3, .2, .6*.8]])\nMc2\n\n\\(\\displaystyle \\left[\\begin{matrix}0.5 & 0.32 & 0.42\\\\0.2 & 0.48 & 0.1\\\\0.3 & 0.2 & 0.48\\end{matrix}\\right]\\)\n\n\n\nMc2**50 * sym.Matrix([0.33333, 0.33333, 0.33333])\n\n\\(\\displaystyle \\left[\\begin{matrix}0.431422288077188\\\\0.230872949689869\\\\0.337694762232943\\end{matrix}\\right]\\)\n\n\n\nSo you’ve shown here that the long-term behavior is the same for each of the two initial distributions you chose. To convince yourself that this is truly universal, though, you’d want to try a number of different initial distributions, not just two.\n(Of course, using the concepts of eigenvalues and eigenvectors, we can see that the steady state you found corresponds to the dominant eigenvector of the matrix. That means that you will tend to arrive there unless your initial distribution is orthogonal to that eigenvector, exactly equal to one of the other eigenvectors. You wouldn’t have found one of these other eigenvectors anyways just trying initial distributions by chance…)\n\n\nReport\nIt is better to run campaign 2 as the steady state vector for campaign 2 is 43% vs 38% for campaign 1. It does not matter what the starting state is the end state after many iterations would be the same. As such the second campaign has insurance company A with a higher market share.\nBelow is the chart for the net gain our company A gains from either company B for marketing campaign 1 or company C for marketing campaign 2 for each subsequent iteration.\nImplicit assumption of this model is that the effectiveness of each marketing campaign in driving consumer behavior remains constant across each time period. This is most likely not the case in reality. In either case though it looks like marketing campaign 2 is the better campaign unless company B has a lot more customers than company C in the initial period.\nThe below code iterates through states and calculates in each state how much company A gains from B or company C depending on whether marketing strategy 1 or 2 was chosen\n\nc1_state = sym.Matrix([0.33333, 0.33333, 0.33333])\nc2_state = sym.Matrix([0.33333, 0.33333, 0.33333])\ngains = pd.DataFrame([], columns=['Percentage from B', 'Percentage from C'])\ngain_from_b = 0\ngain_from_c = 0\nfor step in range(21):\n    gains.loc[int(step)] = (gain_from_b, gain_from_c)\n    next_state_c1 = Mc1 * c1_state\n    next_state_c2 = Mc2 * c2_state\n    gain_from_b = next_state_c1[1] * Mc1[0, 1] - c1_state[0] * Mc1[1, 0]\n    gain_from_c = next_state_c2[2] * Mc2[0, 2] - c2_state[0] * Mc1[2, 0]\n    c1_state = next_state_c1\n    c2_state = next_state_c2\n\n\ngains = gains.astype(float)\n\n\ngains.plot(kind='line', xlabel='state iterations', ylabel=\"proportion from\", title=\"Market Share from B or C\")\n\n\n\n\n\n\n\n\n\nThis meets expectations now! Grade: M\n\n\n\n\nDiffusion\nFor the diffusion project, we are given the problem of finding the concentration at each of the 7 segments of a tube at each time step. However, we are not given the initial amount of gas released into the middle of the tube and are only given two measurements at time step t240 and t270 as seen below. We must use linear algebra to approximate the diffusion equation using a transition matrix and guess the values of the intial concentration as well as the diffusion constant.\n\nimport sympy as sym\nimport numpy as np\nimport pandas as pd\nsb = sym.Symbol\n\n\n\n\nimage.png\n\n\nIn the below cell I create the transition matrix for each segment of the tube. The transition equation was part of equation 3.10\nThere are a total of 7 segments measured which results in a 7x7 transition matrix, each segment is a function of itself and its adjoining segments and the constants D, h.\n\nN=7\nM = sym.zeros(N)\nfor i in range(N):\n  if (i&gt;0):\n      M[i-1,i]=1\n  M[i,i]=-2\n  if (i&lt;N-1):\n      M[i+1,i]=1\n  # M[0, i] = 0\n  # M[6, i] = 0\n#   M[i, 0] = 0\n#   M[i, 6] = 0\n# M[5, 6] = 0\nM\n\n\\(\\displaystyle \\left[\\begin{matrix}-2 & 1 & 0 & 0 & 0 & 0 & 0\\\\1 & -2 & 1 & 0 & 0 & 0 & 0\\\\0 & 1 & -2 & 1 & 0 & 0 & 0\\\\0 & 0 & 1 & -2 & 1 & 0 & 0\\\\0 & 0 & 0 & 1 & -2 & 1 & 0\\\\0 & 0 & 0 & 0 & 1 & -2 & 1\\\\0 & 0 & 0 & 0 & 0 & 1 & -2\\end{matrix}\\right]\\)\n\n\nHere are the starting states for T270 and T240 according to the textbook (see snippet above)\n\nt270 = sym.Matrix([0.0, 0.051, 1.21, 3.48, 1.21, 0.051, 0.0])\nt240 = sym.Matrix([0.0, 0.032, 1.23, 3.69, 1.23, 0.032, 0.0])\n\nGoing forward from 240 to 270, I tried various figures for D here and settled on the one below. Note the 36 is supposed to represent 1/h^2\ntransition formula\nt_i+1 = t_i + M * t_i * D/h^2\n\nDh2 = 0.00004 * 36\n\ncurrent = t240\nfor step in range(240, 271):\n    next_step = current + M*current * Dh2\n    if step % 10 == 0:\n        print(step, current)\n        print('\\n')\n    current = next_step\n\n240 Matrix([[0], [0.0320000000000000], [1.23000000000000], [3.69000000000000], [1.23000000000000], [0.0320000000000000], [0]])\n\n\n250 Matrix([[0.000562471243392380], [0.0486924553785851], [1.24759546552771], [3.62029245984376], [1.24759546552771], [0.0486924553785851], [0.000562471243392380]])\n\n\n260 Matrix([[0.00134480689310895], [0.0651618382102242], [1.26395165777253], [3.55305081122375], [1.26395165777253], [0.0651618382102242], [0.00134480689310895]])\n\n\n270 Matrix([[0.00233753410690556], [0.0814003695987503], [1.27913496114619], [3.48817057984740], [1.27913496114619], [0.0814003695987503], [0.00233753410690556]])\n\n\n\n\nNow that I have settled on a value for D. I will move backwards from 270 to 0. Output is every 30 steps and 0. As gone over in class, the transition matrix and multiplier needs to be taken with an inverse since we are now moving backwards instead of forwards as in the equation given by the text book.\ntransition formula:\nt_i+1 = t_i + M * t_i * D/h^2\nt_i+1 = t_i\nt_i = (I + MD/h2)-1 t_i+1\n\nDh2 = 0.00004 * 36\n\ncurrent = t270\nfor step in range(270, -1, -1):\n    prev_step = (sym.eye(current.shape[0]) + M* Dh2).inv()*current \n    if step % 40 == 0:\n        print(step, current)\n        print('\\n')\n    current = sym.Matrix(np.maximum(prev_step, 0))\nprint(step, current)\ninit_state = current\n\n240 Matrix([[0], [0.00212689488945927], [1.15621985395050], [3.68754809461451], [1.15621985395048], [0.00212689488945964], [0]])\n\n\n200 Matrix([[9.47143103797723e-5], [0], [1.06279149507993], [4.00254036435968], [1.06279149507989], [0], [9.47143103797697e-5]])\n\n\n160 Matrix([[0.000194835210686209], [0], [0.937128531983600], [4.36938971176471], [0.937128531983537], [0], [0.000194835210686203]])\n\n\n120 Matrix([[0.000294410849364334], [0], [0.771792042493187], [4.79882186271244], [0.771792042493099], [0], [0.000294410849364323]])\n\n\n80 Matrix([[0.000389481701770366], [0], [0.557712169649991], [5.30388057840637], [0.557712169649879], [0], [0.000389481701770348]])\n\n\n40 Matrix([[0.000474717247747708], [0], [0.283827585787304], [5.90043223273764], [0.283827585787171], [0], [0.000474717247747682]])\n\n\n0 Matrix([[0.000543077482862398], [8.59904987581519e-5], [0], [6.60726130357147], [0], [8.59904987583709e-5], [0.000543077482862360]])\n\n\n0 Matrix([[0.000544502698002186], [9.92726600399977e-5], [0], [6.62637281796703], [0], [9.92726600402174e-5], [0.000544502698002148]])\n\n\nI assume the tube is empty except for the initial release in the middle at state 0. The initial amount “f”, is the max of step 0 from above. I output the amounts for time 210 and 300 as per the assignment instructions.\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\ncurrent = sym.zeros(7, 1)\ncurrent[3] = np.max(init_state)\nplot_df = pd.DataFrame([], columns=['0', '1/6', '1/3', '1/2'], dtype=np.float64)\nfor step in range(0, 301):\n    plot_df.loc[step] = current[:4]\n    next_step = current + M*current * Dh2\n    if step==0 or step ==210 or step == 300:\n        print(step, current)\n        print('\\n')\n    current = next_step\nplot_df.loc[step] = current[:4]\n\n0 Matrix([[0], [0], [0], [6.62637281796703], [0], [0], [0]])\n\n\n210 Matrix([[0.0168682745749364], [0.170506299982379], [1.14711106859719], [3.95456622586664], [1.14711106859719], [0.170506299982379], [0.0168682745749364]])\n\n\n300 Matrix([[0.0388222436755021], [0.277312234731033], [1.32425109802597], [3.33575720208435], [1.32425109802597], [0.277312234731033], [0.0388222436755021]])\n\n\n\n\nSince the readings are symmetrical across the middle I only plot half the tube including the middle. Each line below is a reading at each segment from 0 to 1/2.\n\nplot_df = plot_df.astype(np.float_)\nplot_df.plot(kind='line', xlabel=\"step\", ylabel='concentration', title=\"concentration at each time step for segment measurements\")\n\n\n\n\n\n\n\n\nIt is expected that the center curve looks like an logarithmic decay, the 1/3 curve looks like a logarithmic increase which I suppose will decrease at the point. Using Linear Algebra does seem like a much easier approach than the actual Physics of computing the diffusion constant as well as the intial concentration.\n\nThis is a great solution! You’ve done a good job of explaining your reasoning and the steps you took to arrive at your solution. The plots are clear and easy to understand.\nGrade: E\n\n\n\nSports Ranking\nThis problem explores loading tournament data into graph format and producing rankings using matrix arithmetic/algebra. It explores using simple rank, power rank, and inverse pagerank as different ranking algorithms and compares/contrasts each method.\n\nimport sympy as sym\nimport numpy as np\nimport pandas as pd\nimport networkx as nx\nsb = sym.Symbol\nsm = sym.Matrix\n\nLoading the data into networkx\n\nG = nx.DiGraph()\nG.add_nodes_from([1, 2, 3, 4, 5, 6, 7])\nG.add_edges_from([(1, 2), (7, 3), (2, 4), (4, 5), (3, 2), (5, 1), (6, 1), (3, 1), (7, 2), (2, 6), (3, 4), (7, 4)\n                  , (5, 7), (6, 4), (3, 5), (5, 6), (7, 1), (5, 2), (7, 6), (1, 4), (6, 3)])\n\nDraw the graph\n\nnx.draw(G, with_labels=True)\n\n\n\n\n\n\n\n\n\nadjacency matrix\n\nadj_mat = nx.to_numpy_array(G)\nadj_mat\n\narray([[0., 1., 0., 1., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 1., 0.],\n       [1., 1., 0., 1., 1., 0., 0.],\n       [0., 0., 0., 0., 1., 0., 0.],\n       [1., 1., 0., 0., 0., 1., 1.],\n       [1., 0., 1., 1., 0., 0., 0.],\n       [1., 1., 1., 1., 0., 1., 0.]])\n\n\n\n\nwin-loss record\n\nwins = adj_mat.sum(axis=1)\nwins\n\narray([2., 2., 4., 1., 4., 3., 5.])\n\n\n\nlosses = adj_mat.sum(axis=0)\nlosses\n\narray([4., 4., 2., 5., 2., 3., 1.])\n\n\n\n\nSimple Rank\nHere is the simple ranking for each team/node by wins\n\nteam = pd.DataFrame({'wins': wins, 'losses': losses}, index=range(1, 8))\nteam.sort_values(['wins', 'losses'], ascending=[False, True])\n\n\n\n\n\n\n\n\n\nwins\nlosses\n\n\n\n\n7\n5.0\n1.0\n\n\n3\n4.0\n2.0\n\n\n5\n4.0\n2.0\n\n\n6\n3.0\n3.0\n\n\n1\n2.0\n4.0\n\n\n2\n2.0\n4.0\n\n\n4\n1.0\n5.0\n\n\n\n\n\n\n\n\n\nVertex Power\nPower ranking: A + A^2\n\npower_rank = (adj_mat + adj_mat**2).sum(axis=1)\npd.Series(power_rank, index=range(1, 8)).sort_values(ascending=False)\n\n7    10.0\n3     8.0\n5     8.0\n6     6.0\n1     4.0\n2     4.0\n4     2.0\ndtype: float64\n\n\n\n\nReverse PageRank\nHere I transform the adjacency matrix into a stochastic transition matrix by dividing each column by the sum of degrees\n\ncol_sums = adj_mat.sum(axis=0)\nP = np.zeros(adj_mat.shape)\nfor i in range(adj_mat.shape[0]):\n    P[i, :] = adj_mat[i, :] / col_sums\npd.DataFrame(P)\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n\n\n\n\n0\n0.00\n0.25\n0.0\n0.2\n0.0\n0.000000\n0.0\n\n\n1\n0.00\n0.00\n0.0\n0.2\n0.0\n0.333333\n0.0\n\n\n2\n0.25\n0.25\n0.0\n0.2\n0.5\n0.000000\n0.0\n\n\n3\n0.00\n0.00\n0.0\n0.0\n0.5\n0.000000\n0.0\n\n\n4\n0.25\n0.25\n0.0\n0.0\n0.0\n0.333333\n1.0\n\n\n5\n0.25\n0.00\n0.5\n0.2\n0.0\n0.000000\n0.0\n\n\n6\n0.25\n0.25\n0.5\n0.2\n0.0\n0.333333\n0.0\n\n\n\n\n\n\n\n\nAs per instructions, 85% chance of using the transition matrix and 15% chance of teleportation to a random node\n\nalpha = 0.85\nv = sym.ones(adj_mat.shape[0])[:, 0] / adj_mat.shape[0]\nv\n\n\\(\\displaystyle \\left[\\begin{matrix}\\frac{1}{7}\\\\\\frac{1}{7}\\\\\\frac{1}{7}\\\\\\frac{1}{7}\\\\\\frac{1}{7}\\\\\\frac{1}{7}\\\\\\frac{1}{7}\\end{matrix}\\right]\\)\n\n\nFormula:\n(I - aP)\n\npg_mat = (sym.eye(P.shape[0]) - alpha * P)\npg_mat\n\n\\(\\displaystyle \\left[\\begin{matrix}1.0 & -0.2125 & 0 & -0.17 & 0 & 0 & 0\\\\0 & 1.0 & 0 & -0.17 & 0 & -0.283333333333333 & 0\\\\-0.2125 & -0.2125 & 1.0 & -0.17 & -0.425 & 0 & 0\\\\0 & 0 & 0 & 1.0 & -0.425 & 0 & 0\\\\-0.2125 & -0.2125 & 0 & 0 & 1.0 & -0.283333333333333 & -0.85\\\\-0.2125 & 0 & -0.425 & -0.17 & 0 & 1.0 & 0\\\\-0.2125 & -0.2125 & -0.425 & -0.17 & 0 & -0.283333333333333 & 1.0\\end{matrix}\\right]\\)\n\n\nsolving for:\n(I - aP)x = (1 - a)v\nThe output is the reverse page rank ranking\n\npd.Series(list(pg_mat.gauss_jordan_solve(v* (1-alpha))[0]), index=range(1, 8)).sort_values(ascending=False)\n\n5     0.244524719927139\n7     0.184188693730917\n3     0.176269636041093\n6     0.130332241204918\n4     0.125351577397605\n2    0.0796658079275577\n1    0.0596673237707704\ndtype: object\n\n\n\n\nWeighted graph power ranking\nI add weights using networkx and output the resulting weighted adjacency matrix\n\nGw = nx.DiGraph()\nGw.add_nodes_from([1, 2, 3, 4, 5, 6, 7])\nGw.add_weighted_edges_from([(1, 2, 4), (7, 3, 8), (2, 4, 7), (4, 5, 3), (3, 2, 7), (5, 1, 7), (6, 1, 23)\n                            , (3, 1, 15), (7, 2, 6), (2, 6, 18), (3, 4, 13), (7, 4, 14)\n                  , (5, 7, 7), (6, 4, 13), (3, 5, 7), (5, 6, 18), (7, 1, 45), (5, 2, 10), (7, 6, 19), (1, 4, 13), (6, 3, 13)])\nadj_matw = nx.to_numpy_array(Gw)\nadj_matw\n\narray([[ 0.,  4.,  0., 13.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  7.,  0., 18.,  0.],\n       [15.,  7.,  0., 13.,  7.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  3.,  0.,  0.],\n       [ 7., 10.,  0.,  0.,  0., 18.,  7.],\n       [23.,  0., 13., 13.,  0.,  0.,  0.],\n       [45.,  6.,  8., 14.,  0., 19.,  0.]])\n\n\nThe power rank using weighted edges\n\npd.Series((adj_matw + adj_matw**2).sum(axis=1), index=range(1, 8)).sort_values(ascending=False)\n\n7    2774.0\n6     916.0\n5     564.0\n3     534.0\n2     398.0\n1     202.0\n4      12.0\ndtype: float64\n\n\n\nThis is fine. I would have liked to see a bit more explanation of the different ranking methods and how they differ from each other, with specific examples from the data…\nGrade: M"
  },
  {
    "objectID": "submitted/projects_1_revised/proj1XW.html",
    "href": "submitted/projects_1_revised/proj1XW.html",
    "title": "Xiao Wei",
    "section": "",
    "text": "Markov Chains\nFor this problem, we are given the transition stochastic matrices of market shares for a market composed of three players. The exercise asks us to find steady state of market shares under a hypothetical scenario where we have to choose between two different transition matrices.\n\nimport sympy as sym\nsb = sym.Symbol\nimport pandas as pd\n\nThe transition matrix\n\nM = sym.Matrix([[.5, .2, .3],[.2, .6, .1], [.3, .2, .6]])\nM\n\n\\(\\displaystyle \\left[\\begin{matrix}0.5 & 0.2 & 0.3\\\\0.2 & 0.6 & 0.1\\\\0.3 & 0.2 & 0.6\\end{matrix}\\right]\\)\n\n\nThe market shares 3 years later if the starting state is the market split evenly between the 3 players. I was a bit surprised that company 2 got the least amount of share instead of company 1 since company 1 has the smallest value in the diagonal meaning they keep less of their own customers in each subsequent state.\n\nM**3 * sym.Matrix([0.33333, 0.33333, 0.33333])\n\n\\(\\displaystyle \\left[\\begin{matrix}0.33899661\\\\0.27533058\\\\0.38566281\\end{matrix}\\right]\\)\n\n\nFirst campaign transition matrix\n\nMc1 = sym.Matrix([[.5, .32, .3],[.2, .48, .1], [.3, .2, .6]])\nMc1\n\n\\(\\displaystyle \\left[\\begin{matrix}0.5 & 0.32 & 0.3\\\\0.2 & 0.48 & 0.1\\\\0.3 & 0.2 & 0.6\\end{matrix}\\right]\\)\n\n\nTo calculate the steady state of the market shares, we multiply the transition matrix large number of times\n\nMc1**50 * sym.Matrix([0.33333, 0.33333, 0.33333])\n\n\\(\\displaystyle \\left[\\begin{matrix}0.380562995951417\\\\0.22266983805668\\\\0.396757165991903\\end{matrix}\\right]\\)\n\n\nAs a check we can use the transition matrix one more time to see that the market share stays the same\n\nMc1**51 * sym.Matrix([0.33333, 0.33333, 0.33333])\n\n\\(\\displaystyle \\left[\\begin{matrix}0.380562995951417\\\\0.22266983805668\\\\0.396757165991903\\end{matrix}\\right]\\)\n\n\nI’m not sure why this solution doesn’t work, as it would serve as a good proof of the steady state. The steady state is the vector in which the transition matrix applied to the vector results in the same vector.\nformula:\nAx = x\nA(x - I) = 0\n\nI’ve just spent a while playing around with this! The matrix is ill-conditioned, so the solver is having trouble. It ends up deciding that the matrix is singular and giving you the trivial solution. But it’s not singular, it’s just very close to it.\nIn any case, you do get the right result just by finding the eigenvectors of Mc1; the eigenvector corresponding to the eigenvalue 1 is the steady state you found. So odd! But it’s a good reminder that numerical linear algebra can be a bit of a minefield.\n\n\n(sym.eye(3) - Mc1).gauss_jordan_solve(sym.zeros(3, 1))[0]\n\n\\(\\displaystyle \\left[\\begin{matrix}0\\\\0\\\\0\\end{matrix}\\right]\\)\n\n\nWe can also see that the intial state doesn’t matter when calculating the steady state\n\nMc1**50 * sym.Matrix([0, 1, 0])\n\n\\(\\displaystyle \\left[\\begin{matrix}0.380566801619433\\\\0.222672064777328\\\\0.396761133603239\\end{matrix}\\right]\\)\n\n\nsecond campaign transition matrix\n\nMc2 = sym.Matrix([[.5, .32, .3+.6*.2],[.2, .48, .1], [.3, .2, .6*.8]])\nMc2\n\n\\(\\displaystyle \\left[\\begin{matrix}0.5 & 0.32 & 0.42\\\\0.2 & 0.48 & 0.1\\\\0.3 & 0.2 & 0.48\\end{matrix}\\right]\\)\n\n\n\nMc2**50 * sym.Matrix([0.33333, 0.33333, 0.33333])\n\n\\(\\displaystyle \\left[\\begin{matrix}0.431422288077188\\\\0.230872949689869\\\\0.337694762232943\\end{matrix}\\right]\\)\n\n\n\nSo you’ve shown here that the long-term behavior is the same for each of the two initial distributions you chose. To convince yourself that this is truly universal, though, you’d want to try a number of different initial distributions, not just two.\n(Of course, using the concepts of eigenvalues and eigenvectors, we can see that the steady state you found corresponds to the dominant eigenvector of the matrix. That means that you will tend to arrive there unless your initial distribution is orthogonal to that eigenvector, exactly equal to one of the other eigenvectors. You wouldn’t have found one of these other eigenvectors anyways just trying initial distributions by chance…)\n\n\nReport\nIt is better to run campaign 2 as the steady state vector for campaign 2 is 43% vs 38% for campaign 1. It does not matter what the starting state is the end state after many iterations would be the same. As such the second campaign has insurance company A with a higher market share.\nBelow is the chart for the net gain our company A gains from either company B for marketing campaign 1 or company C for marketing campaign 2 for each subsequent iteration.\nImplicit assumption of this model is that the effectiveness of each marketing campaign in driving consumer behavior remains constant across each time period. This is most likely not the case in reality. In either case though it looks like marketing campaign 2 is the better campaign unless company B has a lot more customers than company C in the initial period.\nThe below code iterates through states and calculates in each state how much company A gains from B or company C depending on whether marketing strategy 1 or 2 was chosen\n\nc1_state = sym.Matrix([0.33333, 0.33333, 0.33333])\nc2_state = sym.Matrix([0.33333, 0.33333, 0.33333])\ngains = pd.DataFrame([], columns=['Percentage from B', 'Percentage from C'])\ngain_from_b = 0\ngain_from_c = 0\nfor step in range(21):\n    gains.loc[int(step)] = (gain_from_b, gain_from_c)\n    next_state_c1 = Mc1 * c1_state\n    next_state_c2 = Mc2 * c2_state\n    gain_from_b = next_state_c1[1] * Mc1[0, 1] - c1_state[0] * Mc1[1, 0]\n    gain_from_c = next_state_c2[2] * Mc2[0, 2] - c2_state[0] * Mc1[2, 0]\n    c1_state = next_state_c1\n    c2_state = next_state_c2\n\n\ngains = gains.astype(float)\n\n\ngains.plot(kind='line', xlabel='state iterations', ylabel=\"proportion from\", title=\"Market Share from B or C\")\n\n\n\n\n\n\n\n\n\nThis meets expectations now! Grade: M\n\n\n\n\nDiffusion\nFor the diffusion project, we are given the problem of finding the concentration at each of the 7 segments of a tube at each time step. However, we are not given the initial amount of gas released into the middle of the tube and are only given two measurements at time step t240 and t270 as seen below. We must use linear algebra to approximate the diffusion equation using a transition matrix and guess the values of the intial concentration as well as the diffusion constant.\n\nimport sympy as sym\nimport numpy as np\nimport pandas as pd\nsb = sym.Symbol\n\n\n\n\nimage.png\n\n\nIn the below cell I create the transition matrix for each segment of the tube. The transition equation was part of equation 3.10\nThere are a total of 7 segments measured which results in a 7x7 transition matrix, each segment is a function of itself and its adjoining segments and the constants D, h.\n\nN=7\nM = sym.zeros(N)\nfor i in range(N):\n  if (i&gt;0):\n      M[i-1,i]=1\n  M[i,i]=-2\n  if (i&lt;N-1):\n      M[i+1,i]=1\n  # M[0, i] = 0\n  # M[6, i] = 0\n#   M[i, 0] = 0\n#   M[i, 6] = 0\n# M[5, 6] = 0\nM\n\n\\(\\displaystyle \\left[\\begin{matrix}-2 & 1 & 0 & 0 & 0 & 0 & 0\\\\1 & -2 & 1 & 0 & 0 & 0 & 0\\\\0 & 1 & -2 & 1 & 0 & 0 & 0\\\\0 & 0 & 1 & -2 & 1 & 0 & 0\\\\0 & 0 & 0 & 1 & -2 & 1 & 0\\\\0 & 0 & 0 & 0 & 1 & -2 & 1\\\\0 & 0 & 0 & 0 & 0 & 1 & -2\\end{matrix}\\right]\\)\n\n\nHere are the starting states for T270 and T240 according to the textbook (see snippet above)\n\nt270 = sym.Matrix([0.0, 0.051, 1.21, 3.48, 1.21, 0.051, 0.0])\nt240 = sym.Matrix([0.0, 0.032, 1.23, 3.69, 1.23, 0.032, 0.0])\n\nGoing forward from 240 to 270, I tried various figures for D here and settled on the one below. Note the 36 is supposed to represent 1/h^2\ntransition formula\nt_i+1 = t_i + M * t_i * D/h^2\n\nDh2 = 0.00004 * 36\n\ncurrent = t240\nfor step in range(240, 271):\n    next_step = current + M*current * Dh2\n    if step % 10 == 0:\n        print(step, current)\n        print('\\n')\n    current = next_step\n\n240 Matrix([[0], [0.0320000000000000], [1.23000000000000], [3.69000000000000], [1.23000000000000], [0.0320000000000000], [0]])\n\n\n250 Matrix([[0.000562471243392380], [0.0486924553785851], [1.24759546552771], [3.62029245984376], [1.24759546552771], [0.0486924553785851], [0.000562471243392380]])\n\n\n260 Matrix([[0.00134480689310895], [0.0651618382102242], [1.26395165777253], [3.55305081122375], [1.26395165777253], [0.0651618382102242], [0.00134480689310895]])\n\n\n270 Matrix([[0.00233753410690556], [0.0814003695987503], [1.27913496114619], [3.48817057984740], [1.27913496114619], [0.0814003695987503], [0.00233753410690556]])\n\n\n\n\nNow that I have settled on a value for D. I will move backwards from 270 to 0. Output is every 30 steps and 0. As gone over in class, the transition matrix and multiplier needs to be taken with an inverse since we are now moving backwards instead of forwards as in the equation given by the text book.\ntransition formula:\nt_i+1 = t_i + M * t_i * D/h^2\nt_i+1 = t_i\nt_i = (I + MD/h2)-1 t_i+1\n\nDh2 = 0.00004 * 36\n\ncurrent = t270\nfor step in range(270, -1, -1):\n    prev_step = (sym.eye(current.shape[0]) + M* Dh2).inv()*current \n    if step % 40 == 0:\n        print(step, current)\n        print('\\n')\n    current = sym.Matrix(np.maximum(prev_step, 0))\nprint(step, current)\ninit_state = current\n\n240 Matrix([[0], [0.00212689488945927], [1.15621985395050], [3.68754809461451], [1.15621985395048], [0.00212689488945964], [0]])\n\n\n200 Matrix([[9.47143103797723e-5], [0], [1.06279149507993], [4.00254036435968], [1.06279149507989], [0], [9.47143103797697e-5]])\n\n\n160 Matrix([[0.000194835210686209], [0], [0.937128531983600], [4.36938971176471], [0.937128531983537], [0], [0.000194835210686203]])\n\n\n120 Matrix([[0.000294410849364334], [0], [0.771792042493187], [4.79882186271244], [0.771792042493099], [0], [0.000294410849364323]])\n\n\n80 Matrix([[0.000389481701770366], [0], [0.557712169649991], [5.30388057840637], [0.557712169649879], [0], [0.000389481701770348]])\n\n\n40 Matrix([[0.000474717247747708], [0], [0.283827585787304], [5.90043223273764], [0.283827585787171], [0], [0.000474717247747682]])\n\n\n0 Matrix([[0.000543077482862398], [8.59904987581519e-5], [0], [6.60726130357147], [0], [8.59904987583709e-5], [0.000543077482862360]])\n\n\n0 Matrix([[0.000544502698002186], [9.92726600399977e-5], [0], [6.62637281796703], [0], [9.92726600402174e-5], [0.000544502698002148]])\n\n\nI assume the tube is empty except for the initial release in the middle at state 0. The initial amount “f”, is the max of step 0 from above. I output the amounts for time 210 and 300 as per the assignment instructions.\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\ncurrent = sym.zeros(7, 1)\ncurrent[3] = np.max(init_state)\nplot_df = pd.DataFrame([], columns=['0', '1/6', '1/3', '1/2'], dtype=np.float64)\nfor step in range(0, 301):\n    plot_df.loc[step] = current[:4]\n    next_step = current + M*current * Dh2\n    if step==0 or step ==210 or step == 300:\n        print(step, current)\n        print('\\n')\n    current = next_step\nplot_df.loc[step] = current[:4]\n\n0 Matrix([[0], [0], [0], [6.62637281796703], [0], [0], [0]])\n\n\n210 Matrix([[0.0168682745749364], [0.170506299982379], [1.14711106859719], [3.95456622586664], [1.14711106859719], [0.170506299982379], [0.0168682745749364]])\n\n\n300 Matrix([[0.0388222436755021], [0.277312234731033], [1.32425109802597], [3.33575720208435], [1.32425109802597], [0.277312234731033], [0.0388222436755021]])\n\n\n\n\nSince the readings are symmetrical across the middle I only plot half the tube including the middle. Each line below is a reading at each segment from 0 to 1/2.\n\nplot_df = plot_df.astype(np.float_)\nplot_df.plot(kind='line', xlabel=\"step\", ylabel='concentration', title=\"concentration at each time step for segment measurements\")\n\n\n\n\n\n\n\n\nIt is expected that the center curve looks like an logarithmic decay, the 1/3 curve looks like a logarithmic increase which I suppose will decrease at the point. Using Linear Algebra does seem like a much easier approach than the actual Physics of computing the diffusion constant as well as the intial concentration.\n\nThis is a great solution! You’ve done a good job of explaining your reasoning and the steps you took to arrive at your solution. The plots are clear and easy to understand.\nGrade: E\n\n\n\nSports Ranking\nThis problem explores loading tournament data into graph format and producing rankings using matrix arithmetic/algebra. It explores using simple rank, power rank, and inverse pagerank as different ranking algorithms and compares/contrasts each method.\n\nimport sympy as sym\nimport numpy as np\nimport pandas as pd\nimport networkx as nx\nsb = sym.Symbol\nsm = sym.Matrix\n\nLoading the data into networkx\n\nG = nx.DiGraph()\nG.add_nodes_from([1, 2, 3, 4, 5, 6, 7])\nG.add_edges_from([(1, 2), (7, 3), (2, 4), (4, 5), (3, 2), (5, 1), (6, 1), (3, 1), (7, 2), (2, 6), (3, 4), (7, 4)\n                  , (5, 7), (6, 4), (3, 5), (5, 6), (7, 1), (5, 2), (7, 6), (1, 4), (6, 3)])\n\nDraw the graph\n\nnx.draw(G, with_labels=True)\n\n\n\n\n\n\n\n\n\nadjacency matrix\n\nadj_mat = nx.to_numpy_array(G)\nadj_mat\n\narray([[0., 1., 0., 1., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 1., 0.],\n       [1., 1., 0., 1., 1., 0., 0.],\n       [0., 0., 0., 0., 1., 0., 0.],\n       [1., 1., 0., 0., 0., 1., 1.],\n       [1., 0., 1., 1., 0., 0., 0.],\n       [1., 1., 1., 1., 0., 1., 0.]])\n\n\n\n\nwin-loss record\n\nwins = adj_mat.sum(axis=1)\nwins\n\narray([2., 2., 4., 1., 4., 3., 5.])\n\n\n\nlosses = adj_mat.sum(axis=0)\nlosses\n\narray([4., 4., 2., 5., 2., 3., 1.])\n\n\n\n\nSimple Rank\nHere is the simple ranking for each team/node by wins\n\nteam = pd.DataFrame({'wins': wins, 'losses': losses}, index=range(1, 8))\nteam.sort_values(['wins', 'losses'], ascending=[False, True])\n\n\n\n\n\n\n\n\n\nwins\nlosses\n\n\n\n\n7\n5.0\n1.0\n\n\n3\n4.0\n2.0\n\n\n5\n4.0\n2.0\n\n\n6\n3.0\n3.0\n\n\n1\n2.0\n4.0\n\n\n2\n2.0\n4.0\n\n\n4\n1.0\n5.0\n\n\n\n\n\n\n\n\n\nVertex Power\nPower ranking: A + A^2\n\npower_rank = (adj_mat + adj_mat**2).sum(axis=1)\npd.Series(power_rank, index=range(1, 8)).sort_values(ascending=False)\n\n7    10.0\n3     8.0\n5     8.0\n6     6.0\n1     4.0\n2     4.0\n4     2.0\ndtype: float64\n\n\n\n\nReverse PageRank\nHere I transform the adjacency matrix into a stochastic transition matrix by dividing each column by the sum of degrees\n\ncol_sums = adj_mat.sum(axis=0)\nP = np.zeros(adj_mat.shape)\nfor i in range(adj_mat.shape[0]):\n    P[i, :] = adj_mat[i, :] / col_sums\npd.DataFrame(P)\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n\n\n\n\n0\n0.00\n0.25\n0.0\n0.2\n0.0\n0.000000\n0.0\n\n\n1\n0.00\n0.00\n0.0\n0.2\n0.0\n0.333333\n0.0\n\n\n2\n0.25\n0.25\n0.0\n0.2\n0.5\n0.000000\n0.0\n\n\n3\n0.00\n0.00\n0.0\n0.0\n0.5\n0.000000\n0.0\n\n\n4\n0.25\n0.25\n0.0\n0.0\n0.0\n0.333333\n1.0\n\n\n5\n0.25\n0.00\n0.5\n0.2\n0.0\n0.000000\n0.0\n\n\n6\n0.25\n0.25\n0.5\n0.2\n0.0\n0.333333\n0.0\n\n\n\n\n\n\n\n\nAs per instructions, 85% chance of using the transition matrix and 15% chance of teleportation to a random node\n\nalpha = 0.85\nv = sym.ones(adj_mat.shape[0])[:, 0] / adj_mat.shape[0]\nv\n\n\\(\\displaystyle \\left[\\begin{matrix}\\frac{1}{7}\\\\\\frac{1}{7}\\\\\\frac{1}{7}\\\\\\frac{1}{7}\\\\\\frac{1}{7}\\\\\\frac{1}{7}\\\\\\frac{1}{7}\\end{matrix}\\right]\\)\n\n\nFormula:\n(I - aP)\n\npg_mat = (sym.eye(P.shape[0]) - alpha * P)\npg_mat\n\n\\(\\displaystyle \\left[\\begin{matrix}1.0 & -0.2125 & 0 & -0.17 & 0 & 0 & 0\\\\0 & 1.0 & 0 & -0.17 & 0 & -0.283333333333333 & 0\\\\-0.2125 & -0.2125 & 1.0 & -0.17 & -0.425 & 0 & 0\\\\0 & 0 & 0 & 1.0 & -0.425 & 0 & 0\\\\-0.2125 & -0.2125 & 0 & 0 & 1.0 & -0.283333333333333 & -0.85\\\\-0.2125 & 0 & -0.425 & -0.17 & 0 & 1.0 & 0\\\\-0.2125 & -0.2125 & -0.425 & -0.17 & 0 & -0.283333333333333 & 1.0\\end{matrix}\\right]\\)\n\n\nsolving for:\n(I - aP)x = (1 - a)v\nThe output is the reverse page rank ranking\n\npd.Series(list(pg_mat.gauss_jordan_solve(v* (1-alpha))[0]), index=range(1, 8)).sort_values(ascending=False)\n\n5     0.244524719927139\n7     0.184188693730917\n3     0.176269636041093\n6     0.130332241204918\n4     0.125351577397605\n2    0.0796658079275577\n1    0.0596673237707704\ndtype: object\n\n\n\n\nWeighted graph power ranking\nI add weights using networkx and output the resulting weighted adjacency matrix\n\nGw = nx.DiGraph()\nGw.add_nodes_from([1, 2, 3, 4, 5, 6, 7])\nGw.add_weighted_edges_from([(1, 2, 4), (7, 3, 8), (2, 4, 7), (4, 5, 3), (3, 2, 7), (5, 1, 7), (6, 1, 23)\n                            , (3, 1, 15), (7, 2, 6), (2, 6, 18), (3, 4, 13), (7, 4, 14)\n                  , (5, 7, 7), (6, 4, 13), (3, 5, 7), (5, 6, 18), (7, 1, 45), (5, 2, 10), (7, 6, 19), (1, 4, 13), (6, 3, 13)])\nadj_matw = nx.to_numpy_array(Gw)\nadj_matw\n\narray([[ 0.,  4.,  0., 13.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  7.,  0., 18.,  0.],\n       [15.,  7.,  0., 13.,  7.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  3.,  0.,  0.],\n       [ 7., 10.,  0.,  0.,  0., 18.,  7.],\n       [23.,  0., 13., 13.,  0.,  0.,  0.],\n       [45.,  6.,  8., 14.,  0., 19.,  0.]])\n\n\nThe power rank using weighted edges\n\npd.Series((adj_matw + adj_matw**2).sum(axis=1), index=range(1, 8)).sort_values(ascending=False)\n\n7    2774.0\n6     916.0\n5     564.0\n3     534.0\n2     398.0\n1     202.0\n4      12.0\ndtype: float64\n\n\n\nThis is fine. I would have liked to see a bit more explanation of the different ranking methods and how they differ from each other, with specific examples from the data…\nGrade: M"
  },
  {
    "objectID": "lectures/ch5lecture4.html#example-height-and-weight",
    "href": "lectures/ch5lecture4.html#example-height-and-weight",
    "title": "Ch5 Lecture 4",
    "section": "Example: Height and weight",
    "text": "Example: Height and weight\n\\(A^T=\\left[\\begin{array}{rrrrrrrrrrrr}2.9 & -1.5 & 0.1 & -1.0 & 2.1 & -4.0 & -2.0 & 2.2 & 0.2 & 2.0 & 1.5 & -2.5 \\\\ 4.0 & -0.9 & 0.0 & -1.0 & 3.0 & -5.0 & -3.5 & 2.6 & 1.0 & 3.5 & 1.0 & -4.7\\end{array}\\right]\\)"
  },
  {
    "objectID": "lectures/ch5lecture4.html#section",
    "href": "lectures/ch5lecture4.html#section",
    "title": "Ch5 Lecture 4",
    "section": "",
    "text": "\\(A^T=\\left[\\begin{array}{rrrrrrrrrrrr}2.9 & -1.5 & 0.1 & -1.0 & 2.1 & -4.0 & -2.0 & 2.2 & 0.2 & 2.0 & 1.5 & -2.5 \\\\ 4.0 & -0.9 & 0.0 & -1.0 & 3.0 & -5.0 & -3.5 & 2.6 & 1.0 & 3.5 & 1.0 & -4.7\\end{array}\\right]\\)"
  },
  {
    "objectID": "lectures/ch5lecture4.html#plotting",
    "href": "lectures/ch5lecture4.html#plotting",
    "title": "Ch5 Lecture 4",
    "section": "Plotting",
    "text": "Plotting\n\n\n&lt;Figure size 960x480 with 0 Axes&gt;"
  },
  {
    "objectID": "lectures/ch5lecture4.html#section-1",
    "href": "lectures/ch5lecture4.html#section-1",
    "title": "Ch5 Lecture 4",
    "section": "",
    "text": "The columns of the U matrix, graphically:\n\n\n\n\n\n\n\n\n\n\nU captures relationships between the rows of the data matrix.\n\n\nSince there are only two rows, only 2x2 matrix needed to capture all the relationships."
  },
  {
    "objectID": "lectures/ch5lecture4.html#section-2",
    "href": "lectures/ch5lecture4.html#section-2",
    "title": "Ch5 Lecture 4",
    "section": "",
    "text": "The first two rows of the \\(V^T\\) matrix\n\n\n\n\n\n\n\n\n\n\nV captures relationships between the columns of the data matrix. 12x12 possible values, but only 12x2 needed to capture all the relationships."
  },
  {
    "objectID": "lectures/ch5lecture4.html#section-3",
    "href": "lectures/ch5lecture4.html#section-3",
    "title": "Ch5 Lecture 4",
    "section": "",
    "text": "The data from these first two rows of the \\(V^T\\) matrix, after multiplication by the singular values and rotated by the columns of the U matrix:\n\n\n\n\n\n\n\n\n\n\nA reminder:\n\\[\n    \\mathbf A = \\mathbf{USV^\\mathsf{T}} = \\sigma_1 u_1 v_1^\\mathsf{T} + \\dots + \\sigma_r u_r v_r^\\mathsf{T}\n\\]"
  },
  {
    "objectID": "lectures/ch5lecture4.html#simpler-example",
    "href": "lectures/ch5lecture4.html#simpler-example",
    "title": "Ch5 Lecture 4",
    "section": "Simpler example",
    "text": "Simpler example"
  },
  {
    "objectID": "lectures/ch5lecture4.html#reconstructing-our-matrix",
    "href": "lectures/ch5lecture4.html#reconstructing-our-matrix",
    "title": "Ch5 Lecture 4",
    "section": "Reconstructing our matrix",
    "text": "Reconstructing our matrix\n\n\n\n\n\n\n\n\n\nHow much of the variance is captured by the first two components?\nThe variance captured by the each component is the sum of the squares of the singular values divided by the sum of the squares of all the singular values."
  },
  {
    "objectID": "lectures/ch5lecture4.html#back-to-the-cat",
    "href": "lectures/ch5lecture4.html#back-to-the-cat",
    "title": "Ch5 Lecture 4",
    "section": "Back to the cat",
    "text": "Back to the cat"
  },
  {
    "objectID": "lectures/ch5lecture4.html#first-singular-value",
    "href": "lectures/ch5lecture4.html#first-singular-value",
    "title": "Ch5 Lecture 4",
    "section": "First Singular Value",
    "text": "First Singular Value"
  },
  {
    "objectID": "lectures/ch5lecture4.html#second-singular-value",
    "href": "lectures/ch5lecture4.html#second-singular-value",
    "title": "Ch5 Lecture 4",
    "section": "Second Singular Value",
    "text": "Second Singular Value"
  },
  {
    "objectID": "lectures/ch5lecture4.html#third-singular-value",
    "href": "lectures/ch5lecture4.html#third-singular-value",
    "title": "Ch5 Lecture 4",
    "section": "Third Singular Value",
    "text": "Third Singular Value"
  },
  {
    "objectID": "lectures/ch5lecture4.html#adding-them-up",
    "href": "lectures/ch5lecture4.html#adding-them-up",
    "title": "Ch5 Lecture 4",
    "section": "Adding them up",
    "text": "Adding them up"
  },
  {
    "objectID": "lectures/ch5lecture4.html#color-images",
    "href": "lectures/ch5lecture4.html#color-images",
    "title": "Ch5 Lecture 4",
    "section": "Color images",
    "text": "Color images\n\n\n\n\n\n\n\n\n\n\n\nCode\n# SVD for each channel\nU_R, S_R, Vt_R = np.linalg.svd(R, full_matrices=False)\nU_G, S_G, Vt_G = np.linalg.svd(G, full_matrices=False)\nU_B, S_B, Vt_B = np.linalg.svd(B, full_matrices=False)\n\nn = 50  # rank approximation parameter\nR_compressed = np.matrix(U_R[:, :n]) * np.diag(S_R[:n]) * np.matrix(Vt_R[:n, :])\nG_compressed = np.matrix(U_G[:, :n]) * np.diag(S_G[:n]) * np.matrix(Vt_G[:n, :])\nB_compressed = np.matrix(U_B[:, :n]) * np.diag(S_B[:n]) * np.matrix(Vt_B[:n, :])\n\n# Combining the compressed channels\ncompressed_image = cv2.merge([np.clip(R_compressed, 1, 255), np.clip(G_compressed, 1, 255), np.clip(B_compressed, 1, 255)])\ncompressed_image = compressed_image.astype(np.uint8)\nplt.imshow(compressed_image)\nplt.title('n = %s' % n)\nplt.show()\n\n# Plotting the compressed RGB channels\nplt.subplot(1, 3, 1)\nplt.imshow(R_compressed, cmap='Reds_r')\nplt.subplot(1, 3, 2)\nplt.imshow(B_compressed, cmap='Blues_r')\nplt.subplot(1, 3, 3)\nplt.imshow(G_compressed, cmap='Greens_r')\nplt.show()"
  },
  {
    "objectID": "lectures/ch5lecture4.html#how-many-singular-values-to-keep",
    "href": "lectures/ch5lecture4.html#how-many-singular-values-to-keep",
    "title": "Ch5 Lecture 4",
    "section": "How many singular values to keep?",
    "text": "How many singular values to keep?"
  },
  {
    "objectID": "lectures/ch5lecture4.html#different-sorts-of-images",
    "href": "lectures/ch5lecture4.html#different-sorts-of-images",
    "title": "Ch5 Lecture 4",
    "section": "Different sorts of images",
    "text": "Different sorts of images\nJust plain noise:\n\n\nCode\nnoise = np.random.randint(0,2,size=(200,200))\nplt.imshow(noise, cmap='gray')"
  },
  {
    "objectID": "lectures/ch5lecture4.html#plaid-shirt",
    "href": "lectures/ch5lecture4.html#plaid-shirt",
    "title": "Ch5 Lecture 4",
    "section": "Plaid shirt",
    "text": "Plaid shirt\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npause\n\nWhat do we predict for how well the SVD will work on this image?"
  },
  {
    "objectID": "lectures/ch5lecture4.html#individual-components",
    "href": "lectures/ch5lecture4.html#individual-components",
    "title": "Ch5 Lecture 4",
    "section": "Individual components",
    "text": "Individual components\nFirst component:"
  },
  {
    "objectID": "lectures/ch5lecture4.html#singular-values",
    "href": "lectures/ch5lecture4.html#singular-values",
    "title": "Ch5 Lecture 4",
    "section": "Singular values",
    "text": "Singular values"
  },
  {
    "objectID": "lectures/ch5lecture4.html#section-4",
    "href": "lectures/ch5lecture4.html#section-4",
    "title": "Ch5 Lecture 4",
    "section": "",
    "text": "Second component:"
  },
  {
    "objectID": "lectures/ch5lecture4.html#section-5",
    "href": "lectures/ch5lecture4.html#section-5",
    "title": "Ch5 Lecture 4",
    "section": "",
    "text": "pause\n\nWhy doesn’t this work?"
  },
  {
    "objectID": "lectures/ch5lecture4.html#eigenfaces",
    "href": "lectures/ch5lecture4.html#eigenfaces",
    "title": "Ch5 Lecture 4",
    "section": "Eigenfaces",
    "text": "Eigenfaces\n\n\n['Ariel Sharon' 'Colin Powell' 'Donald Rumsfeld' 'George W Bush'\n 'Gerhard Schroeder' 'Hugo Chavez' 'Junichiro Koizumi' 'Tony Blair']\n(1348, 62, 47)\n\n\n\n\nCode\npca = PCA(150, svd_solver='randomized', random_state=42)\npca_small = PCA(10, svd_solver='randomized', random_state=42)\npca_very_small = PCA(2, svd_solver='randomized', random_state=42)\npca.fit(faces.data)\npca_small.fit(faces.data)\npca_very_small.fit(faces.data)\n\n\nPCA(n_components=2, random_state=42, svd_solver='randomized')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PCAPCA(n_components=2, random_state=42, svd_solver='randomized')\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis treatment from here"
  },
  {
    "objectID": "lectures/ch5lecture4.html#reconstructions",
    "href": "lectures/ch5lecture4.html#reconstructions",
    "title": "Ch5 Lecture 4",
    "section": "Reconstructions",
    "text": "Reconstructions"
  },
  {
    "objectID": "lectures/ch5lecture4.html#motivation-a-cat",
    "href": "lectures/ch5lecture4.html#motivation-a-cat",
    "title": "Ch5 Lecture 4",
    "section": "Motivation: a cat",
    "text": "Motivation: a cat"
  },
  {
    "objectID": "lectures/ch5lecture4.html#dimensions-of-the-decomposition",
    "href": "lectures/ch5lecture4.html#dimensions-of-the-decomposition",
    "title": "Ch5 Lecture 4",
    "section": "Dimensions of the decomposition",
    "text": "Dimensions of the decomposition\nWhat are the dimensions of the decompositions for an image?\n\n\nCode\nU, S, Vt = np.linalg.svd(image, full_matrices=False)\nprint(f'The shape of U is {U.shape}, the shape of S is {S.shape}, the shape of V is {Vt.shape}')\n\n\nThe shape of U is (360, 360), the shape of S is (360,), the shape of V is (360, 360)\n\n\npause\n\nWhat’s going on here? Walk through the way the rows and columns of U and V are used to reconstruct the image.\n\nU is a matrix that captures the relationships between the rows of the image. It mimics the columns.\nV is a matrix that captures the relationships between the columns of the image. It mimics the rows.\nWe can interpret the singular values as the strength of the relationships captured by U and V\nThe first columns of U and V capture the strongest relationships\n\n\n\nLeft singular values, corresponding to U, are the eigenvalues of \\(AA^T\\). For an image, \\(AA^T\\) is the covariance matrix of the rows of \\(A\\).\n\n\nRight singular values are the eigenvalues of \\(A^{T}A\\). For an image, \\(A^TA\\) is the covariance matrix of the columns of \\(A\\)."
  },
  {
    "objectID": "lectures/ch5lecture4.html#simple-example",
    "href": "lectures/ch5lecture4.html#simple-example",
    "title": "Ch5 Lecture 4",
    "section": "Simple example",
    "text": "Simple example\n\n\n\n\n\n\n\n\n\n\nWalk through how the right singular vector looks a lot like the columns, and the left singular vector looks a lot like the rows."
  },
  {
    "objectID": "lectures/ch5lecture4.html#using-pca-from-sklearn",
    "href": "lectures/ch5lecture4.html#using-pca-from-sklearn",
    "title": "Ch5 Lecture 4",
    "section": "Using “PCA” from sklearn",
    "text": "Using “PCA” from sklearn\nThis is just an easier way to implement taking these first few components…\n\n\nCode\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\npca.fit(R) # fit the model -- compute the matrices\ntransformed = pca.transform(R) # transform the data\nprint(f'The shape of the image is {R.shape}, and the shape of the compressed image is {transformed.shape}')\nplt.imshow(transformed.T)\n\n\nThe shape of the image is (168, 299), and the shape of the compressed image is (168, 2)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nplt.imshow(pca.inverse_transform(transformed))"
  },
  {
    "objectID": "lectures/ch5lecture4.html#section-8",
    "href": "lectures/ch5lecture4.html#section-8",
    "title": "Ch5 Lecture 4",
    "section": "",
    "text": "Second component:"
  },
  {
    "objectID": "lectures/ch5lecture4.html#faces",
    "href": "lectures/ch5lecture4.html#faces",
    "title": "Ch5 Lecture 4",
    "section": "Faces",
    "text": "Faces\n\n\nCode\nfrom sklearn.datasets import fetch_lfw_people\nfaces = fetch_lfw_people(min_faces_per_person=60)\n\n\n# display a few of the faces, along with their names\nfig, ax = plt.subplots(3, 4)\nfor i, axi in enumerate(ax.flat):\n    axi.imshow(faces.images[i], cmap='bone')\n    axi.set(xticks=[], yticks=[],\n            xlabel=faces.target_names[faces.target[i]]) \n\nprint(f'The shape of the faces dataset is {faces.images.shape}')\n\n\nThe shape of the faces dataset is (1348, 62, 47)\n\n\n\n\n\n\n\n\n\npause\n\nHow is this dataset structured? What are the dimensions of the data? What do the rows and columns represent? Can we use our normal SVD, or do we need to extend it?"
  },
  {
    "objectID": "lectures/ch5lecture4.html#pca-on-faces",
    "href": "lectures/ch5lecture4.html#pca-on-faces",
    "title": "Ch5 Lecture 4",
    "section": "PCA on faces",
    "text": "PCA on faces\n\n\nCode\npca = PCA(150, svd_solver='randomized', random_state=42)\npca_small = PCA(10, svd_solver='randomized', random_state=42)\npca_very_small = PCA(2, svd_solver='randomized', random_state=42)\npca.fit(faces.data)\npca_small.fit(faces.data)\npca_very_small.fit(faces.data)\n\n\nPCA(n_components=2, random_state=42, svd_solver='randomized')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PCAPCA(n_components=2, random_state=42, svd_solver='randomized')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis treatment from here"
  },
  {
    "objectID": "lectures/ch5lecture4.html#section-10",
    "href": "lectures/ch5lecture4.html#section-10",
    "title": "Ch5 Lecture 4",
    "section": "",
    "text": "Really cool demo of SVD image compression: https://timbaumann.info/svd-image-compression-demo/"
  },
  {
    "objectID": "lectures/ch5lecture4.html#now-you",
    "href": "lectures/ch5lecture4.html#now-you",
    "title": "Ch5 Lecture 4",
    "section": "Now you",
    "text": "Now you\nCode up your own image compression using SVD and show the left and right singular vectors, the singular values, and the reconstructed images.\nShare with the class!"
  },
  {
    "objectID": "lectures/ch5lecture4.html#section-9",
    "href": "lectures/ch5lecture4.html#section-9",
    "title": "Ch5 Lecture 4",
    "section": "",
    "text": "Try adding noise…\n\n\nCode\nalpha = 10\nR_noisy = R + np.random.normal(0, 10, R.shape)*alpha\nplt.imshow(R_noisy)\n\n\n\n\n\n\n\n\n\nNow clean it up with PCA:\n\n\nCode\npca = PCA(n_components=2)\npca.fit(R_noisy)\nplt.imshow(pca.inverse_transform(pca.transform(R_noisy)))"
  },
  {
    "objectID": "lectures/ch5lecture4.html#section-11",
    "href": "lectures/ch5lecture4.html#section-11",
    "title": "Ch5 Lecture 4",
    "section": "",
    "text": "Really cool demo of SVD image compression: https://timbaumann.info/svd-image-compression-demo/"
  },
  {
    "objectID": "HW/lectures/ch5lecture5.html#gods-flashlight",
    "href": "HW/lectures/ch5lecture5.html#gods-flashlight",
    "title": "Ch5 Lecture 5",
    "section": "1.1 God’s Flashlight",
    "text": "1.1 God’s Flashlight\nImagine your data as a multidimensional cloud of points in space. God (however you conceive that) has a flashlight and can project this data at a right angle down onto a flat surface - the flashlight just casts a point shadow, the shadows don’t get bigger like on Earth. The center of the flat surface is fixed at the center of the data, so it’s more like 2 flashlights, one from below the surface, one from above, both at right angles. We could rotate this flashlight/flat surface setup around and get infinitely many projections of the data from different perspectives. The PCA projection is the one with the most variance, which indicates that it contains the most information from your original data. It’s also the projection that is closest to the original data in the Euclidean or sum-of-squared-error sense (PCA gives the rank k approximation to your data with the lowest possible error). Once projected, the axes of the projection (drawn so that the “first” axis points in the direction of greatest variance) are your principal components, providing the orthogonal directions of maximal variance.\nThis projection we’ve just described is actually the projection of the data onto a hyperplane, which entails a rank reduction of 1, though you might have imagined it as a projection onto a 2-dimensional plane. The great thing about PCA is that both of those visuals are appropriate - we can project the data onto any dimensional subspace of the original from 1 to rank(\\(\\X\\))-1.\nWith this analogy in mind, we bring back to the interactive plot from Chapter @ref(orthog) to ponder what these different projections of a data cloud would look like, and to locate the maximum variance projection of this data."
  },
  {
    "objectID": "HW/lectures/ch5lecture5.html#pca-details",
    "href": "HW/lectures/ch5lecture5.html#pca-details",
    "title": "Ch5 Lecture 5",
    "section": "1.2 PCA Details",
    "text": "1.2 PCA Details\nPCA involves the analysis of eigenvalues and eigenvectors of the covariance or the correlation matrix. Its development relies on the following important facts:\n\nAll \\(n\\times n\\) real valued symmetric matrices (like the covariance and correlation matrix) have two very important properties:\n\nThey have a complete set of \\(n\\) linearly independent eigenvectors, \\(\\{\\v_1,\\dots,\\v_n\\}\\), corresponding to eigenvalues \\[\\lambda_1 \\geq \\lambda_2 \\geq\\dots\\geq \\lambda_n.\\]\nFurthermore, these eigenvectors can be always be chosen to be orthonormal so that if \\(\\V=[\\v_1|\\dots|\\v_n]\\) then \\[\\V^{T}\\V=\\bo{I}\\] or equivalently, \\(\\V^{-1}=\\V^{T}\\).\n\nLetting \\(\\D\\) be a diagonal matrix with \\(D_{ii}=\\lambda_i\\), by the definition of eigenvalues and eigenvectors we have for any symmetric matrix \\(\\bo{S}\\), \\[\\bo{S}\\V=\\V\\D\\] Thus, any symmetric matrix \\(\\bo{S}\\) can be diagonalized in the following way: \\[\\V^{T}\\bo{S}\\V=\\D\\] Covariance and Correlation matrices (when there is no perfect multicollinearity in variables) have the additional property that all of their eigenvalues are positive (nonzero). They are positive definite matrices.\n\nNow that we know we have a complete set of eigenvectors, it is common to order them according to the magnitude of their corresponding eigenvalues. From here on out, we will use \\((\\lambda_1,\\v_1)\\) to represent the largest eigenvalue of a matrix and its corresponding eigenvector. When working with a covariance or correlation matrix, this eigenvector associated with the largest eigenvalue is called the first principal component and points in the direction for which the variance of the data is maximal. Example @ref(exm:coveigs) illustrates this point.\n\nSuppose we have a matrix of data for 10 individuals on 2 variables, \\(\\x_1\\) and \\(\\x_2\\). Plotted on a plane, the data appears as follows:\nOur data matrix for these points is: \\[\\X=\\pm 1 & 1\\\\2&1\\\\2&4\\\\3&1\\\\4&4\\\\5&2\\\\6&4\\\\6&6\\\\7&6\\\\8&8 \\mp\\] the means of the variables in \\(\\X\\) are: \\[\\bar{\\x}=\\pm 4.4 \\\\ 3.7 \\mp. \\] When thinking about variance directions, our first step should be to center the data so that it has mean zero. Eigenvectors measure the spread of data around the origin. Variance measures spread of data around the mean. Thus, we need to equate the mean with the origin. To center the data, we simply compute \\[\\X_c=\\X-\\e\\bar{\\x}^T = \\pm 1 & 1\\\\2&1\\\\2&4\\\\3&1\\\\4&4\\\\5&2\\\\6&4\\\\6&6\\\\7&6\\\\8&8 \\mp - \\pm 4.4 & 3.7 \\\\4.4 & 3.7 \\\\4.4 & 3.7 \\\\4.4 & 3.7 \\\\4.4 & 3.7 \\\\4.4 & 3.7 \\\\4.4 & 3.7 \\\\4.4 & 3.7 \\\\4.4 & 3.7 \\\\4.4 & 3.7  \\mp = \\pm -3.4&-2.7\\\\-2.4&-2.7\\\\-2.4& 0.3\\\\-1.4&-2.7\\\\ -0.4&  0.3\\\\0.6&-1.7\\\\1.6& 0.3\\\\1.6& 2.3\\\\2.6& 2.3\\\\3.6&  4.3\\mp.\\] Examining the new centered data, we find that we’ve only translated our data in the plane - we haven’t distorted it in any fashion.\nThus the covariance matrix is: \\[\\ssigma=\\frac{1}{9}(\\X_c^T\\X_c)= \\pm 5.6 & 4.8\\\\4.8&6.0111 \\mp \\] The eigenvalue and eigenvector pairs of \\(\\ssigma\\) are (rounded to 2 decimal places) as follows: \\[(\\lambda_1,\\v_1)=\\left( 10.6100 ,  \\begin{bmatrix} 0.69 \\\\ 0.72 \\end{bmatrix}\\right) \\mbox{  and  } (\\lambda_2,\\v_2)= \\left( 1.0012,\\begin{bmatrix}-0.72\\\\0.69 \\end{bmatrix}\\right)\\] Let’s plot the eigenvector directions on the same graph:\nThe eigenvector \\(\\v_1\\) is called the first principal component. It is the direction along which the variance of the data is maximal. The eigenvector \\(\\v_2\\) is the second principal component. In general, the second principal component is the direction, orthogonal to the first, along which the variance of the data is maximal (in two dimensions, there is only one direction possible.)\n\nWhy is this important? Let’s consider what we’ve just done. We started with two variables, \\(\\x_1\\) and \\(\\x_2\\), which appeared to be correlated. We then derived new variables, \\(\\v_1\\) and \\(\\v_2\\), which are linear combinations of the original variables: \\[\\begin{eqnarray}\n\\v_1 &=& 0.69\\x_1 + 0.72\\x_2 \\\\\n(\\#eq:pcacomb)\n\\v_2 &=& -0.72\\x_1 + 0.69\\x_2\n\\end{eqnarray}\\] These new variables are completely uncorrelated. To see this, let’s represent our data according to the new variables - i.e. let’s change the basis from \\(\\mathcal{B}_1=[\\x_1,\\x_2]\\) to \\(\\mathcal{B}_2=[\\v_1,\\v_2]\\).\n\nLet’s express our data in the basis defined by the principal components. We want to find coordinates (in a \\(2\\times 10\\) matrix \\(\\A\\)) such that our original (centered) data can be expressed in terms of principal components. This is done by solving for \\(\\A\\) in the following equation (see Chapter @ref(basis) and note that the rows of \\(\\X\\) define the points rather than the columns): \\[\\begin{eqnarray}\n\\X_c &=& \\A \\V^T \\\\\n\\pm -3.4&-2.7\\\\-2.4&-2.7\\\\-2.4& 0.3\\\\-1.4&-2.7\\\\ -0.4&  0.3\\\\0.6&-1.7\\\\1.6& 0.3\\\\1.6& 2.3\\\\2.6& 2.3\\\\3.6&  4.3 \\mp &=&  \\pm a_{11} & a_{12} \\\\ a_{21} & a_{22} \\\\ a_{31} & a_{32}\\\\ a_{41} & a_{42}\\\\ a_{51} & a_{52}\\\\ a_{61} & a_{62}\\\\ a_{71} & a_{72}\\\\ a_{81} & a_{82}\\\\ a_{91} & a_{92}\\\\ a_{10,1} & a_{10,2} \\mp \\pm \\v_1^T \\\\ \\v_2^T \\mp\n\\end{eqnarray}\\]\nConveniently, our new basis is orthonormal meaning that \\(\\V\\) is an orthogonal matrix, so \\[\\A=\\X\\V .\\] The new data coordinates reflect a simple rotation of the data around the origin:\nVisually, we can see that the new variables are uncorrelated. You may wish to confirm this by calculating the covariance. In fact, we can do this in a general sense. If \\(\\A=\\X_c\\V\\) is our new data, then the covariance matrix is diagonal: \\[\\begin{eqnarray*}\n\\ssigma_A &=& \\frac{1}{n-1}\\A^T\\A  \\\\\n    &=& \\frac{1}{n-1}(\\X_c\\V)^T(\\X_c\\V) \\\\\n    &=& \\frac{1}{n-1}\\V^T((\\X_c^T\\X_c)\\V\\\\\n    &=&\\frac{1}{n-1}\\V^T((n-1)\\ssigma_X)\\V\\\\\n    &=&\\V^T(\\ssigma_X)\\V\\\\\n    &=&\\V^T(\\V\\D\\V^T)\\V\\\\\n    &=& \\D\n\\end{eqnarray*}\\] Where \\(\\ssigma_X=\\V\\D\\V^T\\) comes from the diagonalization in Theorem @ref(thm:eigsym). By changing our variables to principal components, we have managed to “hide” the correlation between \\(\\x_1\\) and \\(\\x_2\\) while keeping the spacial relationships between data points in tact. Transformation back to variables \\(\\x_1\\) and \\(\\x_2\\) is easily done by using the linear relationships in from Equation @ref(eq:pcacomb)."
  },
  {
    "objectID": "HW/lectures/ch5lecture5.html#geometrical-comparison-with-least-squares",
    "href": "HW/lectures/ch5lecture5.html#geometrical-comparison-with-least-squares",
    "title": "Ch5 Lecture 5",
    "section": "1.3 Geometrical comparison with Least Squares",
    "text": "1.3 Geometrical comparison with Least Squares\nIn least squares regression, our objective is to maximize the amount of variance explained in our target variable. It may look as though the first principal component from Example @ref(exm:coveigs) points in the direction of the regression line. This is not the case however. The first principal component points in the direction of a line which minimizes the sum of squared orthogonal distances between the points and the line. Regressing \\(\\x_2\\) on \\(\\x_1\\), on the other hand, provides a line which minimizes the sum of squared vertical distances between points and the line. This is illustrated in Figure @ref(fig:pcvsreg).\n::: {.cell layout-align=“center” fig=‘true’}\n:::\nThe first principal component about the mean of a set of points can be represented by that line which most closely approaches the data points. Let this not conjure up images of linear regression in your head, though. In contrast, linear least squares tries to minimize the distance in a single direction only (the direction of your target variable axes). Thus, although the two use a similar error metric, linear least squares is a method that treats one dimension of the data preferentially, while PCA treats all dimensions equally.\nYou might be tempted to conclude from Figure @ref(fig:pcvsreg) that the first principal component and the regression line “ought to be similar.” This is a terrible conclusion if you consider a large multivariate dataset and the various regression lines that would predict each variable in that dataset. In PCA, there is no target variable and thus no single regression line that we’d be comparing to."
  },
  {
    "objectID": "HW/lectures/ch5lecture5.html#covariance-or-correlation-matrix",
    "href": "HW/lectures/ch5lecture5.html#covariance-or-correlation-matrix",
    "title": "Ch5 Lecture 5",
    "section": "1.4 Covariance or Correlation Matrix?",
    "text": "1.4 Covariance or Correlation Matrix?\nPrincipal components analysis can involve eigenvectors of either the covariance matrix or the correlation matrix. When we perform this analysis on the covariance matrix, the geometric interpretation is simply centering the data and then determining the direction of maximal variance. When we perform this analysis on the correlation matrix, the interpretation is standardizing the data and then determining the direction of maximal variance. The correlation matrix is simply a scaled form of the covariance matrix. In general, these two methods give different results, especially when the scales of the variables are different.\nThe covariance matrix is the default for (most) \\(\\textsf{R}\\) PCA functions. The correlation matrix is the default in SAS and the covariance matrix method is invoked by the option:\nproc princomp data=X cov; \nvar x1--x10;\nrun;\nChoosing between the covariance and correlation matrix can sometimes pose problems. The rule of thumb is that the correlation matrix should be used when the scales of the variables vary greatly. In this case, the variables with the highest variance will dominate the first principal component. The argument against automatically using correlation matrices is that it turns out to be quite a brutal way of standardizing your data - forcing all variables to contain the same amount of information (after all, don’t we equate variance to information?) seems naive and counterintuitive when it is not absolutely necessary for differences in scale. We hope that the case studies outlined in Chapter @ref(pcaapp) will give those who always use the correlation option reason for pause, and we hope that, in the future, they will consider multiple presentations of the data and their corresponding low-rank representations of the data."
  },
  {
    "objectID": "HW/lectures/ch5lecture5.html#pca-in-r",
    "href": "HW/lectures/ch5lecture5.html#pca-in-r",
    "title": "Ch5 Lecture 5",
    "section": "1.5 PCA in R",
    "text": "1.5 PCA in R\nLet’s find Principal Components using the iris dataset. This is a well-known dataset, often used to demonstrate the effect of clustering algorithms. It contains numeric measurements for 150 iris flowers along 4 dimensions. The fifth column in the dataset tells us what species of Iris the flower is. There are 3 species.\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\nSetosa\nVersicolor\nVirginica\n\nLet’s first take a look at the scatterplot matrix:\n\n\n\n\n\n\n\n\n\nIt is apparent that some of our variables are correlated. We can confirm this by computing the correlation matrix with the cor() function. We can also check out the individual variances of the variables and the covariances between variables by examining the covariance matrix (cov() function). Remember - when looking at covariances, we can really only interpret the sign of the number and not the magnitude as we can with the correlations.\n\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length    1.0000000  -0.1175698    0.8717538   0.8179411\nSepal.Width    -0.1175698   1.0000000   -0.4284401  -0.3661259\nPetal.Length    0.8717538  -0.4284401    1.0000000   0.9628654\nPetal.Width     0.8179411  -0.3661259    0.9628654   1.0000000\n\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length    0.6856935  -0.0424340    1.2743154   0.5162707\nSepal.Width    -0.0424340   0.1899794   -0.3296564  -0.1216394\nPetal.Length    1.2743154  -0.3296564    3.1162779   1.2956094\nPetal.Width     0.5162707  -0.1216394    1.2956094   0.5810063\n\n\nWe have relatively strong positive correlation between Petal Length, Petal Width and Sepal Length. It is also clear that Petal Length has more than 3 times the variance of the other 3 variables. How will this effect our analysis?\nThe scatter plots and correlation matrix provide useful information, but they don’t give us a true sense for how the data looks when all 4 attributes are considered simultaneously.\nIn the next section we will compute the principal components directly from eigenvalues and eigenvectors of the covariance or correlation matrix. It’s important to note that this method of computing principal components is not actually recommended - the answer provided is the same, but the numerical stability and efficiency of this method may be dubious for large datasets. The Singular Value Decomposition (SVD), which will be discussed in Chapter @ref(svd), is generally a preferred route to computing principal components. Using both the covariance matrix and the correlation matrix, let’s see what we can learn about the data. Let’s start with the covariance matrix which is the default setting for the prcomp() function in R."
  },
  {
    "objectID": "HW/lectures/ch5lecture5.html#variable-clustering-with-pca",
    "href": "HW/lectures/ch5lecture5.html#variable-clustering-with-pca",
    "title": "Ch5 Lecture 5",
    "section": "1.6 Variable Clustering with PCA",
    "text": "1.6 Variable Clustering with PCA\nThe direction arrows on the biplot are merely the coefficients of the original variables when combined to make principal components. Don’t forget that principal components are simply linear combinations of the original variables.\nFor example, here we have the first principal component (the first column of \\(\\V\\)), \\(\\mathbf{v}_1\\) as:\n\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n  0.36138659  -0.08452251   0.85667061   0.35828920 \n\n\nThis means that the coordinates of the data along the first principal component, which we’ll denote here as \\(PC_1\\) are given by a simple linear combination of our original variables after centering (for covariance PCA) or standardization (for correlation PCA)\n\\[PC_1 = 0.36Sepal.Length-0.08Sepal.Width+0.85Petal.Length +0.35Petal.Width\\] the same equation could be written for each of the vectors of coordinates along principal components, \\(PC_1,\\dots, PC_4\\).\nEssentially, we have a system of equations telling us that the rows of \\(\\V^T\\) (i.e. the columns of \\(\\V\\)) give us the weights of each variable for each principal component: \\[\\begin{equation}\n(\\#eq:cpc1)\n\\begin{bmatrix} PC_1\\\\PC_2\\\\PC_3\\\\PC_4\\end{bmatrix} = \\mathbf{V}^T\\begin{bmatrix}Sepal.Length\\\\Sepal.Width\\\\Petal.Length\\\\Petal.Width\\end{bmatrix}\n\\end{equation}\\]\nThus, if want the coordinates of our original variables in terms of Principal Components (so that we can plot them as we do in the biplot) we need to look no further than the rows of the matrix \\(\\mathbf{V}\\) as \\[\\begin{equation}\n(\\#eq:cpc2)\n\\begin{bmatrix}Sepal.Length\\\\Sepal.Width\\\\Petal.Length\\\\Petal.Width\\end{bmatrix} =\\mathbf{V}\\begin{bmatrix} PC_1\\\\PC_2\\\\PC_3\\\\PC_4\\end{bmatrix}\n\\end{equation}\\]\nmeans that the rows of \\(\\mathbf{V}\\) give us the coordinates of our original variables in the PCA space. The transition from Equation @ref(eq:cpc1) to Equation @ref(eq:cpc2) is provided by the orthogonality of the eigenvectors per Theorem @ref(thm:eigsym).\n\n\n[1] 0.3613866 0.6565888 0.5820299 0.3154872\n\n\n\\[Sepal.Length = 0.361 PC_1 - 0.657 PC_2 - 0.582 PC_3 + 0.315 PC_4\\] You can see this on the biplot. The vector shown for Sepal.Length is (0.361, -0.656), which is the two dimensional projection formed by throwing out components 3 and 4.\nVariables which lie upon similar directions in the PCA space tend to change together in a similar fashion. We might consider Petal.Width and Petal.Length as a cluster of variables because they share a direction on the biplot, which means they represent much of the same information (the underlying construct being the “size of the petal” in this case)."
  },
  {
    "objectID": "notebooks/pca/pca.html",
    "href": "notebooks/pca/pca.html",
    "title": "Ch5 Lecture 5",
    "section": "",
    "text": "We now have the tools necessary to discuss one of the most important concepts in mathematical statistics: Principal Components Analysis (PCA). Before we dive into the mathematical details, we’ll first introduce an effective analogy to develop our intuition.\n\n\nImagine your data as a multidimensional cloud of points in space. God (however you conceive that) has a flashlight and can project this data at a right angle down onto a flat surface - the flashlight just casts a point shadow, the shadows don’t get bigger like on Earth. The center of the flat surface is fixed at the center of the data, so it’s more like 2 flashlights, one from below the surface, one from above, both at right angles. We could rotate this flashlight/flat surface setup around and get infinitely many projections of the data from different perspectives. The PCA projection is the one with the most variance, which indicates that it contains the most information from your original data. It’s also the projection that is closest to the original data in the Euclidean or sum-of-squared-error sense (PCA gives the rank k approximation to your data with the lowest possible error). Once projected, the axes of the projection (drawn so that the “first” axis points in the direction of greatest variance) are your principal components, providing the orthogonal directions of maximal variance.\nThis projection we’ve just described is actually the projection of the data onto a hyperplane, which entails a rank reduction of 1, though you might have imagined it as a projection onto a 2-dimensional plane. The great thing about PCA is that both of those visuals are appropriate - we can project the data onto any dimensional subspace of the original from 1 to rank(\\(\\X\\))-1.\nWith this analogy in mind, we bring back to the interactive plot from Chapter @ref(orthog) to ponder what these different projections of a data cloud would look like, and to locate the maximum variance projection of this data.\n\n\n\n\n\n\n\n\n\nPCA involves the analysis of eigenvalues and eigenvectors of the covariance or the correlation matrix. Its development relies on the following important facts:\n\nAll \\(n\\times n\\) real valued symmetric matrices (like the covariance and correlation matrix) have two very important properties:\n\nThey have a complete set of \\(n\\) linearly independent eigenvectors, \\(\\{\\v_1,\\dots,\\v_n\\}\\), corresponding to eigenvalues \\[\\lambda_1 \\geq \\lambda_2 \\geq\\dots\\geq \\lambda_n.\\]\nFurthermore, these eigenvectors can be always be chosen to be orthonormal so that if \\(\\V=[\\v_1|\\dots|\\v_n]\\) then \\[\\V^{T}\\V=\\bo{I}\\] or equivalently, \\(\\V^{-1}=\\V^{T}\\).\n\nLetting \\(\\D\\) be a diagonal matrix with \\(D_{ii}=\\lambda_i\\), by the definition of eigenvalues and eigenvectors we have for any symmetric matrix \\(\\bo{S}\\), \\[\\bo{S}\\V=\\V\\D\\] Thus, any symmetric matrix \\(\\bo{S}\\) can be diagonalized in the following way: \\[\\V^{T}\\bo{S}\\V=\\D\\] Covariance and Correlation matrices (when there is no perfect multicollinearity in variables) have the additional property that all of their eigenvalues are positive (nonzero). They are positive definite matrices.\n\nNow that we know we have a complete set of eigenvectors, it is common to order them according to the magnitude of their corresponding eigenvalues. From here on out, we will use \\((\\lambda_1,\\v_1)\\) to represent the largest eigenvalue of a matrix and its corresponding eigenvector. When working with a covariance or correlation matrix, this eigenvector associated with the largest eigenvalue is called the first principal component and points in the direction for which the variance of the data is maximal. Example @ref(exm:coveigs) illustrates this point.\n\nSuppose we have a matrix of data for 10 individuals on 2 variables, \\(\\x_1\\) and \\(\\x_2\\). Plotted on a plane, the data appears as follows:\n\n\n\n\n\n\n\n\n\nOur data matrix for these points is: \\[\\X=\\pm 1 & 1\\\\2&1\\\\2&4\\\\3&1\\\\4&4\\\\5&2\\\\6&4\\\\6&6\\\\7&6\\\\8&8 \\mp\\] the means of the variables in \\(\\X\\) are: \\[\\bar{\\x}=\\pm 4.4 \\\\ 3.7 \\mp. \\] When thinking about variance directions, our first step should be to center the data so that it has mean zero. Eigenvectors measure the spread of data around the origin. Variance measures spread of data around the mean. Thus, we need to equate the mean with the origin. To center the data, we simply compute \\[\\X_c=\\X-\\e\\bar{\\x}^T = \\pm 1 & 1\\\\2&1\\\\2&4\\\\3&1\\\\4&4\\\\5&2\\\\6&4\\\\6&6\\\\7&6\\\\8&8 \\mp - \\pm 4.4 & 3.7 \\\\4.4 & 3.7 \\\\4.4 & 3.7 \\\\4.4 & 3.7 \\\\4.4 & 3.7 \\\\4.4 & 3.7 \\\\4.4 & 3.7 \\\\4.4 & 3.7 \\\\4.4 & 3.7 \\\\4.4 & 3.7  \\mp = \\pm -3.4&-2.7\\\\-2.4&-2.7\\\\-2.4& 0.3\\\\-1.4&-2.7\\\\ -0.4&  0.3\\\\0.6&-1.7\\\\1.6& 0.3\\\\1.6& 2.3\\\\2.6& 2.3\\\\3.6&  4.3\\mp.\\] Examining the new centered data, we find that we’ve only translated our data in the plane - we haven’t distorted it in any fashion.\n\n\n\n\n\n\n\n\n\nThus the covariance matrix is: \\[\\ssigma=\\frac{1}{9}(\\X_c^T\\X_c)= \\pm 5.6 & 4.8\\\\4.8&6.0111 \\mp \\] The eigenvalue and eigenvector pairs of \\(\\ssigma\\) are (rounded to 2 decimal places) as follows: \\[(\\lambda_1,\\v_1)=\\left( 10.6100 ,  \\begin{bmatrix} 0.69 \\\\ 0.72 \\end{bmatrix}\\right) \\mbox{  and  } (\\lambda_2,\\v_2)= \\left( 1.0012,\\begin{bmatrix}-0.72\\\\0.69 \\end{bmatrix}\\right)\\] Let’s plot the eigenvector directions on the same graph:\n\n\n\n\n\n\n\n\n\nThe eigenvector \\(\\v_1\\) is called the first principal component. It is the direction along which the variance of the data is maximal. The eigenvector \\(\\v_2\\) is the second principal component. In general, the second principal component is the direction, orthogonal to the first, along which the variance of the data is maximal (in two dimensions, there is only one direction possible.)\n\nWhy is this important? Let’s consider what we’ve just done. We started with two variables, \\(\\x_1\\) and \\(\\x_2\\), which appeared to be correlated. We then derived new variables, \\(\\v_1\\) and \\(\\v_2\\), which are linear combinations of the original variables: \\[\\begin{eqnarray}\n\\v_1 &=& 0.69\\x_1 + 0.72\\x_2 \\\\\n(\\#eq:pcacomb)\n\\v_2 &=& -0.72\\x_1 + 0.69\\x_2\n\\end{eqnarray}\\] These new variables are completely uncorrelated. To see this, let’s represent our data according to the new variables - i.e. let’s change the basis from \\(\\mathcal{B}_1=[\\x_1,\\x_2]\\) to \\(\\mathcal{B}_2=[\\v_1,\\v_2]\\).\n\nLet’s express our data in the basis defined by the principal components. We want to find coordinates (in a \\(2\\times 10\\) matrix \\(\\A\\)) such that our original (centered) data can be expressed in terms of principal components. This is done by solving for \\(\\A\\) in the following equation (see Chapter @ref(basis) and note that the rows of \\(\\X\\) define the points rather than the columns): \\[\\begin{eqnarray}\n\\X_c &=& \\A \\V^T \\\\\n\\pm -3.4&-2.7\\\\-2.4&-2.7\\\\-2.4& 0.3\\\\-1.4&-2.7\\\\ -0.4&  0.3\\\\0.6&-1.7\\\\1.6& 0.3\\\\1.6& 2.3\\\\2.6& 2.3\\\\3.6&  4.3 \\mp &=&  \\pm a_{11} & a_{12} \\\\ a_{21} & a_{22} \\\\ a_{31} & a_{32}\\\\ a_{41} & a_{42}\\\\ a_{51} & a_{52}\\\\ a_{61} & a_{62}\\\\ a_{71} & a_{72}\\\\ a_{81} & a_{82}\\\\ a_{91} & a_{92}\\\\ a_{10,1} & a_{10,2} \\mp \\pm \\v_1^T \\\\ \\v_2^T \\mp\n\\end{eqnarray}\\]\nConveniently, our new basis is orthonormal meaning that \\(\\V\\) is an orthogonal matrix, so \\[\\A=\\X\\V .\\] The new data coordinates reflect a simple rotation of the data around the origin:\n\n\n\n\n\n\n\n\n\nVisually, we can see that the new variables are uncorrelated. You may wish to confirm this by calculating the covariance. In fact, we can do this in a general sense. If \\(\\A=\\X_c\\V\\) is our new data, then the covariance matrix is diagonal: \\[\\begin{eqnarray*}\n\\ssigma_A &=& \\frac{1}{n-1}\\A^T\\A  \\\\\n    &=& \\frac{1}{n-1}(\\X_c\\V)^T(\\X_c\\V) \\\\\n    &=& \\frac{1}{n-1}\\V^T((\\X_c^T\\X_c)\\V\\\\\n    &=&\\frac{1}{n-1}\\V^T((n-1)\\ssigma_X)\\V\\\\\n    &=&\\V^T(\\ssigma_X)\\V\\\\\n    &=&\\V^T(\\V\\D\\V^T)\\V\\\\\n    &=& \\D\n\\end{eqnarray*}\\] Where \\(\\ssigma_X=\\V\\D\\V^T\\) comes from the diagonalization in Theorem @ref(thm:eigsym). By changing our variables to principal components, we have managed to “hide” the correlation between \\(\\x_1\\) and \\(\\x_2\\) while keeping the spacial relationships between data points in tact. Transformation back to variables \\(\\x_1\\) and \\(\\x_2\\) is easily done by using the linear relationships in from Equation @ref(eq:pcacomb).\n\n\n\n\nIn least squares regression, our objective is to maximize the amount of variance explained in our target variable. It may look as though the first principal component from Example @ref(exm:coveigs) points in the direction of the regression line. This is not the case however. The first principal component points in the direction of a line which minimizes the sum of squared orthogonal distances between the points and the line. Regressing \\(\\x_2\\) on \\(\\x_1\\), on the other hand, provides a line which minimizes the sum of squared vertical distances between points and the line. This is illustrated in Figure @ref(fig:pcvsreg).\n::: {.cell layout-align=“center” fig=‘true’} ::: {.cell-output-display}  ::: :::\nThe first principal component about the mean of a set of points can be represented by that line which most closely approaches the data points. Let this not conjure up images of linear regression in your head, though. In contrast, linear least squares tries to minimize the distance in a single direction only (the direction of your target variable axes). Thus, although the two use a similar error metric, linear least squares is a method that treats one dimension of the data preferentially, while PCA treats all dimensions equally.\nYou might be tempted to conclude from Figure @ref(fig:pcvsreg) that the first principal component and the regression line “ought to be similar.” This is a terrible conclusion if you consider a large multivariate dataset and the various regression lines that would predict each variable in that dataset. In PCA, there is no target variable and thus no single regression line that we’d be comparing to.\n\n\n\nPrincipal components analysis can involve eigenvectors of either the covariance matrix or the correlation matrix. When we perform this analysis on the covariance matrix, the geometric interpretation is simply centering the data and then determining the direction of maximal variance. When we perform this analysis on the correlation matrix, the interpretation is standardizing the data and then determining the direction of maximal variance. The correlation matrix is simply a scaled form of the covariance matrix. In general, these two methods give different results, especially when the scales of the variables are different.\nThe covariance matrix is the default for (most) \\(\\textsf{R}\\) PCA functions. The correlation matrix is the default in SAS and the covariance matrix method is invoked by the option:\nproc princomp data=X cov; \nvar x1--x10;\nrun;\nChoosing between the covariance and correlation matrix can sometimes pose problems. The rule of thumb is that the correlation matrix should be used when the scales of the variables vary greatly. In this case, the variables with the highest variance will dominate the first principal component. The argument against automatically using correlation matrices is that it turns out to be quite a brutal way of standardizing your data - forcing all variables to contain the same amount of information (after all, don’t we equate variance to information?) seems naive and counterintuitive when it is not absolutely necessary for differences in scale. We hope that the case studies outlined in Chapter @ref(pcaapp) will give those who always use the correlation option reason for pause, and we hope that, in the future, they will consider multiple presentations of the data and their corresponding low-rank representations of the data.\n\n\n\nLet’s find Principal Components using the iris dataset. This is a well-known dataset, often used to demonstrate the effect of clustering algorithms. It contains numeric measurements for 150 iris flowers along 4 dimensions. The fifth column in the dataset tells us what species of Iris the flower is. There are 3 species.\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\nSetosa\nVersicolor\nVirginica\n\nLet’s first take a look at the scatterplot matrix:\n\npairs(~Sepal.Length+Sepal.Width+Petal.Length+Petal.Width,data=iris,col=c(\"red\",\"green3\",\"blue\")[iris$Species])\n\n\n\n\n\n\n\n\nIt is apparent that some of our variables are correlated. We can confirm this by computing the correlation matrix with the cor() function. We can also check out the individual variances of the variables and the covariances between variables by examining the covariance matrix (cov() function). Remember - when looking at covariances, we can really only interpret the sign of the number and not the magnitude as we can with the correlations.\n\ncor(iris[1:4])\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length    1.0000000  -0.1175698    0.8717538   0.8179411\nSepal.Width    -0.1175698   1.0000000   -0.4284401  -0.3661259\nPetal.Length    0.8717538  -0.4284401    1.0000000   0.9628654\nPetal.Width     0.8179411  -0.3661259    0.9628654   1.0000000\n\ncov(iris[1:4])\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length    0.6856935  -0.0424340    1.2743154   0.5162707\nSepal.Width    -0.0424340   0.1899794   -0.3296564  -0.1216394\nPetal.Length    1.2743154  -0.3296564    3.1162779   1.2956094\nPetal.Width     0.5162707  -0.1216394    1.2956094   0.5810063\n\n\nWe have relatively strong positive correlation between Petal Length, Petal Width and Sepal Length. It is also clear that Petal Length has more than 3 times the variance of the other 3 variables. How will this effect our analysis?\nThe scatter plots and correlation matrix provide useful information, but they don’t give us a true sense for how the data looks when all 4 attributes are considered simultaneously.\nIn the next section we will compute the principal components directly from eigenvalues and eigenvectors of the covariance or correlation matrix. It’s important to note that this method of computing principal components is not actually recommended - the answer provided is the same, but the numerical stability and efficiency of this method may be dubious for large datasets. The Singular Value Decomposition (SVD), which will be discussed in Chapter @ref(svd), is generally a preferred route to computing principal components. Using both the covariance matrix and the correlation matrix, let’s see what we can learn about the data. Let’s start with the covariance matrix which is the default setting for the prcomp() function in R.\n\n\nLet’s start with the covariance matrix which is the default setting for the prcomp function in R. It’s worth repeating that a dedicated principal component function like prcomp() is superior in numerical stability and efficiency to the lines of code in the next section. The only reason for directly computing the covariance matrix and its eigenvalues and eigenvectors (as opposed to prcomp()) is for edification. Computing a PCA in this manner, just this once, will help us grasp the exact mathematics of the situation and empower us to use built in functions with greater flexibility and understanding.\n\n\n\n\ncovM = cov(iris[1:4])\neig=eigen(covM,symmetric=TRUE,only.values=FALSE)\nc=colnames(iris[1:4])\neig$values\n\n[1] 4.22824171 0.24267075 0.07820950 0.02383509\n\n# Label the loadings\nrownames(eig$vectors)=c(colnames(iris[1:4]))\n# eig$vectors\n\nThe eigenvalues tell us how much of the total variance in the data is directed along each eigenvector. Thus, the amount of variance along \\(\\mathbf{v}_1\\) is \\(\\lambda_1\\) and the proportion of variance explained by the first principal component is \\[\\frac{\\lambda_1}{\\lambda_1+\\lambda_2+\\lambda_3+\\lambda_4}\\]\n\neig$values[1]/sum(eig$values)\n\n[1] 0.9246187\n\n\nThus 92% of the variation in the Iris data is explained by the first component alone. What if we consider the first and second principal component directions? Using this two dimensional representation (approximation/projection) we can capture the following proportion of variance: \\[\\frac{\\lambda_1+\\lambda_2}{\\lambda_1+\\lambda_2+\\lambda_3+\\lambda_4}\\]\n\nsum(eig$values[1:2])/sum(eig$values)\n\n[1] 0.9776852\n\n\nWith two dimensions, we explain 97.8% of the variance in these 4 variables! The entries in each eigenvector are called the loadings of the variables on the component. The loadings give us an idea how important each variable is to each component. For example, it seems that the third variable in our dataset (Petal Length) is dominating the first principal component. This should not come as too much of a shock - that variable had (by far) the largest amount of variation of the four. In order to capture the most amount of variance in a single dimension, we should certainly be considering this variable strongly. The variable with the next largest variance, Sepal Length, dominates the second principal component.\nNote: Had Petal Length and Sepal Length been correlated, they would not have dominated separate principal components, they would have shared one. These two variables are not correlated and thus their variation cannot be captured along the same direction.\n\n\n\nLets plot the projection of the four-dimensional iris data onto the two dimensional space spanned by the first 2 principal components. To do this, we need coordinates. These coordinates are commonly called scores in statistical texts. We can find the coordinates of the data on the principal components by solving the system \\[\\mathbf{X}=\\mathbf{A}\\mathbf{V}^T\\] where \\(\\mathbf{X}\\) is our original iris data (centered to have mean = 0) and \\(\\mathbf{A}\\) is a matrix of coordinates in the new principal component space, spanned by the eigenvectors in \\(\\mathbf{V}\\).\nSolving this system is simple enough - since \\(\\mathbf{V}\\) is an orthogonal matrix per Theorem @ref(thm:eigsym). Let’s confirm this:\n\neig$vectors %*% t(eig$vectors)\n\n              Sepal.Length   Sepal.Width  Petal.Length   Petal.Width\nSepal.Length  1.000000e+00 -6.480855e-17 -5.086909e-17 -5.294610e-17\nSepal.Width  -6.480855e-17  1.000000e+00 -2.095187e-16 -1.773774e-17\nPetal.Length -5.086909e-17 -2.095187e-16  1.000000e+00  2.294136e-16\nPetal.Width  -5.294610e-17 -1.773774e-17  2.294136e-16  1.000000e+00\n\nt(eig$vectors) %*% eig$vectors\n\n              [,1]          [,2]         [,3]          [,4]\n[1,]  1.000000e+00 -2.190804e-16 6.787568e-17 -2.951998e-17\n[2,] -2.190804e-16  1.000000e+00 1.708956e-16  1.568895e-16\n[3,]  6.787568e-17  1.708956e-16 1.000000e+00  7.101298e-17\n[4,] -2.951998e-17  1.568895e-16 7.101298e-17  1.000000e+00\n\n\nWe’ll have to settle for precision at 15 decimal places. Close enough!\nTo find the scores, we simply subtract the means from our original variables to create the data matrix \\(\\mathbf{X}\\) and compute \\[\\mathbf{A}=\\mathbf{X}\\mathbf{V}\\]\n\n# The scale function centers and scales by default\nX=scale(iris[1:4],center=TRUE,scale=FALSE)\n# Create data.frame from matrix for plotting purposes.\nscores=data.frame(X %*% eig$vectors)\n# Change default variable names\ncolnames(scores)=c(\"Prin1\",\"Prin2\",\"Prin3\",\"Prin4\")\n# Print coordinates/scores of first 10 observations\nscores[1:10, ]\n\n       Prin1       Prin2       Prin3        Prin4\n1  -2.684126  0.31939725  0.02791483  0.002262437\n2  -2.714142 -0.17700123  0.21046427  0.099026550\n3  -2.888991 -0.14494943 -0.01790026  0.019968390\n4  -2.745343 -0.31829898 -0.03155937 -0.075575817\n5  -2.728717  0.32675451 -0.09007924 -0.061258593\n6  -2.280860  0.74133045 -0.16867766 -0.024200858\n7  -2.820538 -0.08946138 -0.25789216 -0.048143106\n8  -2.626145  0.16338496  0.02187932 -0.045297871\n9  -2.886383 -0.57831175 -0.02075957 -0.026744736\n10 -2.672756 -0.11377425  0.19763272 -0.056295401\n\n\nTo this point, we have simply computed coordinates (scores) on a new set of axis (formed by the principal components, i.e. the eigenvectors). These axes are orthogonal and are aligned with the directions of maximal variance in the data. When we consider only a subset of principal components (like 2 the two components here that account for 97.8% of the variance), we are projecting the data onto a lower dimensional space. Generally, this is one of the primary goals of PCA: Project the data down into a lower dimensional space (onto the span of the principal components) while keeping the maximum amount of information (i.e. variance).\nThus, we know that almost 98% of the data’s variance can be seen in two-dimensions using the first two principal components. Let’s go ahead and see what this looks like:\n\nplot(scores$Prin1, scores$Prin2, \n     main=\"Data Projected on First 2 Principal Components\",\n     xlab=\"First Principal Component\", \n     ylab=\"Second Principal Component\", \n     col=c(\"red\",\"green3\",\"blue\")[iris$Species])\n\n\n\n\n\n\n\n\n\n\n\n\nirispca=prcomp(iris[1:4])\n# Variance Explained\nsummary(irispca)\n\nImportance of components:\n                          PC1     PC2    PC3     PC4\nStandard deviation     2.0563 0.49262 0.2797 0.15439\nProportion of Variance 0.9246 0.05307 0.0171 0.00521\nCumulative Proportion  0.9246 0.97769 0.9948 1.00000\n\n# Eigenvectors:\nirispca$rotation\n\n                     PC1         PC2         PC3        PC4\nSepal.Length  0.36138659 -0.65658877  0.58202985  0.3154872\nSepal.Width  -0.08452251 -0.73016143 -0.59791083 -0.3197231\nPetal.Length  0.85667061  0.17337266 -0.07623608 -0.4798390\nPetal.Width   0.35828920  0.07548102 -0.54583143  0.7536574\n\n# Coordinates of first 10 observations along PCs:\nirispca$x[1:10, ]\n\n            PC1         PC2         PC3          PC4\n [1,] -2.684126 -0.31939725  0.02791483  0.002262437\n [2,] -2.714142  0.17700123  0.21046427  0.099026550\n [3,] -2.888991  0.14494943 -0.01790026  0.019968390\n [4,] -2.745343  0.31829898 -0.03155937 -0.075575817\n [5,] -2.728717 -0.32675451 -0.09007924 -0.061258593\n [6,] -2.280860 -0.74133045 -0.16867766 -0.024200858\n [7,] -2.820538  0.08946138 -0.25789216 -0.048143106\n [8,] -2.626145 -0.16338496  0.02187932 -0.045297871\n [9,] -2.886383  0.57831175 -0.02075957 -0.026744736\n[10,] -2.672756  0.11377425  0.19763272 -0.056295401\n\n\nAll of the information we computed using eigenvectors aligns with what we see here, except that the coordinates/scores and the loadings of Principal Component 3 are of the opposite sign. In light of what we know about eigenvectors representing directions, this should be no cause for alarm. The prcomp function arrived at the unit basis vector pointing in the negative direction of the one we found directly from the eig function - which should negate all the coordinates and leave us with an equivalent mirror image in all of our projections.\n\n\n\nOne additional feature that R users have created is the biplot. The PCA biplot allows us to see where our original variables fall in the space of the principal components. Highly correlated variables will fall along the same direction (or exactly opposite directions) as a change in one of these variables correlates to a change in the other. Uncorrelated variables will appear further apart. The length of the variable vectors on the biplot tell us the degree to which variability in variable is explained in that direction. Shorter vectors have less variability than longer vectors. So in the biplot below, petal width and petal length point in the same direction indicating that these variables share a relatively high degree of correlation. However, the vector for petal width is much shorter than that of petal length, which means you can expect a higher degree of change in petal length as you proceed to the right along PC1. PC1 explains more of the variance in petal length than it does petal width. If we were to imagine a third PC orthogonal to the plane shown, petal width is likely to exist at much larger angle off the plane - here, it is being projected down from that 3-dimensional picture.\n\nbiplot(irispca, col = c(\"gray\", \"blue\"))\n\n\n\n\n\n\n\n\nWe can examine some of the outlying observations to see how they align with these projected variable directions. It helps to compare them to the quartiles of the data. Also keep in mind the direction of the arrows in the plot. If the arrow points down then the positive direction is down - indicating observations which are greater than the mean. Let’s pick out observations 42 and 132 and see what the actual data points look like in comparison to the rest of the sample population.\n\nsummary(iris[1:4])\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n\n# Consider orientation of outlying observations:\niris[42, ]\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n42          4.5         2.3          1.3         0.3  setosa\n\niris[132, ]\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n132          7.9         3.8          6.4           2 virginica\n\n\n\n\n\n\nThe direction arrows on the biplot are merely the coefficients of the original variables when combined to make principal components. Don’t forget that principal components are simply linear combinations of the original variables.\nFor example, here we have the first principal component (the first column of \\(\\V\\)), \\(\\mathbf{v}_1\\) as:\n\neig$vectors[,1]\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n  0.36138659  -0.08452251   0.85667061   0.35828920 \n\n\nThis means that the coordinates of the data along the first principal component, which we’ll denote here as \\(PC_1\\) are given by a simple linear combination of our original variables after centering (for covariance PCA) or standardization (for correlation PCA)\n\\[PC_1 = 0.36Sepal.Length-0.08Sepal.Width+0.85Petal.Length +0.35Petal.Width\\] the same equation could be written for each of the vectors of coordinates along principal components, \\(PC_1,\\dots, PC_4\\).\nEssentially, we have a system of equations telling us that the rows of \\(\\V^T\\) (i.e. the columns of \\(\\V\\)) give us the weights of each variable for each principal component: \\[\\begin{equation}\n(\\#eq:cpc1)\n\\begin{bmatrix} PC_1\\\\PC_2\\\\PC_3\\\\PC_4\\end{bmatrix} = \\mathbf{V}^T\\begin{bmatrix}Sepal.Length\\\\Sepal.Width\\\\Petal.Length\\\\Petal.Width\\end{bmatrix}\n\\end{equation}\\]\nThus, if want the coordinates of our original variables in terms of Principal Components (so that we can plot them as we do in the biplot) we need to look no further than the rows of the matrix \\(\\mathbf{V}\\) as \\[\\begin{equation}\n(\\#eq:cpc2)\n\\begin{bmatrix}Sepal.Length\\\\Sepal.Width\\\\Petal.Length\\\\Petal.Width\\end{bmatrix} =\\mathbf{V}\\begin{bmatrix} PC_1\\\\PC_2\\\\PC_3\\\\PC_4\\end{bmatrix}\n\\end{equation}\\]\nmeans that the rows of \\(\\mathbf{V}\\) give us the coordinates of our original variables in the PCA space. The transition from Equation @ref(eq:cpc1) to Equation @ref(eq:cpc2) is provided by the orthogonality of the eigenvectors per Theorem @ref(thm:eigsym).\n\n#First entry in each eigenvectors give coefficients for Variable 1:\neig$vectors[1,]\n\n[1] 0.3613866 0.6565888 0.5820299 0.3154872\n\n\n\\[Sepal.Length = 0.361 PC_1 - 0.657 PC_2 - 0.582 PC_3 + 0.315 PC_4\\] You can see this on the biplot. The vector shown for Sepal.Length is (0.361, -0.656), which is the two dimensional projection formed by throwing out components 3 and 4.\nVariables which lie upon similar directions in the PCA space tend to change together in a similar fashion. We might consider Petal.Width and Petal.Length as a cluster of variables because they share a direction on the biplot, which means they represent much of the same information (the underlying construct being the “size of the petal” in this case).\n\n\nWe can complete the same analysis using the correlation matrix. I’ll leave it as an exercise to compute the Principal Component loadings and scores and variance explained directly from eigenvectors and eigenvalues. You should do this and compare your results to the R output. (Beware: you must transform your data before solving for the scores. With the covariance version, this meant centering - for the correlation version, this means standardization as well)\n\nirispca2=prcomp(iris[1:4], cor=TRUE)\n\nWarning: In prcomp.default(iris[1:4], cor = TRUE) :\n extra argument 'cor' will be disregarded\n\nsummary(irispca2)\n\nImportance of components:\n                          PC1     PC2    PC3     PC4\nStandard deviation     2.0563 0.49262 0.2797 0.15439\nProportion of Variance 0.9246 0.05307 0.0171 0.00521\nCumulative Proportion  0.9246 0.97769 0.9948 1.00000\n\nirispca2$rotation\n\n                     PC1         PC2         PC3        PC4\nSepal.Length  0.36138659 -0.65658877  0.58202985  0.3154872\nSepal.Width  -0.08452251 -0.73016143 -0.59791083 -0.3197231\nPetal.Length  0.85667061  0.17337266 -0.07623608 -0.4798390\nPetal.Width   0.35828920  0.07548102 -0.54583143  0.7536574\n\nirispca2$x[1:10,]\n\n            PC1         PC2         PC3          PC4\n [1,] -2.684126 -0.31939725  0.02791483  0.002262437\n [2,] -2.714142  0.17700123  0.21046427  0.099026550\n [3,] -2.888991  0.14494943 -0.01790026  0.019968390\n [4,] -2.745343  0.31829898 -0.03155937 -0.075575817\n [5,] -2.728717 -0.32675451 -0.09007924 -0.061258593\n [6,] -2.280860 -0.74133045 -0.16867766 -0.024200858\n [7,] -2.820538  0.08946138 -0.25789216 -0.048143106\n [8,] -2.626145 -0.16338496  0.02187932 -0.045297871\n [9,] -2.886383  0.57831175 -0.02075957 -0.026744736\n[10,] -2.672756  0.11377425  0.19763272 -0.056295401\n\nplot(irispca2$x[,1],irispca2$x[,2],\n     main=\"Data Projected on First 2  Principal Components\",\n     xlab=\"First Principal Component\",\n     ylab=\"Second Principal Component\",\n     col=c(\"red\",\"green3\",\"blue\")[iris$Species])\n\n\n\n\n\n\n\n\n\nbiplot(irispca2)\n\n\n\n\n\n\n\n\nHere you can see the direction vectors of the original variables are relatively uniform in length in the PCA space. This is due to the standardization in the correlation matrix. However, the general message is the same: Petal.Width and Petal.Length Cluster together, and many of the same observations appear “on the fray” on the PCA space - although not all of them!\n\n\n\nWhat do you think? It depends on the task, and it depends on the data. One flavor of PCA is not “better” than the other. Correlation PCA is appropriate when the scales of your attributes differ wildly, and covariance PCA would be inappropriate in that situation. But in all other scenarios, when the scales of our attributes are roughly the same, we should always consider both dimension reductions and make a decision based upon the resulting output (variance explained, projection plots, loadings).\nFor the iris data, The results in terms of variable clustering are pretty much the same. For clustering/classifying the 3 species of flowers, we can see better separation in the covariance version.\n\n\n\nBe careful not to draw improper conclusions from biplots. Particularly, be careful about situations where the first two principal components do not summarize the majority of the variance. If a large amount of variance is captured by the 3rd or 4th (or higher) principal components, then we must keep in mind that the variable projections on the first two principal components are flattened out versions of a higher dimensional picture. If a variable vector appears short in the 2-dimensional projection, it means one of two things:\n\nThat variable has small variance\nThat variable appears to have small variance when depicted in the space of the first two principal components, but truly has a larger variance which is represented by 3rd or higher principal components.\n\nLet’s take a look at an example of this. We’ll generate 500 rows of data on 4 nearly independent normal random variables. Since these variables are uncorrelated, we might expect that the 4 orthogonal principal components will line up relatively close to the original variables. If this doesn’t happen, then at the very least we can expect the biplot to show little to no correlation between the variables. We’ll give variables \\(2\\) and \\(3\\) the largest variance. Multiple runs of this code will generate different results with similar implications.\n\nmeans=c(2,4,1,3)\nsigmas=c(7,9,10,8)\nsample.size=500\ndata=mapply(function(mu,sig){rnorm(mu,sig, n=sample.size)},mu=means,sig=sigmas)\ncor(data)\n\n            [,1]        [,2]        [,3]        [,4]\n[1,]  1.00000000 -0.02175493 -0.02616236 -0.03828080\n[2,] -0.02175493  1.00000000 -0.05596491 -0.02800197\n[3,] -0.02616236 -0.05596491  1.00000000  0.06162748\n[4,] -0.03828080 -0.02800197  0.06162748  1.00000000\n\npc=prcomp(data,scale=TRUE)\nsummary(pc)\n\nImportance of components:\n                          PC1    PC2    PC3    PC4\nStandard deviation     1.0517 1.0126 0.9706 0.9625\nProportion of Variance 0.2765 0.2564 0.2355 0.2316\nCumulative Proportion  0.2765 0.5329 0.7684 1.0000\n\npc$rotation\n\n            PC1         PC2        PC3        PC4\n[1,] -0.2774310  0.74816466  0.5275858  0.2914360\n[2,] -0.4301634 -0.62263259  0.3527535  0.5503209\n[3,]  0.6327338  0.09598568 -0.2114630  0.7387273\n[4,]  0.5810675 -0.20824275  0.7433045 -0.2578641\n\nbiplot(pc)\n\n\n\n\n\n\n\n\nBiPlot of Iris Data\n\n\n\n\nObviously, the wrong conclusion to make from this biplot is that Variables 1 and 4 are correlated. Variables 1 and 4 do not load highly on the first two principal components - in the whole 4-dimensional principal component space they are nearly orthogonal to each other and to variables 1 and 2. Thus, their orthogonal projections appear near the origin of this 2-dimensional subspace.\nThe morals of the story:\n\n\nAlways corroborate your results using the variable loadings and the amount of variation explained by each variable.\n\nWhen a variable shows up near the origin in a biplot, it is generally not well represented by your two-dimensional approximation of the data.\n\n\n\n\nBiPlot of Iris Data"
  },
  {
    "objectID": "notebooks/pca/pca.html#gods-flashlight",
    "href": "notebooks/pca/pca.html#gods-flashlight",
    "title": "Ch5 Lecture 5",
    "section": "",
    "text": "Imagine your data as a multidimensional cloud of points in space. God (however you conceive that) has a flashlight and can project this data at a right angle down onto a flat surface - the flashlight just casts a point shadow, the shadows don’t get bigger like on Earth. The center of the flat surface is fixed at the center of the data, so it’s more like 2 flashlights, one from below the surface, one from above, both at right angles. We could rotate this flashlight/flat surface setup around and get infinitely many projections of the data from different perspectives. The PCA projection is the one with the most variance, which indicates that it contains the most information from your original data. It’s also the projection that is closest to the original data in the Euclidean or sum-of-squared-error sense (PCA gives the rank k approximation to your data with the lowest possible error). Once projected, the axes of the projection (drawn so that the “first” axis points in the direction of greatest variance) are your principal components, providing the orthogonal directions of maximal variance.\nThis projection we’ve just described is actually the projection of the data onto a hyperplane, which entails a rank reduction of 1, though you might have imagined it as a projection onto a 2-dimensional plane. The great thing about PCA is that both of those visuals are appropriate - we can project the data onto any dimensional subspace of the original from 1 to rank(\\(\\X\\))-1.\nWith this analogy in mind, we bring back to the interactive plot from Chapter @ref(orthog) to ponder what these different projections of a data cloud would look like, and to locate the maximum variance projection of this data."
  },
  {
    "objectID": "notebooks/pca/pca.html#pca-details",
    "href": "notebooks/pca/pca.html#pca-details",
    "title": "Ch5 Lecture 5",
    "section": "",
    "text": "PCA involves the analysis of eigenvalues and eigenvectors of the covariance or the correlation matrix. Its development relies on the following important facts:\n\nAll \\(n\\times n\\) real valued symmetric matrices (like the covariance and correlation matrix) have two very important properties:\n\nThey have a complete set of \\(n\\) linearly independent eigenvectors, \\(\\{\\v_1,\\dots,\\v_n\\}\\), corresponding to eigenvalues \\[\\lambda_1 \\geq \\lambda_2 \\geq\\dots\\geq \\lambda_n.\\]\nFurthermore, these eigenvectors can be always be chosen to be orthonormal so that if \\(\\V=[\\v_1|\\dots|\\v_n]\\) then \\[\\V^{T}\\V=\\bo{I}\\] or equivalently, \\(\\V^{-1}=\\V^{T}\\).\n\nLetting \\(\\D\\) be a diagonal matrix with \\(D_{ii}=\\lambda_i\\), by the definition of eigenvalues and eigenvectors we have for any symmetric matrix \\(\\bo{S}\\), \\[\\bo{S}\\V=\\V\\D\\] Thus, any symmetric matrix \\(\\bo{S}\\) can be diagonalized in the following way: \\[\\V^{T}\\bo{S}\\V=\\D\\] Covariance and Correlation matrices (when there is no perfect multicollinearity in variables) have the additional property that all of their eigenvalues are positive (nonzero). They are positive definite matrices.\n\nNow that we know we have a complete set of eigenvectors, it is common to order them according to the magnitude of their corresponding eigenvalues. From here on out, we will use \\((\\lambda_1,\\v_1)\\) to represent the largest eigenvalue of a matrix and its corresponding eigenvector. When working with a covariance or correlation matrix, this eigenvector associated with the largest eigenvalue is called the first principal component and points in the direction for which the variance of the data is maximal. Example @ref(exm:coveigs) illustrates this point.\n\nSuppose we have a matrix of data for 10 individuals on 2 variables, \\(\\x_1\\) and \\(\\x_2\\). Plotted on a plane, the data appears as follows:\n\n\n\n\n\n\n\n\n\nOur data matrix for these points is: \\[\\X=\\pm 1 & 1\\\\2&1\\\\2&4\\\\3&1\\\\4&4\\\\5&2\\\\6&4\\\\6&6\\\\7&6\\\\8&8 \\mp\\] the means of the variables in \\(\\X\\) are: \\[\\bar{\\x}=\\pm 4.4 \\\\ 3.7 \\mp. \\] When thinking about variance directions, our first step should be to center the data so that it has mean zero. Eigenvectors measure the spread of data around the origin. Variance measures spread of data around the mean. Thus, we need to equate the mean with the origin. To center the data, we simply compute \\[\\X_c=\\X-\\e\\bar{\\x}^T = \\pm 1 & 1\\\\2&1\\\\2&4\\\\3&1\\\\4&4\\\\5&2\\\\6&4\\\\6&6\\\\7&6\\\\8&8 \\mp - \\pm 4.4 & 3.7 \\\\4.4 & 3.7 \\\\4.4 & 3.7 \\\\4.4 & 3.7 \\\\4.4 & 3.7 \\\\4.4 & 3.7 \\\\4.4 & 3.7 \\\\4.4 & 3.7 \\\\4.4 & 3.7 \\\\4.4 & 3.7  \\mp = \\pm -3.4&-2.7\\\\-2.4&-2.7\\\\-2.4& 0.3\\\\-1.4&-2.7\\\\ -0.4&  0.3\\\\0.6&-1.7\\\\1.6& 0.3\\\\1.6& 2.3\\\\2.6& 2.3\\\\3.6&  4.3\\mp.\\] Examining the new centered data, we find that we’ve only translated our data in the plane - we haven’t distorted it in any fashion.\n\n\n\n\n\n\n\n\n\nThus the covariance matrix is: \\[\\ssigma=\\frac{1}{9}(\\X_c^T\\X_c)= \\pm 5.6 & 4.8\\\\4.8&6.0111 \\mp \\] The eigenvalue and eigenvector pairs of \\(\\ssigma\\) are (rounded to 2 decimal places) as follows: \\[(\\lambda_1,\\v_1)=\\left( 10.6100 ,  \\begin{bmatrix} 0.69 \\\\ 0.72 \\end{bmatrix}\\right) \\mbox{  and  } (\\lambda_2,\\v_2)= \\left( 1.0012,\\begin{bmatrix}-0.72\\\\0.69 \\end{bmatrix}\\right)\\] Let’s plot the eigenvector directions on the same graph:\n\n\n\n\n\n\n\n\n\nThe eigenvector \\(\\v_1\\) is called the first principal component. It is the direction along which the variance of the data is maximal. The eigenvector \\(\\v_2\\) is the second principal component. In general, the second principal component is the direction, orthogonal to the first, along which the variance of the data is maximal (in two dimensions, there is only one direction possible.)\n\nWhy is this important? Let’s consider what we’ve just done. We started with two variables, \\(\\x_1\\) and \\(\\x_2\\), which appeared to be correlated. We then derived new variables, \\(\\v_1\\) and \\(\\v_2\\), which are linear combinations of the original variables: \\[\\begin{eqnarray}\n\\v_1 &=& 0.69\\x_1 + 0.72\\x_2 \\\\\n(\\#eq:pcacomb)\n\\v_2 &=& -0.72\\x_1 + 0.69\\x_2\n\\end{eqnarray}\\] These new variables are completely uncorrelated. To see this, let’s represent our data according to the new variables - i.e. let’s change the basis from \\(\\mathcal{B}_1=[\\x_1,\\x_2]\\) to \\(\\mathcal{B}_2=[\\v_1,\\v_2]\\).\n\nLet’s express our data in the basis defined by the principal components. We want to find coordinates (in a \\(2\\times 10\\) matrix \\(\\A\\)) such that our original (centered) data can be expressed in terms of principal components. This is done by solving for \\(\\A\\) in the following equation (see Chapter @ref(basis) and note that the rows of \\(\\X\\) define the points rather than the columns): \\[\\begin{eqnarray}\n\\X_c &=& \\A \\V^T \\\\\n\\pm -3.4&-2.7\\\\-2.4&-2.7\\\\-2.4& 0.3\\\\-1.4&-2.7\\\\ -0.4&  0.3\\\\0.6&-1.7\\\\1.6& 0.3\\\\1.6& 2.3\\\\2.6& 2.3\\\\3.6&  4.3 \\mp &=&  \\pm a_{11} & a_{12} \\\\ a_{21} & a_{22} \\\\ a_{31} & a_{32}\\\\ a_{41} & a_{42}\\\\ a_{51} & a_{52}\\\\ a_{61} & a_{62}\\\\ a_{71} & a_{72}\\\\ a_{81} & a_{82}\\\\ a_{91} & a_{92}\\\\ a_{10,1} & a_{10,2} \\mp \\pm \\v_1^T \\\\ \\v_2^T \\mp\n\\end{eqnarray}\\]\nConveniently, our new basis is orthonormal meaning that \\(\\V\\) is an orthogonal matrix, so \\[\\A=\\X\\V .\\] The new data coordinates reflect a simple rotation of the data around the origin:\n\n\n\n\n\n\n\n\n\nVisually, we can see that the new variables are uncorrelated. You may wish to confirm this by calculating the covariance. In fact, we can do this in a general sense. If \\(\\A=\\X_c\\V\\) is our new data, then the covariance matrix is diagonal: \\[\\begin{eqnarray*}\n\\ssigma_A &=& \\frac{1}{n-1}\\A^T\\A  \\\\\n    &=& \\frac{1}{n-1}(\\X_c\\V)^T(\\X_c\\V) \\\\\n    &=& \\frac{1}{n-1}\\V^T((\\X_c^T\\X_c)\\V\\\\\n    &=&\\frac{1}{n-1}\\V^T((n-1)\\ssigma_X)\\V\\\\\n    &=&\\V^T(\\ssigma_X)\\V\\\\\n    &=&\\V^T(\\V\\D\\V^T)\\V\\\\\n    &=& \\D\n\\end{eqnarray*}\\] Where \\(\\ssigma_X=\\V\\D\\V^T\\) comes from the diagonalization in Theorem @ref(thm:eigsym). By changing our variables to principal components, we have managed to “hide” the correlation between \\(\\x_1\\) and \\(\\x_2\\) while keeping the spacial relationships between data points in tact. Transformation back to variables \\(\\x_1\\) and \\(\\x_2\\) is easily done by using the linear relationships in from Equation @ref(eq:pcacomb)."
  },
  {
    "objectID": "notebooks/pca/pca.html#geometrical-comparison-with-least-squares",
    "href": "notebooks/pca/pca.html#geometrical-comparison-with-least-squares",
    "title": "Ch5 Lecture 5",
    "section": "",
    "text": "In least squares regression, our objective is to maximize the amount of variance explained in our target variable. It may look as though the first principal component from Example @ref(exm:coveigs) points in the direction of the regression line. This is not the case however. The first principal component points in the direction of a line which minimizes the sum of squared orthogonal distances between the points and the line. Regressing \\(\\x_2\\) on \\(\\x_1\\), on the other hand, provides a line which minimizes the sum of squared vertical distances between points and the line. This is illustrated in Figure @ref(fig:pcvsreg).\n::: {.cell layout-align=“center” fig=‘true’} ::: {.cell-output-display}  ::: :::\nThe first principal component about the mean of a set of points can be represented by that line which most closely approaches the data points. Let this not conjure up images of linear regression in your head, though. In contrast, linear least squares tries to minimize the distance in a single direction only (the direction of your target variable axes). Thus, although the two use a similar error metric, linear least squares is a method that treats one dimension of the data preferentially, while PCA treats all dimensions equally.\nYou might be tempted to conclude from Figure @ref(fig:pcvsreg) that the first principal component and the regression line “ought to be similar.” This is a terrible conclusion if you consider a large multivariate dataset and the various regression lines that would predict each variable in that dataset. In PCA, there is no target variable and thus no single regression line that we’d be comparing to."
  },
  {
    "objectID": "notebooks/pca/pca.html#covariance-or-correlation-matrix",
    "href": "notebooks/pca/pca.html#covariance-or-correlation-matrix",
    "title": "Ch5 Lecture 5",
    "section": "",
    "text": "Principal components analysis can involve eigenvectors of either the covariance matrix or the correlation matrix. When we perform this analysis on the covariance matrix, the geometric interpretation is simply centering the data and then determining the direction of maximal variance. When we perform this analysis on the correlation matrix, the interpretation is standardizing the data and then determining the direction of maximal variance. The correlation matrix is simply a scaled form of the covariance matrix. In general, these two methods give different results, especially when the scales of the variables are different.\nThe covariance matrix is the default for (most) \\(\\textsf{R}\\) PCA functions. The correlation matrix is the default in SAS and the covariance matrix method is invoked by the option:\nproc princomp data=X cov; \nvar x1--x10;\nrun;\nChoosing between the covariance and correlation matrix can sometimes pose problems. The rule of thumb is that the correlation matrix should be used when the scales of the variables vary greatly. In this case, the variables with the highest variance will dominate the first principal component. The argument against automatically using correlation matrices is that it turns out to be quite a brutal way of standardizing your data - forcing all variables to contain the same amount of information (after all, don’t we equate variance to information?) seems naive and counterintuitive when it is not absolutely necessary for differences in scale. We hope that the case studies outlined in Chapter @ref(pcaapp) will give those who always use the correlation option reason for pause, and we hope that, in the future, they will consider multiple presentations of the data and their corresponding low-rank representations of the data."
  },
  {
    "objectID": "notebooks/pca/pca.html#pca-in-r",
    "href": "notebooks/pca/pca.html#pca-in-r",
    "title": "Ch5 Lecture 5",
    "section": "",
    "text": "Let’s find Principal Components using the iris dataset. This is a well-known dataset, often used to demonstrate the effect of clustering algorithms. It contains numeric measurements for 150 iris flowers along 4 dimensions. The fifth column in the dataset tells us what species of Iris the flower is. There are 3 species.\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\nSetosa\nVersicolor\nVirginica\n\nLet’s first take a look at the scatterplot matrix:\n\npairs(~Sepal.Length+Sepal.Width+Petal.Length+Petal.Width,data=iris,col=c(\"red\",\"green3\",\"blue\")[iris$Species])\n\n\n\n\n\n\n\n\nIt is apparent that some of our variables are correlated. We can confirm this by computing the correlation matrix with the cor() function. We can also check out the individual variances of the variables and the covariances between variables by examining the covariance matrix (cov() function). Remember - when looking at covariances, we can really only interpret the sign of the number and not the magnitude as we can with the correlations.\n\ncor(iris[1:4])\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length    1.0000000  -0.1175698    0.8717538   0.8179411\nSepal.Width    -0.1175698   1.0000000   -0.4284401  -0.3661259\nPetal.Length    0.8717538  -0.4284401    1.0000000   0.9628654\nPetal.Width     0.8179411  -0.3661259    0.9628654   1.0000000\n\ncov(iris[1:4])\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length    0.6856935  -0.0424340    1.2743154   0.5162707\nSepal.Width    -0.0424340   0.1899794   -0.3296564  -0.1216394\nPetal.Length    1.2743154  -0.3296564    3.1162779   1.2956094\nPetal.Width     0.5162707  -0.1216394    1.2956094   0.5810063\n\n\nWe have relatively strong positive correlation between Petal Length, Petal Width and Sepal Length. It is also clear that Petal Length has more than 3 times the variance of the other 3 variables. How will this effect our analysis?\nThe scatter plots and correlation matrix provide useful information, but they don’t give us a true sense for how the data looks when all 4 attributes are considered simultaneously.\nIn the next section we will compute the principal components directly from eigenvalues and eigenvectors of the covariance or correlation matrix. It’s important to note that this method of computing principal components is not actually recommended - the answer provided is the same, but the numerical stability and efficiency of this method may be dubious for large datasets. The Singular Value Decomposition (SVD), which will be discussed in Chapter @ref(svd), is generally a preferred route to computing principal components. Using both the covariance matrix and the correlation matrix, let’s see what we can learn about the data. Let’s start with the covariance matrix which is the default setting for the prcomp() function in R.\n\n\nLet’s start with the covariance matrix which is the default setting for the prcomp function in R. It’s worth repeating that a dedicated principal component function like prcomp() is superior in numerical stability and efficiency to the lines of code in the next section. The only reason for directly computing the covariance matrix and its eigenvalues and eigenvectors (as opposed to prcomp()) is for edification. Computing a PCA in this manner, just this once, will help us grasp the exact mathematics of the situation and empower us to use built in functions with greater flexibility and understanding.\n\n\n\n\ncovM = cov(iris[1:4])\neig=eigen(covM,symmetric=TRUE,only.values=FALSE)\nc=colnames(iris[1:4])\neig$values\n\n[1] 4.22824171 0.24267075 0.07820950 0.02383509\n\n# Label the loadings\nrownames(eig$vectors)=c(colnames(iris[1:4]))\n# eig$vectors\n\nThe eigenvalues tell us how much of the total variance in the data is directed along each eigenvector. Thus, the amount of variance along \\(\\mathbf{v}_1\\) is \\(\\lambda_1\\) and the proportion of variance explained by the first principal component is \\[\\frac{\\lambda_1}{\\lambda_1+\\lambda_2+\\lambda_3+\\lambda_4}\\]\n\neig$values[1]/sum(eig$values)\n\n[1] 0.9246187\n\n\nThus 92% of the variation in the Iris data is explained by the first component alone. What if we consider the first and second principal component directions? Using this two dimensional representation (approximation/projection) we can capture the following proportion of variance: \\[\\frac{\\lambda_1+\\lambda_2}{\\lambda_1+\\lambda_2+\\lambda_3+\\lambda_4}\\]\n\nsum(eig$values[1:2])/sum(eig$values)\n\n[1] 0.9776852\n\n\nWith two dimensions, we explain 97.8% of the variance in these 4 variables! The entries in each eigenvector are called the loadings of the variables on the component. The loadings give us an idea how important each variable is to each component. For example, it seems that the third variable in our dataset (Petal Length) is dominating the first principal component. This should not come as too much of a shock - that variable had (by far) the largest amount of variation of the four. In order to capture the most amount of variance in a single dimension, we should certainly be considering this variable strongly. The variable with the next largest variance, Sepal Length, dominates the second principal component.\nNote: Had Petal Length and Sepal Length been correlated, they would not have dominated separate principal components, they would have shared one. These two variables are not correlated and thus their variation cannot be captured along the same direction.\n\n\n\nLets plot the projection of the four-dimensional iris data onto the two dimensional space spanned by the first 2 principal components. To do this, we need coordinates. These coordinates are commonly called scores in statistical texts. We can find the coordinates of the data on the principal components by solving the system \\[\\mathbf{X}=\\mathbf{A}\\mathbf{V}^T\\] where \\(\\mathbf{X}\\) is our original iris data (centered to have mean = 0) and \\(\\mathbf{A}\\) is a matrix of coordinates in the new principal component space, spanned by the eigenvectors in \\(\\mathbf{V}\\).\nSolving this system is simple enough - since \\(\\mathbf{V}\\) is an orthogonal matrix per Theorem @ref(thm:eigsym). Let’s confirm this:\n\neig$vectors %*% t(eig$vectors)\n\n              Sepal.Length   Sepal.Width  Petal.Length   Petal.Width\nSepal.Length  1.000000e+00 -6.480855e-17 -5.086909e-17 -5.294610e-17\nSepal.Width  -6.480855e-17  1.000000e+00 -2.095187e-16 -1.773774e-17\nPetal.Length -5.086909e-17 -2.095187e-16  1.000000e+00  2.294136e-16\nPetal.Width  -5.294610e-17 -1.773774e-17  2.294136e-16  1.000000e+00\n\nt(eig$vectors) %*% eig$vectors\n\n              [,1]          [,2]         [,3]          [,4]\n[1,]  1.000000e+00 -2.190804e-16 6.787568e-17 -2.951998e-17\n[2,] -2.190804e-16  1.000000e+00 1.708956e-16  1.568895e-16\n[3,]  6.787568e-17  1.708956e-16 1.000000e+00  7.101298e-17\n[4,] -2.951998e-17  1.568895e-16 7.101298e-17  1.000000e+00\n\n\nWe’ll have to settle for precision at 15 decimal places. Close enough!\nTo find the scores, we simply subtract the means from our original variables to create the data matrix \\(\\mathbf{X}\\) and compute \\[\\mathbf{A}=\\mathbf{X}\\mathbf{V}\\]\n\n# The scale function centers and scales by default\nX=scale(iris[1:4],center=TRUE,scale=FALSE)\n# Create data.frame from matrix for plotting purposes.\nscores=data.frame(X %*% eig$vectors)\n# Change default variable names\ncolnames(scores)=c(\"Prin1\",\"Prin2\",\"Prin3\",\"Prin4\")\n# Print coordinates/scores of first 10 observations\nscores[1:10, ]\n\n       Prin1       Prin2       Prin3        Prin4\n1  -2.684126  0.31939725  0.02791483  0.002262437\n2  -2.714142 -0.17700123  0.21046427  0.099026550\n3  -2.888991 -0.14494943 -0.01790026  0.019968390\n4  -2.745343 -0.31829898 -0.03155937 -0.075575817\n5  -2.728717  0.32675451 -0.09007924 -0.061258593\n6  -2.280860  0.74133045 -0.16867766 -0.024200858\n7  -2.820538 -0.08946138 -0.25789216 -0.048143106\n8  -2.626145  0.16338496  0.02187932 -0.045297871\n9  -2.886383 -0.57831175 -0.02075957 -0.026744736\n10 -2.672756 -0.11377425  0.19763272 -0.056295401\n\n\nTo this point, we have simply computed coordinates (scores) on a new set of axis (formed by the principal components, i.e. the eigenvectors). These axes are orthogonal and are aligned with the directions of maximal variance in the data. When we consider only a subset of principal components (like 2 the two components here that account for 97.8% of the variance), we are projecting the data onto a lower dimensional space. Generally, this is one of the primary goals of PCA: Project the data down into a lower dimensional space (onto the span of the principal components) while keeping the maximum amount of information (i.e. variance).\nThus, we know that almost 98% of the data’s variance can be seen in two-dimensions using the first two principal components. Let’s go ahead and see what this looks like:\n\nplot(scores$Prin1, scores$Prin2, \n     main=\"Data Projected on First 2 Principal Components\",\n     xlab=\"First Principal Component\", \n     ylab=\"Second Principal Component\", \n     col=c(\"red\",\"green3\",\"blue\")[iris$Species])\n\n\n\n\n\n\n\n\n\n\n\n\nirispca=prcomp(iris[1:4])\n# Variance Explained\nsummary(irispca)\n\nImportance of components:\n                          PC1     PC2    PC3     PC4\nStandard deviation     2.0563 0.49262 0.2797 0.15439\nProportion of Variance 0.9246 0.05307 0.0171 0.00521\nCumulative Proportion  0.9246 0.97769 0.9948 1.00000\n\n# Eigenvectors:\nirispca$rotation\n\n                     PC1         PC2         PC3        PC4\nSepal.Length  0.36138659 -0.65658877  0.58202985  0.3154872\nSepal.Width  -0.08452251 -0.73016143 -0.59791083 -0.3197231\nPetal.Length  0.85667061  0.17337266 -0.07623608 -0.4798390\nPetal.Width   0.35828920  0.07548102 -0.54583143  0.7536574\n\n# Coordinates of first 10 observations along PCs:\nirispca$x[1:10, ]\n\n            PC1         PC2         PC3          PC4\n [1,] -2.684126 -0.31939725  0.02791483  0.002262437\n [2,] -2.714142  0.17700123  0.21046427  0.099026550\n [3,] -2.888991  0.14494943 -0.01790026  0.019968390\n [4,] -2.745343  0.31829898 -0.03155937 -0.075575817\n [5,] -2.728717 -0.32675451 -0.09007924 -0.061258593\n [6,] -2.280860 -0.74133045 -0.16867766 -0.024200858\n [7,] -2.820538  0.08946138 -0.25789216 -0.048143106\n [8,] -2.626145 -0.16338496  0.02187932 -0.045297871\n [9,] -2.886383  0.57831175 -0.02075957 -0.026744736\n[10,] -2.672756  0.11377425  0.19763272 -0.056295401\n\n\nAll of the information we computed using eigenvectors aligns with what we see here, except that the coordinates/scores and the loadings of Principal Component 3 are of the opposite sign. In light of what we know about eigenvectors representing directions, this should be no cause for alarm. The prcomp function arrived at the unit basis vector pointing in the negative direction of the one we found directly from the eig function - which should negate all the coordinates and leave us with an equivalent mirror image in all of our projections.\n\n\n\nOne additional feature that R users have created is the biplot. The PCA biplot allows us to see where our original variables fall in the space of the principal components. Highly correlated variables will fall along the same direction (or exactly opposite directions) as a change in one of these variables correlates to a change in the other. Uncorrelated variables will appear further apart. The length of the variable vectors on the biplot tell us the degree to which variability in variable is explained in that direction. Shorter vectors have less variability than longer vectors. So in the biplot below, petal width and petal length point in the same direction indicating that these variables share a relatively high degree of correlation. However, the vector for petal width is much shorter than that of petal length, which means you can expect a higher degree of change in petal length as you proceed to the right along PC1. PC1 explains more of the variance in petal length than it does petal width. If we were to imagine a third PC orthogonal to the plane shown, petal width is likely to exist at much larger angle off the plane - here, it is being projected down from that 3-dimensional picture.\n\nbiplot(irispca, col = c(\"gray\", \"blue\"))\n\n\n\n\n\n\n\n\nWe can examine some of the outlying observations to see how they align with these projected variable directions. It helps to compare them to the quartiles of the data. Also keep in mind the direction of the arrows in the plot. If the arrow points down then the positive direction is down - indicating observations which are greater than the mean. Let’s pick out observations 42 and 132 and see what the actual data points look like in comparison to the rest of the sample population.\n\nsummary(iris[1:4])\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n\n# Consider orientation of outlying observations:\niris[42, ]\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n42          4.5         2.3          1.3         0.3  setosa\n\niris[132, ]\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n132          7.9         3.8          6.4           2 virginica"
  },
  {
    "objectID": "notebooks/pca/pca.html#variable-clustering-with-pca",
    "href": "notebooks/pca/pca.html#variable-clustering-with-pca",
    "title": "Ch5 Lecture 5",
    "section": "",
    "text": "The direction arrows on the biplot are merely the coefficients of the original variables when combined to make principal components. Don’t forget that principal components are simply linear combinations of the original variables.\nFor example, here we have the first principal component (the first column of \\(\\V\\)), \\(\\mathbf{v}_1\\) as:\n\neig$vectors[,1]\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n  0.36138659  -0.08452251   0.85667061   0.35828920 \n\n\nThis means that the coordinates of the data along the first principal component, which we’ll denote here as \\(PC_1\\) are given by a simple linear combination of our original variables after centering (for covariance PCA) or standardization (for correlation PCA)\n\\[PC_1 = 0.36Sepal.Length-0.08Sepal.Width+0.85Petal.Length +0.35Petal.Width\\] the same equation could be written for each of the vectors of coordinates along principal components, \\(PC_1,\\dots, PC_4\\).\nEssentially, we have a system of equations telling us that the rows of \\(\\V^T\\) (i.e. the columns of \\(\\V\\)) give us the weights of each variable for each principal component: \\[\\begin{equation}\n(\\#eq:cpc1)\n\\begin{bmatrix} PC_1\\\\PC_2\\\\PC_3\\\\PC_4\\end{bmatrix} = \\mathbf{V}^T\\begin{bmatrix}Sepal.Length\\\\Sepal.Width\\\\Petal.Length\\\\Petal.Width\\end{bmatrix}\n\\end{equation}\\]\nThus, if want the coordinates of our original variables in terms of Principal Components (so that we can plot them as we do in the biplot) we need to look no further than the rows of the matrix \\(\\mathbf{V}\\) as \\[\\begin{equation}\n(\\#eq:cpc2)\n\\begin{bmatrix}Sepal.Length\\\\Sepal.Width\\\\Petal.Length\\\\Petal.Width\\end{bmatrix} =\\mathbf{V}\\begin{bmatrix} PC_1\\\\PC_2\\\\PC_3\\\\PC_4\\end{bmatrix}\n\\end{equation}\\]\nmeans that the rows of \\(\\mathbf{V}\\) give us the coordinates of our original variables in the PCA space. The transition from Equation @ref(eq:cpc1) to Equation @ref(eq:cpc2) is provided by the orthogonality of the eigenvectors per Theorem @ref(thm:eigsym).\n\n#First entry in each eigenvectors give coefficients for Variable 1:\neig$vectors[1,]\n\n[1] 0.3613866 0.6565888 0.5820299 0.3154872\n\n\n\\[Sepal.Length = 0.361 PC_1 - 0.657 PC_2 - 0.582 PC_3 + 0.315 PC_4\\] You can see this on the biplot. The vector shown for Sepal.Length is (0.361, -0.656), which is the two dimensional projection formed by throwing out components 3 and 4.\nVariables which lie upon similar directions in the PCA space tend to change together in a similar fashion. We might consider Petal.Width and Petal.Length as a cluster of variables because they share a direction on the biplot, which means they represent much of the same information (the underlying construct being the “size of the petal” in this case).\n\n\nWe can complete the same analysis using the correlation matrix. I’ll leave it as an exercise to compute the Principal Component loadings and scores and variance explained directly from eigenvectors and eigenvalues. You should do this and compare your results to the R output. (Beware: you must transform your data before solving for the scores. With the covariance version, this meant centering - for the correlation version, this means standardization as well)\n\nirispca2=prcomp(iris[1:4], cor=TRUE)\n\nWarning: In prcomp.default(iris[1:4], cor = TRUE) :\n extra argument 'cor' will be disregarded\n\nsummary(irispca2)\n\nImportance of components:\n                          PC1     PC2    PC3     PC4\nStandard deviation     2.0563 0.49262 0.2797 0.15439\nProportion of Variance 0.9246 0.05307 0.0171 0.00521\nCumulative Proportion  0.9246 0.97769 0.9948 1.00000\n\nirispca2$rotation\n\n                     PC1         PC2         PC3        PC4\nSepal.Length  0.36138659 -0.65658877  0.58202985  0.3154872\nSepal.Width  -0.08452251 -0.73016143 -0.59791083 -0.3197231\nPetal.Length  0.85667061  0.17337266 -0.07623608 -0.4798390\nPetal.Width   0.35828920  0.07548102 -0.54583143  0.7536574\n\nirispca2$x[1:10,]\n\n            PC1         PC2         PC3          PC4\n [1,] -2.684126 -0.31939725  0.02791483  0.002262437\n [2,] -2.714142  0.17700123  0.21046427  0.099026550\n [3,] -2.888991  0.14494943 -0.01790026  0.019968390\n [4,] -2.745343  0.31829898 -0.03155937 -0.075575817\n [5,] -2.728717 -0.32675451 -0.09007924 -0.061258593\n [6,] -2.280860 -0.74133045 -0.16867766 -0.024200858\n [7,] -2.820538  0.08946138 -0.25789216 -0.048143106\n [8,] -2.626145 -0.16338496  0.02187932 -0.045297871\n [9,] -2.886383  0.57831175 -0.02075957 -0.026744736\n[10,] -2.672756  0.11377425  0.19763272 -0.056295401\n\nplot(irispca2$x[,1],irispca2$x[,2],\n     main=\"Data Projected on First 2  Principal Components\",\n     xlab=\"First Principal Component\",\n     ylab=\"Second Principal Component\",\n     col=c(\"red\",\"green3\",\"blue\")[iris$Species])\n\n\n\n\n\n\n\n\n\nbiplot(irispca2)\n\n\n\n\n\n\n\n\nHere you can see the direction vectors of the original variables are relatively uniform in length in the PCA space. This is due to the standardization in the correlation matrix. However, the general message is the same: Petal.Width and Petal.Length Cluster together, and many of the same observations appear “on the fray” on the PCA space - although not all of them!\n\n\n\nWhat do you think? It depends on the task, and it depends on the data. One flavor of PCA is not “better” than the other. Correlation PCA is appropriate when the scales of your attributes differ wildly, and covariance PCA would be inappropriate in that situation. But in all other scenarios, when the scales of our attributes are roughly the same, we should always consider both dimension reductions and make a decision based upon the resulting output (variance explained, projection plots, loadings).\nFor the iris data, The results in terms of variable clustering are pretty much the same. For clustering/classifying the 3 species of flowers, we can see better separation in the covariance version.\n\n\n\nBe careful not to draw improper conclusions from biplots. Particularly, be careful about situations where the first two principal components do not summarize the majority of the variance. If a large amount of variance is captured by the 3rd or 4th (or higher) principal components, then we must keep in mind that the variable projections on the first two principal components are flattened out versions of a higher dimensional picture. If a variable vector appears short in the 2-dimensional projection, it means one of two things:\n\nThat variable has small variance\nThat variable appears to have small variance when depicted in the space of the first two principal components, but truly has a larger variance which is represented by 3rd or higher principal components.\n\nLet’s take a look at an example of this. We’ll generate 500 rows of data on 4 nearly independent normal random variables. Since these variables are uncorrelated, we might expect that the 4 orthogonal principal components will line up relatively close to the original variables. If this doesn’t happen, then at the very least we can expect the biplot to show little to no correlation between the variables. We’ll give variables \\(2\\) and \\(3\\) the largest variance. Multiple runs of this code will generate different results with similar implications.\n\nmeans=c(2,4,1,3)\nsigmas=c(7,9,10,8)\nsample.size=500\ndata=mapply(function(mu,sig){rnorm(mu,sig, n=sample.size)},mu=means,sig=sigmas)\ncor(data)\n\n            [,1]        [,2]        [,3]        [,4]\n[1,]  1.00000000 -0.02175493 -0.02616236 -0.03828080\n[2,] -0.02175493  1.00000000 -0.05596491 -0.02800197\n[3,] -0.02616236 -0.05596491  1.00000000  0.06162748\n[4,] -0.03828080 -0.02800197  0.06162748  1.00000000\n\npc=prcomp(data,scale=TRUE)\nsummary(pc)\n\nImportance of components:\n                          PC1    PC2    PC3    PC4\nStandard deviation     1.0517 1.0126 0.9706 0.9625\nProportion of Variance 0.2765 0.2564 0.2355 0.2316\nCumulative Proportion  0.2765 0.5329 0.7684 1.0000\n\npc$rotation\n\n            PC1         PC2        PC3        PC4\n[1,] -0.2774310  0.74816466  0.5275858  0.2914360\n[2,] -0.4301634 -0.62263259  0.3527535  0.5503209\n[3,]  0.6327338  0.09598568 -0.2114630  0.7387273\n[4,]  0.5810675 -0.20824275  0.7433045 -0.2578641\n\nbiplot(pc)\n\n\n\n\n\n\n\n\nBiPlot of Iris Data\n\n\n\n\nObviously, the wrong conclusion to make from this biplot is that Variables 1 and 4 are correlated. Variables 1 and 4 do not load highly on the first two principal components - in the whole 4-dimensional principal component space they are nearly orthogonal to each other and to variables 1 and 2. Thus, their orthogonal projections appear near the origin of this 2-dimensional subspace.\nThe morals of the story:\n\n\nAlways corroborate your results using the variable loadings and the amount of variation explained by each variable.\n\nWhen a variable shows up near the origin in a biplot, it is generally not well represented by your two-dimensional approximation of the data.\n\n\n\n\nBiPlot of Iris Data"
  },
  {
    "objectID": "lectures/ch5lecture5.html#how-spread-out-is-the-data-along-a-particular-direction",
    "href": "lectures/ch5lecture5.html#how-spread-out-is-the-data-along-a-particular-direction",
    "title": "Ch5 Lecture 5",
    "section": "How spread out is the data along a particular direction?",
    "text": "How spread out is the data along a particular direction?\nSuppose we have n data points in p dimensions. We can represent the data as a matrix \\(X\\) of size \\(n \\times p\\). The data points are represented as rows in the matrix, and we have subtracted the mean along each dimension from the data."
  },
  {
    "objectID": "lectures/ch5lecture5.html#visualizing-the-high-dimensional-data",
    "href": "lectures/ch5lecture5.html#visualizing-the-high-dimensional-data",
    "title": "Ch5 Lecture 5",
    "section": "Visualizing the high-dimensional data",
    "text": "Visualizing the high-dimensional data\n\n\n\n\n\n\n\n\n\n\nKIDEN\ncity\nregion\nprice_index_no_rent\nprice_index_with_rent\ngross_salaries\nnet_salaries\nwork_hours_year\npaid_vacations_year\ngross_buying_power\n...\nmechanic\nconstruction_worker\nmetalworker\ncook_chef\nfactory_manager\nengineer\nbank_clerk\nexecutive_secretary\nsalesperson\ntextile_worker\n\n\n\n\n0\nAmsterdam91\nAmsterdam\nCentral Europe\n65.6\n65.7\n56.9\n49.0\n1714.0\n31.9\n86.7\n...\n11924.0\n12661.0\n14536.0\n14402.0\n25924.0\n24786.0\n14871.0\n14871.0\n11857.0\n10852.0\n\n\n1\nAthenes91\nAthens\nSouthern Europe\n53.8\n55.6\n30.2\n30.4\n1792.0\n23.5\n56.1\n...\n8574.0\n9847.0\n14402.0\n14068.0\n13800.0\n14804.0\n9914.0\n6900.0\n4555.0\n5761.0\n\n\n2\nBogota91\nBogota\nSouth America\n37.9\n39.3\n10.1\n11.5\n2152.0\n17.4\n26.6\n...\n4354.0\n1206.0\n4823.0\n13934.0\n12192.0\n12259.0\n2345.0\n5024.0\n2278.0\n2814.0\n\n\n3\nBombay91\nMumbai\nSouth Asia and Australia\n30.3\n39.9\n6.0\n5.3\n2052.0\n30.6\n19.9\n...\n1809.0\n737.0\n2479.0\n2412.0\n3751.0\n2880.0\n2345.0\n1809.0\n1072.0\n1206.0\n\n\n4\nBruxelles91\nBrussels\nCentral Europe\n73.8\n72.2\n68.2\n50.5\n1708.0\n24.6\n92.4\n...\n10450.0\n12192.0\n17350.0\n19159.0\n31016.0\n24518.0\n19293.0\n13800.0\n10718.0\n10182.0\n\n\n\n\n5 rows × 41 columns\n\n\n\n\n\n\nDataset taken from here"
  },
  {
    "objectID": "lectures/ch5lecture5.html#clouds-of-row-points",
    "href": "lectures/ch5lecture5.html#clouds-of-row-points",
    "title": "Ch5 Lecture 5",
    "section": "Clouds of row-points",
    "text": "Clouds of row-points\n\n\ndistances between points represent similarity between cities…"
  },
  {
    "objectID": "lectures/ch5lecture5.html#clouds-of-column-points",
    "href": "lectures/ch5lecture5.html#clouds-of-column-points",
    "title": "Ch5 Lecture 5",
    "section": "Clouds of column-points",
    "text": "Clouds of column-points\n\n\neach point represents one feature, plotted in one dimension for each city\ndistances between points represent similarity between features, such as (in this case) salaries in specific occupations"
  },
  {
    "objectID": "lectures/ch5lecture5.html#projection-onto-fewer-dimensions",
    "href": "lectures/ch5lecture5.html#projection-onto-fewer-dimensions",
    "title": "Ch5 Lecture 5",
    "section": "Projection onto fewer dimensions",
    "text": "Projection onto fewer dimensions\nTo visualize data, we need to project it onto 2d (or 3d) subspaces. But which ones?\nThese are all equivalent:\n\nmaximize variance of projected data\nminimize squared distances between data points and their projections\nkeep distances between points as similar as possible in original vs projected space"
  },
  {
    "objectID": "lectures/ch5lecture5.html#example-in-the-space-of-column-points",
    "href": "lectures/ch5lecture5.html#example-in-the-space-of-column-points",
    "title": "Ch5 Lecture 5",
    "section": "Example in the space of column points",
    "text": "Example in the space of column points"
  },
  {
    "objectID": "lectures/ch5lecture5.html#example",
    "href": "lectures/ch5lecture5.html#example",
    "title": "Ch5 Lecture 5",
    "section": "Example",
    "text": "Example"
  },
  {
    "objectID": "lectures/ch5lecture5.html#direction-of-maximum-variance",
    "href": "lectures/ch5lecture5.html#direction-of-maximum-variance",
    "title": "Ch5 Lecture 5",
    "section": "Direction of maximum variance",
    "text": "Direction of maximum variance\nTo find the direction of maximum variance, we need to find the unit vector \\(\\mathbf{u}\\) that maximizes \\(\\mathbf{u}^T C \\mathbf{u}\\).\n. . . ::: notes We start by finding the eigendecomposition of the covariance matrix \\(C\\): \\(C = V \\Lambda V^T\\).\n\\(V\\) is a matrix whose columns are the eigenvectors of \\(C\\), and \\(\\Lambda\\) is a diagonal matrix whose diagonal elements are the eigenvalues of \\(C\\).\n(Note that these are simply the right singular vectors and singular values of the data matrix \\(X\\).)\nThen we can express \\(\\mathbf{u}\\) in terms of the eigenvectors of \\(C\\): \\(\\mathbf{u} = \\sum_{i=1}^p a_i \\mathbf{v}_i\\), where \\(\\mathbf{v}_i\\) are the eigenvectors of \\(C\\). Because \\(\\mathbf{u}\\) is a unit vector, the coefficients \\(a_i\\) must sum to 1.\nNow we have that \\(C \\mathbf{u} = \\sum_{i=1}^p C v_i a_i =  \\sum_{i=1}^p a_i v_i\\), where \\(\\lambda_i\\) are the eigenvalues of \\(C\\).\nSo then \\(\\mathbf{u}^T C \\mathbf{u} = \\sum_{i,j=1}^p a_i a_j \\mathbf{v_j}\\mathbf{v_j} = \\sum_{i,j=1}^p a_i a_j \\delta_{i,j}||v_i||  \\lambda_i = \\sum_{i=1}^p a_i^2 \\lambda_i\\). :::"
  },
  {
    "objectID": "lectures/ch5lecture5.html#which-direction-gives-the-maximum-variance",
    "href": "lectures/ch5lecture5.html#which-direction-gives-the-maximum-variance",
    "title": "Ch5 Lecture 5",
    "section": "Which direction gives the maximum variance?",
    "text": "Which direction gives the maximum variance?\n\nTo find the direction that gives the maximum variance, we need to find the set of coefficients \\(a_i\\) that maximize \\(\\sum_{i=1}^p a_i^2 \\lambda_i\\) subject to the constraint that \\(\\sum_{i=1}^p a_i = 1\\). This will have its maximum value when \\(a_i = 1\\) for the eigenvector corresponding to the largest eigenvalue, and \\(a_i = 0\\) for all other eigenvectors.\nSo the direction of maximum variance is given by the eigenvector corresponding to the largest eigenvalue of the covariance matrix. This is the first singular vector of the data matrix, and it is also called the first principal component.\n\npause\n\n\nThe first principal component of a data matrix \\(X\\) is the eigenvector corresponding to the largest eigenvalue of the covariance matrix of the data.\nIn terms of the singular value decomposition of \\(X\\), the first principal component is the first right singular vector of \\(X\\):\n\\(\\mathbf{v_1}\\).\n\nThe variance of the data along each principal component is given by the corresponding eigenvalue, or the square of the corresponding singular value."
  },
  {
    "objectID": "lectures/ch5lecture5.html#example-dataset",
    "href": "lectures/ch5lecture5.html#example-dataset",
    "title": "Ch5 Lecture 5",
    "section": "Example dataset",
    "text": "Example dataset\nFrom here"
  },
  {
    "objectID": "lectures/ch5lecture5.html#food",
    "href": "lectures/ch5lecture5.html#food",
    "title": "Ch5 Lecture 5",
    "section": "Food",
    "text": "Food\n\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n1990\n1991\n1992\n1993\n1994\n1995\n1996\n1997\n1998\n1999\n\n\nname\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7up\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n...\n1\n1\n0\n0\n0\n0\n2\n0\n0\n1\n\n\nlasagna\n0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n...\n0\n2\n1\n0\n0\n0\n0\n1\n1\n0\n\n\npepsi\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n1\n0\n2\n0\n0\n2\n0\n0\n0\n0\n\n\nyop\n0\n0\n0\n2\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\nred.wine\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n2\n2\n0\n0\n0\n0\n\n\n\n\n5 rows × 2000 columns"
  },
  {
    "objectID": "lectures/ch5lecture5.html#section",
    "href": "lectures/ch5lecture5.html#section",
    "title": "Ch5 Lecture 5",
    "section": "",
    "text": "We might choose to focus on only 12 (!) of the 41 variables in the dataset, corresponding to the average wages of workers in 12 specific occupations in each city.\n\n\n\n\n\n\n\n\n\n\ncity\nteacher\nbus_driver\nmechanic\nconstruction_worker\nmetalworker\ncook_chef\nfactory_manager\nengineer\nbank_clerk\nexecutive_secretary\nsalesperson\ntextile_worker\n\n\n\n\n0\nAmsterdam\n15608.0\n17819.0\n11924.0\n12661.0\n14536.0\n14402.0\n25924.0\n24786.0\n14871.0\n14871.0\n11857.0\n10852.0\n\n\n1\nAthens\n7972.0\n9445.0\n8574.0\n9847.0\n14402.0\n14068.0\n13800.0\n14804.0\n9914.0\n6900.0\n4555.0\n5761.0\n\n\n2\nBogota\n2144.0\n2412.0\n4354.0\n1206.0\n4823.0\n13934.0\n12192.0\n12259.0\n2345.0\n5024.0\n2278.0\n2814.0\n\n\n3\nMumbai\n1005.0\n1340.0\n1809.0\n737.0\n2479.0\n2412.0\n3751.0\n2880.0\n2345.0\n1809.0\n1072.0\n1206.0\n\n\n4\nBrussels\n14001.0\n14068.0\n10450.0\n12192.0\n17350.0\n19159.0\n31016.0\n24518.0\n19293.0\n13800.0\n10718.0\n10182.0\n\n\n\n\n\n\n\n\nHow can we think about the data in this 12-dimensional space?"
  },
  {
    "objectID": "lectures/ch5lecture5.html#section-1",
    "href": "lectures/ch5lecture5.html#section-1",
    "title": "Ch5 Lecture 5",
    "section": "",
    "text": "The data consist of 2000 observations of 42 variables each! The variables are the number of times each of 42 different food items was purchased in a particular shopping trip.\nLet’s try visualizing the data in a few of the dimensions of the original space.\n\n\n\n\n\n\n\n\n\n\n\nBut there’s a problem here. We know from looking at this that there are at least one of each of these combinations where there’s a dot, but how many?"
  },
  {
    "objectID": "lectures/ch5lecture5.html#section-2",
    "href": "lectures/ch5lecture5.html#section-2",
    "title": "Ch5 Lecture 5",
    "section": "",
    "text": "We can look at many combinations…"
  },
  {
    "objectID": "lectures/ch5lecture5.html#section-3",
    "href": "lectures/ch5lecture5.html#section-3",
    "title": "Ch5 Lecture 5",
    "section": "",
    "text": "We can look at many combinations…"
  },
  {
    "objectID": "lectures/ch5lecture5.html#meaning-of-the-principal-components",
    "href": "lectures/ch5lecture5.html#meaning-of-the-principal-components",
    "title": "Ch5 Lecture 5",
    "section": "Meaning of the principal components",
    "text": "Meaning of the principal components"
  },
  {
    "objectID": "lectures/ch5lecture5.html#section-4",
    "href": "lectures/ch5lecture5.html#section-4",
    "title": "Ch5 Lecture 5",
    "section": "",
    "text": "Maybe we can learn more from the correlations?"
  },
  {
    "objectID": "lectures/ch5lecture5.html#some-real-world-data",
    "href": "lectures/ch5lecture5.html#some-real-world-data",
    "title": "Ch5 Lecture 5",
    "section": "Some real-world data",
    "text": "Some real-world data"
  },
  {
    "objectID": "lectures/ch5lecture5.html#example-dataset-shopping-baskets",
    "href": "lectures/ch5lecture5.html#example-dataset-shopping-baskets",
    "title": "Ch5 Lecture 5",
    "section": "Example dataset: shopping baskets",
    "text": "Example dataset: shopping baskets\n\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n1990\n1991\n1992\n1993\n1994\n1995\n1996\n1997\n1998\n1999\n\n\nname\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7up\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n...\n1\n1\n0\n0\n0\n0\n2\n0\n0\n1\n\n\nlasagna\n0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n...\n0\n2\n1\n0\n0\n0\n0\n1\n1\n0\n\n\npepsi\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n1\n0\n2\n0\n0\n2\n0\n0\n0\n0\n\n\nyop\n0\n0\n0\n2\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\nred.wine\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n2\n2\n0\n0\n0\n0\n\n\n\n\n5 rows × 2000 columns"
  },
  {
    "objectID": "lectures/ch5lecture5.html#section-5",
    "href": "lectures/ch5lecture5.html#section-5",
    "title": "Ch5 Lecture 5",
    "section": "",
    "text": "OK, it looks like there are some patterns here. But it’s hard to get a real sense for it."
  },
  {
    "objectID": "lectures/ch5lecture5.html#section-6",
    "href": "lectures/ch5lecture5.html#section-6",
    "title": "Ch5 Lecture 5",
    "section": "",
    "text": "Now perform PCA on the data.\n\n\nCode\n# standardize the data\nscaler = StandardScaler()\nfood_scaled = scaler.fit_transform(food)\n# find the first four principal components of the data\npca = PCA(n_components=4)\npca.fit(food_scaled);\nprint(f'Explained variance %: {pca.explained_variance_ratio_*100}')\n\n\nExplained variance %: [8.82557338 8.3553662  7.78271493 5.81230637]"
  },
  {
    "objectID": "lectures/ch5lecture5.html#section-10",
    "href": "lectures/ch5lecture5.html#section-10",
    "title": "Ch5 Lecture 5",
    "section": "",
    "text": "3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n...\n10094\n10095\n10096\n10097\n10098\n10099\n10100\n10101\n10102\n10103\n\n\n\n\n0\nG\nG\nT\nT\nA\nA\nC\nA\nC\nC\n...\nT\nA\nT\nA\nA\nT\nT\nT\nG\nA\n\n\n1\nA\nA\nT\nT\nA\nG\nC\nA\nT\nT\n...\nG\nC\nT\nG\nA\nT\nC\nT\nG\nG\n\n\n2\nA\nA\nT\nT\nA\nA\nG\nA\nC\nC\n...\nG\nC\nT\nG\nA\nT\nC\nT\nG\nG\n\n\n3\nA\nA\nT\nC\nA\nA\nG\nA\nC\nC\n...\nG\nA\nT\nG\nA\nT\nC\nT\nG\nG\n\n\n4\nG\nA\nT\nC\nG\nA\nC\nA\nC\nC\n...\nG\nC\nT\nG\nA\nT\nC\nT\nG\nG\n\n\n\n\n5 rows × 10101 columns"
  },
  {
    "objectID": "lectures/ch5lecture5.html#section-11",
    "href": "lectures/ch5lecture5.html#section-11",
    "title": "Ch5 Lecture 5",
    "section": "",
    "text": "3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n...\n10094\n10095\n10096\n10097\n10098\n10099\n10100\n10101\n10102\n10103\n\n\n\n\n0\nG\nG\nT\nT\nA\nA\nC\nA\nC\nC\n...\nT\nA\nT\nA\nA\nT\nT\nT\nG\nA\n\n\n1\nA\nA\nT\nT\nA\nG\nC\nA\nT\nT\n...\nG\nC\nT\nG\nA\nT\nC\nT\nG\nG\n\n\n2\nA\nA\nT\nT\nA\nA\nG\nA\nC\nC\n...\nG\nC\nT\nG\nA\nT\nC\nT\nG\nG\n\n\n3\nA\nA\nT\nC\nA\nA\nG\nA\nC\nC\n...\nG\nA\nT\nG\nA\nT\nC\nT\nG\nG\n\n\n4\nG\nA\nT\nC\nG\nA\nC\nA\nC\nC\n...\nG\nC\nT\nG\nA\nT\nC\nT\nG\nG\n\n\n\n\n5 rows × 10101 columns"
  },
  {
    "objectID": "lectures/ch5lecture5.html#section-12",
    "href": "lectures/ch5lecture5.html#section-12",
    "title": "Ch5 Lecture 5",
    "section": "",
    "text": "0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n10091\n10092\n10093\n10094\n10095\n10096\n10097\n10098\n10099\n10100\n\n\n\n\n0\n0\n1\n0\n1\n0\n1\n1\n0\n0\n0\n...\n0\n1\n1\n1\n0\n0\n1\n0\n0\n1\n\n\n1\n1\n0\n0\n1\n0\n0\n1\n0\n1\n1\n...\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n1\n0\n0\n1\n0\n1\n0\n0\n0\n0\n...\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n...\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n0\n0\n0\n0\n1\n1\n1\n0\n0\n0\n...\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n5 rows × 10101 columns"
  },
  {
    "objectID": "lectures/ch5lecture5.html#goal",
    "href": "lectures/ch5lecture5.html#goal",
    "title": "Ch5 Lecture 5",
    "section": "Goal",
    "text": "Goal\nWe’d like to know in which directions in \\(R^p\\) the data has the highest variance.\n\n. . .\nFirst: understand how much the data is spread out along a particular direction, given by a unit vector \\(\\mathbf{u}\\).\n. . .\nRemember, for each point \\(\\mathbf{x}_i\\) in the data, we can project it onto the direction \\(\\mathbf{u}\\) by computing \\(\\psi_i = \\mathbf{x}_i \\cdot \\mathbf{u}\\).\n\n. . .\nWe then define the vector of all the projected points as \\(\\mathbf{\\psi} = X \\mathbf{u}\\).\n. . .\nWhat is the variance of the projected data? Since the data has been centered, the variance is given by \\(\\frac{1}{n} \\sum_{i=1}^n v_i^2 = \\frac{1}{n} \\mathbf{v}^T \\mathbf{v}\\).\n. . .\nWe can rewrite this in terms of the original data matrix as \\(\\frac{1}{n} \\mathbf{v}^T \\mathbf{v} = \\frac{1}{n} \\mathbf{u}^T X^T X \\mathbf{u}\\).\nWe recognize that \\(X^T X\\) is the covariance matrix of the data.\n. . .\nSo the variance of the projected data is given by \\(\\mathbf{u}^T C \\mathbf{u}\\), where \\(C = \\frac{1}{n} X^T X\\) is the covariance matrix of the data."
  },
  {
    "objectID": "lectures/ch5lecture5.html#section-7",
    "href": "lectures/ch5lecture5.html#section-7",
    "title": "Ch5 Lecture 5",
    "section": "",
    "text": "pause\n\nFirst of all, clearly the data is artificial. It’s way too neat! (This came from a machine learning textbook.)\nWhat this is saying is that there seem to be roughly three groups of people who buy food in a similar way, and only differ in the amount."
  },
  {
    "objectID": "lectures/ch5lecture5.html#section-13",
    "href": "lectures/ch5lecture5.html#section-13",
    "title": "Ch5 Lecture 5",
    "section": "",
    "text": "Code\npca = PCA(n_components=6)\npca.fit(X);\n#Data points projected along the principal components\n\n\n\n\n\n\n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\nPC5\nPC6\npopulation\ngender\n\n\n\n\n0\n4.786586\n-0.877840\n3.265087\n-0.743684\n-2.809567\n1.820446\nACB\n1\n\n\n1\n12.951468\n2.355097\n-0.677190\n-3.943423\n-2.863389\n-0.260862\nACB\n2\n\n\n2\n9.841647\n0.674617\n-1.285491\n-2.410303\n-2.508126\n0.389504\nACB\n2\n\n\n3\n-0.340635\n-0.725657\n3.516283\n0.537464\n-0.717723\n-1.025561\nACB\n1\n\n\n4\n3.782420\n0.508652\n-2.742794\n-1.065664\n1.119973\n-1.237596\nACB\n2"
  },
  {
    "objectID": "submitted/projects_2/vincent_notes.html",
    "href": "submitted/projects_2/vincent_notes.html",
    "title": "Projects 2",
    "section": "",
    "text": "Vincent:\nProject 1, part 1: The proof works, but it is not very clear. It would be better to start by stating what \\(b\\) and \\(y\\) represent (\\(b\\) is a possible vector of potential differences, \\(y\\) is a vector of potentials). Then you’ll want to say what Kirchoff’s law says in terms of \\(b\\), \\(A\\), and \\(y\\), and then you’ll go in and do the proof.\nAlso, use latex for the math expressions! Just put dollar signs around them…\npart 2: you’ve correctly noted that the condition is that the sums of the columns and the sums of the rows on a weighted incidence matrix all need to be zero. Can you put this requirement in matrix form, given a weight vector w and your original incidence matrix A?\n(But I really like the graph!!)\nGrade: R\nProject 2: This is really nice! Only improvement I’d suggest is to find some way of displaying the edge quadrants better – maybe plotting each quadrant separately with its own cmap, because right now it’s very hard to see the pixels since they’re so close to black.\nGrade: E\nProject 3:\nOof, that’s a lot of hand-entered data! As a challenge, I just tried to reproduce this using some built-in nx functions. Althought I burned dinner in the process, I eventually succeeded, using these two functions: nx.from_numpy_array(spreadn, create_using=nx.DiGraph) and nx.incidence_matrix(G,oriented=True,weight=None).todense() .\n(I also found the matrix of point spreads from the game scores that you loaded in – that’s just a one-liner, see if you can figure it out! But then I needed to take the lower-triangular portion of it, to feed into nx.from_numpy_array, which I did using np.tril)\nAnyways. You can optimize your code if you like, but regardless of that, I have some questions about the final results. First, I’m confused about the missing games. Why were these the ones that were played twice? I’m not getting something.\nSecond, I like that you’ve computed the error, but I wonder if percentage is the best way to measure it. When I look at the predictions and I see that they are at least predicting the correct team to win, that seems like a good start to me… something to think about.\nGrade: M"
  },
  {
    "objectID": "submitted/projects_2/buyalosbob.html",
    "href": "submitted/projects_2/buyalosbob.html",
    "title": "Project 2 - Stat 243",
    "section": "",
    "text": "Project 1 was completed with Hanyan Cai. Our joint-write up is on his submission. We worked together for every part of this problem, devising our solutions together on a whiteboard and then writing them up.\nProject 2 and 3 were completed and written up independently, although with lots of discussion between us and using the same dataset for project 3."
  },
  {
    "objectID": "submitted/projects_2/buyalosbob.html#project-2",
    "href": "submitted/projects_2/buyalosbob.html#project-2",
    "title": "Project 2 - Stat 243",
    "section": "Project 2",
    "text": "Project 2\nLets begin by choosing a photo! Of course the only reasonable choice is that of a cat…\n(if you wish to follow along and need the specific image I’m using, on a uchicago email you can download it here: https://drive.google.com/file/d/1Kl3gDaQGHWJVXOLnf35M2FOf_LuUJKe6/view?usp=sharing )\n\nfrom PIL import Image\nimport numpy as np\n\n# Load image\nimage = Image.open(\"catimage.png\").convert(\"L\")  # Convert to grayscale\n\n# Convert image to numpy array\nimage_array = np.array(image)\n\nimage\n\n\n\n\n\n\n\n\nThe first thing we will do in program in the Harr Wavelet transform for a given n. We will show with n=6 what this looks like. See comments for summary of code.\n\nimport math\ndef harr(n: int):\n\n    #Ensure that we have an even size\n    if n % 2 != 0:\n        raise ValueError(\"n must be even!\")\n\n    output = []\n\n    #Create upper block by iterating over even indicies\n    for i in range(0, n, 2):\n        row = [0]*n\n        row[i] = 1\n        row[i+1] = 1\n        output.append(row)\n    \n    #Create lower block by iterating over even indicies\n    for i in range(0, n, 2):\n        row = [0]*n\n        row[i] = -1\n        row[i+1] = 1\n        output.append(row)\n\n    #use nested list comprehensions to apply our scalar sqrt(2)/2, and then cast our list a numpy array.\n    return np.array([[(math.sqrt(2)/2)*i for i in row] for row in output])\n\n#Example with n = 6\nharr(6)\n\narray([[ 0.70710678,  0.70710678,  0.        ,  0.        ,  0.        ,\n         0.        ],\n       [ 0.        ,  0.        ,  0.70710678,  0.70710678,  0.        ,\n         0.        ],\n       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.70710678,\n         0.70710678],\n       [-0.70710678,  0.70710678,  0.        ,  0.        ,  0.        ,\n         0.        ],\n       [ 0.        ,  0.        , -0.70710678,  0.70710678,  0.        ,\n         0.        ],\n       [ 0.        ,  0.        ,  0.        ,  0.        , -0.70710678,\n         0.70710678]])\n\n\nNow, since we have our image as an mxn matrix, lets get the dimensions of this matrix. Observe that these are both even numbers so we do not need to cut anything and can make our Harr matrices for both of these even numbers.\n\nprint(\"Image shape:\", image_array.shape)\nm, n = image_array.shape\n\nImage shape: (2912, 5000)\n\n\nNow, we know that the Haar Wavelet Transform equation is \\(W_mAW_n^T\\). So, we have everything we need to represent this:\n\ntransform = 1/2 * (harr(m) @ image_array @ harr(n).transpose())\n\n# Convert numpy array back to image\nimage_from_array = Image.fromarray(transform).convert(\"L\")\n\nimage_from_array\n\n\n\n\n\n\n\n\nLet’s now view just the blurred version:\n\nblurred_array = transform[:int((m/2) + 1), :int((n/2) + 1)]\n\n# Convert numpy array back to image\nblurred_image_from_array = Image.fromarray(blurred_array).convert(\"L\")\n\nblurred_image_from_array\n\n\n\n\n\n\n\n\nThis is cool! But is this still a very nice and clear image of a cat. I predict that we will need to compress it a bunch until it is unrecognizable, so lets write a function that will compress it n times.\n\ndef compress(k, image_array: int, edge_mats = []):\n    assert k&gt;0, \"must compress atleast n=1 times!\"\n    m, n = image_array.shape\n    if m%2 == 1:\n        m = m-1\n        image_array = image_array[:-1]\n    if n%2 == 1:\n        n = n-1\n        image_array = image_array[:, :-1]\n\n    transform = 1/2 * (harr(m) @ image_array @ harr(n).transpose())\n    edge_mats.append(transform)\n    transform_cut = transform[:int((m/2) + 1), :int((n/2) + 1)]\n    if k-1 &gt; 0:\n        return compress(k-1, transform_cut, edge_mats)\n    \n    image_from_array = Image.fromarray(transform_cut).convert(\"L\")\n\n    return (image_from_array, edge_mats)\n\nLets test this! First lets reload in our cat image and run for n = 2, 3, 4, 5, 6 compressions (may take up to 30 seconds; if you need to return this cell, rerun the one above it first!)\n\ncatimage = Image.open(\"catimage.png\").convert(\"L\")  # Convert to grayscale\ncat_image_array = np.array(image)\n\n_, edge_mats = compress(6, cat_image_array, [])\n\nfor i, mat in enumerate(edge_mats):\n    print (f\"Image after {i+1} compressions:\")\n    m, n = mat.shape\n    transform_cut = mat[:int((m/2) + 1), :int((n/2) + 1)]\n    image_from_array = Image.fromarray(transform_cut).convert(\"L\")\n    display(image_from_array.resize((5000,2912)))\n\nImage after 1 compressions:\nImage after 2 compressions:\nImage after 3 compressions:\nImage after 4 compressions:\nImage after 5 compressions:\nImage after 6 compressions:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGreat! I would subjectively say that the image that has been compresed three times is the lowest quality I would go before it becomes unreasonable. However, would if we could store this image using less storage? One key thing to observe is that we can actual reconstruct the image using just the edge matrices (lossless compression):\nWe have after a compression our transformation \\(C = \\frac{1}{2}W_mAW_n^T\\), where W_n, W_m are the Haar transformations, A is our original image, and C is the block matrix produced after applying our compression. Since \\(W_n\\), \\(W_m\\) are orthogonal then we can rearrange this to have \\(A = 2W_m^TCW_n\\). Therefore, we only need to store \\(C\\) in order to recreate \\(A\\)!\nThis is very as \\(C\\) contains much less data than \\(A\\). For example, let’s see their relative sizes after compression 0, 1, 2, and 3 times (I saved these images to my computer and have written out their storage units here):\n\n\n\n# Compressions\nStorage space\n\n\n\n\n0 Compressions\n662 KB\n\n\n1 Compression\n206 KB\n\n\n2 Compressions\n61 KB\n\n\n3 Compressions\n20 KB\n\n\n\nAmazing! So, since we have reasonable image quality after 3 compressions, we can store our image with 20KB, which is about 33x smaller than 662KB!\nBut, I think we can do even better by creating a threshold condition. Let us look distribution of the coefficents in our matrix C:\n\nimport matplotlib.pyplot as plt\nC = edge_mats[2]\n\ncoefficients, bins = np.histogram(C, bins=50)  # Adjust the number of bins as needed\n\n# Plot the distribution\nplt.bar(bins[:-1], coefficients, width=np.diff(bins), edgecolor='black')\nplt.xlabel('Value')\nplt.ylabel('Frequency')\nplt.title('Distribution of Coefficients')\nplt.show()\n\nthirty_percentile = np.percentile(C, 30)\nprint(\"Thirty-th Percentile:\", thirty_percentile)\n\nforty_percentile = np.percentile(C, 40)\nprint(\"Forty-th Percentile:\", forty_percentile)\n\n\n\n\n\n\n\n\nThirty-th Percentile: 0.0\nForty-th Percentile: 0.0\n\n\nSo, we see that the value 0 is between our 30th and 40th percentile, This makes since looking at our histogram because the clustering right below 0. So, it would us space if we set all the values below a certain threshold to 0. Lets make our threshold 0 itself! So, for all negative numbers, we can just store them as 0. Lets do this and view it! Recall that \\(A = 2W_m^TCW_n\\) where \\(A\\) is the array of our image and \\(C\\) is the image after applying the Haar Wavelet transform\n\nimport copy\nC_copy = copy.deepcopy(C)\nfor i, row in enumerate(C):\n    for j, coeff in enumerate(row):\n        if coeff &lt; 0:\n            C_copy[i][j] = 0\n\nm, n = C_copy.shape\nif m%2 == 1:\n    m = m-1\n    C_copy = C_copy[:-1]\nif n%2 == 1:\n    n = n-1\n    C_copy = C_copy[:, :-1]\nA = 2*(harr(m).transpose()) @  C_copy @ harr(n)\nA_image = Image.fromarray(A).convert(\"L\")\nA_image\n\n\n\n\n\n\n\n\nLets compare this to what the third compression looks like without the threshold applied:\n\nmat = C\nm, n = mat.shape\ntransform_cut = mat[:int((m/2) + 1), :int((n/2) + 1)]\nimage_from_array = Image.fromarray(transform_cut).convert(\"L\")\ndisplay(image_from_array.resize((5000,2912)))\n\n\n\n\n\n\n\n\nWe can see that the third compression with the threshold is more pixelated. You can see this clearly looking at the cat’s eyes. However, overall it is not bad! Saving the matrix C is more efficent than saving the image \\(A\\) of the third compression because the space taken up by the edge images take up less space than the storage the image. And, once we apply this threshold compression, we can make store this matrix C even more efficently, as we now have more 0 (we are not storing very time negative values like -2.0 x 10^8). Making a class of values 0 also make create more ‘blocks’ of 0’s in the matrix, allowing the computer to store it with less space.\n\nThis is just excellent. Two comments: first, using the threshold alone shouldn’t result in much savings, because you are still storing all the 0s. You allude to this when you talk about the “blocks” that the computer can use – that’s correct, but it won’t happen automatically, it needs to be implemented somehow. There are many algorithms designed to efficiently store sparse matrices, and you could look into those.\nSecond, when you say “we only need to store C in order to recreate A”, I’m not sure how to interpret this. Yes, you can recreate A from the entire block matrix – but what you are storing when you’re doing the iterative compressions is just the one quadrant of C, from which you can’t entirely recreate A. I think you know what you meant here, it’s just a little confusing to read.\nLastly: looping over the pixels to apply your threshold isn’t efficient. (In general, loops are the slowest way to do anything.) Here is a bit of code that will do it much more quickly:\nC_copy = np.where(C &lt; 0, 0, C)\nGrade: E"
  },
  {
    "objectID": "submitted/projects_2/buyalosbob.html#project-3",
    "href": "submitted/projects_2/buyalosbob.html#project-3",
    "title": "Project 2 - Stat 243",
    "section": "Project 3:",
    "text": "Project 3:\nData Source: https://github.com/devstopfix/nfl_results/blob/master/nfl%202014.csv\nThe team that we will choose to investigate is the 2014 season of the NFL! We will begin by important the raw data and investigating it. I reccomend downloading the raw file from the link above so that you can run the code as you read. We will begin by inspecting the code:\n\nimport pandas as pd\ndf = pd.read_csv('nfl 2014.csv')\ndf\n\n\n\n\n\n\n\n\n\nseason\nweek\nkickoff\nhome_team\nhome_score\nvisitors_score\nvisiting_team\n\n\n\n\n0\n2014\n1\n2014-09-04T19:30:00-05:00\nSeahawks\n36\n16\nPackers\n\n\n1\n2014\n1\n2014-09-07T12:00:00-05:00\nBears\n20\n23\nBills\n\n\n2\n2014\n1\n2014-09-07T12:00:00-05:00\nChiefs\n10\n26\nTitans\n\n\n3\n2014\n1\n2014-09-07T12:00:00-05:00\nDolphins\n33\n20\nPatriots\n\n\n4\n2014\n1\n2014-09-07T12:00:00-05:00\nEagles\n34\n17\nJaguars\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n262\n2014\n19\n2015-01-11T13:05:00-05:00\nPackers\n26\n21\nCowboys\n\n\n263\n2014\n19\n2015-01-11T16:40:00-05:00\nBroncos\n13\n24\nColts\n\n\n264\n2014\n20\n2015-01-18T15:00:00-05:00\nSeahawks\n28\n22\nPackers\n\n\n265\n2014\n20\n2015-01-18T18:30:00-05:00\nPatriots\n45\n7\nColts\n\n\n266\n2014\n21\n2015-02-01T18:30:00-05:00\nSeahawks\n24\n28\nPatriots\n\n\n\n\n267 rows × 7 columns\n\n\n\n\nNow we will clean this up this data! We will perform the following: 1. remove columns so that we are only left with the home team, home score, visiting team, visiting score. 2. We will split our dataset in half, only looking at the first 133 rows. That way we can ‘train’ on the first half and then test how effective this is on the second half of the data. 3. From our 133 rows, we need to ensure that there is at most 1 game between any two teams (so that we do not have multiple edges between two nodes). We will do this and remove such duplicates (keeping the first instance of a game between two teams)\nOnce we have done this cleaning, we can put out data into a graph system! To do this, we will create an edge from home_team –&gt; visitng team. To calculate the weight of this edge, we will use home team score - visiting team score.\n\nimport networkx as nx\n# 1. and 2. Only Keep Relevany Columns and the top 133 rows\ncolumns_to_keep = ['home_team', 'home_score', 'visitors_score', 'visiting_team'] \ndf_filtered = df[columns_to_keep].head(133)\n\n# 3. Remove duplicate games between teams\nstorage = []\nEdges = []\nfor index, row in df_filtered.iterrows():\n    id = set()\n    id.add(row['home_team'])\n    id.add(row['visiting_team'])\n    if id not in storage:\n        Edges.append((row['home_team'], row['home_score'], row['visitors_score'], row['visiting_team']))\n        storage.append(id)\n\n#Create list of all the teams in our database -- used to create verticies when constructing the graph (add_nodes_from function below)\nvertex_set = set()\nfor i, team in enumerate(df_filtered['home_team']):\n    vertex_set.add(team)\nvertex_set = list(vertex_set)\n\n#Construct graph such that edges go from home team --&gt; visitng team and weights are home score - visitng score; meanwhile construct weights list\nG = nx.DiGraph()\nG.add_nodes_from(vertex_set)\nfor tup in Edges:\n    score_difference = (tup[1] - tup[2])\n    G.add_edge(tup[0], tup[3], weight = score_difference)\n \n\n# Draw the graph\nnx.draw_circular(G, with_labels=True)\nnx.draw_networkx_edge_labels(G, pos= nx.spring_layout(G))\nplt.show()\n\n\n\n\n\n\n\n\n\nWow, that’s a busy graph! For fun, I just made it even busier for you, by labelling the edges…\n\nNow, observe what the incidence matrix of this graph will look like. Each columns will represent an edge. Each edge represents a game, so there will be one 1 and one -1 in each column. Now, observe what happens if we take the transpose of this matrix, call it A. Any row of A will have one 1, one -1, and the rest 0. Say our 1 is in column i and our -1 is in row j. Then, the equation that corresponds to this row is x(i) - x(j). So, if let y = x(i) - x(j), and do this for every row, and put it in a vector of the respective row order, then we have the observed potential differences vector! Thus, we have crated a system Ax = b and we can use least squares to find the best fit for our vector x of potentials.\nSo, lets begin by forming the incidence matrix transpose and our observed potential differences vector b: 1. to get incidence matrix transpose we can directly get this from our graph G using built in functions of networkx 2. to get the observed potential differnces vector, we just have to take home score - column score, and make sure that it is in the correct row.\n\nfrom networkx.linalg.graphmatrix import incidence_matrix\n#Create incidence matrix\ninc_matrix = incidence_matrix(G, oriented=True)\ninc_matrix = inc_matrix.toarray()\ninc_transpose = inc_matrix.transpose()\nprint(\"incidence transpose:\", inc_transpose)\n\n\n#Create observed potential differences vector b through three steps:\n#a) Get edge weights as a dictionary\nedge_weights_dict = dict(nx.get_edge_attributes(G, 'weight'))\n#b) Get a list of edge weights in the same order as the incidence matrix\nedge_weights_list = [edge_weights_dict[edge] for edge in G.edges]\n#c) Convert the list to a NumPy array\nedge_weights_array = np.array(edge_weights_list)\nb = edge_weights_array\nb\n\nincidence transpose: [[-1.  0.  0. ...  0.  0.  0.]\n [-1.  0.  0. ...  0.  0.  0.]\n [-1.  0.  0. ...  0.  0.  0.]\n ...\n [ 0.  0.  0. ...  0.  0. -1.]\n [ 0.  0.  0. ...  1.  0. -1.]\n [ 0.  0.  0. ...  0.  0. -1.]]\n\n\narray([  7,   7,  21,  25,  14,  11,   6,  -5, -10,  21,  12,  -3,   1,\n       -16,  27,  27,  14,   1,   9,  10,   4,   7,  26,   2,  28,  22,\n        -6,  -2, -31,  -6, -11,  13,  10, -16,  -1,   2, -14,   7,  32,\n        21, -23,  13, -14,   3,  11,   6,  21,  17, -18,   7,  -4, -18,\n       -11,  21,   3,  10,  -3, -11,   3,  42, -14,  -1, -28,  -3, -14,\n         2,  19, -12, -15,   1,  -8,   5,   5,  -7,  20,  28,  22,  -3,\n       -21, -13,   2,  -2,  10,   5, -27,  -8,  18, -14,  13, -19,  -3,\n        37,   9,  19,  31,  -3,  14,  26,   0,  10,   5,  -8,  -7, -14,\n       -20,   3,  -3,   7,  17,  -3,  24,   7,  27, -16, -24,  -3, -11,\n        20,   6,  -7,   6,  17,   3,   6,  27,  31, -31, -10,   2])\n\n\n\nThat’s a nice and elegent way of getting the weighted incidence matrix!\n\nLet’s do some testing to amake sure that our incidence transpose matrix and vector b are what we expect them to be. To do this, let’s create a dictionary that maps vertex numbers (as indicies in vertex_set), and the name of the vertex (the team name). This is important because this index number of each vertex corresponds to the column of our incidence transpose matrix represents, by the way we iterately defined our graph through the vertex_set list.\nOnce we have created this dictionary, vert_to_team, let’s check a few rows to make sure that the game that row represents has the correct spread in the vector b.\n(I will put many of these outputs onto the markdown - because early on we cast a set to a list, then our rows may be in shuffled order–the rows that the reader runs will almost surely be in a different order than when I am writing this on my computer–but long as we check for the correct correspondence between the rows of our matrix and the vector b, then it does not matter if the rows are shuffled when you run it)\nMy first row is: [[-1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\nUsing code below, I see that my first team is the Ravens, and the 24th team is the Bengals. Going back to my original data source, in this game the Ravens scored 16 and were at home, and the Bengals scored 23 and were visiting. So, we expect our vector b to have -7 in the first row. And it does!\nLet’s test row 101. This is [ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. -1. 0. 0. 0. 0. 0. 0.]\nSo, the 17th team is the Browns, and our 26th team are the Steelers. Going back to our data source, we see that the Browns were at home and scored 30, and the Steelers were visiting and scored 27. So, we expect our vector b to have 3 in the 101-th row, And it does!\nSo, we can say that our matrix and our vector be have the correct row correspondence.\n\nI love these checks. Getting the row indices right is a common source of error, and it’s great to see you’re being careful about it.\n\n\nvert_to_team = {}\nfor i, team in enumerate(vertex_set):\n    vert_to_team[i] = team\n\n#test row 1\nprint(\"test row 1:\")\nprint(vert_to_team[0])\nprint(vert_to_team[23])\nprint(b[0])\nprint(\"\\n\")\n\n#test row 101\nprint(\"test row 101:\")\nprint(inc_transpose[100])\nprint(vert_to_team[16])\nprint(vert_to_team[25])\nprint(b[100])\n\ntest row 1:\nBroncos\nChargers\n7\n\n\ntest row 101:\n[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0. -1.  0.  0.  1.  0.  0.  0.]\nBills\nJets\n5\n\n\nNow, we will perform least squares using the normal equations!\nThe matrix defined as B can be thought of as the \\(AA^T\\), and so we have that \\(x = B^{-1}A^Tb\\). Note that this may be slightly confusing because our \\(A\\) is “inc_transpose” and so our \\(A^T\\) is “inc_transpose.transpose()”\n\nb_column_vector = b.reshape(-1, 1)  # Assuming b is a 1D array with 129 elements\nB = inc_transpose.transpose() @ inc_transpose\nBinv = np.linalg.inv(B)\nx = Binv @ inc_transpose.transpose() @ b_column_vector\n\nNow, we have found our potentials vector x! Let’s test it on some testing data!\nWe will essentially do all of the same things, but this time restricting our raw data to only the last 133 entires before filtering. We will do all of the same work we did earlier to arrive at the vector b, so feel free to not read this next code block since we are essentially copying and pasting everything we have done up to now. Let’s print vector b of our spreads of our testing data:\n\nA more elegant way to do this would have been to put all your code into a function, and then call it twice – once with the first half of the data, and once with the second half. However, then you wouldn’t have been able to show your work step by step with the markdown, so this way was probably better!\n\n\n# 1. and 2. Only Keep Relevany Columns and the bottom 133 rows\ncolumns_to_keep = ['home_team', 'home_score', 'visitors_score', 'visiting_team'] \ndf_filtered = df[columns_to_keep].tail(133)\n\n# 3. Remove duplicate games between teams\nstorage = []\nEdges = []\nfor index, row in df_filtered.iterrows():\n    id = set()\n    id.add(row['home_team'])\n    id.add(row['visiting_team'])\n    if id not in storage:\n        Edges.append((row['home_team'], row['home_score'], row['visitors_score'], row['visiting_team']))\n        storage.append(id)\n\n#Create list of all the teams in our database -- used to create verticies when constructing the graph (add_nodes_from function below)\nvertex_set = set()\nfor i, team in enumerate(df_filtered['home_team']):\n    vertex_set.add(team)\nvertex_set = list(vertex_set)\n\n#Construct graph such that edges go from home team --&gt; visitng team and weights are home score - visitng score; meanwhile construct weights list\nG = nx.DiGraph()\nG.add_nodes_from(vertex_set)\nfor tup in Edges:\n    score_difference = (tup[1] - tup[2])\n    G.add_edge(tup[0], tup[3], weight = score_difference)\n\n\ninc_matrix = incidence_matrix(G, oriented=True)\ninc_matrix = inc_matrix.toarray()\ninc_transpose = inc_matrix.transpose()\n\n#Create observed potential differences vector b through three steps:\n#a) Get edge weights as a dictionary\nedge_weights_dict = dict(nx.get_edge_attributes(G, 'weight'))\n#b) Get a list of edge weights in the same order as the incidence matrix\nedge_weights_list = [edge_weights_dict[edge] for edge in G.edges]\n#c) Convert the list to a NumPy array\nedge_weights_array = np.array(edge_weights_list)\nb = edge_weights_array\nb_column_vector = b.reshape(-1, 1)\nprint(b_column_vector)\n\n[[  3]\n [  7]\n [-11]\n [ -9]\n [ 24]\n [ 12]\n [  4]\n [ 17]\n [ 17]\n [  2]\n [  4]\n [-13]\n [ 12]\n [ 17]\n [  8]\n [  3]\n [ 25]\n [ 28]\n [ -8]\n [  4]\n [-10]\n [ -1]\n [-17]\n [ -3]\n [ -6]\n [ -3]\n [ 11]\n [ -8]\n [ 41]\n [ 33]\n [  5]\n [  6]\n [ 10]\n [  5]\n [ -3]\n [-29]\n [ -5]\n [-17]\n [ -3]\n [ 18]\n [  6]\n [ -3]\n [-17]\n [ -7]\n [-31]\n [-16]\n [ -2]\n [  2]\n [  4]\n [ 11]\n [-23]\n [ 35]\n [  4]\n [ -2]\n [ 11]\n [ -7]\n [ 15]\n [ 52]\n [-10]\n [ -4]\n [ 35]\n [ 16]\n [  8]\n [  4]\n [-16]\n [ -3]\n [  3]\n [ 14]\n [ -1]\n [  8]\n [ 10]\n [  8]\n [  8]\n [-13]\n [-16]\n [-14]\n [  1]\n [-14]\n [  8]\n [-16]\n [ -1]\n [ 13]\n [-15]\n [  2]\n [  7]\n [  3]\n [ -9]\n [-12]\n [-21]\n [-21]\n [  9]\n [  7]\n [ -3]\n [ -1]\n [ -3]\n [  8]\n [-13]\n [-22]\n [ 20]\n [ 22]\n [  7]\n [ 16]\n [-24]\n [  4]\n [ 11]\n [  2]\n [ 21]\n [ 16]\n [ 14]\n [ 14]\n [  6]\n [ -4]\n [ 24]\n [ 19]\n [-10]\n [-20]\n [-24]\n [  3]\n [-27]]\n\n\nNow, above we created an “inc_transpose” matrix, (i.e. matrix A) for the testing data in the same way we created an “inc_transpose” matrix for our training data. Now, using the least squares vector x we got on our earlier matrix, we want to compare how Ax compares to b, where A is the matrix obtained from the testing data, x is the least squares matrix from the training data, and b is the observed potential differences vector of our testing data.\nTo compare, we will take the absolute value of their difference and their percent differences:\n\\(|y_i - b_i|\\) and \\(\\frac{b_i - y_i}{y_i}\\) where \\(y_i\\) is the i-th component of \\(Ax\\) for the \\(x\\) we got from least squares and the \\(A\\) of the testing data, and \\(b_i\\) is the i-th component of the b vector. So, if for a given row, Ax has 4 and b has 6, then its absolute value ‘score’ would be \\(2\\) and its percent change would be \\(.5\\). We will compute these two metrics for every component and take the average value. This will give us a sense of how ‘good’ our vector \\(x\\) is at testing \\(b\\):\n\noutput_abs = []\noutput_perc = []\nax = inc_transpose @ x\nfor i in range(len(b_column_vector)):\n    output_abs.append(abs(float(ax[i]) - float(b_column_vector[i])))\n    output_perc.append((float(b_column_vector[i]) - float(ax[i]))/float(ax[i]))\nprint(\"Average Value of absolute value 'score':\", sum(output_abs)/len(output_abs))\nprint(\"Average Value of percent change 'score':\", sum(output_perc)/len(output_perc))\n\nAverage Value of absolute value 'score': 15.592436974789916\nAverage Value of percent change 'score': -4.835796049014146\n\n\nAfter doing this analysis, we see that our potentials vector x from the training data does a largely bad job of predicting the observed potential differences of the testing dataset. The average difference between the predicted spread and the actual spread is about 22 points! And the percent change between the predicted spread and the observed spread was a decrease of 56% This is quite bad. There could be a few reasons for this: 1. a ‘potential difference’ does not give much information about the magnitude of a win or a loss. If one game is 2-4 and another is 200-202, we will have the same potential differencce. This may create issues when extracting it to other teams. For example, a 2 potential diference of one game and a 2 potential difference of another game might not reflect the differences in magnitude between these two games and thus will not be helpful in predicting the potential differences when you match off these teams against others.\n\nThis is a very good point.\n\n\nOur data set only includes one game for any two given teams, and many teams only appear in the dataset a handful of times. Therefore, our data is very suspetible to outliers, or even to random chance where a good team is only playing bad teams, and vice versa. Also, due to the structure of the NFL, most of the games are played wtihin divisions, and so we may have worse predictions when two teams from different divisons play off. Thus, we need a larger sample size, and every team to play each other, in order to really create a more accurate x vector.\n\n\nExcellent. Grade: E"
  },
  {
    "objectID": "submitted/projects_2/test.html",
    "href": "submitted/projects_2/test.html",
    "title": "Projects 2",
    "section": "",
    "text": "Code\nimport sys\nprint(sys.executable)\n\n\n/usr/local/bin/python3"
  },
  {
    "objectID": "submitted/projects_2/yangsally.html",
    "href": "submitted/projects_2/yangsally.html",
    "title": "Projects 2",
    "section": "",
    "text": "Modelling with Directed Graphs\nComment: I think the hints really helped a lot with the intuition. I couldn’t really understand what the basis vectors for the null space of an incidence matrix represent intuitively until the hints are given, especially when I was trying to compute a basis of directed loops.\n\nGreat, I’m glad they helped!\n\n\nPlot the Digraph\n\n\nprint the path of the python executable\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport gravis as gv\nfrom collections import defaultdict\n\nG = nx.DiGraph()\nG.add_node(\"1\")\nG.add_node(\"2\")\nG.add_node(\"3\")\nG.add_node(\"4\")\nG.add_node(\"5\")\nG.add_node(\"6\")\nG.add_edge(\"1\",\"2\")\nG.add_edge(\"2\",\"3\")\nG.add_edge(\"3\",\"4\")\nG.add_edge(\"4\",\"2\")\nG.add_edge(\"1\",\"4\")\nG.add_edge(\"3\",\"1\")\nG.add_edge(\"3\",\"6\")\nG.add_edge(\"6\",\"3\")\nG.add_edge(\"4\",\"5\")\nG.add_edge(\"5\",\"6\")\npos = nx.spring_layout(G)\nnx.draw_networkx_nodes(G, pos)\nnx.draw_networkx_labels(G, pos)\nnx.draw_networkx_edges(G, pos, edge_color='r', arrows = True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nExhibit the incidence matrix A of this digraph and find a basis for N(A) using its reduced row echelon form.\n\n\n\nCode\nimport numpy as np\nfrom sympy import Matrix\n\n# Define the vertices and edges\nV = [1, 2, 3, 4, 5, 6]\nE = [(1, 2), (2, 3), (3, 4), (4, 2), (1, 4), (3, 1), (3, 6), (6, 3), (4, 5), (5, 6)]\nnum_vertices = len(V)\nnum_edges = len(E)\n\n# Initialize the incidence matrix \nincidence = np.zeros((num_vertices, num_edges))\n\n# Fill in the matrix with entries corresponding to each vertex and incoming & outgoing edges\nfor col, edge in enumerate(E):\n    from_vertex, to_vertex = edge\n    from_index = V.index(from_vertex)\n    to_index = V.index(to_vertex)\n    incidence[from_index, col] = 1  # Outgoing edge\n    incidence[to_index, col] = -1  # Incoming edge\nprint(\"The incidence matrix is:\")\nprint(incidence)\n\n# Then we find the RREF of A to find a basis for N(A) by hand (details in attached pictures)\nx1 = np.array([0,1,1,1,0,0,0,0,0,0])\nx2 = np.array([-1,-1,-1,0,1,0,0,0,0,0])\nx3 = np.array([1,1,0,0,0,1,0,0,0,0])\nx4 = np.array([0,0,0,0,0,0,1,1,0,0])\nx5 = np.array([0,0,1,0,0,0,-1,0,1,1])\nprint(\"The basis for null space of A is:\")\nprint(x1.T)\nprint(x2.T)\nprint(x3.T)\nprint(x4.T)\nprint(x5.T)\n\n\nThe incidence matrix is:\n[[ 1.  0.  0.  0.  1. -1.  0.  0.  0.  0.]\n [-1.  1.  0. -1.  0.  0.  0.  0.  0.  0.]\n [ 0. -1.  1.  0.  0.  1.  1. -1.  0.  0.]\n [ 0.  0. -1.  1. -1.  0.  0.  0.  1.  0.]\n [ 0.  0.  0.  0.  0.  0.  0.  0. -1.  1.]\n [ 0.  0.  0.  0.  0.  0. -1.  1.  0. -1.]]\nThe basis for null space of A is:\n[0 1 1 1 0 0 0 0 0 0]\n[-1 -1 -1  0  1  0  0  0  0  0]\n[1 1 0 0 0 1 0 0 0 0]\n[0 0 0 0 0 0 1 1 0 0]\n[ 0  0  1  0  0  0 -1  0  1  1]\n\n\n\nNx has a built-in function for finding the incidence matrix, but I appreciate that you’ve done it by hand here.\n(For that matter, sympy has a function for finding the rref and the nullspace of a matrix… these let me double-check your work):\n\n\nCode\nfrom sympy import Matrix, nsimplify\n# E but map all the numbers to strings\nE_str = [(str(e[0]), str(e[1])) for e in E]\ninc_matrix = nsimplify(Matrix(nx.incidence_matrix(G,edgelist=E_str, oriented=True).toarray()),rational=True)\nprint(\"Incidence matrix:\")\ndisplay(inc_matrix)\n(np.equal(np.array(incidence),-np.array(inc_matrix))).all() # Check if #the two matrices are the same\nnullspace = nsimplify(Matrix.hstack(*inc_matrix.nullspace()),rational=True)\nprint(\"Nullspace basis:\")\ndisplay(nullspace)\n\n\nIncidence matrix:\nNullspace basis:\n\n\n\\(\\displaystyle \\left[\\begin{matrix}-1 & 0 & 0 & 0 & -1 & 1 & 0 & 0 & 0 & 0\\\\1 & -1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & 1 & -1 & 0 & 0 & -1 & -1 & 1 & 0 & 0\\\\0 & 0 & 1 & -1 & 1 & 0 & 0 & 0 & -1 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & -1\\\\0 & 0 & 0 & 0 & 0 & 0 & 1 & -1 & 0 & 1\\end{matrix}\\right]\\)\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0 & -1 & 1 & 0 & 0\\\\1 & -1 & 1 & 0 & 0\\\\1 & -1 & 0 & 0 & 1\\\\1 & 0 & 0 & 0 & 0\\\\0 & 1 & 0 & 0 & 0\\\\0 & 0 & 1 & 0 & 0\\\\0 & 0 & 0 & 1 & -1\\\\0 & 0 & 0 & 1 & 0\\\\0 & 0 & 0 & 0 & 1\\\\0 & 0 & 0 & 0 & 1\\end{matrix}\\right]\\)\n\n\n\nThese five vectors together span the null space of A.\nJust as given by the hint “each column in the nullspace tells us a set of coefficients which can multiply the columns of the incidence matrix so that the rows can sum to zero. Example: if our nullspace has a column vector [0,1,0,1]^T, this tells us that column 2 + column 4 of the incidence matrix will sum to zero,” we can verify this with the basis vectors we get.\nFor instance, x1 is [0,1,1,1,0,0,0,0,0,0]^T, which means column 2 + column 3 + column 4 of the incidence matrix A will sum to zero, which is true.\nNow we use the computed basis to find a basis of directed loops For x1, we may see that it is a directed loop : column 2, 3, and 4 together is a loop from vertex 2 to 3, and then 3 to 4, and 4 back to 2. Direction does matter in this loop since if we switch the direction it would become 2 to 4, 4 to 3, and 3 back to 2, which is different.\n\nThat’s not what this is asking about. The difficulty is when a loop in the basis is something like 1 to 2, and 3 to 2, and 1 to 3. You can’t actually follow that loop if you are going in the directions of the edges.\nSome of the loops in the basis are actually directed loops, but some are not. You need to figure out which are which, then see if you can add a directed loop to some of the undirected loops to make a new basis of directed loops.\n\nFor x2, direction also matters. This loop is slightly more complicated but the idea is similar - if we switch the direction the loop would be different. For x3, direction matters as well just as for x1. For x4, we shall see this is actually a non-directed loop consisted of only two vertices 3 and 6. Switching the direction, we still get the same loop as before. For x5, direction matters so it’s a directed loop.\nTherefore, considering adding a directed basis element to the non-directed basis element x4 just like what’s given in the hint, we may replace x4 by x4 + x5, which is [0,0,1,0,0,0,0,1,1,1]^T and this loop is directed as expected.\n\n\nCode\n# Now the \"corrected\" basis of directed loops is:\nx1 = np.array([0,1,1,1,0,0,0,0,0,0])\nx2 = np.array([-1,-1,-1,0,1,0,0,0,0,0])\nx3 = np.array([1,1,0,0,0,1,0,0,0,0])\nx4 = np.array([0,0,1,0,0,0,0,1,1,1])\nx5 = np.array([0,0,1,0,0,0,-1,0,1,1])\nprint(\"The (directed-loop) basis for null space of incidence matrix is:\")\nprint(x1.T)\nprint(x2.T)\nprint(x3.T)\nprint(x4.T)\nprint(x5.T)\n\n\nThe (directed-loop) basis for null space of incidence matrix is:\n[0 1 1 1 0 0 0 0 0 0]\n[-1 -1 -1  0  1  0  0  0  0  0]\n[1 1 0 0 0 1 0 0 0 0]\n[0 0 1 0 0 0 0 1 1 1]\n[ 0  0  1  0  0  0 -1  0  1  1]\n\n\nNow we move on to solve 3 problems selected from the 5 problems given.\n\nThink of the digraph as representing an electrical circuit where an edge represents some electrical object like a resistor or capacitor. Each node represents the circuit space between these objects. and we can attach a potential value to each node, say the potentials are x1, …., x6. The potential difference across an edge is the potential value of head minus tail. Kirchhoff’s second law of electrical circuits says that the sum of potential differences around a circuit loop must be zero. Assume and use the fact (p. 422) that Ax = b implies that for all y in N(A^T), y^Tb = 0, to find conditions that a vector b must satisfy in order for it to be a vector of potential differences for some potential distribution on the vertices.\n\n\n\nCode\n# We should obtain the basis for null space of A^T, which is simpler than that of A\n# In fact, as N(A) has 5 basis elements, it's easy to see that N（A^T) has 6-5 = 1 basis element\ne = np.array([1,1,1,1,1,1]) \n# Now we want to find conditions on b to make y^Tb = 0 to hold for y in null space of A^T\n# Which means for any y that is a scalar multiple of [1,1,1,1,1,1]^T, y^Tb = 0 should hold.\n\n\n# Start with any vector b we want to test\nb = np.array([1, 2, 3, 0, -4, -2])  \n\n# Check if the condition holds for any scalar multiple of [1,1,1,1,1,1]\ncondition_met = True\nfor scalar in range(-10, 11):  # Check scalar multiples from -10 to 10\n    y = scalar * np.ones(6)  # Create a scalar multiple of [1,1,1,1,1,1]\n    inner_product = np.dot(y, b)\n    if inner_product != 0:\n        condition_met = False\n        break\n\nif condition_met:\n    print(\"The condition holds.\")\nelse:\n    print(\"The condition does not hold for the inputted vector b.\")\n\n\nThe condition holds.\n\n\nIntuitively speaking, the yTb = 0 condition essentially means the entries of b should sum to 0, so the inner product of any scalar multiple ( let it be a ) of [1,1,1,1,1,1] and b would be ab1 + ab2 +…+ ab6 = a (b1 +…+b6) = 0. I think this makes intuitive sense —— b has entries summing up to 0 means the “distribution” of potential differences between the 6 nodes must be cancelled out after being added up, otherwise it wouldn’t make sense.\n\nVery nice!\n\n\nAssume that across each edge of a circuit a current flows. Thus, we can assign to each edge a “weight,” namely the current flow along the edge. This is an example of a weighted digraph. However, not just any set of current weights will do, since Kirchhoff’s first law of circuits says that the total flow of current in and out of any node should be 0 . Use this law to find a matrix condition that must be satisfied by the currents and solve it to exhibit some current flows.\n\nLet Iij denote the current flowing from node i to node j. Iij essentially represents the amount of current flowing in the (i,j) edge (if exists). As Kirchhoff’s first law of circuits says that the total flow of current in and out of any node should be 0, this means for any node i that has edges connected to it, the currents should sum to zero. Let I be the vector of currents, so it should be a 10 by 1 vector since the number of currents should exactly match the number of edges. A matrix condition that the current vector I must satisfy, as required by Kirchoff’s first law, should be AI = 0.\n\nYou can write the math using LaTeX in the markdown cells. For example, you can write the “Iij” as $I_{ij}$, which becomes \\(I_{ij}\\)\n\n\n\nCode\nimport numpy as np\n\n# Use the singular value decomposition (SVD) to solve Ax = 0\nU, S, Vt = np.linalg.svd(incidence)\n\n# The solution vector x is the last column of Vt (corresponding to the smallest singular value)\nI = Vt[-1]\n\nprint(\"Solution I:\")\nprint(I)\n\n\nSolution I:\n[-0.13114072 -0.00822184 -0.1078173   0.12291888  0.60876909  0.47762838\n -0.18901646  0.18901646  0.37803292  0.37803292]\n\n\n\nThe solution space for Ax = 0 is the nullspace of A, which you’ve already found.\nNow, your solution works in this case because the last singular value is zero; this will be the case in general if A isn’t square or full rank. However, using the nullspace of A is a more direct way to find the solution, and it happens to give a much more clean basis in this case.\n\nThe solution we get is indeed a 10 by 1 vector (A is a 6 by 10 matrix, so I should be 10 by 1)\n\nThink of the digraph as representing a directed communications network. Here loops determine which nodes have bidirectional communication since any two nodes of a loop can only communicate with each other by way of a loop. By examining only a basis of directed loops how could you determine which nodes in the network can communicate with each other?\n\nFrom only the basis of directed loops, which is consisted of x1 = [0,1,1,1,0,0,0,0,0,0] x2 = [-1,-1,-1,0,1,0,0,0,0,0] x3 = [1,1,0,0,0,1,0,0,0,0] x4 = [0,0,1,0,0,0,0,1,1,1] x5 = [0,0,1,0,0,0,-1,0,1,1], we want to determine which nodes among {1,2,3,4,5,6} communicate with each other.\nNodes 1 and 2: There is a nonzero entry in position 2 of x1 and x2, so nodes 1 and 2 communicate. Nodes 1 and 3: There is a nonzero entry in position 1 of x1 and x3, so nodes 1 and 3 communicate. Nodes 1 and 4: There is a nonzero entry in position 3 of x1 and x4, so nodes 1 and 4 communicate. Nodes 1 and 5: No nonzero entries in the positions corresponding to nodes 1 and 5 in any basis vectors, so nodes 1 and 5 do not communicate. Nodes 1 and 6: No nonzero entries in the positions corresponding to nodes 1 and 6 in any basis vectors, so nodes 1 and 6 do not communicate. Nodes 2 and 3: There is a nonzero entry in position 2 of x2 and x3, so nodes 2 and 3 communicate. Nodes 2 and 4: There is a nonzero entry in position 4 of x2 and x4, so nodes 2 and 4 communicate. …..\nTherefore, the pairs of nodes that communicate with each other are:\nNodes 1 and 2, Nodes 1 and 3, Nodes 1 and 4 Nodes 2 and 3, Nodes 2 and 4, Nodes 3 and 4, Nodes 3 and 6, Nodes 4 and 5, Nodes 5 and 6\n\nThe question is asking you to figure out which nodes can communicate even with potential intermediate nodes that may not be part of the same basis elements in your basis of directed loops. So you will have to figure out what happens if you have combinations of the loops in your basis.\n\n\nThere were some conceptual issues I’d like to see fixed here!\nGrade: R\n\nImage Conpression and Edge Detection\n\nSelected image in grayscale with 8-bit precision (using Gimp)\n\n\nI didn’t have your images, so to make this compile I replaced all the images with cat images…\n\n\n\nCode\nfrom IPython.display import Image\nImage(filename='catimage.png')\n\n\n\n\n\n\n\n\n\n\nMatrix generated by the Grayscale Image & Haar Wavelet Transform\n\nComment : Honestly I really tried writing a program to perform the Haar Wavelet Transform but I kept failing to get the representation of the Haar Wavelet Transform matrix right. I ended up using a package called Pywavelets to perform the transformation on the matrix. This package is really simple to use in practice but it doesn’t help with understanding the algorithm.\n\nThat sounds frustrating! I would have liked to see your attempts, so I could help you debug.\nThe library you found can actually be of help here. It does implement the Haar transform, although you need to specify it by putting “haar” as the wavelet name (instead of “bior1.3” as you did in the code below).\n\n\nCode\n!pip install PyWavelets\n\n\nRequirement already satisfied: PyWavelets in /usr/local/lib/python3.11/site-packages (1.6.0)\nRequirement already satisfied: numpy&lt;3,&gt;=1.22.4 in /usr/local/lib/python3.11/site-packages (from PyWavelets) (1.26.4)\n\n\nExample:\n\n\nCode\nimport numpy as np\n\nimport pywt\n# generate a simple 5x5 matrix\nwt = pywt.Wavelet('haar')\nwt.filter_bank\nplt.show()\n\n\nIt doesn’t seem like there is any way to get the matrix representation of the Haar wavelet transform from this package, but you can still use the package to perform the transform and visualize the results.\n\n\n\nCode\nimport matplotlib.image as img\nimport pywt\nfile_name = 'catimage.png'\nimage_matrix = img.imread(file_name)\nimage_matrix = np.sum(image_matrix, axis=2) / 3\nprint(image_matrix.shape)\n# We shall see that the generated matrix is a 1170 by 1492 matrix\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pywt\nimport pywt.data\n\n# Load image\noriginal = image_matrix\n\n# Wavelet transform of image, and plot approximation and details\ntitles = ['Approximation', ' Horizontal detail',\n          'Vertical detail', 'Diagonal detail']\ncoeffs2 = pywt.dwt2(original, 'bior1.3')\nLL, (LH, HL, HH) = coeffs2\nfig = plt.figure(figsize=(12, 3))\nfor i, a in enumerate([LL, LH, HL, HH]):\n    ax = fig.add_subplot(1, 4, i + 1)\n    ax.imshow(a, interpolation=\"nearest\", cmap=plt.cm.gray)\n    ax.set_title(titles[i], fontsize=10)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nfig.tight_layout()\nplt.show()\n\noriginal = LL\n\n# Wavelet transform of image, and plot approximation and details\ntitles = ['Approximation', ' Horizontal detail',\n          'Vertical detail', 'Diagonal detail']\ncoeffs2 = pywt.dwt2(original, 'bior1.3')\nLL, (LH, HL, HH) = coeffs2\nfig = plt.figure(figsize=(12, 3))\nfor i, a in enumerate([LL, LH, HL, HH]):\n    ax = fig.add_subplot(1, 4, i + 1)\n    ax.imshow(a, interpolation=\"nearest\", cmap=plt.cm.gray)\n    ax.set_title(titles[i], fontsize=10)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nfig.tight_layout()\nplt.show()\n\noriginal = LL\n\n# Wavelet transform of image, and plot approximation and details\ntitles = ['Approximation', ' Horizontal detail',\n          'Vertical detail', 'Diagonal detail']\ncoeffs2 = pywt.dwt2(original, 'bior1.3')\nLL, (LH, HL, HH) = coeffs2\nfig = plt.figure(figsize=(12, 3))\nfor i, a in enumerate([LL, LH, HL, HH]):\n    ax = fig.add_subplot(1, 4, i + 1)\n    ax.imshow(a, interpolation=\"nearest\", cmap=plt.cm.gray)\n    ax.set_title(titles[i], fontsize=10)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nfig.tight_layout()\nplt.show()\n\noriginal = LL\n\n# Wavelet transform of image, and plot approximation and details\ntitles = ['Approximation', ' Horizontal detail',\n          'Vertical detail', 'Diagonal detail']\ncoeffs2 = pywt.dwt2(original, 'bior1.3')\nLL, (LH, HL, HH) = coeffs2\nfig = plt.figure(figsize=(12, 3))\nfor i, a in enumerate([LL, LH, HL, HH]):\n    ax = fig.add_subplot(1, 4, i + 1)\n    ax.imshow(a, interpolation=\"nearest\", cmap=plt.cm.gray)\n    ax.set_title(titles[i], fontsize=10)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nfig.tight_layout()\nplt.show()\n\noriginal = LL\n\n# Wavelet transform of image, and plot approximation and details\ntitles = ['Approximation', ' Horizontal detail',\n          'Vertical detail', 'Diagonal detail']\ncoeffs2 = pywt.dwt2(original, 'bior1.3')\nLL, (LH, HL, HH) = coeffs2\nfig = plt.figure(figsize=(12, 3))\nfor i, a in enumerate([LL, LH, HL, HH]):\n    ax = fig.add_subplot(1, 4, i + 1)\n    ax.imshow(a, interpolation=\"nearest\", cmap=plt.cm.gray)\n    ax.set_title(titles[i], fontsize=10)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nfig.tight_layout()\nplt.show()\n\noriginal = LL\n\n# Wavelet transform of image, and plot approximation and details\ntitles = ['Approximation', ' Horizontal detail',\n          'Vertical detail', 'Diagonal detail']\ncoeffs2 = pywt.dwt2(original, 'bior1.3')\nLL, (LH, HL, HH) = coeffs2\nfig = plt.figure(figsize=(12, 3))\nfor i, a in enumerate([LL, LH, HL, HH]):\n    ax = fig.add_subplot(1, 4, i + 1)\n    ax.imshow(a, interpolation=\"nearest\", cmap=plt.cm.gray)\n    ax.set_title(titles[i], fontsize=10)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nfig.tight_layout()\nplt.show()\n\noriginal = LL\n\n# Wavelet transform of image, and plot approximation and details\ntitles = ['Approximation', ' Horizontal detail',\n          'Vertical detail', 'Diagonal detail']\ncoeffs2 = pywt.dwt2(original, 'bior1.3')\nLL, (LH, HL, HH) = coeffs2\nfig = plt.figure(figsize=(12, 3))\nfor i, a in enumerate([LL, LH, HL, HH]):\n    ax = fig.add_subplot(1, 4, i + 1)\n    ax.imshow(a, interpolation=\"nearest\", cmap=plt.cm.gray)\n    ax.set_title(titles[i], fontsize=10)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nfig.tight_layout()\nplt.show()\n\n\n(2912, 5000)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSince each application of the transform will reduce storage requirements by a factor of four, applying the transform to the image 7 times will reduce storage requirements by a factor of 4^7. To make sure that this is indeed “unacceptably far” from the original image, we try applying the transform to an image with text 7 times to check if we could still read the text.\n\n\nCode\nfrom IPython.display import Image\nImage(filename='catimage.png')\n\nimport matplotlib.image as img\nfile_name = 'catimage.png'\ntext_matrix = img.imread(file_name)\nprint(text_matrix.shape)\n# We can see that this is a 563 by 1000 matrix\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pywt\nimport pywt.data\n\n# Load image\noriginal = text_matrix\n\n# Wavelet transform of image, and plot approximation and details\ntitles = ['Approximation', ' Horizontal detail',\n          'Vertical detail', 'Diagonal detail']\ncoeffs2 = pywt.dwt2(original, 'bior1.3')\nLL, (LH, HL, HH) = coeffs2\nfig = plt.figure(figsize=(12, 3))\nfor i, a in enumerate([LL, LH, HL, HH]):\n    ax = fig.add_subplot(1, 4, i + 1)\n    ax.imshow(a, interpolation=\"nearest\", cmap=plt.cm.gray)\n    ax.set_title(titles[i], fontsize=10)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nfig.tight_layout()\nplt.show()\n\noriginal = LL\n\n# Wavelet transform of image, and plot approximation and details\ntitles = ['Approximation', ' Horizontal detail',\n          'Vertical detail', 'Diagonal detail']\ncoeffs2 = pywt.dwt2(original, 'bior1.3')\nLL, (LH, HL, HH) = coeffs2\nfig = plt.figure(figsize=(12, 3))\nfor i, a in enumerate([LL, LH, HL, HH]):\n    ax = fig.add_subplot(1, 4, i + 1)\n    ax.imshow(a, interpolation=\"nearest\", cmap=plt.cm.gray)\n    ax.set_title(titles[i], fontsize=10)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nfig.tight_layout()\nplt.show()\n\noriginal = LL\n\n# Wavelet transform of image, and plot approximation and details\ntitles = ['Approximation', ' Horizontal detail',\n          'Vertical detail', 'Diagonal detail']\ncoeffs2 = pywt.dwt2(original, 'bior1.3')\nLL, (LH, HL, HH) = coeffs2\nfig = plt.figure(figsize=(12, 3))\nfor i, a in enumerate([LL, LH, HL, HH]):\n    ax = fig.add_subplot(1, 4, i + 1)\n    ax.imshow(a, interpolation=\"nearest\", cmap=plt.cm.gray)\n    ax.set_title(titles[i], fontsize=10)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nfig.tight_layout()\nplt.show()\n\noriginal = LL\n\n# Wavelet transform of image, and plot approximation and details\ntitles = ['Approximation', ' Horizontal detail',\n          'Vertical detail', 'Diagonal detail']\ncoeffs2 = pywt.dwt2(original, 'bior1.3')\nLL, (LH, HL, HH) = coeffs2\nfig = plt.figure(figsize=(12, 3))\nfor i, a in enumerate([LL, LH, HL, HH]):\n    ax = fig.add_subplot(1, 4, i + 1)\n    ax.imshow(a, interpolation=\"nearest\", cmap=plt.cm.gray)\n    ax.set_title(titles[i], fontsize=10)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nfig.tight_layout()\nplt.show()\n\noriginal = LL\n\n# Wavelet transform of image, and plot approximation and details\ntitles = ['Approximation', ' Horizontal detail',\n          'Vertical detail', 'Diagonal detail']\ncoeffs2 = pywt.dwt2(original, 'bior1.3')\nLL, (LH, HL, HH) = coeffs2\nfig = plt.figure(figsize=(12, 3))\nfor i, a in enumerate([LL, LH, HL, HH]):\n    ax = fig.add_subplot(1, 4, i + 1)\n    ax.imshow(a, interpolation=\"nearest\", cmap=plt.cm.gray)\n    ax.set_title(titles[i], fontsize=10)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nfig.tight_layout()\nplt.show()\n\noriginal = LL\n\n# Wavelet transform of image, and plot approximation and details\ntitles = ['Approximation', ' Horizontal detail',\n          'Vertical detail', 'Diagonal detail']\ncoeffs2 = pywt.dwt2(original, 'bior1.3')\nLL, (LH, HL, HH) = coeffs2\nfig = plt.figure(figsize=(12, 3))\nfor i, a in enumerate([LL, LH, HL, HH]):\n    ax = fig.add_subplot(1, 4, i + 1)\n    ax.imshow(a, interpolation=\"nearest\", cmap=plt.cm.gray)\n    ax.set_title(titles[i], fontsize=10)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nfig.tight_layout()\nplt.show()\n\noriginal = LL\n\n# Wavelet transform of image, and plot approximation and details\ntitles = ['Approximation', ' Horizontal detail',\n          'Vertical detail', 'Diagonal detail']\ncoeffs2 = pywt.dwt2(original, 'bior1.3')\nLL, (LH, HL, HH) = coeffs2\nfig = plt.figure(figsize=(12, 3))\nfor i, a in enumerate([LL, LH, HL, HH]):\n    ax = fig.add_subplot(1, 4, i + 1)\n    ax.imshow(a, interpolation=\"nearest\", cmap=plt.cm.gray)\n    ax.set_title(titles[i], fontsize=10)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nfig.tight_layout()\nplt.show()\n\n\n(2912, 5000, 3)\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfter applying the transform for 7 times, the text is truly unrecognizable. In fact, after 6 times of transformation, one can no longer read the text. Given the larger size of the matrix generated by the cartoon image, I would believe applying the transform 7 times results in enough blur in the image so that it is basically unrecognizable.\n\nYou definitely shouldn’t be pasting and repasting code here like this – it makes it hard to read and easy to make mistakes! If you will be reusing a bit of code, it needs to be in a function.\n\n“Consider the amount of savings if, in addition to saving the blurs in all their detail, you were to to save a very good approximation to the edges portion of the transformed picture. For example, consider what you might achieve by first applying some thresholding condition to edge portions of the picture that sets all pixels below a certain level to zero, then accounting for the large number of resulting zeros by some compression technique. You might even suggest a format for such a compression format.”\nNow as suggested, we shall try applying some thresholding condition to edge portions of the picture that sets all pixels below a certain level, then accounting for the large number of resulting zeros by some compression technique. To figure out what threshold we should use, we can try using 0 (same as the no-condition case), 0.1, 0.2, 0.3, and 0.4. I first try imposing this condition on the whole picture and then only on the edge portions.\n\n\nCode\nimport matplotlib.image as img\nfile_name = 'catimage.png'\nimage_matrix = img.imread(file_name)\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pywt\nimport pywt.data\n\n# Load image\noriginal = image_matrix\n\n# Wavelet transform of image, and plot approximation and details\ntitles = ['Approximation', ' Horizontal detail',\n          'Vertical detail', 'Diagonal detail']\ncoeffs2 = pywt.dwt2(original, 'bior1.3')\nLL, (LH, HL, HH) = coeffs2\nfig = plt.figure(figsize=(12, 3))\nfor i, a in enumerate([LL, LH, HL, HH]):\n    ax = fig.add_subplot(1, 4, i + 1)\n    ax.imshow(a, interpolation=\"nearest\", cmap=plt.cm.gray)\n    ax.set_title(titles[i], fontsize=10)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nfig.tight_layout()\nplt.show()\n\n# Now impose the condition that sets entries with values &lt; 0.1 to zero\nimage_matrix[image_matrix &lt; 0.1] = 0\n\n# Load image\noriginal = image_matrix\n\n# Wavelet transform of image, and plot approximation and details\ntitles = ['Approximation', ' Horizontal detail',\n          'Vertical detail', 'Diagonal detail']\ncoeffs2 = pywt.dwt2(original, 'bior1.3')\nLL, (LH, HL, HH) = coeffs2\nfig = plt.figure(figsize=(12, 3))\nfor i, a in enumerate([LL, LH, HL, HH]):\n    ax = fig.add_subplot(1, 4, i + 1)\n    ax.imshow(a, interpolation=\"nearest\", cmap=plt.cm.gray)\n    ax.set_title(titles[i], fontsize=10)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nfig.tight_layout()\nplt.show()\n\nimage_matrix[image_matrix &lt; 0.2] = 0\n\n# Load image\noriginal = image_matrix\n\n# Wavelet transform of image, and plot approximation and details\ntitles = ['Approximation', ' Horizontal detail',\n          'Vertical detail', 'Diagonal detail']\ncoeffs2 = pywt.dwt2(original, 'bior1.3')\nLL, (LH, HL, HH) = coeffs2\nfig = plt.figure(figsize=(12, 3))\nfor i, a in enumerate([LL, LH, HL, HH]):\n    ax = fig.add_subplot(1, 4, i + 1)\n    ax.imshow(a, interpolation=\"nearest\", cmap=plt.cm.gray)\n    ax.set_title(titles[i], fontsize=10)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nfig.tight_layout()\nplt.show()\n\nimage_matrix[image_matrix &lt; 0.3] = 0\n\n# Load image\noriginal = image_matrix\n\n# Wavelet transform of image, and plot approximation and details\ntitles = ['Approximation', ' Horizontal detail',\n          'Vertical detail', 'Diagonal detail']\ncoeffs2 = pywt.dwt2(original, 'bior1.3')\nLL, (LH, HL, HH) = coeffs2\nfig = plt.figure(figsize=(12, 3))\nfor i, a in enumerate([LL, LH, HL, HH]):\n    ax = fig.add_subplot(1, 4, i + 1)\n    ax.imshow(a, interpolation=\"nearest\", cmap=plt.cm.gray)\n    ax.set_title(titles[i], fontsize=10)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nfig.tight_layout()\nplt.show()\n\nimage_matrix[image_matrix &lt; 0.4] = 0\n\n# Load image\noriginal = image_matrix\n\n# Wavelet transform of image, and plot approximation and details\ntitles = ['Approximation', ' Horizontal detail',\n          'Vertical detail', 'Diagonal detail']\ncoeffs2 = pywt.dwt2(original, 'bior1.3')\nLL, (LH, HL, HH) = coeffs2\nfig = plt.figure(figsize=(12, 3))\nfor i, a in enumerate([LL, LH, HL, HH]):\n    ax = fig.add_subplot(1, 4, i + 1)\n    ax.imshow(a, interpolation=\"nearest\", cmap=plt.cm.gray)\n    ax.set_title(titles[i], fontsize=10)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nfig.tight_layout()\nplt.show()\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt looks like you are thresholding the image matrix directly here, not the edges – that’s not what you want!\nAfter you do the thresholding of the edges, there’s an additional step, which is to save the matrix in a way that takes advantage of the large number of zeros. You could, for example, save the matrix as a sparse matrix, which only stores the non-zero entries. This would be a good way to compress the data.\n\nI’m also curious about what happens when we set entries with values larger than a certain threshold value to 1. This is accounted for in the following:\n\n\nCode\nimport matplotlib.image as img\nfile_name = 'catimage.png'\nimage_matrix = img.imread(file_name)\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pywt\nimport pywt.data\n\n# Now impose the condition that sets entries with values &gt; 0.9 to one\nimage_matrix[image_matrix &gt; 0.9] = 1\n# Load image\noriginal = image_matrix\n\n# Wavelet transform of image, and plot approximation and details\ntitles = ['Approximation', ' Horizontal detail',\n          'Vertical detail', 'Diagonal detail']\ncoeffs2 = pywt.dwt2(original, 'bior1.3')\nLL, (LH, HL, HH) = coeffs2\nfig = plt.figure(figsize=(12, 3))\nfor i, a in enumerate([LL, LH, HL, HH]):\n    ax = fig.add_subplot(1, 4, i + 1)\n    ax.imshow(a, interpolation=\"nearest\", cmap=plt.cm.gray)\n    ax.set_title(titles[i], fontsize=10)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nfig.tight_layout()\nplt.show()\n\n\nimage_matrix[image_matrix &gt; 0.7] = 1\n# Load image\noriginal = image_matrix\n\n# Wavelet transform of image, and plot approximation and details\ntitles = ['Approximation', ' Horizontal detail',\n          'Vertical detail', 'Diagonal detail']\ncoeffs2 = pywt.dwt2(original, 'bior1.3')\nLL, (LH, HL, HH) = coeffs2\nfig = plt.figure(figsize=(12, 3))\nfor i, a in enumerate([LL, LH, HL, HH]):\n    ax = fig.add_subplot(1, 4, i + 1)\n    ax.imshow(a, interpolation=\"nearest\", cmap=plt.cm.gray)\n    ax.set_title(titles[i], fontsize=10)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nfig.tight_layout()\nplt.show()\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA number of issues here. If you will use the PyWavelets library, you should try examining the filter banks or using it on some simple toy images (or 1d data) to understand exactly what it’s doing. And of course use ‘haar’!\nGrade: R\n\n\n\nLeast Squares\nFrom WNBA’s 2023 first half stats, we get the following table just as in the given example.\n\n\nCode\nfrom IPython.display import Image\n#Image(filename='table.png')\n\n\nWe shall first obtain the matrix from the table and transform it by setting the (i,j)th entry to the score difference x(i) - x(j). For each team, it could potentially play against the other 11 teams. So the total number of games score differences (potential differences) we wish to predict is 11*12/2 = 66.\n\n\nCode\nimport numpy as np\nwnba_stats = np.array([[0, 74.5, 71, 75, 66, 69, 72, 80, 77.5, 88, 0, 0],\n                      [76, 0, 70, 89, 69, 81, 104, 0, 84, 77, 91, 73],\n                      [69, 79.5, 0, 82, 94.5, 75, 0, 81, 72, 65, 62, 80],\n                      [74, 94, 81.5, 0, 83, 87, 95, 93, 74, 85, 61, 0],\n                      [87, 71, 79.5, 90, 0, 63, 80, 0, 61, 90, 0, 80],\n                      [69, 90, 69, 78, 85, 0, 73, 69.5, 0, 0, 78.3, 79],\n                      [65.5, 93, 0, 91, 68, 77, 0, 78, 79, 0, 75.5, 64],\n                      [64, 0, 79.5, 102, 90, 0, 86, 0, 69, 89.5, 0, 77],\n                      [84, 89, 96, 80, 75.5, 0, 85, 64, 0, 87, 83, 89],\n                      [76, 83, 83, 78, 87, 0, 0, 84.5, 72.5, 0, 0, 87],\n                      [0, 86, 77, 79, 0, 94.3, 77.5, 0, 74, 0, 0, 75],\n                      [0, 94, 93, 0, 84, 99, 105, 84, 83.5, 92, 93.5, 0]])\n\n\nAs given in the problem, we need to set up the actual matrix A using the information x(i) - x(j) = bij, where bij is the actual observed score differences. An intuitive explanation of how A should be formed is in an attached picture on canvas.\n\n\nCode\nimport numpy as np\n\n# Given matrix\nmatrix = np.array([[0, 74.5, 71, 75, 66, 69, 72, 80, 77.5, 88, 0, 0],\n                   [76, 0, 70, 89, 69, 81, 104, 0, 84, 77, 91, 73],\n                   [69, 79.5, 0, 82, 94.5, 75, 0, 81, 72, 65, 62, 80],\n                   [74, 94, 81.5, 0, 83, 87, 95, 93, 74, 85, 61, 0],\n                   [87, 71, 79.5, 90, 0, 63, 80, 0, 61, 90, 0, 80],\n                   [69, 90, 69, 78, 85, 0, 73, 69.5, 0, 0, 78.3, 79],\n                   [65.5, 93, 0, 91, 68, 77, 0, 78, 79, 0, 75.5, 64],\n                   [64, 0, 79.5, 102, 90, 0, 86, 0, 69, 89.5, 0, 77],\n                   [84, 89, 96, 80, 75.5, 0, 85, 64, 0, 87, 83, 89],\n                   [76, 83, 83, 78, 87, 0, 0, 84.5, 72.5, 0, 0, 87],\n                   [0, 86, 77, 79, 0, 94.3, 77.5, 0, 74, 0, 0, 75],\n                   [0, 94, 93, 0, 84, 99, 105, 84, 83.5, 92, 93.5, 0]])\n\n# Initialize result matrix\nresult_matrix = np.zeros((132, 12))\n\n# Construct the matrix\ngame_count = 0\nfor i in range(12):\n    for j in range(i + 1, 12):\n        result_matrix[game_count * 2, i] = 1\n        result_matrix[game_count * 2, j] = -1\n        result_matrix[game_count * 2 + 1, i] = -1\n        result_matrix[game_count * 2 + 1, j] = 1\n        game_count += 1\n\n# Print the result matrix\nprint(result_matrix)\n\n\n# Initialize b vector\nb = np.zeros((132, 1))\n\n# Construct the b vector\ngame_count = 0\nfor i in range(12):\n    for j in range(i + 1, 12):\n        b[game_count] = matrix[i, j] - matrix[j, i]\n        game_count += 1\n\n# Print the b vector\nprint(b)\n\n\n[[ 1. -1.  0. ...  0.  0.  0.]\n [-1.  1.  0. ...  0.  0.  0.]\n [ 1.  0. -1. ...  0.  0.  0.]\n ...\n [ 0.  0.  0. ... -1.  0.  1.]\n [ 0.  0.  0. ...  0.  1. -1.]\n [ 0.  0.  0. ...  0. -1.  1.]]\n[[ -1.5]\n [  2. ]\n [  1. ]\n [-21. ]\n [  0. ]\n [  6.5]\n [ 16. ]\n [ -6.5]\n [ 12. ]\n [  0. ]\n [  0. ]\n [ -9.5]\n [ -5. ]\n [ -2. ]\n [ -9. ]\n [ 11. ]\n [  0. ]\n [ -5. ]\n [ -6. ]\n [  5. ]\n [-21. ]\n [  0.5]\n [ 15. ]\n [  6. ]\n [  0. ]\n [  1.5]\n [-24. ]\n [-18. ]\n [-15. ]\n [-13. ]\n [ -7. ]\n [  9. ]\n [  4. ]\n [ -9. ]\n [ -6. ]\n [  7. ]\n [-18. ]\n [  0. ]\n [-22. ]\n [ 12. ]\n [-90. ]\n [-14.5]\n [  3. ]\n [  0. ]\n [ -4. ]\n [ -4. ]\n [ 69.5]\n [  0. ]\n [  0. ]\n [-16. ]\n [-20. ]\n [ -8. ]\n [ -6. ]\n [  0. ]\n [ -2. ]\n [-41. ]\n [  5. ]\n [  5. ]\n [  0. ]\n [ -7. ]\n [ 14.5]\n [  9. ]\n [  5.5]\n [  0. ]\n [ -5. ]\n [-18.5]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]\n [  0. ]]\n\n\nNow we want to solve for Ax = b using the normal equation ATAx = ATb\n\n\nCode\nimport numpy as np\nx = np.linalg.solve(result_matrix.T@result_matrix, result_matrix.T@b)\nprint(x)\n\n\nI think I need more efforts on refining the programming part, particularly when trying to define the matrix and the vector b, but I think the idea could work somehow???\n\nOK, I get your approach here. But there’s no exact solution, which is why your last cell isn’t working. You’ll want to use least squares instead, using np.linalg.lstsq instead of np.linalg.solve. Then you’ll need to continue the problem, using the vector x that you found to predict the results of some new games.\nGrade: R\n\n\n\n\n\n\n:::{#quarto-navigation-envelope .hidden}\n[Stat 24320]{.hidden render-id=\"quarto-int-sidebar-title\"}\n[Stat 24320]{.hidden render-id=\"quarto-int-navbar-title\"}\n:::\n\n\n\n:::{#quarto-meta-markdown .hidden}\n[Stat 24320 - Projects 2]{.hidden render-id=\"quarto-metatitle\"}\n[Stat 24320 - Projects 2]{.hidden render-id=\"quarto-twittercardtitle\"}\n[Stat 24320 - Projects 2]{.hidden render-id=\"quarto-ogcardtitle\"}\n[Stat 24320]{.hidden render-id=\"quarto-metasitename\"}\n[]{.hidden render-id=\"quarto-twittercarddesc\"}\n[]{.hidden render-id=\"quarto-ogcardddesc\"}\n:::\n\n\n\n\n&lt;!-- --&gt;\n\n::: {.quarto-embedded-source-code}\n```````````````````{.markdown shortcodes=\"false\"}\n---\ntitle: Projects 2\nformat:\n  html:\n    code-fold: show\n    code-tools: \n      source: true\n      toggle: true\n---\n\nModelling with Directed Graphs\n\nComment: I think the hints really helped a lot with the intuition. I couldn't really understand what the basis vectors for the null space of an incidence matrix represent intuitively until the hints are given, especially when I was trying to compute a basis of directed loops. \n\n::: comments\nGreat, I'm glad they helped!\n:::\n\na. Plot the Digraph\n\n# print the path of the python executable\n\n\n\nquarto-executable-code-5450563D\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport gravis as gv\nfrom collections import defaultdict\n\nG = nx.DiGraph()\nG.add_node(\"1\")\nG.add_node(\"2\")\nG.add_node(\"3\")\nG.add_node(\"4\")\nG.add_node(\"5\")\nG.add_node(\"6\")\nG.add_edge(\"1\",\"2\")\nG.add_edge(\"2\",\"3\")\nG.add_edge(\"3\",\"4\")\nG.add_edge(\"4\",\"2\")\nG.add_edge(\"1\",\"4\")\nG.add_edge(\"3\",\"1\")\nG.add_edge(\"3\",\"6\")\nG.add_edge(\"6\",\"3\")\nG.add_edge(\"4\",\"5\")\nG.add_edge(\"5\",\"6\")\npos = nx.spring_layout(G)\nnx.draw_networkx_nodes(G, pos)\nnx.draw_networkx_labels(G, pos)\nnx.draw_networkx_edges(G, pos, edge_color='r', arrows = True)\nplt.show()\n\nExhibit the incidence matrix A of this digraph and find a basis for N(A) using its reduced row echelon form.\n\nquarto-executable-code-5450563D\nimport numpy as np\nfrom sympy import Matrix\n\n# Define the vertices and edges\nV = [1, 2, 3, 4, 5, 6]\nE = [(1, 2), (2, 3), (3, 4), (4, 2), (1, 4), (3, 1), (3, 6), (6, 3), (4, 5), (5, 6)]\nnum_vertices = len(V)\nnum_edges = len(E)\n\n# Initialize the incidence matrix \nincidence = np.zeros((num_vertices, num_edges))\n\n# Fill in the matrix with entries corresponding to each vertex and incoming & outgoing edges\nfor col, edge in enumerate(E):\n    from_vertex, to_vertex = edge\n    from_index = V.index(from_vertex)\n    to_index = V.index(to_vertex)\n    incidence[from_index, col] = 1  # Outgoing edge\n    incidence[to_index, col] = -1  # Incoming edge\nprint(\"The incidence matrix is:\")\nprint(incidence)\n\n# Then we find the RREF of A to find a basis for N(A) by hand (details in attached pictures)\nx1 = np.array([0,1,1,1,0,0,0,0,0,0])\nx2 = np.array([-1,-1,-1,0,1,0,0,0,0,0])\nx3 = np.array([1,1,0,0,0,1,0,0,0,0])\nx4 = np.array([0,0,0,0,0,0,1,1,0,0])\nx5 = np.array([0,0,1,0,0,0,-1,0,1,1])\nprint(\"The basis for null space of A is:\")\nprint(x1.T)\nprint(x2.T)\nprint(x3.T)\nprint(x4.T)\nprint(x5.T)\n\nNx has a built-in function for finding the incidence matrix, but I appreciate that you’ve done it by hand here.\n(For that matter, sympy has a function for finding the rref and the nullspace of a matrix… these let me double-check your work):\nquarto-executable-code-5450563D\nfrom sympy import Matrix, nsimplify\n# E but map all the numbers to strings\nE_str = [(str(e[0]), str(e[1])) for e in E]\ninc_matrix = nsimplify(Matrix(nx.incidence_matrix(G,edgelist=E_str, oriented=True).toarray()),rational=True)\nprint(\"Incidence matrix:\")\ndisplay(inc_matrix)\n(np.equal(np.array(incidence),-np.array(inc_matrix))).all() # Check if #the two matrices are the same\nnullspace = nsimplify(Matrix.hstack(*inc_matrix.nullspace()),rational=True)\nprint(\"Nullspace basis:\")\ndisplay(nullspace)\n\n\nThese five vectors together span the null space of A.\nJust as given by the hint “each column in the nullspace tells us a set of coefficients which can multiply the columns of the incidence matrix so that the rows can sum to zero. Example: if our nullspace has a column vector [0,1,0,1]^T, this tells us that column 2 + column 4 of the incidence matrix will sum to zero,” we can verify this with the basis vectors we get.\nFor instance, x1 is [0,1,1,1,0,0,0,0,0,0]^T, which means column 2 + column 3 + column 4 of the incidence matrix A will sum to zero, which is true.\nNow we use the computed basis to find a basis of directed loops For x1, we may see that it is a directed loop : column 2, 3, and 4 together is a loop from vertex 2 to 3, and then 3 to 4, and 4 back to 2. Direction does matter in this loop since if we switch the direction it would become 2 to 4, 4 to 3, and 3 back to 2, which is different.\n\nThat’s not what this is asking about. The difficulty is when a loop in the basis is something like 1 to 2, and 3 to 2, and 1 to 3. You can’t actually follow that loop if you are going in the directions of the edges.\nSome of the loops in the basis are actually directed loops, but some are not. You need to figure out which are which, then see if you can add a directed loop to some of the undirected loops to make a new basis of directed loops.\n\nFor x2, direction also matters. This loop is slightly more complicated but the idea is similar - if we switch the direction the loop would be different. For x3, direction matters as well just as for x1. For x4, we shall see this is actually a non-directed loop consisted of only two vertices 3 and 6. Switching the direction, we still get the same loop as before. For x5, direction matters so it’s a directed loop.\nTherefore, considering adding a directed basis element to the non-directed basis element x4 just like what’s given in the hint, we may replace x4 by x4 + x5, which is [0,0,1,0,0,0,0,1,1,1]^T and this loop is directed as expected.\nquarto-executable-code-5450563D\n# Now the \"corrected\" basis of directed loops is:\nx1 = np.array([0,1,1,1,0,0,0,0,0,0])\nx2 = np.array([-1,-1,-1,0,1,0,0,0,0,0])\nx3 = np.array([1,1,0,0,0,1,0,0,0,0])\nx4 = np.array([0,0,1,0,0,0,0,1,1,1])\nx5 = np.array([0,0,1,0,0,0,-1,0,1,1])\nprint(\"The (directed-loop) basis for null space of incidence matrix is:\")\nprint(x1.T)\nprint(x2.T)\nprint(x3.T)\nprint(x4.T)\nprint(x5.T)\nNow we move on to solve 3 problems selected from the 5 problems given.\n\nThink of the digraph as representing an electrical circuit where an edge represents some electrical object like a resistor or capacitor. Each node represents the circuit space between these objects. and we can attach a potential value to each node, say the potentials are x1, …., x6. The potential difference across an edge is the potential value of head minus tail. Kirchhoff’s second law of electrical circuits says that the sum of potential differences around a circuit loop must be zero. Assume and use the fact (p. 422) that Ax = b implies that for all y in N(A^T), y^Tb = 0, to find conditions that a vector b must satisfy in order for it to be a vector of potential differences for some potential distribution on the vertices.\n\nquarto-executable-code-5450563D\n# We should obtain the basis for null space of A^T, which is simpler than that of A\n# In fact, as N(A) has 5 basis elements, it's easy to see that N（A^T) has 6-5 = 1 basis element\ne = np.array([1,1,1,1,1,1]) \n# Now we want to find conditions on b to make y^Tb = 0 to hold for y in null space of A^T\n# Which means for any y that is a scalar multiple of [1,1,1,1,1,1]^T, y^Tb = 0 should hold.\n\n\n# Start with any vector b we want to test\nb = np.array([1, 2, 3, 0, -4, -2])  \n\n# Check if the condition holds for any scalar multiple of [1,1,1,1,1,1]\ncondition_met = True\nfor scalar in range(-10, 11):  # Check scalar multiples from -10 to 10\n    y = scalar * np.ones(6)  # Create a scalar multiple of [1,1,1,1,1,1]\n    inner_product = np.dot(y, b)\n    if inner_product != 0:\n        condition_met = False\n        break\n\nif condition_met:\n    print(\"The condition holds.\")\nelse:\n    print(\"The condition does not hold for the inputted vector b.\")\nIntuitively speaking, the yTb = 0 condition essentially means the entries of b should sum to 0, so the inner product of any scalar multiple ( let it be a ) of [1,1,1,1,1,1] and b would be ab1 + ab2 +…+ ab6 = a (b1 +…+b6) = 0. I think this makes intuitive sense —— b has entries summing up to 0 means the “distribution” of potential differences between the 6 nodes must be cancelled out after being added up, otherwise it wouldn’t make sense.\n\nVery nice!\n\n\nAssume that across each edge of a circuit a current flows. Thus, we can assign to each edge a “weight,” namely the current flow along the edge. This is an example of a weighted digraph. However, not just any set of current weights will do, since Kirchhoff’s first law of circuits says that the total flow of current in and out of any node should be 0 . Use this law to find a matrix condition that must be satisfied by the currents and solve it to exhibit some current flows.\n\nLet Iij denote the current flowing from node i to node j. Iij essentially represents the amount of current flowing in the (i,j) edge (if exists). As Kirchhoff’s first law of circuits says that the total flow of current in and out of any node should be 0, this means for any node i that has edges connected to it, the currents should sum to zero. Let I be the vector of currents, so it should be a 10 by 1 vector since the number of currents should exactly match the number of edges. A matrix condition that the current vector I must satisfy, as required by Kirchoff’s first law, should be AI = 0.\n\nYou can write the math using LaTeX in the markdown cells. For example, you can write the “Iij” as $I_{ij}$, which becomes \\(I_{ij}\\)\n\nquarto-executable-code-5450563D\nimport numpy as np\n\n# Use the singular value decomposition (SVD) to solve Ax = 0\nU, S, Vt = np.linalg.svd(incidence)\n\n# The solution vector x is the last column of Vt (corresponding to the smallest singular value)\nI = Vt[-1]\n\nprint(\"Solution I:\")\nprint(I)\n\nThe solution space for Ax = 0 is the nullspace of A, which you’ve already found.\nNow, your solution works in this case because the last singular value is zero; this will be the case in general if A isn’t square or full rank. However, using the nullspace of A is a more direct way to find the solution, and it happens to give a much more clean basis in this case.\n\nThe solution we get is indeed a 10 by 1 vector (A is a 6 by 10 matrix, so I should be 10 by 1)\n\nThink of the digraph as representing a directed communications network. Here loops determine which nodes have bidirectional communication since any two nodes of a loop can only communicate with each other by way of a loop. By examining only a basis of directed loops how could you determine which nodes in the network can communicate with each other?\n\nFrom only the basis of directed loops, which is consisted of x1 = [0,1,1,1,0,0,0,0,0,0] x2 = [-1,-1,-1,0,1,0,0,0,0,0] x3 = [1,1,0,0,0,1,0,0,0,0] x4 = [0,0,1,0,0,0,0,1,1,1] x5 = [0,0,1,0,0,0,-1,0,1,1], we want to determine which nodes among {1,2,3,4,5,6} communicate with each other.\nNodes 1 and 2: There is a nonzero entry in position 2 of x1 and x2, so nodes 1 and 2 communicate. Nodes 1 and 3: There is a nonzero entry in position 1 of x1 and x3, so nodes 1 and 3 communicate. Nodes 1 and 4: There is a nonzero entry in position 3 of x1 and x4, so nodes 1 and 4 communicate. Nodes 1 and 5: No nonzero entries in the positions corresponding to nodes 1 and 5 in any basis vectors, so nodes 1 and 5 do not communicate. Nodes 1 and 6: No nonzero entries in the positions corresponding to nodes 1 and 6 in any basis vectors, so nodes 1 and 6 do not communicate. Nodes 2 and 3: There is a nonzero entry in position 2 of x2 and x3, so nodes 2 and 3 communicate. Nodes 2 and 4: There is a nonzero entry in position 4 of x2 and x4, so nodes 2 and 4 communicate. …..\nTherefore, the pairs of nodes that communicate with each other are:\nNodes 1 and 2, Nodes 1 and 3, Nodes 1 and 4 Nodes 2 and 3, Nodes 2 and 4, Nodes 3 and 4, Nodes 3 and 6, Nodes 4 and 5, Nodes 5 and 6\n\nThe question is asking you to figure out which nodes can communicate even with potential intermediate nodes that may not be part of the same basis elements in your basis of directed loops. So you will have to figure out what happens if you have combinations of the loops in your basis.\n\n\nThere were some conceptual issues I’d like to see fixed here!\nGrade: R\n\nImage Conpression and Edge Detection\n\nSelected image in grayscale with 8-bit precision (using Gimp)\n\n\nI didn’t have your images, so to make this compile I replaced all the images with cat images…\n\nquarto-executable-code-5450563D\nfrom IPython.display import Image\nImage(filename='catimage.png')\n\nMatrix generated by the Grayscale Image & Haar Wavelet Transform\n\nComment : Honestly I really tried writing a program to perform the Haar Wavelet Transform but I kept failing to get the representation of the Haar Wavelet Transform matrix right. I ended up using a package called Pywavelets to perform the transformation on the matrix. This package is really simple to use in practice but it doesn’t help with understanding the algorithm.\n\nThat sounds frustrating! I would have liked to see your attempts, so I could help you debug.\nThe library you found can actually be of help here. It does implement the Haar transform, although you need to specify it by putting “haar” as the wavelet name (instead of “bior1.3” as you did in the code below).\nquarto-executable-code-5450563D\n!pip install PyWavelets\nExample:\nquarto-executable-code-5450563D\nimport numpy as np\n\nimport pywt\n# generate a simple 5x5 matrix\nwt = pywt.Wavelet('haar')\nwt.filter_bank\nplt.show()\nIt doesn’t seem like there is any way to get the matrix representation of the Haar wavelet transform from this package, but you can still use the package to perform the transform and visualize the results.\n\nquarto-executable-code-5450563D\nimport matplotlib.image as img\nimport pywt\nfile_name = 'catimage.png'\nimage_matrix = img.imread(file_name)\nimage_matrix = np.sum(image_matrix, axis=2) / 3\nprint(image_matrix.shape)\n# We shall see that the generated matrix is a 1170 by 1492 matrix\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pywt\nimport pywt.data\n\n# Load image\noriginal = image_matrix\n\n# Wavelet transform of image, and plot approximation and details\ntitles = ['Approximation', ' Horizontal detail',\n          'Vertical detail', 'Diagonal detail']\ncoeffs2 = pywt.dwt2(original, 'bior1.3')\nLL, (LH, HL, HH) = coeffs2\nfig = plt.figure(figsize=(12, 3))\nfor i, a in enumerate([LL, LH, HL, HH]):\n    ax = fig.add_subplot(1, 4, i + 1)\n    ax.imshow(a, interpolation=\"nearest\", cmap=plt.cm.gray)\n    ax.set_title(titles[i], fontsize=10)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nfig.tight_layout()\nplt.show()\n\noriginal = LL\n\n# Wavelet transform of image, and plot approximation and details\ntitles = ['Approximation', ' Horizontal detail',\n          'Vertical detail', 'Diagonal detail']\ncoeffs2 = pywt.dwt2(original, 'bior1.3')\nLL, (LH, HL, HH) = coeffs2\nfig = plt.figure(figsize=(12, 3))\nfor i, a in enumerate([LL, LH, HL, HH]):\n    ax = fig.add_subplot(1, 4, i + 1)\n    ax.imshow(a, interpolation=\"nearest\", cmap=plt.cm.gray)\n    ax.set_title(titles[i], fontsize=10)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nfig.tight_layout()\nplt.show()\n\noriginal = LL\n\n# Wavelet transform of image, and plot approximation and details\ntitles = ['Approximation', ' Horizontal detail',\n          'Vertical detail', 'Diagonal detail']\ncoeffs2 = pywt.dwt2(original, 'bior1.3')\nLL, (LH, HL, HH) = coeffs2\nfig = plt.figure(figsize=(12, 3))\nfor i, a in enumerate([LL, LH, HL, HH]):\n    ax = fig.add_subplot(1, 4, i + 1)\n    ax.imshow(a, interpolation=\"nearest\", cmap=plt.cm.gray)\n    ax.set_title(titles[i], fontsize=10)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nfig.tight_layout()\nplt.show()\n\noriginal = LL\n\n# Wavelet transform of image, and plot approximation and details\ntitles = ['Approximation', ' Horizontal detail',\n          'Vertical detail', 'Diagonal detail']\ncoeffs2 = pywt.dwt2(original, 'bior1.3')\nLL, (LH, HL, HH) = coeffs2\nfig = plt.figure(figsize=(12, 3))\nfor i, a in enumerate([LL, LH, HL, HH]):\n    ax = fig.add_subplot(1, 4, i + 1)\n    ax.imshow(a, interpolation=\"nearest\", cmap=plt.cm.gray)\n    ax.set_title(titles[i], fontsize=10)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nfig.tight_layout()\nplt.show()\n\noriginal = LL\n\n# Wavelet transform of image, and plot approximation and details\ntitles = ['Approximation', ' Horizontal detail',\n          'Vertical detail', 'Diagonal detail']\ncoeffs2 = pywt.dwt2(original, 'bior1.3')\nLL, (LH, HL, HH) = coeffs2\nfig = plt.figure(figsize=(12, 3))\nfor i, a in enumerate([LL, LH, HL, HH]):\n    ax = fig.add_subplot(1, 4, i + 1)\n    ax.imshow(a, interpolation=\"nearest\", cmap=plt.cm.gray)\n    ax.set_title(titles[i], fontsize=10)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nfig.tight_layout()\nplt.show()\n\noriginal = LL\n\n# Wavelet transform of image, and plot approximation and details\ntitles = ['Approximation', ' Horizontal detail',\n          'Vertical detail', 'Diagonal detail']\ncoeffs2 = pywt.dwt2(original, 'bior1.3')\nLL, (LH, HL, HH) = coeffs2\nfig = plt.figure(figsize=(12, 3))\nfor i, a in enumerate([LL, LH, HL, HH]):\n    ax = fig.add_subplot(1, 4, i + 1)\n    ax.imshow(a, interpolation=\"nearest\", cmap=plt.cm.gray)\n    ax.set_title(titles[i], fontsize=10)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nfig.tight_layout()\nplt.show()\n\noriginal = LL\n\n# Wavelet transform of image, and plot approximation and details\ntitles = ['Approximation', ' Horizontal detail',\n          'Vertical detail', 'Diagonal detail']\ncoeffs2 = pywt.dwt2(original, 'bior1.3')\nLL, (LH, HL, HH) = coeffs2\nfig = plt.figure(figsize=(12, 3))\nfor i, a in enumerate([LL, LH, HL, HH]):\n    ax = fig.add_subplot(1, 4, i + 1)\n    ax.imshow(a, interpolation=\"nearest\", cmap=plt.cm.gray)\n    ax.set_title(titles[i], fontsize=10)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nfig.tight_layout()\nplt.show()\nSince each application of the transform will reduce storage requirements by a factor of four, applying the transform to the image 7 times will reduce storage requirements by a factor of 4^7. To make sure that this is indeed “unacceptably far” from the original image, we try applying the transform to an image with text 7 times to check if we could still read the text.\nquarto-executable-code-5450563D\nfrom IPython.display import Image\nImage(filename='catimage.png')\n\nimport matplotlib.image as img\nfile_name = 'catimage.png'\ntext_matrix = img.imread(file_name)\nprint(text_matrix.shape)\n# We can see that this is a 563 by 1000 matrix\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pywt\nimport pywt.data\n\n# Load image\noriginal = text_matrix\n\n# Wavelet transform of image, and plot approximation and details\ntitles = ['Approximation', ' Horizontal detail',\n          'Vertical detail', 'Diagonal detail']\ncoeffs2 = pywt.dwt2(original, 'bior1.3')\nLL, (LH, HL, HH) = coeffs2\nfig = plt.figure(figsize=(12, 3))\nfor i, a in enumerate([LL, LH, HL, HH]):\n    ax = fig.add_subplot(1, 4, i + 1)\n    ax.imshow(a, interpolation=\"nearest\", cmap=plt.cm.gray)\n    ax.set_title(titles[i], fontsize=10)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nfig.tight_layout()\nplt.show()\n\noriginal = LL\n\n# Wavelet transform of image, and plot approximation and details\ntitles = ['Approximation', ' Horizontal detail',\n          'Vertical detail', 'Diagonal detail']\ncoeffs2 = pywt.dwt2(original, 'bior1.3')\nLL, (LH, HL, HH) = coeffs2\nfig = plt.figure(figsize=(12, 3))\nfor i, a in enumerate([LL, LH, HL, HH]):\n    ax = fig.add_subplot(1, 4, i + 1)\n    ax.imshow(a, interpolation=\"nearest\", cmap=plt.cm.gray)\n    ax.set_title(titles[i], fontsize=10)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nfig.tight_layout()\nplt.show()\n\noriginal = LL\n\n# Wavelet transform of image, and plot approximation and details\ntitles = ['Approximation', ' Horizontal detail',\n          'Vertical detail', 'Diagonal detail']\ncoeffs2 = pywt.dwt2(original, 'bior1.3')\nLL, (LH, HL, HH) = coeffs2\nfig = plt.figure(figsize=(12, 3))\nfor i, a in enumerate([LL, LH, HL, HH]):\n    ax = fig.add_subplot(1, 4, i + 1)\n    ax.imshow(a, interpolation=\"nearest\", cmap=plt.cm.gray)\n    ax.set_title(titles[i], fontsize=10)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nfig.tight_layout()\nplt.show()\n\noriginal = LL\n\n# Wavelet transform of image, and plot approximation and details\ntitles = ['Approximation', ' Horizontal detail',\n          'Vertical detail', 'Diagonal detail']\ncoeffs2 = pywt.dwt2(original, 'bior1.3')\nLL, (LH, HL, HH) = coeffs2\nfig = plt.figure(figsize=(12, 3))\nfor i, a in enumerate([LL, LH, HL, HH]):\n    ax = fig.add_subplot(1, 4, i + 1)\n    ax.imshow(a, interpolation=\"nearest\", cmap=plt.cm.gray)\n    ax.set_title(titles[i], fontsize=10)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nfig.tight_layout()\nplt.show()\n\noriginal = LL\n\n# Wavelet transform of image, and plot approximation and details\ntitles = ['Approximation', ' Horizontal detail',\n          'Vertical detail', 'Diagonal detail']\ncoeffs2 = pywt.dwt2(original, 'bior1.3')\nLL, (LH, HL, HH) = coeffs2\nfig = plt.figure(figsize=(12, 3))\nfor i, a in enumerate([LL, LH, HL, HH]):\n    ax = fig.add_subplot(1, 4, i + 1)\n    ax.imshow(a, interpolation=\"nearest\", cmap=plt.cm.gray)\n    ax.set_title(titles[i], fontsize=10)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nfig.tight_layout()\nplt.show()\n\noriginal = LL\n\n# Wavelet transform of image, and plot approximation and details\ntitles = ['Approximation', ' Horizontal detail',\n          'Vertical detail', 'Diagonal detail']\ncoeffs2 = pywt.dwt2(original, 'bior1.3')\nLL, (LH, HL, HH) = coeffs2\nfig = plt.figure(figsize=(12, 3))\nfor i, a in enumerate([LL, LH, HL, HH]):\n    ax = fig.add_subplot(1, 4, i + 1)\n    ax.imshow(a, interpolation=\"nearest\", cmap=plt.cm.gray)\n    ax.set_title(titles[i], fontsize=10)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nfig.tight_layout()\nplt.show()\n\noriginal = LL\n\n# Wavelet transform of image, and plot approximation and details\ntitles = ['Approximation', ' Horizontal detail',\n          'Vertical detail', 'Diagonal detail']\ncoeffs2 = pywt.dwt2(original, 'bior1.3')\nLL, (LH, HL, HH) = coeffs2\nfig = plt.figure(figsize=(12, 3))\nfor i, a in enumerate([LL, LH, HL, HH]):\n    ax = fig.add_subplot(1, 4, i + 1)\n    ax.imshow(a, interpolation=\"nearest\", cmap=plt.cm.gray)\n    ax.set_title(titles[i], fontsize=10)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nfig.tight_layout()\nplt.show()\nAfter applying the transform for 7 times, the text is truly unrecognizable. In fact, after 6 times of transformation, one can no longer read the text. Given the larger size of the matrix generated by the cartoon image, I would believe applying the transform 7 times results in enough blur in the image so that it is basically unrecognizable.\n\nYou definitely shouldn’t be pasting and repasting code here like this – it makes it hard to read and easy to make mistakes! If you will be reusing a bit of code, it needs to be in a function.\n\n“Consider the amount of savings if, in addition to saving the blurs in all their detail, you were to to save a very good approximation to the edges portion of the transformed picture. For example, consider what you might achieve by first applying some thresholding condition to edge portions of the picture that sets all pixels below a certain level to zero, then accounting for the large number of resulting zeros by some compression technique. You might even suggest a format for such a compression format.”\nNow as suggested, we shall try applying some thresholding condition to edge portions of the picture that sets all pixels below a certain level, then accounting for the large number of resulting zeros by some compression technique. To figure out what threshold we should use, we can try using 0 (same as the no-condition case), 0.1, 0.2, 0.3, and 0.4. I first try imposing this condition on the whole picture and then only on the edge portions.\nquarto-executable-code-5450563D\nimport matplotlib.image as img\nfile_name = 'catimage.png'\nimage_matrix = img.imread(file_name)\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pywt\nimport pywt.data\n\n# Load image\noriginal = image_matrix\n\n# Wavelet transform of image, and plot approximation and details\ntitles = ['Approximation', ' Horizontal detail',\n          'Vertical detail', 'Diagonal detail']\ncoeffs2 = pywt.dwt2(original, 'bior1.3')\nLL, (LH, HL, HH) = coeffs2\nfig = plt.figure(figsize=(12, 3))\nfor i, a in enumerate([LL, LH, HL, HH]):\n    ax = fig.add_subplot(1, 4, i + 1)\n    ax.imshow(a, interpolation=\"nearest\", cmap=plt.cm.gray)\n    ax.set_title(titles[i], fontsize=10)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nfig.tight_layout()\nplt.show()\n\n# Now impose the condition that sets entries with values &lt; 0.1 to zero\nimage_matrix[image_matrix &lt; 0.1] = 0\n\n# Load image\noriginal = image_matrix\n\n# Wavelet transform of image, and plot approximation and details\ntitles = ['Approximation', ' Horizontal detail',\n          'Vertical detail', 'Diagonal detail']\ncoeffs2 = pywt.dwt2(original, 'bior1.3')\nLL, (LH, HL, HH) = coeffs2\nfig = plt.figure(figsize=(12, 3))\nfor i, a in enumerate([LL, LH, HL, HH]):\n    ax = fig.add_subplot(1, 4, i + 1)\n    ax.imshow(a, interpolation=\"nearest\", cmap=plt.cm.gray)\n    ax.set_title(titles[i], fontsize=10)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nfig.tight_layout()\nplt.show()\n\nimage_matrix[image_matrix &lt; 0.2] = 0\n\n# Load image\noriginal = image_matrix\n\n# Wavelet transform of image, and plot approximation and details\ntitles = ['Approximation', ' Horizontal detail',\n          'Vertical detail', 'Diagonal detail']\ncoeffs2 = pywt.dwt2(original, 'bior1.3')\nLL, (LH, HL, HH) = coeffs2\nfig = plt.figure(figsize=(12, 3))\nfor i, a in enumerate([LL, LH, HL, HH]):\n    ax = fig.add_subplot(1, 4, i + 1)\n    ax.imshow(a, interpolation=\"nearest\", cmap=plt.cm.gray)\n    ax.set_title(titles[i], fontsize=10)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nfig.tight_layout()\nplt.show()\n\nimage_matrix[image_matrix &lt; 0.3] = 0\n\n# Load image\noriginal = image_matrix\n\n# Wavelet transform of image, and plot approximation and details\ntitles = ['Approximation', ' Horizontal detail',\n          'Vertical detail', 'Diagonal detail']\ncoeffs2 = pywt.dwt2(original, 'bior1.3')\nLL, (LH, HL, HH) = coeffs2\nfig = plt.figure(figsize=(12, 3))\nfor i, a in enumerate([LL, LH, HL, HH]):\n    ax = fig.add_subplot(1, 4, i + 1)\n    ax.imshow(a, interpolation=\"nearest\", cmap=plt.cm.gray)\n    ax.set_title(titles[i], fontsize=10)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nfig.tight_layout()\nplt.show()\n\nimage_matrix[image_matrix &lt; 0.4] = 0\n\n# Load image\noriginal = image_matrix\n\n# Wavelet transform of image, and plot approximation and details\ntitles = ['Approximation', ' Horizontal detail',\n          'Vertical detail', 'Diagonal detail']\ncoeffs2 = pywt.dwt2(original, 'bior1.3')\nLL, (LH, HL, HH) = coeffs2\nfig = plt.figure(figsize=(12, 3))\nfor i, a in enumerate([LL, LH, HL, HH]):\n    ax = fig.add_subplot(1, 4, i + 1)\n    ax.imshow(a, interpolation=\"nearest\", cmap=plt.cm.gray)\n    ax.set_title(titles[i], fontsize=10)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nfig.tight_layout()\nplt.show()\n\nIt looks like you are thresholding the image matrix directly here, not the edges – that’s not what you want!\nAfter you do the thresholding of the edges, there’s an additional step, which is to save the matrix in a way that takes advantage of the large number of zeros. You could, for example, save the matrix as a sparse matrix, which only stores the non-zero entries. This would be a good way to compress the data.\n\nI’m also curious about what happens when we set entries with values larger than a certain threshold value to 1. This is accounted for in the following:\nquarto-executable-code-5450563D\nimport matplotlib.image as img\nfile_name = 'catimage.png'\nimage_matrix = img.imread(file_name)\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pywt\nimport pywt.data\n\n# Now impose the condition that sets entries with values &gt; 0.9 to one\nimage_matrix[image_matrix &gt; 0.9] = 1\n# Load image\noriginal = image_matrix\n\n# Wavelet transform of image, and plot approximation and details\ntitles = ['Approximation', ' Horizontal detail',\n          'Vertical detail', 'Diagonal detail']\ncoeffs2 = pywt.dwt2(original, 'bior1.3')\nLL, (LH, HL, HH) = coeffs2\nfig = plt.figure(figsize=(12, 3))\nfor i, a in enumerate([LL, LH, HL, HH]):\n    ax = fig.add_subplot(1, 4, i + 1)\n    ax.imshow(a, interpolation=\"nearest\", cmap=plt.cm.gray)\n    ax.set_title(titles[i], fontsize=10)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nfig.tight_layout()\nplt.show()\n\n\nimage_matrix[image_matrix &gt; 0.7] = 1\n# Load image\noriginal = image_matrix\n\n# Wavelet transform of image, and plot approximation and details\ntitles = ['Approximation', ' Horizontal detail',\n          'Vertical detail', 'Diagonal detail']\ncoeffs2 = pywt.dwt2(original, 'bior1.3')\nLL, (LH, HL, HH) = coeffs2\nfig = plt.figure(figsize=(12, 3))\nfor i, a in enumerate([LL, LH, HL, HH]):\n    ax = fig.add_subplot(1, 4, i + 1)\n    ax.imshow(a, interpolation=\"nearest\", cmap=plt.cm.gray)\n    ax.set_title(titles[i], fontsize=10)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nfig.tight_layout()\nplt.show()\n\nA number of issues here. If you will use the PyWavelets library, you should try examining the filter banks or using it on some simple toy images (or 1d data) to understand exactly what it’s doing. And of course use ‘haar’!\nGrade: R\n\n\n\nLeast Squares\nFrom WNBA’s 2023 first half stats, we get the following table just as in the given example.\nquarto-executable-code-5450563D\nfrom IPython.display import Image\n#Image(filename='table.png')\nWe shall first obtain the matrix from the table and transform it by setting the (i,j)th entry to the score difference x(i) - x(j). For each team, it could potentially play against the other 11 teams. So the total number of games score differences (potential differences) we wish to predict is 11*12/2 = 66.\nquarto-executable-code-5450563D\nimport numpy as np\nwnba_stats = np.array([[0, 74.5, 71, 75, 66, 69, 72, 80, 77.5, 88, 0, 0],\n                      [76, 0, 70, 89, 69, 81, 104, 0, 84, 77, 91, 73],\n                      [69, 79.5, 0, 82, 94.5, 75, 0, 81, 72, 65, 62, 80],\n                      [74, 94, 81.5, 0, 83, 87, 95, 93, 74, 85, 61, 0],\n                      [87, 71, 79.5, 90, 0, 63, 80, 0, 61, 90, 0, 80],\n                      [69, 90, 69, 78, 85, 0, 73, 69.5, 0, 0, 78.3, 79],\n                      [65.5, 93, 0, 91, 68, 77, 0, 78, 79, 0, 75.5, 64],\n                      [64, 0, 79.5, 102, 90, 0, 86, 0, 69, 89.5, 0, 77],\n                      [84, 89, 96, 80, 75.5, 0, 85, 64, 0, 87, 83, 89],\n                      [76, 83, 83, 78, 87, 0, 0, 84.5, 72.5, 0, 0, 87],\n                      [0, 86, 77, 79, 0, 94.3, 77.5, 0, 74, 0, 0, 75],\n                      [0, 94, 93, 0, 84, 99, 105, 84, 83.5, 92, 93.5, 0]])\nAs given in the problem, we need to set up the actual matrix A using the information x(i) - x(j) = bij, where bij is the actual observed score differences. An intuitive explanation of how A should be formed is in an attached picture on canvas.\nquarto-executable-code-5450563D\nimport numpy as np\n\n# Given matrix\nmatrix = np.array([[0, 74.5, 71, 75, 66, 69, 72, 80, 77.5, 88, 0, 0],\n                   [76, 0, 70, 89, 69, 81, 104, 0, 84, 77, 91, 73],\n                   [69, 79.5, 0, 82, 94.5, 75, 0, 81, 72, 65, 62, 80],\n                   [74, 94, 81.5, 0, 83, 87, 95, 93, 74, 85, 61, 0],\n                   [87, 71, 79.5, 90, 0, 63, 80, 0, 61, 90, 0, 80],\n                   [69, 90, 69, 78, 85, 0, 73, 69.5, 0, 0, 78.3, 79],\n                   [65.5, 93, 0, 91, 68, 77, 0, 78, 79, 0, 75.5, 64],\n                   [64, 0, 79.5, 102, 90, 0, 86, 0, 69, 89.5, 0, 77],\n                   [84, 89, 96, 80, 75.5, 0, 85, 64, 0, 87, 83, 89],\n                   [76, 83, 83, 78, 87, 0, 0, 84.5, 72.5, 0, 0, 87],\n                   [0, 86, 77, 79, 0, 94.3, 77.5, 0, 74, 0, 0, 75],\n                   [0, 94, 93, 0, 84, 99, 105, 84, 83.5, 92, 93.5, 0]])\n\n# Initialize result matrix\nresult_matrix = np.zeros((132, 12))\n\n# Construct the matrix\ngame_count = 0\nfor i in range(12):\n    for j in range(i + 1, 12):\n        result_matrix[game_count * 2, i] = 1\n        result_matrix[game_count * 2, j] = -1\n        result_matrix[game_count * 2 + 1, i] = -1\n        result_matrix[game_count * 2 + 1, j] = 1\n        game_count += 1\n\n# Print the result matrix\nprint(result_matrix)\n\n\n# Initialize b vector\nb = np.zeros((132, 1))\n\n# Construct the b vector\ngame_count = 0\nfor i in range(12):\n    for j in range(i + 1, 12):\n        b[game_count] = matrix[i, j] - matrix[j, i]\n        game_count += 1\n\n# Print the b vector\nprint(b)\nNow we want to solve for Ax = b using the normal equation ATAx = ATb\nquarto-executable-code-5450563D\n#| eval: false\nimport numpy as np\nx = np.linalg.solve(result_matrix.T@result_matrix, result_matrix.T@b)\nprint(x)\nI think I need more efforts on refining the programming part, particularly when trying to define the matrix and the vector b, but I think the idea could work somehow???\n\nOK, I get your approach here. But there’s no exact solution, which is why your last cell isn’t working. You’ll want to use least squares instead, using np.linalg.lstsq instead of np.linalg.solve. Then you’ll need to continue the problem, using the vector x that you found to predict the results of some new games.\nGrade: R\n\n\n\n:::"
  },
  {
    "objectID": "submitted/projects_2/tandocalec.html",
    "href": "submitted/projects_2/tandocalec.html",
    "title": "Problem 1 Modeling with Directed Graphs",
    "section": "",
    "text": "import sympy as sp\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport networkx as nx\nimport scipy as sci\nfrom PIL import Image as im\nimport pandas as pd\nfrom scipy.linalg import svd\n\nWe first construct the digraph\n\n# Create a directed graph\nG = nx.DiGraph()\n\n# Add nodes\nG.add_nodes_from([1, 2, 3, 4, 5, 6])\n\n# Add vertices\nedges = [(1,2),(2,3),(3,4),(4,2),(1,4),(3,1),(3,6),(6,3),(4,5),(5,6)]\nG.add_edges_from(edges)\n\n# Draw the graph\nnx.draw_circular(G, with_labels=True)\nplt.show()\n\n\n\n\n\n\n\n\nWe use networkx to determine the incidence matrix of the graph. Each row represents a node, each column represents an edge, with a -1 corresponding to the tail of an edge and a 1 representing the head.\n\ninc_sparse = -nx.incidence_matrix(G, oriented=True)\nA = sp.Matrix(inc_sparse.toarray())\nA\n\n\\(\\displaystyle \\left[\\begin{matrix}1.0 & 1.0 & 0 & 0 & -1.0 & 0 & 0 & 0 & 0 & 0\\\\-1.0 & 0 & 1.0 & 0 & 0 & 0 & -1.0 & 0 & 0 & 0\\\\0 & 0 & -1.0 & 1.0 & 1.0 & 1.0 & 0 & 0 & 0 & -1.0\\\\0 & -1.0 & 0 & -1.0 & 0 & 0 & 1.0 & 1.0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & -1.0 & 1.0 & 0\\\\0 & 0 & 0 & 0 & 0 & -1.0 & 0 & 0 & -1.0 & 1.0\\end{matrix}\\right]\\)\n\n\nWe then determine the nullspace of A and find a basis.\n\niter = A.nullspace()\nbasis = iter[0]\nfor vec in iter[1:]:\n  basis = basis.row_join(vec)\nbasis\n\n\\(\\displaystyle \\left[\\begin{matrix}1.0 & 1.0 & -1.0 & -1.0 & 0\\\\-1.0 & 0 & 1.0 & 1.0 & 0\\\\1.0 & 1.0 & 0 & -1.0 & 0\\\\1 & 0 & 0 & 0 & 0\\\\0 & 1 & 0 & 0 & 0\\\\0 & 0 & 0 & -1.0 & 1.0\\\\0 & 0 & 1 & 0 & 0\\\\0 & 0 & 0 & 1.0 & 0\\\\0 & 0 & 0 & 1 & 0\\\\0 & 0 & 0 & 0 & 1\\end{matrix}\\right]\\)\n\n\nAs evident from our basis, there are two directed loops: column 2 and column 5. Moreover, we can construct more loops using linear combinations. Specifically, col1 + col3, col2 + col4 + col 5, and col2 + col3\n\ndir_basis = (basis.col(1)).row_join(basis.col(4))\ndir_basis = dir_basis.row_join(basis.col(0) + basis.col(2))\ndir_basis = dir_basis.row_join(basis.col(1) + basis.col(3) + basis.col(4))\ndir_basis = dir_basis.row_join(basis.col(1) + basis.col(2))\ndir_basis\n\n\\(\\displaystyle \\left[\\begin{matrix}1.0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 1.0 & 1.0\\\\1.0 & 0 & 1.0 & 0 & 1.0\\\\0 & 0 & 1 & 0 & 0\\\\1 & 0 & 0 & 1 & 1\\\\0 & 1.0 & 0 & 0 & 0\\\\0 & 0 & 1 & 0 & 1\\\\0 & 0 & 0 & 1.0 & 0\\\\0 & 0 & 0 & 1 & 0\\\\0 & 1 & 0 & 1 & 0\\end{matrix}\\right]\\)\n\n\nWe can visually confirm the nodes in these loops. Columns:\n\n1, 2, 3\n3, 6\n2, 3, 4\n1, 4, 5, 6, 3\n3, 4, 5, 6\n\n\nProblem 1.1\nWe are looking for Ax = b such that A represents a matrix where the rows are edges and columns represent whether a node is a head or tail for that edge (-1 for tail, +1 for head). x is some distribution of potential value on the nodes and b is the resulting potential differences of our edges. We know that for y in N(AT), yT*b = 0. We first construct A\n\nA = np.zeros((10, 6))\ni = 0\nfor tail, head in edges:\n  A[i, tail-1] = -1\n  A[i, head-1] = 1\n  i+=1\nA = sp.Matrix(A)\nA\n\n\\(\\displaystyle \\left[\\begin{matrix}-1.0 & 1.0 & 0 & 0 & 0 & 0\\\\0 & -1.0 & 1.0 & 0 & 0 & 0\\\\0 & 0 & -1.0 & 1.0 & 0 & 0\\\\0 & 1.0 & 0 & -1.0 & 0 & 0\\\\-1.0 & 0 & 0 & 1.0 & 0 & 0\\\\1.0 & 0 & -1.0 & 0 & 0 & 0\\\\0 & 0 & -1.0 & 0 & 0 & 1.0\\\\0 & 0 & 1.0 & 0 & 0 & -1.0\\\\0 & 0 & 0 & -1.0 & 1.0 & 0\\\\0 & 0 & 0 & 0 & -1.0 & 1.0\\end{matrix}\\right]\\)\n\n\n\nNetworkx has an “incidence_matrix” function that will do this… nx.incidence_matrix(G).toarray()\n\n\nA.T\n\n\\(\\displaystyle \\left[\\begin{matrix}-1.0 & 0 & 0 & 0 & -1.0 & 1.0 & 0 & 0 & 0 & 0\\\\1.0 & -1.0 & 0 & 1.0 & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & 1.0 & -1.0 & 0 & 0 & -1.0 & -1.0 & 1.0 & 0 & 0\\\\0 & 0 & 1.0 & -1.0 & 1.0 & 0 & 0 & 0 & -1.0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1.0 & -1.0\\\\0 & 0 & 0 & 0 & 0 & 0 & 1.0 & -1.0 & 0 & 1.0\\end{matrix}\\right]\\)\n\n\nNow we solve for y in N(AT). AT is a matrix where rows represent nodes and columns represent whether an edge is outgoing (-1) or incoming (+1) into the node, 0 if no connection to the node. Thus, solving the nullspace of AT gives us a basis for a vector with components representing the distribution of potential differences on the edges of our graph\n\nAT_null = (A.T).nullspace()\nAT_null\n\n[Matrix([\n [  0],\n [1.0],\n [1.0],\n [  1],\n [  0],\n [  0],\n [  0],\n [  0],\n [  0],\n [  0]]),\n Matrix([\n [-1.0],\n [-1.0],\n [-1.0],\n [   0],\n [   1],\n [   0],\n [   0],\n [   0],\n [   0],\n [   0]]),\n Matrix([\n [1.0],\n [1.0],\n [  0],\n [  0],\n [  0],\n [  1],\n [  0],\n [  0],\n [  0],\n [  0]]),\n Matrix([\n [  0],\n [  0],\n [  0],\n [  0],\n [  0],\n [  0],\n [1.0],\n [  1],\n [  0],\n [  0]]),\n Matrix([\n [   0],\n [   0],\n [ 1.0],\n [   0],\n [   0],\n [   0],\n [-1.0],\n [   0],\n [ 1.0],\n [   1]])]\n\n\nWe now construct our basis.\n\nnull_basis = AT_null[0].T\nfor vec in AT_null[1:]:\n  null_basis = null_basis.col_join(vec.T)\nnull_basis\n\n\\(\\displaystyle \\left[\\begin{matrix}0 & 1.0 & 1.0 & 1 & 0 & 0 & 0 & 0 & 0 & 0\\\\-1.0 & -1.0 & -1.0 & 0 & 1 & 0 & 0 & 0 & 0 & 0\\\\1.0 & 1.0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 1.0 & 1 & 0 & 0\\\\0 & 0 & 1.0 & 0 & 0 & 0 & -1.0 & 0 & 1.0 & 1\\end{matrix}\\right]\\)\n\n\nWe know that our null basis is composed of yT vectors for y in N(AT). Thus, we know that null_basis * b = 0, meaning if we solve for the null space of matrix we are given potential values of b.\n\nnull_basis.nullspace()\n\n[Matrix([\n [ 1.0],\n [-1.0],\n [   0],\n [   1],\n [   0],\n [   0],\n [   0],\n [   0],\n [   0],\n [   0]]),\n Matrix([\n [-1.0],\n [   0],\n [   0],\n [   0],\n [-1.0],\n [   1],\n [   0],\n [   0],\n [   0],\n [   0]]),\n Matrix([\n [-1.0],\n [ 1.0],\n [-1.0],\n [   0],\n [-1.0],\n [   0],\n [-1.0],\n [   1],\n [   0],\n [   0]]),\n Matrix([\n [-1.0],\n [ 1.0],\n [-1.0],\n [   0],\n [-1.0],\n [   0],\n [   0],\n [   0],\n [   1],\n [   0]]),\n Matrix([\n [-1.0],\n [ 1.0],\n [-1.0],\n [   0],\n [-1.0],\n [   0],\n [   0],\n [   0],\n [   0],\n [   1]])]\n\n\nThus, we know that b must be some linear combination of these vectors. Moreover, we know that since a vector yT in null_basis represents a loop of the matrix, solving b in yT * b = 0 gives us a vector that will distribute potential differences along edges of the graph such that potential differences between loops will be 0.\n\nI think your logic here isn’t quite clear. It should go something like this:\n\nThe valid values for \\(b\\) are those which can be written as \\(Ax=b\\) for some \\(x\\). That is, \\(b\\) must be in the column space of \\(A\\).\nSeparately, we know a subset of edgesform a loop in the graph iff the sum of the rows in A corresponding to this subset sums to 0. If \\(y\\) is a column vector with 1s for each edge in the subset, and 0s elsewhere, then the sum of the corresponding rows is \\(y^T A\\). Since we want this sum to be zero, we have \\(y^T A = 0\\) and therefore \\(A^T y=0\\) so we conclude that if \\(y\\) is a subset of edges representing a loop, \\(y\\) must be in the nullspace of \\(A^T\\).\n\nPutting these two together, we can use matrix algebra to show that if \\(y\\) is in the nullspace of \\(A^T\\) and \\(b\\) is in the column space of \\(A\\), then \\(y^T b = 0\\).\nThat means that for any valid set of potential differences (\\(b\\)), and any set of edges that form a loop (\\(y\\)), the potential differences along the edges of the loop, as given by \\(y^T b\\), will be zero.\nSo we don’t use this to find \\(b\\), but rather to show that Kirchoff’s law is satisfied for any valid \\(b\\).\n(It should be valid to do it the way you have done it, because the left nullspace of \\(A\\) is the orthogonal complement of the column space of \\(A\\), but I think it’s clearer to think about it the way I just described.)\n\n\n\nProblem 1.2\nWe consider an incoming edge to all outgoing edges of a node. Thus, we can describe each node as an equation, using edge labels 1-10 as seen from our edge list. Node 1: e6 = e1 + e4 &lt;-&gt; e6 - e1 - e4 = 0, Node 2: e1 + e4 = e2 &lt;-&gt; e1 + e4 - e2 = 0. We follow this pattern for each edge, giving us a 6x10 matrix.\n\n# im.open('/content/drive/MyDrive/24320 - Linear Algebra Applications/24320_datasets/24320_graph_crop 2024-05-05 03_38_02.jpg')\n\n\ncur_mat = sp.Matrix([\n    [-1, 0, 0, 0, -1, 1, 0, 0, 0, 0],\n    [1, -1, 0, 1, 0, 0, 0, 0, 0, 0],\n    [0, 1, -1, 0, 0, -1, -1, 1, 0, 0],\n    [0 ,0, 1, -1, 1, 0, 0, 0, -1, 0],\n    [0, 0, 0, 0, 0, 0, 0, 0, 1, -1],\n    [0, 0, 0, 0, 0, 0, 1, -1, 0, 1]\n])\ncur_mat\n\n\\(\\displaystyle \\left[\\begin{matrix}-1 & 0 & 0 & 0 & -1 & 1 & 0 & 0 & 0 & 0\\\\1 & -1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & 1 & -1 & 0 & 0 & -1 & -1 & 1 & 0 & 0\\\\0 & 0 & 1 & -1 & 1 & 0 & 0 & 0 & -1 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & -1\\\\0 & 0 & 0 & 0 & 0 & 0 & 1 & -1 & 0 & 1\\end{matrix}\\right]\\)\n\n\nAs shown, we now have a matrix condition that must be satisfied, such that net total current in and out of each node is 0. To find this, we can solve for the nullspace of the system. This nullspace will form a basis for a vector distribution of currents. However, it will include -1 components that indicate a vector being “flipped.” Thus, we will use vector combinations that result in positive components only. Moreover, if a component is 0, that indicates that no flow is going through that edge.\n\nnull_basis = cur_mat.nullspace()\nnull_basis\n\n[Matrix([\n [0],\n [1],\n [1],\n [1],\n [0],\n [0],\n [0],\n [0],\n [0],\n [0]]),\n Matrix([\n [-1],\n [-1],\n [-1],\n [ 0],\n [ 1],\n [ 0],\n [ 0],\n [ 0],\n [ 0],\n [ 0]]),\n Matrix([\n [1],\n [1],\n [0],\n [0],\n [0],\n [1],\n [0],\n [0],\n [0],\n [0]]),\n Matrix([\n [0],\n [0],\n [0],\n [0],\n [0],\n [0],\n [1],\n [1],\n [0],\n [0]]),\n Matrix([\n [ 0],\n [ 0],\n [ 1],\n [ 0],\n [ 0],\n [ 0],\n [-1],\n [ 0],\n [ 1],\n [ 1]])]\n\n\nWe demonstrate that a linear combination of these vectors satsifies our conditions\n\ntest_vec = null_basis[0]\nfor vec in null_basis[1:]:\n  test_vec += vec\ntest_vec\n\n\\(\\displaystyle \\left[\\begin{matrix}0\\\\1\\\\1\\\\1\\\\1\\\\1\\\\0\\\\1\\\\1\\\\1\\end{matrix}\\right]\\)\n\n\n\ncur_mat * test_vec\n\n\\(\\displaystyle \\left[\\begin{matrix}0\\\\0\\\\0\\\\0\\\\0\\\\0\\end{matrix}\\right]\\)\n\n\nThus, we can see that we have found a basis for potential current distributions for our matrix.\n\nTo be precise, what you have demonstrated with code is that one specific vector in the span of the null_basis is a valid current distribution. You have not shown that all vectors in the span of the null_basis are valid current distributions. (Although it is true; that’s from the definition of the nullspace.)\nTo show this using code, you could do something like this, to make a generic sum:\n\nsyms = sp.symbols('x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 x11')\ntest_vec = 0*null_basis[0]\nfor i in range(len(null_basis)):\n  print(i)\n  test_vec += null_basis[i]*syms[i]\ndisplay(test_vec)\ndisplay(cur_mat*test_vec)\n\n0\n1\n2\n3\n4\n\n\n\\(\\displaystyle \\left[\\begin{matrix}- x_{2} + x_{3}\\\\x_{1} - x_{2} + x_{3}\\\\x_{1} - x_{2} + x_{5}\\\\x_{1}\\\\x_{2}\\\\x_{3}\\\\x_{4} - x_{5}\\\\x_{4}\\\\x_{5}\\\\x_{5}\\end{matrix}\\right]\\)\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0\\\\0\\\\0\\\\0\\\\0\\\\0\\end{matrix}\\right]\\)\n\n\n\n#Problem 1.3: When examining the basis of directed loops, you can take a sum of all the columns in the basis. This will give you a vector along with components indicating how many times an edge is used in a loop of the graph. We can then compare this sum vector to our edges. If a coefficient for an edge is non-zero, that indicates that both ends of the edge, or the nodes, must be contained in our sum loop such that they are connected to every other node in the loop. Otherwise, if a node is not contained, then it is not connected. I will demonstrate an example below.\n\nsum_vec = dir_basis.col(0)\nfor i in range(1, dir_basis.cols):\n  sum_vec += dir_basis.col(i)\nsum_vec\n\n\\(\\displaystyle \\left[\\begin{matrix}1.0\\\\2.0\\\\3.0\\\\1\\\\3\\\\1.0\\\\2\\\\1.0\\\\1\\\\2\\end{matrix}\\right]\\)\n\n\nThus, we can see that all nodes have bidirectional communication, as every edge is utilized in our basis. This can be further confirmed by analyzing our graph: we can draw a path 1, 2, 3, 4, 5, 6, 3, 1, such that every node can reach another node in our path.\n\nWould this approach work though if you had some disconnected loops? For example, suppose that nodes 5 and 6 only connected to each other. Or, for that matter, suppose that node 4 linked to node 5, but then 5 and 6 only link to each other.\n\n\nThis problem is generally done at an “M” level. However, I will give it an “R” because I’d really like to see you fix the issue I mentioned just above, about the disconnected loops.\nIf you address that, I will give it an “M” level. If you also have time to address the other points, I will give it an “E”.\nGrade: R\n\n\n\nProblem 2: Image Compression and Edge Detection\nIn this problem, we will be attempting to compress an image using a Haar Wavelet Transform\nhaar_wavelet: Creates an n-dimension Haar Wavelet transformation matrix.\n\ndef haar_wavelet(dim):\n  result = 0*np.identity(dim)\n  jump = dim//2\n  i = 0\n  for j in range(0, dim, 2):\n    result[i][j] = 1\n    result[i][j+1] = 1\n    result[i+jump][j] = -1\n    result[i+jump][j+1] = 1\n    i+=1\n  return ((2**0.5)/2)*result\n\nhaar_wavelet_transform: performs the haar wavelet transform on a matrix A given m-dimension and n-dimension haar wavelet matrices\n\ndef haar_wavelet_transform(A, Hm, Hn):\n  mat = np.copy(A)\n  transpose = Hn.transpose()\n  return 0.5*(np.matmul(np.matmul(Hm, mat), transpose))\n\nhaar_wavelet_transform_reverse: Given a haar_wavelet transform C, converts the trasnformation back to its original image\n\ndef haar_wavelet_transform_reverse(C, Hm, Hn):\n  mat = np.copy(C)\n  transpose = Hm.transpose()\n  return 2*np.matmul(np.matmul(transpose, mat), Hn)\n\nedge_transform: performs the edge transform on an already transformed haar_wavelet image to obtain the edge image\n\ndef edge_transform(transform, Hm, Hn):\n  zerod = np.copy(transform)\n  rows = Hm.shape[0]//2\n  cols = Hn.shape[0]//2\n  for col in range(cols):\n    for row in range(rows):\n      zerod[row][col] = 0\n  transpose = Hm.transpose()\n  return 2 * np.matmul(np.matmul(transpose, zerod), Hn)\n\nstack: stacks a 2D matrix onto itself to give it a 3D shape such that there is a Red, Green, and Blue value for an RGB image reader. Sets type to uint8 to make it compatible with PIL.Image\n\ndef stack(matrix):\n  return (np.stack([matrix, matrix, matrix], axis=2)).astype(np.uint8)\n\nclean_matrix: ensures that the given matrix has an even number of rows and columns\n\ndef clean_matrix(matrix):\n  rows, cols = matrix.shape\n  result = np.copy(matrix)\n  if rows % 2 == 0 and cols % 2 == 0:\n      return result\n  if rows % 2 != 0:\n    result = np.delete(result, -1, 0)\n  if cols % 2 != 0:\n    result = np.delete(result, -1, 1)\n  return result\n\nblur: blurs a given array by a factor of 4\n\ndef blur(matrix):\n  cur = clean_matrix(matrix)\n  rows, cols = cur.shape\n  hm = haar_wavelet(rows)\n  hn = haar_wavelet(cols)\n  transformation = haar_wavelet_transform(cur, hm, hn)\n  result = np.copy(transformation[0:(rows//2), 0:(cols//2)])\n  return result\n\n\nhaar_wavelet(4)\n\narray([[ 0.70710678,  0.70710678,  0.        ,  0.        ],\n       [ 0.        ,  0.        ,  0.70710678,  0.70710678],\n       [-0.70710678,  0.70710678,  0.        ,  0.        ],\n       [ 0.        ,  0.        , -0.70710678,  0.70710678]])\n\n\n\nI love how you’ve put all the above code into functions. Here, I’ve displayed the haar_wavelet visually (so I can see that it makes sense – which it does!):\n\nplt.imshow(haar_wavelet(10))\n\n\n\n\n\n\n\n\n\n\nSince I don’t have your image file, I am using a cat image instead.\n\n\nimage = im.open('catimage.png')\nimage\n\n\n\n\n\n\n\n\nWe convert the image into a numpy array and then remove a row and/or column if there is an odd count of them. This is because the Haar Wavelet Matrix requires an even dimension as per definition.\n\nimage_arr = np.asarray(image)[:,:,0]\nworking = clean_matrix(image_arr)\nworking\n\narray([[117, 117, 117, ..., 138, 140, 140],\n       [117, 117, 117, ..., 138, 140, 140],\n       [117, 117, 117, ..., 138, 140, 140],\n       ...,\n       [123, 123, 123, ..., 129, 130, 132],\n       [123, 123, 123, ..., 129, 130, 132],\n       [127, 127, 127, ..., 133, 134, 135]], dtype=uint8)\n\n\nUsing the dimensions of the cleaned matrix, we construct the m-dimension and n-dimension haar wavelet matrices needed for the transformation.\n\nhaar_m = haar_wavelet(working.shape[0])\nhaar_n = haar_wavelet(working.shape[1])\n\nWe now perform the first iteration of the transformation.\n\ntransform = haar_wavelet_transform(working, haar_m, haar_n)\n\n\nstacked = stack(transform)\nim.fromarray(stacked)\n\n\n\n\n\n\n\n\n\nWow, that’s seriously nice-looking! I’m impressed.\n\nWe can also find the edge-only version using our transformation\n\nedges = edge_transform(transform, haar_m, haar_n)\nstacked_edges = stack(edges)\nim.fromarray(stacked_edges)\n\n\n\n\n\n\n\n\nWe can also convert the image back to its original state to demonstrate the compression value of this transformation.\n\noriginal = haar_wavelet_transform_reverse(transform, haar_m, haar_n)\nstacked_original = stack(original)\nim.fromarray(stacked_original)\n\n\n\n\n\n\n\n\nWe can then begin the process of blurring the original image until it is unrecognizable. Although this can be done as a function, I believe it will help to see the step-by-step process.\ncontinuous_blur: given an image, returns a list of blurred images of length step+1 starting with the original, with each subsequent item blurred 4x the previous.\n\ndef continuous_blur(image, steps):\n  blurs = [image]\n  stacked_blurs = [stack(image)]\n  for i in range(steps):\n    next_blur = blur(blurs[i])\n    blurs.append(next_blur)\n    stacked_blurs.append(stack(next_blur))\n  return stacked_blurs\n\n\nblurs = continuous_blur(working, 5)\n\n\nim.fromarray(blurs[0])\n\n\n\n\n\n\n\n\n\nim.fromarray(blurs[1])\n\n\n\n\n\n\n\n\n\nim.fromarray(blurs[2])\n\n\n\n\n\n\n\n\n\nim.fromarray(blurs[3])\n\n\n\n\n\n\n\n\n\nim.fromarray(blurs[4])\n\n\n\n\n\n\n\n\n\nim.fromarray(blurs[5])\n\n\n\n\n\n\n\n\nAs shown by our constant image blurring, these images become so small that it is almost impossible to recognize their original form.\n\nThe images do become small – but also blurred. (With my cat in here, it’s hard to see the blur actually, but I think it is there.)\n\n\n\nProblem 2 Writeup\nAs demonstrated by the Haar Wavelet transformation, we have an effective method of image compression that is cheaper than storing the original image. This is because storing the edges of an image is cheaper than storing the actual image since we are only storing a fraction of the image’s data. Parts of the image’s data that are near-uniform in color are stored in the blurred image of the data. One way we can make this image compression more effective is by identifying the average pixel color of edges. Using that information from our edge images, we can then choose to 0 out pixel blocks below that average color value in the converted edges image. Given the original dimensions of the edge image to be M x N pixels, we can convert this approximation into a matrix A of size M’ x N’, where we choose a box of size P, such that M’ = M/P, N’ = N/P. Then for each element a_ji in this matrix, we can choose a_ji = 0 if that box of size PxP is a zeroed out area, otherwise we choose a_ji = 1 such that the information in this area must be preserved. We can then save all boxes set to 1 as respective to their element a_ji, representing a box in the original image of size [jP:(j+1)P] x [iP: (i+1)P], into a list B. This list B will contain all boxes of size P of image data that has been conserved. When we want to convert back to the matrix, we can first construct a matrix called final of size (M’* P) x (N’ * P). Then, we iterate through our matrix A. For each section of size PxP in final, we identify whether it should be represented by a zeroed entry (a_ji = 0) or a data entry (a_ji = 1). If it is represented by 0’s, we construct a box of zeros of size PxP and insert it into final. Otherwise, we take the current entry of B and insert that box of size PxP into the matrix. We then iterate B to the next entry in the list. We repeat this process until we have fully constructed the edge image with zeroed sections. We can then use this full constructed edge image as our approximation for all edges in the image. Since we know that both Wn and Wm are orthogonal, we can use the original equation that found our edge image: edges = 2* Wm.T * transform.z * Wn, where transform.z represents the Haar Wavelet Transform matrix with the blurred section zeroed out. Due to this formula, we know that transform.z = 0.5 * Wm * edges * Wn.T. Then we can reconstruct our original transform by replacing the zeroed out section of tranform.z with the pixel values from our blurred image. Thus, we have found a way to store the edges in a more compressed form than originally found by the Haar Wavelet Transform.\nWe can demonstrate this method through a simple 2x3 approximation of a 4x6 edge matrix. This assumes that P = 2\n\n# original matrix A:\nA = sp.Matrix([\n    [4, 2, 155, 155, 50, 50],\n    [8, 10, 155, 155, 100, 100],\n    [155, 155, 20, 25, 2, 4],\n    [155, 155, 20, 15, 40, 40]\n])\nA\n\n\\(\\displaystyle \\left[\\begin{matrix}4 & 2 & 155 & 155 & 50 & 50\\\\8 & 10 & 155 & 155 & 100 & 100\\\\155 & 155 & 20 & 25 & 2 & 4\\\\155 & 155 & 20 & 15 & 40 & 40\\end{matrix}\\right]\\)\n\n\nWe notice that our edge color seems to be 155. Thus, we choose to zero out all blocks that have pixel values less than our edge value.\n\nAz = A[:, :]\nsize = sp.shape(Az)[0] * sp.shape(Az)[1]\nfor i in range(size):\n  if Az[i] &lt; 155:\n    Az[i] = 0\nAz\n\n\\(\\displaystyle \\left[\\begin{matrix}0 & 0 & 155 & 155 & 0 & 0\\\\0 & 0 & 155 & 155 & 0 & 0\\\\155 & 155 & 0 & 0 & 0 & 0\\\\155 & 155 & 0 & 0 & 0 & 0\\end{matrix}\\right]\\)\n\n\nWe approximate this matrix by identifying data sections and zero sections\n\napprox = sp.Matrix([\n    [0, 1, 0],\n    [1, 0, 0]\n])\nB = [sp.Matrix([\n    [155, 155],\n    [155, 155]\n]),\n  sp.Matrix([\n      [155,155],\n      [155,155]\n  ])]\nsp.pprint(approx)\nB\n\n⎡0  1  0⎤\n⎢       ⎥\n⎣1  0  0⎦\n\n\n[Matrix([\n [155, 155],\n [155, 155]]),\n Matrix([\n [155, 155],\n [155, 155]])]\n\n\nAs can be seen, our compression would allow us to recreate an image that could approximate the edges of a matrix and store it in as small as P^2 less area.\n\nHah I was just about to start writing a comment “but what about image compression?”, and then I saw this section. Very very well done!\nGrade E\n\n\n\nProblem 3 Least Squares\nIn this problem, we will attempt to predict the end results of the Ivy League football season using the midseason table. We first read in the midseason table of the Ivy League. Both rows and columns indicate teams. Given row index j, column index j, the element a[j][i] represents how many points team j scored against team i.\n#Problem 3 Writeup\nIn this project, I attempt to use SVD to complete the matrix. First, I find the current leaders of the league using the power matrix influence. Then, I approximate the missing values of our matrix to be the average defensive performance a team of column index i has put on prior to the midseason review. Afterwards, I calculate the singular value decomposition and then choose k singular values, such that the remaining n-k singular values are much smaller than the ones I chose. This allows me to weight the “important” vectors of U and Vt (the ones with large singular values), more heavily, creating an approimate final table that is influenced by the results of the midterm review. Then, I compare it to the actual final table to see how the results differ.\nThe reason I chose to take their average defensive performance (e.g. the average score other teams put up against them) was because specifically in college football, standout players are often found on the offensive side, mainly quarterbacks. Because quarterbacks have a heavy influence on the game, they often are in control of how well an offense plays or not. If they’re in a slump, their team suffers, if they’re on fire, the team scores. However, a football team’s defense is often more consistent because their overall performance isn’t dependent on one player. Thus, the amount of point scored against a team will often be more consistent than the amount of points a team scores because the defensive side of the team is likely more consistent than the quarterback.\nThis solution is limited by the strength of schedule a team has before the midseason. If a team has a really easy schedule, such that they only play easy opponents before we take a look midway through the season, then they will look much better on paper than they may really be in the league. Because of that, I ran into issues where Cornell, which should not have been ranked number 1, was consistently ranked number 1 in my approximations due to their strong performance early on.\n\n\nProblem 3 Code\nfind_ranking_power_matrix: finds the ranking of teams using a power matrix given a weighted adjacency matrix\n\ndef find_ranking_power_matrix(weight_adj):\n  weight_power = weight_adj + weight_adj**2\n  weight_power_sum = [sum(weight_power[i]) for i in range(8)]\n  rrank_weight = np.argsort(weight_power_sum)\n  rank_weight = np.flip(rrank_weight)\n  for num in rank_weight:\n    print(teams[num]+'\\n')\n\nweighted_adjacency_matrix: given a table of scores, finds the weighted adjacency matrix of the table\n\ndef weighted_adjacency_matrix(table):\n  weighted_adjacency = np.copy(table)\n  for j in range(rows):\n    for i in range(cols):\n      if weighted_adjacency[j][i] &gt;= weighted_adjacency[i][j]:\n        weighted_adjacency[j][i] = weighted_adjacency[j][i] - weighted_adjacency[i][j]\n        weighted_adjacency[i][j] = 0.0\n      else:\n        weighted_adjacency[i][j] = weighted_adjacency[i][j] - weighted_adjacency[j][i]\n        weighted_adjacency[j][i] = 0.0\n  return weighted_adjacency\n\n\nmidseason_view = pd.read_csv('Ivy League Stats - Midseason.csv')\nmidseason_view\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nHarvard\nYale\nDartmouth\nPrinceton\nPenn\nBrown\nCornell\nColumbia\n\n\n\n\n0\nHarvard\n0.0\nNaN\nNaN\n14.0\nNaN\n34.0\n41.0\nNaN\n\n\n1\nYale\nNaN\n0.0\n31.0\nNaN\n17.0\nNaN\n21.0\nNaN\n\n\n2\nDartmouth\nNaN\n24.0\n0.0\nNaN\n23.0\nNaN\nNaN\n20.0\n\n\n3\nPrinceton\n21.0\nNaN\nNaN\n0.0\nNaN\n27.0\nNaN\n10.0\n\n\n4\nPenn\nNaN\n27.0\n20.0\nNaN\n0.0\nNaN\nNaN\n20.0\n\n\n5\nBrown\n31.0\nNaN\nNaN\n28.0\nNaN\n0.0\n14.0\nNaN\n\n\n6\nCornell\n23.0\n23.0\nNaN\nNaN\nNaN\n36.0\n0.0\nNaN\n\n\n7\nColumbia\nNaN\nNaN\n9.0\n7.0\n17.0\nNaN\nNaN\n0.0\n\n\n\n\n\n\n\n\nWe collect all the team names from this table.\n\nteams = list(midseason_view)[1:]\nteams\n\n['Harvard',\n 'Yale',\n 'Dartmouth',\n 'Princeton',\n 'Penn',\n 'Brown',\n 'Cornell',\n 'Columbia']\n\n\nHowever, since having team names will be messy for data analysis, we transform the dataframe into a numpy array and delete the row names.\n\nmidseason = midseason_view.to_numpy()\nmidseason\n\narray([['Harvard', 0.0, nan, nan, 14.0, nan, 34.0, 41.0, nan],\n       ['Yale', nan, 0.0, 31.0, nan, 17.0, nan, 21.0, nan],\n       ['Dartmouth', nan, 24.0, 0.0, nan, 23.0, nan, nan, 20.0],\n       ['Princeton', 21.0, nan, nan, 0.0, nan, 27.0, nan, 10.0],\n       ['Penn', nan, 27.0, 20.0, nan, 0.0, nan, nan, 20.0],\n       ['Brown', 31.0, nan, nan, 28.0, nan, 0.0, 14.0, nan],\n       ['Cornell', 23.0, 23.0, nan, nan, nan, 36.0, 0.0, nan],\n       ['Columbia', nan, nan, 9.0, 7.0, 17.0, nan, nan, 0.0]],\n      dtype=object)\n\n\n\nmidseason = np.delete(midseason, 0, 1)\nsp.Matrix(midseason)\n\n\\(\\displaystyle \\left[\\begin{matrix}0 & \\text{NaN} & \\text{NaN} & 14.0 & \\text{NaN} & 34.0 & 41.0 & \\text{NaN}\\\\\\text{NaN} & 0 & 31.0 & \\text{NaN} & 17.0 & \\text{NaN} & 21.0 & \\text{NaN}\\\\\\text{NaN} & 24.0 & 0 & \\text{NaN} & 23.0 & \\text{NaN} & \\text{NaN} & 20.0\\\\21.0 & \\text{NaN} & \\text{NaN} & 0 & \\text{NaN} & 27.0 & \\text{NaN} & 10.0\\\\\\text{NaN} & 27.0 & 20.0 & \\text{NaN} & 0 & \\text{NaN} & \\text{NaN} & 20.0\\\\31.0 & \\text{NaN} & \\text{NaN} & 28.0 & \\text{NaN} & 0 & 14.0 & \\text{NaN}\\\\23.0 & 23.0 & \\text{NaN} & \\text{NaN} & \\text{NaN} & 36.0 & 0 & \\text{NaN}\\\\\\text{NaN} & \\text{NaN} & 9.0 & 7.0 & 17.0 & \\text{NaN} & \\text{NaN} & 0\\end{matrix}\\right]\\)\n\n\nWe notice that this graph can represent an adjacency matrix, with weights. To get to this point, we first zero out every nan value, such that we every nan represents a lack of an adjacency between the two teams.\n\nrows, cols = np.shape(midseason)\nfor j in range(rows):\n  for i in range(cols):\n    if not midseason[j][i] &gt;= 0:\n      midseason[j][i] = 0.0\nmidseason = midseason.astype('float')\nmidseason\n\narray([[ 0.,  0.,  0., 14.,  0., 34., 41.,  0.],\n       [ 0.,  0., 31.,  0., 17.,  0., 21.,  0.],\n       [ 0., 24.,  0.,  0., 23.,  0.,  0., 20.],\n       [21.,  0.,  0.,  0.,  0., 27.,  0., 10.],\n       [ 0., 27., 20.,  0.,  0.,  0.,  0., 20.],\n       [31.,  0.,  0., 28.,  0.,  0., 14.,  0.],\n       [23., 23.,  0.,  0.,  0., 36.,  0.,  0.],\n       [ 0.,  0.,  9.,  7., 17.,  0.,  0.,  0.]])\n\n\nWe notice that it is similar to the weighted adjacency matrix from our last project. However, the key difference is that both teams have scores for a specific matchup. This can easily be fixed by subtracting the lower score from the higher score and zeroing out the low score. Thus we perform the following.\n\nweighted_adjacency = weighted_adjacency_matrix(midseason)\nweighted_adjacency\n\narray([[ 0.,  0.,  0.,  0.,  0.,  3., 18.,  0.],\n       [ 0.,  0.,  7.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  3.,  0.,  0., 11.],\n       [ 7.,  0.,  0.,  0.,  0.,  0.,  0.,  3.],\n       [ 0., 10.,  0.,  0.,  0.,  0.,  0.,  3.],\n       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n       [ 0.,  2.,  0.,  0.,  0., 22.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])\n\n\nFrom this weighted adjacency, we can then find the power matrix so that we can rank the schools during midseason.\n\nfind_ranking_power_matrix(weighted_adjacency)\n\nCornell\n\nHarvard\n\nDartmouth\n\nPenn\n\nPrinceton\n\nYale\n\nBrown\n\nColumbia\n\n\n\nAs demonstrated by the power matrix, we can see that Cornell is currently the strongest school, followed by Harvard and Dartmouth. We will take this information into consideration as we perform SVD to find a matrix completion.\nThere are many ways to estimate a complete matrix. One way we can do this is by using the Singular Value Decomposition of a matrix and estimate the complete matrix by using k singular values. The choice of k can be decided by estimating how important each singular value is. The most important part, however, is what we choose to do with our missing values. In this case, I believe the best option is replace them with the average value in their respective column. That is to say, given a missing value for an element with row index j and column index i, we input the average points scored against team i as our replacement. The intuition behind this is that a team’s defense in football, especially in college, tends to be consistent, regardless of the teams they play. I will explain my reasoning in more depth in the write-up.\n\nmidseason_svd = np.copy(midseason)\nfor j in range(rows):\n  for i in range(cols):\n    if i != j and midseason[j][i] == 0:\n      midseason_svd[j][i] = np.mean(midseason[:, i])\nmidseason_svd\n\narray([[ 0.   ,  9.25 ,  7.5  , 14.   ,  7.125, 34.   , 41.   ,  6.25 ],\n       [ 9.375,  0.   , 31.   ,  6.125, 17.   , 12.125, 21.   ,  6.25 ],\n       [ 9.375, 24.   ,  0.   ,  6.125, 23.   , 12.125,  9.5  , 20.   ],\n       [21.   ,  9.25 ,  7.5  ,  0.   ,  7.125, 27.   ,  9.5  , 10.   ],\n       [ 9.375, 27.   , 20.   ,  6.125,  0.   , 12.125,  9.5  , 20.   ],\n       [31.   ,  9.25 ,  7.5  , 28.   ,  7.125,  0.   , 14.   ,  6.25 ],\n       [23.   , 23.   ,  7.5  ,  6.125,  7.125, 36.   ,  0.   ,  6.25 ],\n       [ 9.375,  9.25 ,  9.   ,  7.   , 17.   , 12.125,  9.5  ,  0.   ]])\n\n\nWe then use scipy.linalg’s svd function to find the singular value decomposition of the matrix\n\nU, S, Vt = svd(midseason)\nS\n\narray([74.75657202, 53.98601884, 48.45216671, 37.35834194, 25.97844269,\n       23.26194426, 14.72288866,  7.93382333])\n\n\n\nplt.plot(S)\n\n\n\n\n\n\n\n\nWe see our sharpest decline in eigenvalue magnitude between 0 to 1. Afterwards, we have steady decline, but since 4 and 5 maintain similar values, we can choose an approximation to be k=5. Thus, we will perform a low rank approximation with k=2 for analysis and k=5 as our final\n\nk = 2\nfinal_approx_k2 = np.round(np.matmul(np.matmul(U[:, :k], np.diag(S[:k])), Vt[:k, :]))\nsp.Matrix(final_approx_k2)\n\n\\(\\displaystyle \\left[\\begin{matrix}21.0 & 2.0 & -1.0 & 14.0 & -2.0 & 33.0 & 23.0 & -1.0\\\\4.0 & 14.0 & 9.0 & 1.0 & 8.0 & 7.0 & 4.0 & 9.0\\\\1.0 & 22.0 & 15.0 & -3.0 & 14.0 & 1.0 & -1.0 & 15.0\\\\12.0 & 5.0 & 2.0 & 8.0 & 1.0 & 19.0 & 13.0 & 2.0\\\\1.0 & 23.0 & 16.0 & -3.0 & 15.0 & 2.0 & 0 & 16.0\\\\12.0 & 0 & -1.0 & 9.0 & -2.0 & 20.0 & 14.0 & -1.0\\\\16.0 & 13.0 & 7.0 & 9.0 & 5.0 & 26.0 & 17.0 & 7.0\\\\1.0 & 7.0 & 5.0 & 0 & 4.0 & 1.0 & 0 & 5.0\\end{matrix}\\right]\\)\n\n\nAs shown above, given k=2, we have a matrix approximation that is difficult to parse. This is because we have only selected the first two columns and rows of U and Vt respectively. This means that our matrix is the cross product of 4 corresponding vectors, which results in high variance and unpredictability, as small changes in either corresponding vectors results in large changes when you cross them. We can apply our power matrix summation (after zeroing out the i=j entries) to see which teams would win the league according to our prediction.\n\nfor i in range(np.shape(final_approx_k2)[0]):\n  final_approx_k2[i][i] = 0\nk1_weight = weighted_adjacency_matrix(final_approx_k2)\nfind_ranking_power_matrix(k1_weight)\n\nHarvard\n\nPenn\n\nCornell\n\nDartmouth\n\nPrinceton\n\nYale\n\nColumbia\n\nBrown\n\n\n\nAs predicted, this ranking resulted in multiple shifts due to the high unpredictability of using a low k value.\n\nk = 5\nfinal_approx_k5 = np.round(np.matmul(np.matmul(U[:, :k], np.diag(S[:k])), Vt[:k, :]))\nsp.Matrix(np.round(np.matmul(np.matmul(U[:, :k], np.diag(S[:k])), Vt[:k, :])))\n\n\\(\\displaystyle \\left[\\begin{matrix}1.0 & 0 & -1.0 & 13.0 & 2.0 & 34.0 & 41.0 & -1.0\\\\-1.0 & 0 & 32.0 & 2.0 & 15.0 & 1.0 & 20.0 & 1.0\\\\-2.0 & 27.0 & 3.0 & 3.0 & 14.0 & -1.0 & 1.0 & 20.0\\\\20.0 & 8.0 & 1.0 & 2.0 & -3.0 & 25.0 & 1.0 & 1.0\\\\2.0 & 24.0 & 15.0 & -3.0 & 14.0 & 2.0 & -2.0 & 16.0\\\\31.0 & -1.0 & -1.0 & 28.0 & 3.0 & 1.0 & 13.0 & 0\\\\23.0 & 19.0 & 1.0 & -1.0 & -2.0 & 36.0 & 0 & 7.0\\\\2.0 & 3.0 & 11.0 & 4.0 & 7.0 & -4.0 & 5.0 & 3.0\\end{matrix}\\right]\\)\n\n\nMeanwhile, when k=5, we see lower variance in terms of scores. We now see how our power-ranking looks according to this\n\nfor i in range(np.shape(final_approx_k5)[0]):\n  final_approx_k5[i][i] = 0\nk6_weight = weighted_adjacency_matrix(final_approx_k5)\nfind_ranking_power_matrix(k6_weight)\n\nCornell\n\nHarvard\n\nPenn\n\nPrinceton\n\nDartmouth\n\nYale\n\nBrown\n\nColumbia\n\n\n\nDue to the increase in vectors being used as a basis for our approximation, we see less unpredictable patterns as variance is now regulated: our previous number 1 and 2 seeds after the midseason review have remained.\n\n#Final approximation\nsp.Matrix(final_approx_k5)\n\n\\(\\displaystyle \\left[\\begin{matrix}0 & 0 & -1.0 & 13.0 & 2.0 & 34.0 & 41.0 & -1.0\\\\-1.0 & 0 & 32.0 & 2.0 & 15.0 & 1.0 & 20.0 & 1.0\\\\-2.0 & 27.0 & 0 & 3.0 & 14.0 & -1.0 & 1.0 & 20.0\\\\20.0 & 8.0 & 1.0 & 0 & -3.0 & 25.0 & 1.0 & 1.0\\\\2.0 & 24.0 & 15.0 & -3.0 & 0 & 2.0 & -2.0 & 16.0\\\\31.0 & -1.0 & -1.0 & 28.0 & 3.0 & 0 & 13.0 & 0\\\\23.0 & 19.0 & 1.0 & -1.0 & -2.0 & 36.0 & 0 & 7.0\\\\2.0 & 3.0 & 11.0 & 4.0 & 7.0 & -4.0 & 5.0 & 0\\end{matrix}\\right]\\)\n\n\nWe now compare this approximate final results with the true final results.\n\nfinal_true = pd.read_csv('Ivy League Stats - Full Season.csv')\nfinal = final_true.to_numpy()\nfinal = np.delete(final, 0, 1)\nsp.Matrix(final)\n\n\\(\\displaystyle \\left[\\begin{matrix}0 & 18 & 17 & 14 & 25 & 34 & 41 & 38\\\\23 & 0 & 31 & 36 & 17 & 36 & 21 & 35\\\\9 & 24 & 0 & 23 & 23 & 38 & 30 & 20\\\\21 & 28 & 21 & 0 & 31 & 27 & 14 & 10\\\\23 & 27 & 20 & 24 & 0 & 26 & 23 & 20\\\\31 & 17 & 13 & 28 & 30 & 0 & 14 & 21\\\\23 & 23 & 14 & 3 & 8 & 36 & 0 & 14\\\\24 & 7 & 9 & 7 & 17 & 14 & 29 & 0\\end{matrix}\\right]\\)\n\n\n\nfinal_weight = weighted_adjacency_matrix(final)\nfind_ranking_power_matrix(final_weight)\n\nYale\n\nDartmouth\n\nHarvard\n\nCornell\n\nPenn\n\nPrinceton\n\nColumbia\n\nBrown\n\n\n\nAs we can see, by our approximations we were only able to predict two of the top 4 teams. Moreover, we had both Yale and Dartmouth, the top 2 teams, as bottom 4 teams in our predictions. This misapproximation is likely due to both the midseason results we started with and the way we estimated our missing values.\n\nsp.pprint(teams)\nsp.Matrix(midseason)\n\n[Harvard, Yale, Dartmouth, Princeton, Penn, Brown, Cornell, Columbia]\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0 & 0 & 0 & 14.0 & 0 & 34.0 & 41.0 & 0\\\\0 & 0 & 31.0 & 0 & 17.0 & 0 & 21.0 & 0\\\\0 & 24.0 & 0 & 0 & 23.0 & 0 & 0 & 20.0\\\\21.0 & 0 & 0 & 0 & 0 & 27.0 & 0 & 10.0\\\\0 & 27.0 & 20.0 & 0 & 0 & 0 & 0 & 20.0\\\\31.0 & 0 & 0 & 28.0 & 0 & 0 & 14.0 & 0\\\\23.0 & 23.0 & 0 & 0 & 0 & 36.0 & 0 & 0\\\\0 & 0 & 9.0 & 7.0 & 17.0 & 0 & 0 & 0\\end{matrix}\\right]\\)\n\n\nAs seen in the graph, we can see that before the midseason review, Cornell had its three strongest performances all season. Thus, they were weighted higher in SVD approximations because they performed so well early. Meanwhile, Yale had 2 of its weakest performances scoring wise early. Thus, they were weighted less heavily in our approximation and is likely why we did not consider them as strong beforehand.\n\nThis is a very nice and creative approach. It’s well organized and you’ve explained your thinking well. I really like that you’ve sanity-checked your results against the power ranking techniques.\nWhat you have done here is akin to an “image denoising”. You made a thoughtful choice for what to put in for the missing values; but we can think of them as being the true values plus some unknown “noise”. Then, if there is an underlying low-rank structure to the matrix, which the noise doesn’t follow, your approach should be able to predict the true values.\nI would interpret the fact that your analysis wasn’t very successful in terms of its predictions as evidence that the structure of the data isn’t well-represented by a low-rank matrix.\nI will note that this overall approach is not actually what the problem asked for. There, you were supposed to set up a directed incidence matrix A showing which teams were playing which team in every game before midseason, and then have the score spread as an output, and try to find a vector of strengths such that A * strengths = spread for the games which had already been played. Because this system would be inconsistent, you’d need to use least squares to find the best-fit for \\(strengths\\). Then, once you had the strengths, you’d use a bigger incidence matrix which included all the games to predict their results.\nIf I had much more time, I’d be curious to see the differences in these two approaches. The least squares approach also assumes a certain low-rank underlying structure, but perhaps it deals more elegantly with the missing values…\nAnyways! Your work was great. Grade: E"
  },
  {
    "objectID": "submitted/projects_2/caihanyan.html",
    "href": "submitted/projects_2/caihanyan.html",
    "title": "Modeling with Directed Graphs",
    "section": "",
    "text": "import networkx as nx\nimport numpy as np\nimport matplotlib.pyplot as plt\nNote: Project 1 was a joint effor by me (Hanyan Cai) AND Bobby Buyalos. We worked extensively on the discussions of the theories behind Project 1: Prologue and its three parts. Project 2 and 3 was completed independently by me with discussions with Bobby"
  },
  {
    "objectID": "submitted/projects_2/caihanyan.html#part-1",
    "href": "submitted/projects_2/caihanyan.html#part-1",
    "title": "Modeling with Directed Graphs",
    "section": "Part 1",
    "text": "Part 1\nFor nodes \\(1, \\ldots, 6\\), let \\(x_1, \\ldots, x_6\\) represent the potentials for these nodes. Then, for an edge between node \\(i\\) and node \\(j,\\) we have that \\(x_j - x_i\\) represents the potential of that edge.\nLet’s do this for each edge in our graph:\n\\[\\begin{align*}\n    &x_2 - x_1 \\\\\n    &x_3 - x_2 \\\\\n    &x_4 - x_3\\\\\n    &x_2 - x_4\\\\\n    &x_4 - x_1\\\\\n    &x_1 - x_3\\\\\n    &x_6 - x_3\\\\\n    &x_3 - x_6 \\\\\n    &x_5 - x_4 \\\\\n    &x_6 - x_5\n\\end{align*}\\]\nWe can then put this in a matrix \\(A\\), and have that\nA =\n\\[\\begin{bmatrix}\n-1 & 1 & 0 & 0 & 0 & 0 \\\\\n0 & -1 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & -1 & 1 & 0 & 0 \\\\\n0 & 1 & 0 & -1 & 0 & 0 \\\\\n-1 & 0 & 0 & 1 & 0 & 0 \\\\\n1 & 0 & -1 & 0 & 0 & 0 \\\\\n0 & 0 & -1 & 0 & 0 & 1 \\\\\n0 & 0 & 1 & 0 & 0 & -1 \\\\\n0 & 0 & 0 & -1 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & -1 & 1 \\\\\n\\end{bmatrix}\\]\nx =\n\\[\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\nx_4 \\\\\nx_5 \\\\\nx_6 \\\\\n\\end{bmatrix}\\]\nb =\n\\[\\begin{bmatrix}\nx_2 - x_1 \\\\\nx_3 - x_2 \\\\\nx_4 - x_3 \\\\\nx_2 - x_4 \\\\\nx_4 - x_1 \\\\\nx_1 - x_3 \\\\\nx_6 - x_3 \\\\\nx_3 - x_6 \\\\\nx_5 - x_4 \\\\\nx_6 - x_5 \\\\\n\\end{bmatrix}\\]\nCall this system \\(Ax = b\\).\nNow, here is the crucial observation: \\(A^T\\) is the incidence matrix of our graph! The transpose is\n[\n\\[\\begin{bmatrix}\n-1 & 0 & 0 & 0 & -1 & 1 & 0 & 0 & 0 & 0 \\\\\n1 & -1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & -1 & 0 & 0 & -1 & -1 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & -1 & 1 & 0 & 0 & 0 & -1 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & -1 \\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\\\\n\\end{bmatrix}\\]\n]\nHere, every column represents an edge, and every row represents a node!\nLet \\(b = b_1, \\ldots, b_{10}\\). Our condition for b is that it is consistent with \\(Ax = b,\\) with our \\(A, x, b\\) defined above. If this is the case, then we know that there exists a \\(y\\in \\mathcal{N}(A^T)\\) such that \\(y\\cdot b = 0\\).\nSince \\(A^T\\) is the incidence matrix of our graph, then we have that \\(y\\) is a loop of our graph! And, the \\(i-th\\) edge of our graph is represented by the \\(i-th\\) column of \\(A^T\\) and the \\(i-th\\) row of \\(A\\).\nObserve that \\(y\\cdot b = y_1b_1 + \\ldots + y_{10}b_{10} = y_1(x_2 - x_1) + \\ldots + y_{10}(x_6 - x_5) = 0\\). But, we can think of this as \\(y_i\\) as the coefficient applied to the \\(i-th\\) edge, \\(b_i\\). So, we know we have a loop (since \\(y\\) is a loop) and the sums of our potentials on the loop y are zero (since \\(y \\cdot b = 0)\\), thus \\(b\\) is a vector of potential differences!\n\nVery nicely put!"
  },
  {
    "objectID": "submitted/projects_2/caihanyan.html#part-2",
    "href": "submitted/projects_2/caihanyan.html#part-2",
    "title": "Modeling with Directed Graphs",
    "section": "Part 2",
    "text": "Part 2\nHow should we characterize Kirchoff’s Loop Rule for Currents constraint? Let us first find a way to characterize the summation of each nodes. Recall our incidence matrix: \\(\\begin{bmatrix}\n-1.00 & -1.00 &  0.00 &  0.00 &  1.00 &  0.00 &  0.00 &  0.00 &  0.00 &  0.00\\\\\n  1.00 &  0.00 & -1.00 &  0.00 &  0.00 &  0.00 &  1.00 &  0.00 &  0.00 &  0.00\\\\\n  0.00 &  0.00 &  1.00 & -1.00 & -1.00 & -1.00 &  0.00 &  0.00 &  0.00 &  1.00\\\\\n  0.00 &  1.00 &  0.00 &  1.00 &  0.00 &  0.00 & -1.00 & -1.00 &  0.00 &  0.00\\\\\n  0.00 &  0.00 &  0.00 &  0.00 &  0.00 &  0.00 &  0.00 &  1.00 & -1.00 &  0.00\\\\\n  0.00 &  0.00 &  0.00 &  0.00 &  0.00 &  1.00 &  0.00 &  0.00 &  1.00 & -1.00\n\\end{bmatrix}\\).\nA given row corresponds to a given node. This node could be a resistor or some set junction of the circuit. For this row representing the node, \\(-1\\) represents an outgoing wire connection and \\(1\\) represents an ingoing wire connection. For example, consider row 1 which corresponds to node 1. We can see that there are two outgoing wire connections from node 1 to node 2 and node 4, while we have one incoming wire connection to node 1 from node 3.\nNow, consider the vector \\(d = \\begin{bmatrix} d_1 \\\\ \\vdots \\\\ d_i \\\\ \\vdots \\\\ d_n\\end{bmatrix}\\), where $d_i = $ weight of edge \\(i\\), $n = $ number of edges.\nThe matrix product of row 1 with \\(d\\) gives us the sum of all weighted incoming and outgoing edges. So in our example, we consider: \\(\\begin{bmatrix} -1.00 & -1.00 &  0.00 &  0.00 &  1.00 &  0.00 &  0.00 &  0.00 &  0.00 &  0.00 \\end{bmatrix} \\begin{bmatrix} d_1 \\\\ \\vdots \\\\ d_i \\\\ \\vdots \\\\ d_n\\end{bmatrix} = -d_1 -d_2 + d_4\\). Since we are viewing the weights for the edges as the current that corresponds to the edge (wire), this product gives us the total change in current through node 1! Kirchoff’s Loop Rule for Current states that the total flow of current through any node has to be 0, which is equivalent to the condition that the incidence matrix \\(A^T\\) times \\(d\\): \\(A^Td = 0\\)! Therefore, any weight vector \\(d\\) constructed from the definition above must be in the nullspace of \\(A^T\\). Wow! Therefore, let us recall that $N(A^T) = $ Span\\(\\left\\{\\begin{bmatrix}\n1 \\\\ -1 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\0 \\\\0\n\\end{bmatrix}, \\begin{bmatrix}\n1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\0\\\\0\\\\0\\end{bmatrix}, \\begin{bmatrix}\n-1 \\\\ 1\\\\0\\\\0\\\\0\\\\0\\\\1\\\\0\\\\0\\\\0 \\end{bmatrix}, \\begin{bmatrix} -1\\\\1\\\\-1\\\\0\\\\0\\\\-1\\\\0\\\\1\\\\1\\\\0\\end{bmatrix}, \\begin{bmatrix} 0\\\\0\\\\0\\\\0\\\\0\\\\1\\\\0\\\\0\\\\0\\\\1\\end{bmatrix}\\right\\}\\).\nSo the weight vector must be an element in this span in order for the Kirchoff’s Law to be satisfied. For example, using this requirement, any basis vector in the above span and their scalar products are valid weights. The vector \\(\\begin{bmatrix}\n1 \\\\ -1 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\0 \\\\0\n\\end{bmatrix} + \\begin{bmatrix}\n1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\0\\\\0\\\\0\\end{bmatrix} + \\begin{bmatrix}\n-1 \\\\ 1\\\\0\\\\0\\\\0\\\\0\\\\1\\\\0\\\\0\\\\0 \\end{bmatrix} + \\begin{bmatrix} -1\\\\1\\\\-1\\\\0\\\\0\\\\-1\\\\0\\\\1\\\\1\\\\0\\end{bmatrix} + \\begin{bmatrix} 0\\\\0\\\\0\\\\0\\\\0\\\\1\\\\0\\\\0\\\\0\\\\1\\end{bmatrix}\\)"
  },
  {
    "objectID": "submitted/projects_2/caihanyan.html#part-3",
    "href": "submitted/projects_2/caihanyan.html#part-3",
    "title": "Modeling with Directed Graphs",
    "section": "Part 3",
    "text": "Part 3\nWe need to first clarify what we mean when we talk about a basis of directed loops. Since the problem does not expand in detail about the definition, we will find a clever way to characterize this basis without breaking the rules. Suppose we have a digraph and we have \\(n\\) directed loops that form a basis. Let us denote this basis by \\(\\{b_1, \\dots, b_n\\}\\), where each \\(b_i\\) will be the directed loop vector that is linearly independent from \\(b_j\\), where \\(i,j \\in \\{1, \\dots, n\\}\\).\nNow, we aren’t given any more information about this, so we think it is fair to have some leeway in expanding on the definition of any \\(b_i\\). Since \\(b_i\\) is a directed loop, we claim that it is fair for us to know what edges this directed loop contains. For any \\(i \\in \\{1, \\dots, n\\}\\), \\(b_i = \\{ e_{i_1}, \\dots, e_{i_k}\\}\\) for some finite index sequence \\(i_1, \\dots, i_k\\), where \\(e_{i_l}\\) is some edge vector for the graph (Remember, an arbitrary edge vector for the graph is simply an arbitrary column vector in the incidence matrix of the graph). This is a fair expansion to the assumption that was given because we require no information other than the actual edges that compose the directed loop. It is also clear that the linear independence of the directed loops within the basis is preserved by our expansion of definition.\nNow, let node \\(a\\) and node \\(b\\) of the graph be given. We want to know whether these two nodes can communicate with each other, i.e, whether these two nodes are in a loop.\nConsider the set \\(D = \\{e \\text{ a column vector of the graph incidence matrix :} e \\in  b_i = \\{ e_{i_1}, \\dots, e_{i_k}\\} \\text{for some } i \\in \\{1, \\dots, n\\} \\}\\). The \\(Span(D)\\) is defined because these column vectors have to be linearly independent by zero-nonzero observations.\nDefine $e^* $ as the edge vector from node \\(a\\) to node \\(b\\). For example, if we have a graph with node \\(a\\), \\(b\\), \\(c\\) as row \\(1, 2, 3\\) of the incidence matrix respectively, \\(e^* = \\begin{bmatrix} -1 \\\\ 1 \\\\ 0  \\end{bmatrix}\\).\nTheorem: If \\(e^* \\in Span(D)\\) if and only if node \\(a\\) and node \\(b\\) is in a loop of the graph.\nA formal proof will not be given. A sketch is as follows:\nThe edge vector \\(e^*\\) denotes a path from node \\(a\\) to node \\(b\\). If \\(e^* \\in Span(D)\\), then the path from node \\(a\\) to node \\(b\\) exists by a combination of paths that lie in arbitrary number of loops in the graph. The path from node \\(b\\) to node \\(a\\) also has to exists as we simply multiply the linear combination of the paths in \\(D\\) that make up \\(e^*\\) by \\(-1\\), which gives us the edge vector from node \\(b\\) to node \\(a\\). Therefore, we have found a loop that connects node \\(a\\) and node \\(b\\).\nNow, suppose node \\(a\\) and node \\(b\\) is in a loop of the graph. Then there exists a linear combination of directed loop basis elements that equals the loop that makes up node \\(a\\) and node \\(b\\). It quickly follows that there is a linear combination of elements in \\(D\\) that equals both the path from node \\(a\\) to node \\(b\\) and the path from node \\(b\\) to node \\(a\\).\nTherefore, we have found a way to determine whether two nodes can communicate with each other. That is, they can talk with each other as long as a path between the two nodes is in the column space of \\(D\\). To determine this, we simply build \\(D\\) matrix and solve for the system \\(Dx = b\\), where \\(b\\) denotes the incidence column vector/edge vector that denotes a path between the two nodes. If there is a solution, then the two nodes can talk with each other. If not, then they cannot.\nLet us test this theorem with a real example.\n\nV = [1, 2, 3, 4, 5, 6, 7]\nE = [(1, 2), (2, 3), (3,1), (1,4), (4,5), (5,1), (6,7)]\n\n#construct graph\nG = nx.DiGraph()\nG.add_nodes_from(V)\nG.add_edges_from(E)\n\n# Draw the graph\nnx.draw_circular(G, with_labels=True)\nplt.show()\n\n\n\n\n\n\n\n\nNow, we can create a basis of directed loops for this graph in the following way:\n\\[\n\\left\\{\n\\left\\{\n\\begin{pmatrix}\n-1 \\\\\n1 \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n0\n\\end{pmatrix},\n\\begin{pmatrix}\n0 \\\\\n-1 \\\\\n1 \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n0\n\\end{pmatrix},\n\\begin{pmatrix}\n1 \\\\\n0 \\\\\n-1 \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n0\n\\end{pmatrix}\n\\right\\},\n\\left\\{\n\\begin{pmatrix}\n-1 \\\\\n0 \\\\\n0 \\\\\n1 \\\\\n0 \\\\\n0 \\\\\n0\n\\end{pmatrix},\n\\begin{pmatrix}\n0 \\\\\n0 \\\\\n0 \\\\\n-1 \\\\\n1 \\\\\n0 \\\\\n0\n\\end{pmatrix},\n\\begin{pmatrix}\n1 \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n-1 \\\\\n0 \\\\\n0  \n\\end{pmatrix}\n\\right\\}\n\\right\\}\n\\]\nThis will produce the vector enclosed in curly braces.\nNow, we want to see if this works! Lets first test if our method will work. Lets see if it can detect if node 5 and 2 are connected.\nSo, our first step is to consider the column space of all of the vectors within our sets in our basis of directed loops. We can visualize this with a matrix:\n\\[\n\\begin{pmatrix}\n-1 & 0 & 1 & -1 & 0 & 1 \\\\\n1 & -1 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & -1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & -1 & 0 \\\\\n0 & 0 & 0 & 0 & 1 & -1 \\\\\n0 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 0 \\\\\n\\end{pmatrix}\n\\]\nLets put this as a numpy array. We want to consider the vector from 2 to 5, which looks like \\[\\begin{pmatrix}\n0 \\\\\n-1 \\\\\n0 \\\\\n0 \\\\\n1 \\\\\n0 \\\\\n0  \n\\end{pmatrix} \\].\n\nIt took me a while to understand what you were doing here. You were making a basis not of directed loops, but of edges which are part of directed loops. Nice.\n\nWe want to test if this is in the column space of our matrix! We can check this by solving Ax = b.\nTo do this, we will use a least squares solver and show that this produces a geniune solution to Ax = b, so we know that b is in the column space of A.\n\nI reproduced your A as inc*sp.diagonalize_vector(inc.nullspace()[1]+inc.nullspace()[0]) where inc is the incidence matrix (in sympy)\n\n::: {#09a751bc .cell execution_count=5}\n``` {.python .cell-code}\nA = np.array([\n    [-1, 0, 1, -1, 0, 1],\n    [1, -1, 0, 0, 0, 0],\n    [0, 1, -1, 0, 0, 0],\n    [0, 0, 0, 1, -1, 0],\n    [0, 0, 0, 0, 1, -1],\n    [0, 0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0, 0]\n])\n\nb= np.array([\n    [0],\n    [-1],\n    [0],\n    [0],\n    [1],\n    [0],\n    [0]\n])\n\nx, residuals, _, _ = np.linalg.lstsq(A, b, rcond=None)\n\n\nprint(\"Does Ax = b:\", np.allclose(A@x, b))\n\n\nDoes Ax = b: True\n\n\nNow lets make sure that is doesn’t work when it should not! Lets show that it will not detect a connection between node 7 and 1. Lets see if the following vector is in the column space of the matrix from above. \\[\\begin{pmatrix}\n1 \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n-1  \n\\end{pmatrix} \\]\nTo do this, we will use a least squares solver and show that this will produce a non-genuine solution, i.e. Ax = b is not consistent:\n\nA = np.array([\n    [-1, 0, 1, -1, 0, 1],\n    [1, -1, 0, 0, 0, 0],\n    [0, 1, -1, 0, 0, 0],\n    [0, 0, 0, 1, -1, 0],\n    [0, 0, 0, 0, 1, -1],\n    [0, 0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0, 0]\n])\n\nb = np.array([\n    [1],\n    [0],\n    [0],\n    [0],\n    [1],\n    [0],\n    [-1]\n])\n\nx, residuals, _, _ = np.linalg.lstsq(A, b, rcond=None)\n\n\nprint(\"Does Ax = b:\", np.allclose(A@x, b))\n\nDoes Ax = b: False\n\n\nTherefore, we see that our method is working! In this way, give a basis of directed loops, our theorem gives us an algorithms that we can use to know if any two nodes are connected in a loop!\nA question for future exploration may be: how can one efficiently obtain such a basis, especially for graphs with a large number of nodes (can\n\nVery well done. I hadn’t thought of using the least squares method.\nHere is a way to do this a bit more systematically: once you have your incidence matrix corresponding only to edges which are in directed loops, square it to find an adjacency matrix. Then remove the diagonal elements (so you don’t have self-loops. Finally, you can see which nodes are connected by taking powers of this adjacency matrix – if you go up to like the matrix ^ 10 and there is a nonzero value in {i,j}, that means that eventually you can get from i to j via directed loops.)\nGrade: E"
  },
  {
    "objectID": "submitted/projects_2/Project2_hanyan.html",
    "href": "submitted/projects_2/Project2_hanyan.html",
    "title": "Project2",
    "section": "",
    "text": "For this project, we will use a R to process our data. But before we do, we need to first use Photoshop to double check the property of the image. We can see that it is already in gray scale with 8 bits per channel. Therefore, we do not have to do anything.\n\nNow, let us use R to convert this image into matrix form. I am going to use the R package imager to process this image into a matrix. Note that I will be importing the image below locally. Therefore, if you wish to have the code running, you will have to download the image below and change the path to the image in the code below.\n\n\n# Importing the glue library for fstring equivalent in R\nif (!requireNamespace(\"glue\", quietly = TRUE)) {\n  install.packages(\"glue\")\n}\nlibrary(glue)\n\n# Importing the imager library\nif (!requireNamespace(\"imager\", quietly = TRUE)) {\n  install.packages(\"imager\")\n}\nlibrary(imager)\n\nLoading required package: magrittr\n\n\n\nAttaching package: 'imager'\n\n\nThe following object is masked from 'package:magrittr':\n\n    add\n\n\nThe following objects are masked from 'package:stats':\n\n    convolve, spectrum\n\n\nThe following object is masked from 'package:graphics':\n\n    frame\n\n\nThe following object is masked from 'package:base':\n\n    save.image\n\n# Importing the location of the image and the actual image\n\n# imageFile &lt;- glue('OriginalFeyman.png')\nimage = load.image('/Users/kendra/LinearAlg/LinearAlgLectures/submitted/projects_2/OriginalFeyman.png')\n\n\"image = matrix(0, nrow=10, ncol=10)\nimage[2,5] = 1\nimage[2,4:6] = 1\nimage[4,3:7]\nimage\"\n\n[1] \"image = matrix(0, nrow=10, ncol=10)\\nimage[2,5] = 1\\nimage[2,4:6] = 1\\nimage[4,3:7]\\nimage\"\n\n# Checking the dimensions of the imported file\ndim(image)\n\n[1]  691 1000    1    3\n\n\nNow, we can see from the output that there are three color channels. This seems to be because imager still assumes that all imported images have three color channels, even though our Feyman picture is gray scale. Therefore, we will have to use the imager method to convert to gray scale.\n\n# Converting to grayscale\ngray &lt;- grayscale(image)\n\n# Plotting the picture\nplot(gray)\n\n\n\n\n\n\n\n\nGreat, we should be able to see the imported image as a plot. Now, let us convert this picture into its pixel matrix.\n\n# Convertion to the pixel matrix\ngrayMatrix &lt;- as.matrix(gray)\n\n# Checking to see if the conversion back to picture from the pixel matrix yields the same image.\ntestImg &lt;- as.cimg(grayMatrix)\nplot(testImg)\n\n\n\n\n\n\n\n\nWe can see that the pixel matrix imager gives us contains values between 0 and 1. This is because imager works with normalized values. We can replace these normalized values by the product of these values by 255 to get the original gray scale values.\n\n# Unnormalizing the pixel matrix\nunnormalizedGrayMatrix &lt;- grayMatrix\n\n\n\n\nNow, let us get to the important part of constructing the Haar Wavelet Transform. To do so, we first should make sure that the dimensions of the pixel matrix is even.\n\n# We want to make the pixel matrix have even rows and columns.\n\n# From ChatGBT we get a function to correct the pixel matrix.\n# Function to check and remove the first row or column based on odd counts\nremove_odd_row_or_column &lt;- function(matrix_input) {\n  # Get the number of rows and columns\n  num_rows &lt;- nrow(matrix_input)\n  num_cols &lt;- ncol(matrix_input)\n  \n  # Check if the number of rows is odd\n  if (num_rows %% 2 == 1) {\n    # Remove the first row if odd\n    matrix_output &lt;- matrix_input[-1, ]\n  } else if (num_cols %% 2 == 1) {\n    # Remove the first column if odd\n    matrix_output &lt;- matrix_input[, -1]\n  } else {\n    # If neither is odd, return the original matrix\n    matrix_output &lt;- matrix_input\n  }\n  \n  return(matrix_output)\n}\n\n# Now making our matrix even dimensional\n\"Dimension of the pixel matrix before correction:\" \n\n[1] \"Dimension of the pixel matrix before correction:\"\n\ndim(unnormalizedGrayMatrix)\n\n[1]  691 1000\n\n# Calling the function\nevenMatrix &lt;- remove_odd_row_or_column(unnormalizedGrayMatrix)\n\n\"Dimension fo the pixel matrix after correction:\"\n\n[1] \"Dimension fo the pixel matrix after correction:\"\n\n# Below is code to descale the image for testing.\n\"evenMatrix &lt;- remove_odd_row_or_column(evenMatrix[seq(1,nrow(evenMatrix),32),seq(1,ncol(evenMatrix),32)])\n\ndim(evenMatrix)\"\n\n[1] \"evenMatrix &lt;- remove_odd_row_or_column(evenMatrix[seq(1,nrow(evenMatrix),32),seq(1,ncol(evenMatrix),32)])\\n\\ndim(evenMatrix)\"\n\n\nNow we can begin to construct the Haar Wavelet Transform Matrix. \n\n# The Haar Wavelet Transform matrix is a square matrix and corresponds to the number of rows of the pixel matrix. I am creating a function that finds the Haar Wavelet Transform matrix for any arbitrary dimension.\n\nhaarGet &lt;- function(dimension) {\n  haarMatrix &lt;- matrix(NA, nrow = dimension, ncol = dimension)\n\n  # Creating a dummy row that will be changed and added to the haarMatrix\n  rowDummy &lt;- rep(0, dimension)\n  \n  # Iterating over the rows. We will add each row accordingly\n  for (row in 1:dimension){\n    changedRow &lt;- rowDummy\n    \n    # Getting the upper matrix U correct\n    if (row &lt;= dimension / 2){\n      changedRow[c((2 * row) - 1, 2 * row)] &lt;- 1\n      haarMatrix[row,] &lt;- changedRow\n    }\n    # Getting the lower matrix L right\n    if (row &gt; dimension / 2){\n      changedRow[2 * row - dimension] &lt;- 1\n      changedRow[2 * row - 1 - dimension] &lt;- -1\n      haarMatrix[row, ] &lt;- changedRow\n    }\n  }\n  \n  # Get the true matrix which includes the normalization square root of 2 over 2 term\n  haarMatrix &lt;- (sqrt(2) / 2) * haarMatrix\n  \n  return(haarMatrix)\n}\n\n\n\n\nWith our Haar Wavelet Transform Matrix function ready, we can try performing one instance of the Wavelet Transform.\n\n\n\n\n\n\n# First encoding the dimension of the corrected pixel matrix\ndimension &lt;- dim(evenMatrix)\nnumRow &lt;- dimension[1]\nnumCol &lt;- dimension[2]\n\n# The dimension of the W_m in the picture above corresponds to the number of rows of the pixel matrix\nW_m &lt;- haarGet(numRow)\n\n# Likewise, the dimension of W_n in the picture above corresponds to the number of columns of the pixel matrix. Then we get its transpose\nW_n &lt;- haarGet(numCol)\nW_nT &lt;- t(W_n)\n\n# Get the resulting product\nresultMatrix &lt;- (W_m %*% evenMatrix %*% W_nT) / 2\ndim(resultMatrix)\n\n[1]  690 1000\n\n\nOk, we are doing good here. We know the following:The dimensions of \\(B, V, H, D\\) are \\(\\frac{1}{2} (\\text{row dimension})\\times \\frac{1}{2} (\\text{column dimension})\\).\nTherefore, we can extract these matrices out of our result matrix.\n\nresultMatrix &lt;- resultMatrix\nresult &lt;- as.cimg(resultMatrix) \nplot(result)\n\n\n\n\n\n\n\nB &lt;- resultMatrix[1:(numRow / 2), 1:(numCol / 2)]\nV &lt;- resultMatrix[1:(numRow / 2), (numCol / 2 + 1):numCol]\nD &lt;- resultMatrix[(numRow / 2 + 1):numRow, (numCol / 2 + 1):numCol]\nH &lt;- resultMatrix[(numRow / 2 + 1):numRow, 1:(numCol / 2)]\n\ntestB &lt;- as.cimg(B)\nplot(testB, main=\"B Matrix\")\n\n\n\n\n\n\n\ntestV &lt;- as.cimg(V)\nplot(testV, main=\"V Matrix\")\n\n\n\n\n\n\n\ntestD &lt;- as.cimg(D)\nplot(testD, main=\"D Matrix\")\n\n\n\n\n\n\n\ntestH &lt;- as.cimg(H)\nplot(testH, main=\"H Matrix\")\n\n\n\n\n\n\n\n\nThe first plot is the image converted from the pixel matrix \\(\\begin{bmatrix} B & V \\\\ H & D \\end{bmatrix}.\\) The second plot is the image converted from the pixel matrix B. The third is V, fourth is D, and fifth is H.\n\n\n\nNow, we can construct a function that automatically applies the Haar Wavelet Transform to a give pixel matrix. With this function, we can then see how much compression we can do before Feyman becomes too blurry :(.\nFirst, we will create a function called HaarCompression that takes in a normalized pixel matrix and outputs the resulting pixel matrix after the Haar Wavelet Transform along with B, V, D, and H.\n\nHaarCompression &lt;- function(pixelMatrix){\n  # Correcting to the correct dimensions\n  evenMatrix &lt;- remove_odd_row_or_column(pixelMatrix)\n  \n  dimension &lt;- dim(evenMatrix)\n  numRow &lt;- dimension[1]\n  numCol &lt;- dimension[2]\n  \n  # The dimension of the W_m in the picture above corresponds to the number of rows of the pixel matrix\n  W_m &lt;- haarGet(numRow)\n\n  # Likewise, the dimension of W_n in the picture above corresponds to the number of columns of the pixel matrix. Then we get its transpose\n  W_n &lt;- haarGet(numCol)\n  W_nT &lt;- t(W_n)\n  \n  # Get the resulting product\n  resultMatrix &lt;- (W_m %*% evenMatrix %*% W_nT) / 2\n  \n  # Get the four matrices\n  B &lt;- resultMatrix[1:(numRow / 2), 1:(numCol / 2)]\n  V &lt;- resultMatrix[1:(numRow / 2), (numCol / 2 + 1):numCol]\n  D &lt;- resultMatrix[(numRow / 2 + 1):numRow, (numCol / 2 + 1):numCol]\n  H &lt;- resultMatrix[(numRow / 2 + 1):numRow, 1:(numCol / 2)]\n  \n  # Getting the output ready. This function will output a list containing the original matrix and the five partitioned matrices.\n  return(list(HaarMatrix = resultMatrix, B = B, V = V, D = D, H = H))\n}\n\n# Testing with Feyman\ntest = HaarCompression(grayMatrix)\nplot(as.cimg(test$HaarMatrix))\n\n\n\n\n\n\n\nplot(as.cimg(test$B))\n\n\n\n\n\n\n\nplot(as.cimg(test$V))\n\n\n\n\n\n\n\nplot(as.cimg(test$D))\n\n\n\n\n\n\n\nplot(as.cimg(test$H))\n\n\n\n\n\n\n\n\nWe can see that our HaarCompression function works as intended. Now we can create a new function that applies our HaarCompression function repeatedly.\n\nIterativeHaarCompression &lt;- function(image, numRuns){\n  # Converting the image first to gray scale and then to the normalized pixel matrix and to make the dimensions even.\n  gray &lt;- grayscale(image)\n  pixelMatrix &lt;- as.matrix(gray)\n  startingMatrix &lt;- remove_odd_row_or_column(pixelMatrix)\n\n  # Initializing the selected matrix\n  selectedMatrix &lt;- startingMatrix\n  for (run in 1:numRuns){\n    resultList &lt;- HaarCompression(selectedMatrix)\n    selectedMatrix &lt;- resultList$B\n  }\n  \n  compressedImage &lt;- as.cimg(selectedMatrix)\n  return(compressedImage)\n}\n\ntest &lt;- IterativeHaarCompression(image, 4)\nplot(test, main=\"Feyman after 4 compressions.\")\n\n\n\n\n\n\n\n\nLet us view the results.\n\nfor(runs in 1:8){\n  img &lt;- IterativeHaarCompression(image, runs)\n  plot(img, main=glue(\"Feyman after {runs} compressions.\"))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt is reasonable to conclude that after 4 compressions, Feyman is still acceptable. If one knows Feyman very well, the portrait after 5 compression may even be deemed acceptable. But after that, the portrait clearly becomes unrecognizable.\n\nI think even the 4 compression Feynman is pretty sad! I can’t tell who it is, only the rough time period that the person is from…\n\nEvery compression reduces the totally number of elements in the pixel matrix by 4. Therefore, if we deem that 4 compressions for Feyman is maximum, then the approximate amount of reduction of pixels will be a factor of \\(4^4 = 256\\). Feyman started with \\(690000\\) pixel elements (after we do the even correction). So after four compressions we are left with around \\(2695\\) pixels. I am only saying around because every compression requires an even correction, which may decrease the number of pixels further. If we view the number of pixels as proportional to the size of the image, then we have reduced the size of Feyman by a factor of \\(256\\)! But of course, the size of the image is not linearly proportional to the number of pixels. Let us try saving the 4-times compressed photo to our desktop and compare the size of this photo to our original.\n\noriginalFeyman = image\n#save.image(originalFeyman, glue(\"{path}/originalFeyman.png\"))\n\nfourTimesCompressedFeyman = IterativeHaarCompression(image, 4)\n#save.image(fourTimesCompressedFeyman, glue(\"{path}/fourTimesCompressedFeyman.png\"))\n\n\nWow, 290 times smaller! But maybe four times compressed Feyman is still a little too blurry. 2 compressions will give us a recognizable Feyman and the size of the photo will still decrease by more than a factor of 16.\nAlso, we don’t only have the \\(B\\) matrix after each compression. What can we do about the three edges matrices?\n\n\n\nLet us look at the Haar Wavelet Transform Algorithm another time: \\[\n\\begin{equation}\nW_m A W_n^T = 2\\begin{bmatrix}  B & V \\\\ H & D  \\end{bmatrix},\\\\\n\\Rightarrow W_m^T WA W_n^T W_n = 2 W_m^T \\begin{bmatrix}  B & V \\\\ H & D  \\end{bmatrix}W_n, \\\\\n\\Rightarrow A = 2 W_m^T \\begin{bmatrix}  B & V \\\\ H & D  \\end{bmatrix}W_n ,\n\\end{equation}\n\\]\nwhere the third equation follows from the fact that the inverse of an orthogonal matrix is the transpose of the matrix. This means that we can reconstruct the original image from the compressed matrix. If we don’t know the dimension of the Haar Wavelet Transform matrices \\(W_m\\) and \\(W_n\\), we can simply work backwards to determine their dimensions by looking at the dimension of \\(B\\).\n\n# Let us test this hypothesis. \n\nW_mT &lt;- t(W_m)\n\ntest &lt;- HaarCompression(grayMatrix)\nresultMatrix &lt;- test$HaarMatrix\n\noriginalPixelMatrix &lt;- 2 * W_mT %*% resultMatrix %*% W_n\nplot(as.cimg(originalPixelMatrix))\n\n\n\n\n\n\n\n\nGreat, the idea is valid. So, let us work on a prototype compression idea. We can compress an image as many times until it is still acceptable. Then, we compress the image again to make it unacceptable and record the resulting compression matrix with the \\(B,V,H,D\\) matrices. If we can find a way to store \\(V,H,D\\) in an efficient manner by adding some kind of storing technique that only stores information about the location of the edges, then we can decrease the size of \\(V,H,D\\). To accomplish this, we will be dealing using sparse matrices. We can construct a reconstruction algorithm to find the correct Haar Wavelet Transform Matrices and reconstruct the unacceptable image resolution to an acceptable resolution. If we can do this, we basically managed to decrease the size of a compressed image even further by reducing the number of pixels by another factor of 4 but with some added edge information that needs to be stored. To take this further, maybe if we store the compressed \\(V,H,D\\) information every time we compressed, we may even be able to get a rough reconstruction of the original image from a many-times compressed image!\nBut let us not get too far ahead of ourselves. First, we need to design an algorithm that compresses the three edge matrices in a suitable way. We need a detection algorithm and a reconstruction algorithm.\nWe first need to see what is going on. I have defined a modified function below that gives us the list of the five matrices. Then, I am going to plot the matrix V to see what values the edges will represent.\n\nIterativeHaarCompressionList &lt;- function(image, numRuns){\n  # Converting the image first to gray scale and then to the normalized pixel matrix and to make the dimensions even.\n  gray &lt;- grayscale(image)\n  pixelMatrix &lt;- as.matrix(gray)\n  startingMatrix &lt;- remove_odd_row_or_column(pixelMatrix)\n\n  # Initializing the selected matrix\n  selectedMatrix &lt;- startingMatrix\n  list &lt;- HaarCompression(startingMatrix)\n\n  for (run in 1:numRuns){\n    resultList &lt;- HaarCompression(selectedMatrix)\n    selectedMatrix &lt;- resultList$B\n    list &lt;- resultList\n  }\n  \n  return(list)\n}\n\nlist &lt;- IterativeHaarCompressionList(image, 5)\nB &lt;- list$B\nV &lt;- list$V\nD &lt;- list$D\nH &lt;- list$H\nlibrary(plot.matrix)\n\n# Adjust margins to ensure the color indicator is not cut off\npar(mar = c(5, 4, 4, 7))  # Increase right margin to allow space for the color indicator\n\n# Plot the matrix with `plot.matrix`\nplot(abs(V), main = \"V\")\n\n\n\n\n\n\n\nplot(abs(D), main = \"D\")\n\n\n\n\n\n\n\nplot(abs(H), main = \"H\")\n\n\n\n\n\n\n\n\nHere, I am plotting the edge matrices with their absolute values to get a clearer picture. Now, let us create a function that get an input matrix with a threshold value and outputs the matrix keeping the elements that have absolute values equal to or greater than the threshold value.\nCareful, note that the scale of the values for these matrices are different. Therefore, the threshold value will be defined as the percentage (converted to decimal) of the maximum value of the matrix. To efficiently store the returned matrix, we will use the package Matrix and store the matrix as a sparse matrix. This will allow for more efficient storage.\n\nif (!requireNamespace(\"Matrix\", quietly = TRUE)) {\n  install.packages(\"Matrix\")\n}\nlibrary(Matrix)\n\nzeroCompression &lt;- function(matrix, thresholdVal){\n  dimension &lt;- dim(matrix)\n  numRow &lt;- dimension[1]\n  numCol &lt;- dimension[2]\n  \n  maxVal &lt;- abs(max(matrix))\n  threshold &lt;- maxVal * thresholdVal\n  \n  for (row in 1:numRow){\n    for (col in 1: numCol){\n      if (abs(matrix[row, col]) &lt; threshold){\n        matrix[row, col] &lt;- 0\n      }\n    }\n  }\n  \n  return(as(matrix, \"sparseMatrix\"))\n}\n\nplot(as.matrix(zeroCompression(D, 0.7)))\n\n\n\n\n\n\n\nplot(as.matrix(zeroCompression(D, 0.5)))\n\n\n\n\n\n\n\n\nWe can see that our algorithm works. Let us create a reconstruction function that takes the four matrices and reconstructs the image.\n\nimageReconstruction &lt;- function(B, V, D, H){\n  # Combine B, V, D, H to form the large matrix A\n  top &lt;- cbind(B, V)\n  bottom &lt;- cbind(H, D)\n  largeMatrix &lt;- rbind(top, bottom)\n  \n  dimension &lt;- dim(B)\n  numRow &lt;- 2 * dimension[1]\n  numCol &lt;- 2 * dimension[2]\n  \n  W_mT = t(haarGet(numRow))\n  W_n = haarGet(numCol)\n  \n  originalPixelMatrix &lt;- 2 * W_mT %*% largeMatrix %*% W_n\n  \n  return(as.cimg(originalPixelMatrix))\n}\n\nplot(IterativeHaarCompression(image, 4), main=\"After 4 Compressions\")\n\n\n\n\n\n\n\nplot(imageReconstruction(B, V, D, H), main=\"Reconstruction from 5 Compressions\")\n\n\n\n\n\n\n\npercentage &lt;- 0.5\ncompressedV &lt;- as.matrix(zeroCompression(V, percentage))\ncompressedD &lt;- as.matrix(zeroCompression(D, percentage))\ncompressedH &lt;- as.matrix(zeroCompression(H, percentage))\nplot(imageReconstruction(B, compressedV, compressedD, compressedH), main=\"Reconstruction from 5 Compressions \\n Added Compression of V, D, H \\n Threshold of 50% of max value.\")\n\n\n\n\n\n\n\nplot(IterativeHaarCompression(image, 5), main=\"After 5 Compressions\")\n\n\n\n\n\n\n\n\nThis is super cool! Comparing the third plot to the fourth plot, we can conclude that the reconstruction with the compressed edge matrices is definetly higher in quality that the imag after 5 compressions. Let us see how the reconstruction changes as we vary the threshold values.\n\nfor(percentage in seq(0, 1, by=0.1)){\n  compressedV &lt;- zeroCompression(V, percentage)\n  compressedD &lt;- zeroCompression(D, percentage)\n  compressedH &lt;- zeroCompression(H, percentage)\n  actualPercentage &lt;- percentage * 100\n  \n  plot(imageReconstruction(B, as.matrix(compressedV), as.matrix(compressedD), as.matrix(compressedH)), main=glue(\"Reconstruction from 5 Compressions \\n Added Compression of V, D, H \\n Threshold of {actualPercentage}% of max value \\n {nnzero(compressedV) + nnzero(compressedD) + nnzero(compressedH)} nonzero entries (sum of non-zero entries of V, D, H).\"))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see that the quality of reconstruction decreases as we increase the threshold value. Naturally, the number of nonzero entries of V, D, H decreases as well. We may say that the number of nonzero entries of V, D, H determine the quality of the reconstruction.\n\n\n\nLet us set the threshold value/percentage to 30% of the maximum value and create the entire image compression algorithm function! This function will have an input of an image and a number denoting how many factors less should the size of the compressed image be compared to the original image. This function will then output a list of the B, V, D, H for storage after the appropriate number of Haar Wavelet Transforms and the compressions of V, D, H matrices. Another function, the image reader will be used when we want to convert the list back to an image. Let us call this compression the “combined compression”.\n\nHanyanImageCompression &lt;- function(image, sizeFactor){\n  # Calculating first roughly the number of compressions that we need to get closer to the size factor.\n  numberOfCompressionsRough &lt;- log(sizeFactor, 4)\n  numberOfCompressions &lt;- round(numberOfCompressionsRough)\n  \n  # Getting the list of B, V, D, H matrices\n  matrixList &lt;- IterativeHaarCompressionList(image, numberOfCompressions)\n  \n  # Setting the percentage threshold and compressing V, D, H\n  percentage &lt;- 0.3\n  \n  compressedV &lt;- zeroCompression(matrixList$V, percentage)\n  compressedD &lt;- zeroCompression(matrixList$D, percentage)\n  compressedH &lt;- zeroCompression(matrixList$H, percentage)\n  \n  return(list(B = matrixList$B, D = compressedD, V = compressedV, H = compressedH, numberOfCompressions = numberOfCompressions))\n  \n}\n\n# We have to rewrite the imageReconstruction function so that it takes in the list that the HanyanImageCompression returns.\nimageReader &lt;- function(list){\n  B &lt;- list$B\n  V &lt;- as.matrix(list$V)\n  D &lt;- as.matrix(list$D)\n  H &lt;- as.matrix(list$H)\n  \n  return(imageReconstruction(B, V, D, H))\n  \n}\n\n# Simple testing\ntestCompress &lt;- HanyanImageCompression(image, 100)\nplot(imageReader(testCompress))\n\n\n\n\n\n\n\n\nGreat, the two functions work. Now we can check how well the combined compression method works.\n\ntest2Compress &lt;- HanyanImageCompression(image, 300)\nHanyanImage &lt;- imageReader(test2Compress)\nplot(HanyanImage, main=glue(\"Combined Compression with {test2Compress$numberOfCompressions} Haar Wavelet Compressions\"))\n\n\n\n\n\n\n\n#save.image(HanyanImage, glue(\"{path}/HanyanCompression.png\"))\n\nhaarCompress &lt;- IterativeHaarCompressionList(image, test2Compress$numberOfCompressions - 1)\nplot(as.cimg(haarCompress$B), main=glue(\"{test2Compress$numberOfCompressions - 1} Haar Wavelet Compressions\"))\n\n\n\n\n\n\n\n#save.image(as.cimg(haarCompress$B), glue(\"{path}/{test2Compress$numberOfCompressions - 1}HaarWaveletCompressions.png\"))\n\nWow! The resulting image doesn’t seem to change much, at least when viewed from a distance.\n\n\n\nWe can see that our combined compression achieves a similar result to the three-times Haar Wavelet Compressed image with even smaller size!\n\n\nIf we zoomed in to the two iamges, however, we will see that the combined compression result looks more pixelated than compression with only the Haar Wavelet Transform. This makes sense, as our added compression of V, D, H makes us loss a lot of “intermittent” information between the pixels that we keep and the pixels that we set to 0.\n\n# Let us check the sizes of the data files that we store.\n#save(test2Compress, file = glue(\"{path}/test2Compress.RData\"))\n\"The size of the saved combined compression file (bytes): \"\n\n[1] \"The size of the saved combined compression file (bytes): \"\n\n#print(file.info(glue(\"{path}/test2Compress.RData\"))$size)\n\nsaveMatrix &lt;- haarCompress$B\n\"The size of the Haar Wavelet Compression file (bytes): \"\n\n[1] \"The size of the Haar Wavelet Compression file (bytes): \"\n\n#save(saveMatrix, file = glue(\"{path}/haarCompress.RData\"))\n#print(file.info(glue(\"{path}/haarCompress.RData\"))$size)\n\nWonderful, we can see how we can squeeze more compression of out the originally Haar Wavelet compressed image to reduce the size of the stored information about the image even further by compromising on a further reduced quality of the photo.\n\n\n\nLet us try this compression method on other images.\nHere is a painting named Las Meninas by Diego Velázquez. Unfortunately I have to crop it to make the compression effects are more visible (sorry!)\n\n\nLasMeninasLocation &lt;- glue('{path}/LasMeninas.jpg')\nLasMeninas = load.image(LasMeninasLocation)\nLasMeninasGray &lt;- grayscale(LasMeninas)\nplot(LasMeninasGray)\n\nLasMeninasCombinedCompress &lt;- HanyanImageCompression(LasMeninas, 100)\nHanyanImage &lt;- imageReader(LasMeninasCombinedCompress)\nplot(HanyanImage, main=glue(\"Combined Compression with {test2Compress$numberOfCompressions} Haar Wavelet Compressions\"))\nsave.image(HanyanImage, glue(\"{path}/HanyanCompressionLasMeninas.png\"))\n\nhaarCompress &lt;- IterativeHaarCompressionList(LasMeninas, LasMeninasCombinedCompress$numberOfCompressions - 1)\nplot(as.cimg(haarCompress$B), main=glue(\"{LasMeninasCombinedCompress$numberOfCompressions - 1} Haar Wavelet Compressions\"))\nsave.image(as.cimg(haarCompress$B), glue(\"{path}/{LasMeninasCombinedCompress$numberOfCompressions - 1}HaarWaveletCompressionsLasMeninas.png\"))\n\nThe sizes of the images are shown below.\n\nWe can see the shortcomings of the combined compression technique compared to the Haar Wavelet Compression. Although we can save more space using the combined compression, for images other than portraits where there are many edges, information loss becomes a lot more noticeable. If you open the HanyanCompressionLasMeninas image, it might even look a little unnerving due to the faces becoming pixelated.\n\nThis is amazing. It’s really neat to see how well the compression works, how you can recover so much from even the compressed edges.\nI’m enjoying seeing your thought process as you work through these projects!\nGrade: E"
  },
  {
    "objectID": "submitted/projects_2/Project2_hanyan.html#image-import",
    "href": "submitted/projects_2/Project2_hanyan.html#image-import",
    "title": "Project2",
    "section": "",
    "text": "For this project, we will use a R to process our data. But before we do, we need to first use Photoshop to double check the property of the image. We can see that it is already in gray scale with 8 bits per channel. Therefore, we do not have to do anything.\n\nNow, let us use R to convert this image into matrix form. I am going to use the R package imager to process this image into a matrix. Note that I will be importing the image below locally. Therefore, if you wish to have the code running, you will have to download the image below and change the path to the image in the code below.\n\n\n# Importing the glue library for fstring equivalent in R\nif (!requireNamespace(\"glue\", quietly = TRUE)) {\n  install.packages(\"glue\")\n}\nlibrary(glue)\n\n# Importing the imager library\nif (!requireNamespace(\"imager\", quietly = TRUE)) {\n  install.packages(\"imager\")\n}\nlibrary(imager)\n\nLoading required package: magrittr\n\n\n\nAttaching package: 'imager'\n\n\nThe following object is masked from 'package:magrittr':\n\n    add\n\n\nThe following objects are masked from 'package:stats':\n\n    convolve, spectrum\n\n\nThe following object is masked from 'package:graphics':\n\n    frame\n\n\nThe following object is masked from 'package:base':\n\n    save.image\n\n# Importing the location of the image and the actual image\n\n# imageFile &lt;- glue('OriginalFeyman.png')\nimage = load.image('/Users/kendra/LinearAlg/LinearAlgLectures/submitted/projects_2/OriginalFeyman.png')\n\n\"image = matrix(0, nrow=10, ncol=10)\nimage[2,5] = 1\nimage[2,4:6] = 1\nimage[4,3:7]\nimage\"\n\n[1] \"image = matrix(0, nrow=10, ncol=10)\\nimage[2,5] = 1\\nimage[2,4:6] = 1\\nimage[4,3:7]\\nimage\"\n\n# Checking the dimensions of the imported file\ndim(image)\n\n[1]  691 1000    1    3\n\n\nNow, we can see from the output that there are three color channels. This seems to be because imager still assumes that all imported images have three color channels, even though our Feyman picture is gray scale. Therefore, we will have to use the imager method to convert to gray scale.\n\n# Converting to grayscale\ngray &lt;- grayscale(image)\n\n# Plotting the picture\nplot(gray)\n\n\n\n\n\n\n\n\nGreat, we should be able to see the imported image as a plot. Now, let us convert this picture into its pixel matrix.\n\n# Convertion to the pixel matrix\ngrayMatrix &lt;- as.matrix(gray)\n\n# Checking to see if the conversion back to picture from the pixel matrix yields the same image.\ntestImg &lt;- as.cimg(grayMatrix)\nplot(testImg)\n\n\n\n\n\n\n\n\nWe can see that the pixel matrix imager gives us contains values between 0 and 1. This is because imager works with normalized values. We can replace these normalized values by the product of these values by 255 to get the original gray scale values.\n\n# Unnormalizing the pixel matrix\nunnormalizedGrayMatrix &lt;- grayMatrix"
  },
  {
    "objectID": "submitted/projects_2/Project2_hanyan.html#haar-wavelet-transform-matrix-construction",
    "href": "submitted/projects_2/Project2_hanyan.html#haar-wavelet-transform-matrix-construction",
    "title": "Project2",
    "section": "",
    "text": "Now, let us get to the important part of constructing the Haar Wavelet Transform. To do so, we first should make sure that the dimensions of the pixel matrix is even.\n\n# We want to make the pixel matrix have even rows and columns.\n\n# From ChatGBT we get a function to correct the pixel matrix.\n# Function to check and remove the first row or column based on odd counts\nremove_odd_row_or_column &lt;- function(matrix_input) {\n  # Get the number of rows and columns\n  num_rows &lt;- nrow(matrix_input)\n  num_cols &lt;- ncol(matrix_input)\n  \n  # Check if the number of rows is odd\n  if (num_rows %% 2 == 1) {\n    # Remove the first row if odd\n    matrix_output &lt;- matrix_input[-1, ]\n  } else if (num_cols %% 2 == 1) {\n    # Remove the first column if odd\n    matrix_output &lt;- matrix_input[, -1]\n  } else {\n    # If neither is odd, return the original matrix\n    matrix_output &lt;- matrix_input\n  }\n  \n  return(matrix_output)\n}\n\n# Now making our matrix even dimensional\n\"Dimension of the pixel matrix before correction:\" \n\n[1] \"Dimension of the pixel matrix before correction:\"\n\ndim(unnormalizedGrayMatrix)\n\n[1]  691 1000\n\n# Calling the function\nevenMatrix &lt;- remove_odd_row_or_column(unnormalizedGrayMatrix)\n\n\"Dimension fo the pixel matrix after correction:\"\n\n[1] \"Dimension fo the pixel matrix after correction:\"\n\n# Below is code to descale the image for testing.\n\"evenMatrix &lt;- remove_odd_row_or_column(evenMatrix[seq(1,nrow(evenMatrix),32),seq(1,ncol(evenMatrix),32)])\n\ndim(evenMatrix)\"\n\n[1] \"evenMatrix &lt;- remove_odd_row_or_column(evenMatrix[seq(1,nrow(evenMatrix),32),seq(1,ncol(evenMatrix),32)])\\n\\ndim(evenMatrix)\"\n\n\nNow we can begin to construct the Haar Wavelet Transform Matrix. \n\n# The Haar Wavelet Transform matrix is a square matrix and corresponds to the number of rows of the pixel matrix. I am creating a function that finds the Haar Wavelet Transform matrix for any arbitrary dimension.\n\nhaarGet &lt;- function(dimension) {\n  haarMatrix &lt;- matrix(NA, nrow = dimension, ncol = dimension)\n\n  # Creating a dummy row that will be changed and added to the haarMatrix\n  rowDummy &lt;- rep(0, dimension)\n  \n  # Iterating over the rows. We will add each row accordingly\n  for (row in 1:dimension){\n    changedRow &lt;- rowDummy\n    \n    # Getting the upper matrix U correct\n    if (row &lt;= dimension / 2){\n      changedRow[c((2 * row) - 1, 2 * row)] &lt;- 1\n      haarMatrix[row,] &lt;- changedRow\n    }\n    # Getting the lower matrix L right\n    if (row &gt; dimension / 2){\n      changedRow[2 * row - dimension] &lt;- 1\n      changedRow[2 * row - 1 - dimension] &lt;- -1\n      haarMatrix[row, ] &lt;- changedRow\n    }\n  }\n  \n  # Get the true matrix which includes the normalization square root of 2 over 2 term\n  haarMatrix &lt;- (sqrt(2) / 2) * haarMatrix\n  \n  return(haarMatrix)\n}"
  },
  {
    "objectID": "submitted/projects_2/Project2_hanyan.html#performing-compression",
    "href": "submitted/projects_2/Project2_hanyan.html#performing-compression",
    "title": "Project2",
    "section": "",
    "text": "With our Haar Wavelet Transform Matrix function ready, we can try performing one instance of the Wavelet Transform.\n\n\n\n\n\n\n# First encoding the dimension of the corrected pixel matrix\ndimension &lt;- dim(evenMatrix)\nnumRow &lt;- dimension[1]\nnumCol &lt;- dimension[2]\n\n# The dimension of the W_m in the picture above corresponds to the number of rows of the pixel matrix\nW_m &lt;- haarGet(numRow)\n\n# Likewise, the dimension of W_n in the picture above corresponds to the number of columns of the pixel matrix. Then we get its transpose\nW_n &lt;- haarGet(numCol)\nW_nT &lt;- t(W_n)\n\n# Get the resulting product\nresultMatrix &lt;- (W_m %*% evenMatrix %*% W_nT) / 2\ndim(resultMatrix)\n\n[1]  690 1000\n\n\nOk, we are doing good here. We know the following:The dimensions of \\(B, V, H, D\\) are \\(\\frac{1}{2} (\\text{row dimension})\\times \\frac{1}{2} (\\text{column dimension})\\).\nTherefore, we can extract these matrices out of our result matrix.\n\nresultMatrix &lt;- resultMatrix\nresult &lt;- as.cimg(resultMatrix) \nplot(result)\n\n\n\n\n\n\n\nB &lt;- resultMatrix[1:(numRow / 2), 1:(numCol / 2)]\nV &lt;- resultMatrix[1:(numRow / 2), (numCol / 2 + 1):numCol]\nD &lt;- resultMatrix[(numRow / 2 + 1):numRow, (numCol / 2 + 1):numCol]\nH &lt;- resultMatrix[(numRow / 2 + 1):numRow, 1:(numCol / 2)]\n\ntestB &lt;- as.cimg(B)\nplot(testB, main=\"B Matrix\")\n\n\n\n\n\n\n\ntestV &lt;- as.cimg(V)\nplot(testV, main=\"V Matrix\")\n\n\n\n\n\n\n\ntestD &lt;- as.cimg(D)\nplot(testD, main=\"D Matrix\")\n\n\n\n\n\n\n\ntestH &lt;- as.cimg(H)\nplot(testH, main=\"H Matrix\")\n\n\n\n\n\n\n\n\nThe first plot is the image converted from the pixel matrix \\(\\begin{bmatrix} B & V \\\\ H & D \\end{bmatrix}.\\) The second plot is the image converted from the pixel matrix B. The third is V, fourth is D, and fifth is H."
  },
  {
    "objectID": "submitted/projects_2/Project2_hanyan.html#more-compression",
    "href": "submitted/projects_2/Project2_hanyan.html#more-compression",
    "title": "Project2",
    "section": "",
    "text": "Now, we can construct a function that automatically applies the Haar Wavelet Transform to a give pixel matrix. With this function, we can then see how much compression we can do before Feyman becomes too blurry :(.\nFirst, we will create a function called HaarCompression that takes in a normalized pixel matrix and outputs the resulting pixel matrix after the Haar Wavelet Transform along with B, V, D, and H.\n\nHaarCompression &lt;- function(pixelMatrix){\n  # Correcting to the correct dimensions\n  evenMatrix &lt;- remove_odd_row_or_column(pixelMatrix)\n  \n  dimension &lt;- dim(evenMatrix)\n  numRow &lt;- dimension[1]\n  numCol &lt;- dimension[2]\n  \n  # The dimension of the W_m in the picture above corresponds to the number of rows of the pixel matrix\n  W_m &lt;- haarGet(numRow)\n\n  # Likewise, the dimension of W_n in the picture above corresponds to the number of columns of the pixel matrix. Then we get its transpose\n  W_n &lt;- haarGet(numCol)\n  W_nT &lt;- t(W_n)\n  \n  # Get the resulting product\n  resultMatrix &lt;- (W_m %*% evenMatrix %*% W_nT) / 2\n  \n  # Get the four matrices\n  B &lt;- resultMatrix[1:(numRow / 2), 1:(numCol / 2)]\n  V &lt;- resultMatrix[1:(numRow / 2), (numCol / 2 + 1):numCol]\n  D &lt;- resultMatrix[(numRow / 2 + 1):numRow, (numCol / 2 + 1):numCol]\n  H &lt;- resultMatrix[(numRow / 2 + 1):numRow, 1:(numCol / 2)]\n  \n  # Getting the output ready. This function will output a list containing the original matrix and the five partitioned matrices.\n  return(list(HaarMatrix = resultMatrix, B = B, V = V, D = D, H = H))\n}\n\n# Testing with Feyman\ntest = HaarCompression(grayMatrix)\nplot(as.cimg(test$HaarMatrix))\n\n\n\n\n\n\n\nplot(as.cimg(test$B))\n\n\n\n\n\n\n\nplot(as.cimg(test$V))\n\n\n\n\n\n\n\nplot(as.cimg(test$D))\n\n\n\n\n\n\n\nplot(as.cimg(test$H))\n\n\n\n\n\n\n\n\nWe can see that our HaarCompression function works as intended. Now we can create a new function that applies our HaarCompression function repeatedly.\n\nIterativeHaarCompression &lt;- function(image, numRuns){\n  # Converting the image first to gray scale and then to the normalized pixel matrix and to make the dimensions even.\n  gray &lt;- grayscale(image)\n  pixelMatrix &lt;- as.matrix(gray)\n  startingMatrix &lt;- remove_odd_row_or_column(pixelMatrix)\n\n  # Initializing the selected matrix\n  selectedMatrix &lt;- startingMatrix\n  for (run in 1:numRuns){\n    resultList &lt;- HaarCompression(selectedMatrix)\n    selectedMatrix &lt;- resultList$B\n  }\n  \n  compressedImage &lt;- as.cimg(selectedMatrix)\n  return(compressedImage)\n}\n\ntest &lt;- IterativeHaarCompression(image, 4)\nplot(test, main=\"Feyman after 4 compressions.\")\n\n\n\n\n\n\n\n\nLet us view the results.\n\nfor(runs in 1:8){\n  img &lt;- IterativeHaarCompression(image, runs)\n  plot(img, main=glue(\"Feyman after {runs} compressions.\"))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt is reasonable to conclude that after 4 compressions, Feyman is still acceptable. If one knows Feyman very well, the portrait after 5 compression may even be deemed acceptable. But after that, the portrait clearly becomes unrecognizable.\n\nI think even the 4 compression Feynman is pretty sad! I can’t tell who it is, only the rough time period that the person is from…\n\nEvery compression reduces the totally number of elements in the pixel matrix by 4. Therefore, if we deem that 4 compressions for Feyman is maximum, then the approximate amount of reduction of pixels will be a factor of \\(4^4 = 256\\). Feyman started with \\(690000\\) pixel elements (after we do the even correction). So after four compressions we are left with around \\(2695\\) pixels. I am only saying around because every compression requires an even correction, which may decrease the number of pixels further. If we view the number of pixels as proportional to the size of the image, then we have reduced the size of Feyman by a factor of \\(256\\)! But of course, the size of the image is not linearly proportional to the number of pixels. Let us try saving the 4-times compressed photo to our desktop and compare the size of this photo to our original.\n\noriginalFeyman = image\n#save.image(originalFeyman, glue(\"{path}/originalFeyman.png\"))\n\nfourTimesCompressedFeyman = IterativeHaarCompression(image, 4)\n#save.image(fourTimesCompressedFeyman, glue(\"{path}/fourTimesCompressedFeyman.png\"))\n\n\nWow, 290 times smaller! But maybe four times compressed Feyman is still a little too blurry. 2 compressions will give us a recognizable Feyman and the size of the photo will still decrease by more than a factor of 16.\nAlso, we don’t only have the \\(B\\) matrix after each compression. What can we do about the three edges matrices?"
  },
  {
    "objectID": "submitted/projects_2/Project2_hanyan.html#edge-edge-edge",
    "href": "submitted/projects_2/Project2_hanyan.html#edge-edge-edge",
    "title": "Project2",
    "section": "",
    "text": "Let us look at the Haar Wavelet Transform Algorithm another time: \\[\n\\begin{equation}\nW_m A W_n^T = 2\\begin{bmatrix}  B & V \\\\ H & D  \\end{bmatrix},\\\\\n\\Rightarrow W_m^T WA W_n^T W_n = 2 W_m^T \\begin{bmatrix}  B & V \\\\ H & D  \\end{bmatrix}W_n, \\\\\n\\Rightarrow A = 2 W_m^T \\begin{bmatrix}  B & V \\\\ H & D  \\end{bmatrix}W_n ,\n\\end{equation}\n\\]\nwhere the third equation follows from the fact that the inverse of an orthogonal matrix is the transpose of the matrix. This means that we can reconstruct the original image from the compressed matrix. If we don’t know the dimension of the Haar Wavelet Transform matrices \\(W_m\\) and \\(W_n\\), we can simply work backwards to determine their dimensions by looking at the dimension of \\(B\\).\n\n# Let us test this hypothesis. \n\nW_mT &lt;- t(W_m)\n\ntest &lt;- HaarCompression(grayMatrix)\nresultMatrix &lt;- test$HaarMatrix\n\noriginalPixelMatrix &lt;- 2 * W_mT %*% resultMatrix %*% W_n\nplot(as.cimg(originalPixelMatrix))\n\n\n\n\n\n\n\n\nGreat, the idea is valid. So, let us work on a prototype compression idea. We can compress an image as many times until it is still acceptable. Then, we compress the image again to make it unacceptable and record the resulting compression matrix with the \\(B,V,H,D\\) matrices. If we can find a way to store \\(V,H,D\\) in an efficient manner by adding some kind of storing technique that only stores information about the location of the edges, then we can decrease the size of \\(V,H,D\\). To accomplish this, we will be dealing using sparse matrices. We can construct a reconstruction algorithm to find the correct Haar Wavelet Transform Matrices and reconstruct the unacceptable image resolution to an acceptable resolution. If we can do this, we basically managed to decrease the size of a compressed image even further by reducing the number of pixels by another factor of 4 but with some added edge information that needs to be stored. To take this further, maybe if we store the compressed \\(V,H,D\\) information every time we compressed, we may even be able to get a rough reconstruction of the original image from a many-times compressed image!\nBut let us not get too far ahead of ourselves. First, we need to design an algorithm that compresses the three edge matrices in a suitable way. We need a detection algorithm and a reconstruction algorithm.\nWe first need to see what is going on. I have defined a modified function below that gives us the list of the five matrices. Then, I am going to plot the matrix V to see what values the edges will represent.\n\nIterativeHaarCompressionList &lt;- function(image, numRuns){\n  # Converting the image first to gray scale and then to the normalized pixel matrix and to make the dimensions even.\n  gray &lt;- grayscale(image)\n  pixelMatrix &lt;- as.matrix(gray)\n  startingMatrix &lt;- remove_odd_row_or_column(pixelMatrix)\n\n  # Initializing the selected matrix\n  selectedMatrix &lt;- startingMatrix\n  list &lt;- HaarCompression(startingMatrix)\n\n  for (run in 1:numRuns){\n    resultList &lt;- HaarCompression(selectedMatrix)\n    selectedMatrix &lt;- resultList$B\n    list &lt;- resultList\n  }\n  \n  return(list)\n}\n\nlist &lt;- IterativeHaarCompressionList(image, 5)\nB &lt;- list$B\nV &lt;- list$V\nD &lt;- list$D\nH &lt;- list$H\nlibrary(plot.matrix)\n\n# Adjust margins to ensure the color indicator is not cut off\npar(mar = c(5, 4, 4, 7))  # Increase right margin to allow space for the color indicator\n\n# Plot the matrix with `plot.matrix`\nplot(abs(V), main = \"V\")\n\n\n\n\n\n\n\nplot(abs(D), main = \"D\")\n\n\n\n\n\n\n\nplot(abs(H), main = \"H\")\n\n\n\n\n\n\n\n\nHere, I am plotting the edge matrices with their absolute values to get a clearer picture. Now, let us create a function that get an input matrix with a threshold value and outputs the matrix keeping the elements that have absolute values equal to or greater than the threshold value.\nCareful, note that the scale of the values for these matrices are different. Therefore, the threshold value will be defined as the percentage (converted to decimal) of the maximum value of the matrix. To efficiently store the returned matrix, we will use the package Matrix and store the matrix as a sparse matrix. This will allow for more efficient storage.\n\nif (!requireNamespace(\"Matrix\", quietly = TRUE)) {\n  install.packages(\"Matrix\")\n}\nlibrary(Matrix)\n\nzeroCompression &lt;- function(matrix, thresholdVal){\n  dimension &lt;- dim(matrix)\n  numRow &lt;- dimension[1]\n  numCol &lt;- dimension[2]\n  \n  maxVal &lt;- abs(max(matrix))\n  threshold &lt;- maxVal * thresholdVal\n  \n  for (row in 1:numRow){\n    for (col in 1: numCol){\n      if (abs(matrix[row, col]) &lt; threshold){\n        matrix[row, col] &lt;- 0\n      }\n    }\n  }\n  \n  return(as(matrix, \"sparseMatrix\"))\n}\n\nplot(as.matrix(zeroCompression(D, 0.7)))\n\n\n\n\n\n\n\nplot(as.matrix(zeroCompression(D, 0.5)))\n\n\n\n\n\n\n\n\nWe can see that our algorithm works. Let us create a reconstruction function that takes the four matrices and reconstructs the image.\n\nimageReconstruction &lt;- function(B, V, D, H){\n  # Combine B, V, D, H to form the large matrix A\n  top &lt;- cbind(B, V)\n  bottom &lt;- cbind(H, D)\n  largeMatrix &lt;- rbind(top, bottom)\n  \n  dimension &lt;- dim(B)\n  numRow &lt;- 2 * dimension[1]\n  numCol &lt;- 2 * dimension[2]\n  \n  W_mT = t(haarGet(numRow))\n  W_n = haarGet(numCol)\n  \n  originalPixelMatrix &lt;- 2 * W_mT %*% largeMatrix %*% W_n\n  \n  return(as.cimg(originalPixelMatrix))\n}\n\nplot(IterativeHaarCompression(image, 4), main=\"After 4 Compressions\")\n\n\n\n\n\n\n\nplot(imageReconstruction(B, V, D, H), main=\"Reconstruction from 5 Compressions\")\n\n\n\n\n\n\n\npercentage &lt;- 0.5\ncompressedV &lt;- as.matrix(zeroCompression(V, percentage))\ncompressedD &lt;- as.matrix(zeroCompression(D, percentage))\ncompressedH &lt;- as.matrix(zeroCompression(H, percentage))\nplot(imageReconstruction(B, compressedV, compressedD, compressedH), main=\"Reconstruction from 5 Compressions \\n Added Compression of V, D, H \\n Threshold of 50% of max value.\")\n\n\n\n\n\n\n\nplot(IterativeHaarCompression(image, 5), main=\"After 5 Compressions\")\n\n\n\n\n\n\n\n\nThis is super cool! Comparing the third plot to the fourth plot, we can conclude that the reconstruction with the compressed edge matrices is definetly higher in quality that the imag after 5 compressions. Let us see how the reconstruction changes as we vary the threshold values.\n\nfor(percentage in seq(0, 1, by=0.1)){\n  compressedV &lt;- zeroCompression(V, percentage)\n  compressedD &lt;- zeroCompression(D, percentage)\n  compressedH &lt;- zeroCompression(H, percentage)\n  actualPercentage &lt;- percentage * 100\n  \n  plot(imageReconstruction(B, as.matrix(compressedV), as.matrix(compressedD), as.matrix(compressedH)), main=glue(\"Reconstruction from 5 Compressions \\n Added Compression of V, D, H \\n Threshold of {actualPercentage}% of max value \\n {nnzero(compressedV) + nnzero(compressedD) + nnzero(compressedH)} nonzero entries (sum of non-zero entries of V, D, H).\"))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see that the quality of reconstruction decreases as we increase the threshold value. Naturally, the number of nonzero entries of V, D, H decreases as well. We may say that the number of nonzero entries of V, D, H determine the quality of the reconstruction."
  },
  {
    "objectID": "submitted/projects_2/Project2_hanyan.html#getting-down-to-business",
    "href": "submitted/projects_2/Project2_hanyan.html#getting-down-to-business",
    "title": "Project2",
    "section": "",
    "text": "Let us set the threshold value/percentage to 30% of the maximum value and create the entire image compression algorithm function! This function will have an input of an image and a number denoting how many factors less should the size of the compressed image be compared to the original image. This function will then output a list of the B, V, D, H for storage after the appropriate number of Haar Wavelet Transforms and the compressions of V, D, H matrices. Another function, the image reader will be used when we want to convert the list back to an image. Let us call this compression the “combined compression”.\n\nHanyanImageCompression &lt;- function(image, sizeFactor){\n  # Calculating first roughly the number of compressions that we need to get closer to the size factor.\n  numberOfCompressionsRough &lt;- log(sizeFactor, 4)\n  numberOfCompressions &lt;- round(numberOfCompressionsRough)\n  \n  # Getting the list of B, V, D, H matrices\n  matrixList &lt;- IterativeHaarCompressionList(image, numberOfCompressions)\n  \n  # Setting the percentage threshold and compressing V, D, H\n  percentage &lt;- 0.3\n  \n  compressedV &lt;- zeroCompression(matrixList$V, percentage)\n  compressedD &lt;- zeroCompression(matrixList$D, percentage)\n  compressedH &lt;- zeroCompression(matrixList$H, percentage)\n  \n  return(list(B = matrixList$B, D = compressedD, V = compressedV, H = compressedH, numberOfCompressions = numberOfCompressions))\n  \n}\n\n# We have to rewrite the imageReconstruction function so that it takes in the list that the HanyanImageCompression returns.\nimageReader &lt;- function(list){\n  B &lt;- list$B\n  V &lt;- as.matrix(list$V)\n  D &lt;- as.matrix(list$D)\n  H &lt;- as.matrix(list$H)\n  \n  return(imageReconstruction(B, V, D, H))\n  \n}\n\n# Simple testing\ntestCompress &lt;- HanyanImageCompression(image, 100)\nplot(imageReader(testCompress))\n\n\n\n\n\n\n\n\nGreat, the two functions work. Now we can check how well the combined compression method works.\n\ntest2Compress &lt;- HanyanImageCompression(image, 300)\nHanyanImage &lt;- imageReader(test2Compress)\nplot(HanyanImage, main=glue(\"Combined Compression with {test2Compress$numberOfCompressions} Haar Wavelet Compressions\"))\n\n\n\n\n\n\n\n#save.image(HanyanImage, glue(\"{path}/HanyanCompression.png\"))\n\nhaarCompress &lt;- IterativeHaarCompressionList(image, test2Compress$numberOfCompressions - 1)\nplot(as.cimg(haarCompress$B), main=glue(\"{test2Compress$numberOfCompressions - 1} Haar Wavelet Compressions\"))\n\n\n\n\n\n\n\n#save.image(as.cimg(haarCompress$B), glue(\"{path}/{test2Compress$numberOfCompressions - 1}HaarWaveletCompressions.png\"))\n\nWow! The resulting image doesn’t seem to change much, at least when viewed from a distance.\n\n\n\nWe can see that our combined compression achieves a similar result to the three-times Haar Wavelet Compressed image with even smaller size!\n\n\nIf we zoomed in to the two iamges, however, we will see that the combined compression result looks more pixelated than compression with only the Haar Wavelet Transform. This makes sense, as our added compression of V, D, H makes us loss a lot of “intermittent” information between the pixels that we keep and the pixels that we set to 0.\n\n# Let us check the sizes of the data files that we store.\n#save(test2Compress, file = glue(\"{path}/test2Compress.RData\"))\n\"The size of the saved combined compression file (bytes): \"\n\n[1] \"The size of the saved combined compression file (bytes): \"\n\n#print(file.info(glue(\"{path}/test2Compress.RData\"))$size)\n\nsaveMatrix &lt;- haarCompress$B\n\"The size of the Haar Wavelet Compression file (bytes): \"\n\n[1] \"The size of the Haar Wavelet Compression file (bytes): \"\n\n#save(saveMatrix, file = glue(\"{path}/haarCompress.RData\"))\n#print(file.info(glue(\"{path}/haarCompress.RData\"))$size)\n\nWonderful, we can see how we can squeeze more compression of out the originally Haar Wavelet compressed image to reduce the size of the stored information about the image even further by compromising on a further reduced quality of the photo."
  },
  {
    "objectID": "submitted/projects_2/Project2_hanyan.html#testing-the-combined-compression",
    "href": "submitted/projects_2/Project2_hanyan.html#testing-the-combined-compression",
    "title": "Project2",
    "section": "",
    "text": "Let us try this compression method on other images.\nHere is a painting named Las Meninas by Diego Velázquez. Unfortunately I have to crop it to make the compression effects are more visible (sorry!)\n\n\nLasMeninasLocation &lt;- glue('{path}/LasMeninas.jpg')\nLasMeninas = load.image(LasMeninasLocation)\nLasMeninasGray &lt;- grayscale(LasMeninas)\nplot(LasMeninasGray)\n\nLasMeninasCombinedCompress &lt;- HanyanImageCompression(LasMeninas, 100)\nHanyanImage &lt;- imageReader(LasMeninasCombinedCompress)\nplot(HanyanImage, main=glue(\"Combined Compression with {test2Compress$numberOfCompressions} Haar Wavelet Compressions\"))\nsave.image(HanyanImage, glue(\"{path}/HanyanCompressionLasMeninas.png\"))\n\nhaarCompress &lt;- IterativeHaarCompressionList(LasMeninas, LasMeninasCombinedCompress$numberOfCompressions - 1)\nplot(as.cimg(haarCompress$B), main=glue(\"{LasMeninasCombinedCompress$numberOfCompressions - 1} Haar Wavelet Compressions\"))\nsave.image(as.cimg(haarCompress$B), glue(\"{path}/{LasMeninasCombinedCompress$numberOfCompressions - 1}HaarWaveletCompressionsLasMeninas.png\"))\n\nThe sizes of the images are shown below.\n\nWe can see the shortcomings of the combined compression technique compared to the Haar Wavelet Compression. Although we can save more space using the combined compression, for images other than portraits where there are many edges, information loss becomes a lot more noticeable. If you open the HanyanCompressionLasMeninas image, it might even look a little unnerving due to the faces becoming pixelated.\n\nThis is amazing. It’s really neat to see how well the compression works, how you can recover so much from even the compressed edges.\nI’m enjoying seeing your thought process as you work through these projects!\nGrade: E"
  },
  {
    "objectID": "HW/Projects3.html",
    "href": "HW/Projects3.html",
    "title": "Homework 5",
    "section": "",
    "text": "Revisit one of the previous projects using new concepts which we have learned since you first did the project. (The “Image Compression” project on page 389 of your textbook would be one example of this, but you may take this in any direction you like.)"
  },
  {
    "objectID": "HW/Projects3.html#revisiting-a-previous-project",
    "href": "HW/Projects3.html#revisiting-a-previous-project",
    "title": "Homework 5",
    "section": "",
    "text": "Revisit one of the previous projects using new concepts which we have learned since you first did the project. (The “Image Compression” project on page 389 of your textbook would be one example of this, but you may take this in any direction you like.)"
  },
  {
    "objectID": "HW/Projects3.html#quadratic-forms",
    "href": "HW/Projects3.html#quadratic-forms",
    "title": "Homework 5",
    "section": "2.1 Quadratic Forms",
    "text": "2.1 Quadratic Forms\nIn order to classify quadratic equations in \\(x\\) and \\(y\\) one goes through roughly three steps. First, perform a rotation transformation of coordinates to get rid of mixed terms such as \\(2 x y\\) in the quadratic equation \\(x^{2}+2 x y-y^{2}+x-3 y=4\\). Second, do a translation of coordinates to put the equation in a “standard form.” Third, identify the curve by your knowledge of the shape of a curve in that standard form. Standard forms are equations like \\(x^{2} / 4+y^{2} / 2=1\\), an ellipse with its axes along the \\(x\\) - and \\(y\\)-axes. It is the second-degree terms \\(\\left(x^{2}, 2 x y\\right.\\), and \\(\\left.y^{2}\\right)\\) alone that determine the nature of a quadratic.\nNow you’re ready for the rest of the story. Just as with curves in \\(x\\) and \\(y\\), the basic shape of the surface of a quadric equation in \\(x, y\\), and \\(z\\) is determined by the second-degree terms. So we will focus on an example with no first-degree terms, namely,\n\\[\nQ(x, y, z)=2 x^{2}+4 y^{2}+6 z^{2}-4 x y-2 x z+2 y z=1 .\n\\]\nThe problem is this: find a change of coordinates that will make it clear what standard forms is represented by this surface – is it a sphere, a spheroid, or a tri-axial ellipsoid?\nFirst you must express the so-called quadratic form \\(Q(x, y, z)\\) in matrix form as \\(Q(x, y, z)=\\) \\([x, y, z] A[x, y, z]^{T}\\).\nNext, replace \\(A\\) by the equivalent matrix \\(\\left(A+A^{T}\\right) / 2\\). (Check that if \\(A\\) specifies the quadratic form \\(Q\\), then so will \\(A^\\prime = \\left(A+A^{T}\\right) / 2\\).)\nNext, diagonalize your symmetric matrix. You can use the command diagonalize in sympy to do this, but check that the results are what you’d expect given the eigenvalues and eigenvectors of your symmetric matrix. The result is a diagonal matrix \\(D\\) and an orthogonal matrix \\(P\\) such that \\(P^{T} A^\\prime P=D\\).\nNext, make the linear change of variables \\([x, y, z]^{T}=P\\left[x^{\\prime}, y^{\\prime}, z^{\\prime}\\right]^{T}\\) and deduce that \\(Q(x, y, z)=\\) \\(\\left[x^{\\prime}, y^{\\prime}, z^{\\prime}\\right] P^{T} A^\\prime P\\left[x^{\\prime}, y^{\\prime}, z^{\\prime}\\right]^{T}\\). If \\(P^{T} A^\\prime P\\) is diagonal, we end up with squares of \\(x^{\\prime}, y^{\\prime}\\) and \\(z^{\\prime}\\), and no mixed terms.\nFrom this data alone you should be able to classify the surface represented by the above equation.\nConfirm your results by plotting the surface in the new coordinates.\nAn outstanding reference on this topic and many others relating to matrix analysis is the recently republished textbook [3] by Richard Bellman, widely considered to be a classic in the field.\n(Hint: when I did this problem, I encountered very small imagainary part at certain points. I used the sp.nsimplify(sp.N(f),rational=True) function to get rid of these.)"
  },
  {
    "objectID": "HW/Projects3.html#management-of-sheep-populations",
    "href": "HW/Projects3.html#management-of-sheep-populations",
    "title": "Homework 5",
    "section": "3.1 Management of Sheep Populations",
    "text": "3.1 Management of Sheep Populations\nDescription of the problem: You are working for the New Zealand Department of Agriculture on a project for sheep farmers. The species of sheep that these shepherds raise have a life span of 12 years. Of course, some live longer, but they are sufficiently few in number and their reproductive rate is so low that they may be ignored in your population study. Accordingly, you divide sheep into 12 age classes, namely those in the first year of life, etc. An extensive survey of the demographics of this species of sheep results in the following approximations for the demographic parameters \\(f_{i}\\) and \\(s_{i}\\), where \\(f_{i}\\) is the percapita reproductive rate for sheep in the \\(i\\) th age class and \\(s_{i}\\) is the survival rate for sheep in that age class, i.e., the fraction of sheep in that age class that survive to the \\((i+1)\\) th class. (As a matter of fact, this table is related to real data. The interested reader might consult the article [7] in the bibliography.)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(i\\)\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n\n\\(f_{i}\\)\n.000\n.023\n.145\n.236\n.242\n.273\n.271\n.251\n.234\n.229\n.216\n.210\n\n\n\\(s_{i}\\)\n.845\n.975\n.965\n.950\n.926\n.895\n.850\n.786\n.691\n.561\n.370\n-\n\n\n\nThe problem is as follows: in order to maintain a constant population of sheep, shepherds will harvest a certain number of sheep each year. Harvesting need not mean slaughter; it simply means removing sheep from the population (e.g., selling animals to other shepherds). Denote the fraction of sheep that are removed from the \\(i\\) th age group at the end of each growth period (a year in our case) by \\(h_{i}\\). If these numbers are constant from year to year, they constitute a harvesting policy. If, moreover, the yield of each harvest, i.e., total number of animals harvested each year, is a constant and the age distribution of the remaining populace is essentially constant after each harvest, then the harvesting policy is called sustainable. If all the \\(h_{i}\\) ’s are the same, say \\(h\\), then the harvesting policy is called uniform. Uniform policies are simple to implement: One selects the sheep to be harvested at random.\nYour problem: Find a uniform sustainable harvesting policy to recommend to shepherds, and find the resulting distribution of sheep that they can expect with this policy. Shepherds who raise sheep for sale to markets are also interested in a sustainable policy that gives a maximum yield. If you can find such a policy that has a larger annual yield than the uniform policy, then recommend it. On the other hand, shepherds who raise sheep for their wool may prefer to minimize the annual yield. If you can find a sustainable policy whose yield is smaller than that of the uniform policy, make a recommendation accordingly. In each case find the expected distribution of your harvesting policies. Do you think that there might be other economic factors that should be taken into account in this model? Organize your results for a report to be read by your supervisor and an informed public.\nProcedure: Express this problem as a discrete linear dynamical system \\(\\mathbf{x}^{(k+1)}=L \\mathbf{x}^{(k)}\\), where \\(L\\) is a so-called Leslie matrix of the form\n\\[\nL=\\left[\\begin{array}{cccccc}\nf_{1} & f_{2} & f_{3} & \\cdots & f_{n-1} & f_{n} \\\\\ns_{1} & 0 & 0 & \\cdots & 0 & 0 \\\\\n0 & s_{2} & 0 & \\cdots & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & s_{n-1} & 0\n\\end{array}\\right]\n\\]\nIt is understood that \\(0&lt;s_{i} \\leq 1,0 \\leq f_{i}\\), and at least one \\(f_{i}\\) is nonzero.\nIn regard to harvesting, let \\(H\\) be a diagonal matrix with the harvest fractions \\(h_{i}\\) down the diagonal. (Here \\(0 \\leq h_{i} \\leq 1\\).) Then the population that results from this harvesting at the end of each period is given by \\(\\mathbf{x}^{k+1}=L \\mathbf{x}^{k}-H L \\mathbf{x}^{k}=(I-H) L \\mathbf{x}^{k}\\). There are other theoretical tools, but all you need to do is to find a matrix \\(H\\) such that 1 is the dominant eigenvalue of \\((I-H) L\\). You can do this by trial and error, a method that is applicable to any harvesting policy, uniform or not. However, in the case of uniform policies it’s simpler to note that \\((I-H) L=(1-h) L\\), where \\(h\\) is the diagonal entry of \\(H\\). Find an eigenvector corresponding to this eigenvalue and scale this vector by dividing it by the sum of its components to obtains a probability distribution vector that could be used for any population."
  },
  {
    "objectID": "HW/Projects3.sol.html",
    "href": "HW/Projects3.sol.html",
    "title": "Homework 5 Solutions",
    "section": "",
    "text": "Revisit one of the previous projects using new concepts which we have learned since you first did the project. (The “Image Compression” project on page 389 of your textbook would be one example of this, but you may take this in any direction you like.)"
  },
  {
    "objectID": "HW/Projects3.sol.html#revisiting-a-previous-project",
    "href": "HW/Projects3.sol.html#revisiting-a-previous-project",
    "title": "Homework 5 Solutions",
    "section": "",
    "text": "Revisit one of the previous projects using new concepts which we have learned since you first did the project. (The “Image Compression” project on page 389 of your textbook would be one example of this, but you may take this in any direction you like.)"
  },
  {
    "objectID": "HW/Projects3.sol.html#quadratic-forms",
    "href": "HW/Projects3.sol.html#quadratic-forms",
    "title": "Homework 5 Solutions",
    "section": "2.1 Quadratic Forms",
    "text": "2.1 Quadratic Forms\nIn order to classify quadratic equations in \\(x\\) and \\(y\\) one goes through roughly three steps. First, perform a rotation transformation of coordinates to get rid of mixed terms such as \\(2 x y\\) in the quadratic equation \\(x^{2}+2 x y-y^{2}+x-3 y=4\\). Second, do a translation of coordinates to put the equation in a “standard form.” Third, identify the curve by your knowledge of the shape of a curve in that standard form. Standard forms are equations like \\(x^{2} / 4+y^{2} / 2=1\\), an ellipse with its axes along the \\(x\\) - and \\(y\\)-axes. It is the second-degree terms \\(\\left(x^{2}, 2 x y\\right.\\), and \\(\\left.y^{2}\\right)\\) alone that determine the nature of a quadratic.\nNow you’re ready for the rest of the story. Just as with curves in \\(x\\) and \\(y\\), the basic shape of the surface of a quadric equation in \\(x, y\\), and \\(z\\) is determined by the second-degree terms. So we will focus on an example with no first-degree terms, namely,\n\\[\nQ(x, y, z)=2 x^{2}+4 y^{2}+6 z^{2}-4 x y-2 x z+2 y z=1 .\n\\]\nThe problem is this: find a change of coordinates that will make it clear what standard forms is represented by this surface – is it a sphere, a spheroid, or a tri-axial ellipsoid?\nFirst you must express the so-called quadratic form \\(Q(x, y, z)\\) in matrix form as \\(Q(x, y, z)=\\) \\([x, y, z] A[x, y, z]^{T}\\).\nNext, replace \\(A\\) by the equivalent matrix \\(\\left(A+A^{T}\\right) / 2\\). (Check that if \\(A\\) specifies the quadratic form \\(Q\\), then so will \\(A^\\prime = \\left(A+A^{T}\\right) / 2\\).)\nNext, diagonalize your symmetric matrix. You can use the command diagonalize in sympy to do this, but check that the results are what you’d expect given the eigenvalues and eigenvectors of your symmetric matrix. The result is a diagonal matrix \\(D\\) and an orthogonal matrix \\(P\\) such that \\(P^{T} A^\\prime P=D\\).\nNext, make the linear change of variables \\([x, y, z]^{T}=P\\left[x^{\\prime}, y^{\\prime}, z^{\\prime}\\right]^{T}\\) and deduce that \\(Q(x, y, z)=\\) \\(\\left[x^{\\prime}, y^{\\prime}, z^{\\prime}\\right] P^{T} A^\\prime P\\left[x^{\\prime}, y^{\\prime}, z^{\\prime}\\right]^{T}\\). If \\(P^{T} A^\\prime P\\) is diagonal, we end up with squares of \\(x^{\\prime}, y^{\\prime}\\) and \\(z^{\\prime}\\), and no mixed terms.\nFrom this data alone you should be able to classify the surface represented by the above equation.\nConfirm your results by plotting the surface in the new coordinates.\nAn outstanding reference on this topic and many others relating to matrix analysis is the recently republished textbook [3] by Richard Bellman, widely considered to be a classic in the field.\n(Hint: when I did this problem, I encountered very small imagainary part at certain points. I used the sp.nsimplify(sp.N(f),rational=True) function to get rid of these.)\n\nFirst write the quadratic form in matrix form\n\nimport sympy as sp\nfrom sympy.abc import a, b, c, d, e, f, g, h, i, x, y, z\n\nM = sp.Matrix([[2, 0, 0], [-4, 4, 0], [-2, 2, 6]])\nxx = sp.Matrix([x, y, z])\nquadratic_form = xx.T*M*xx\n# simply pulling all quadratic terms together\nquadratic_form = quadratic_form[0].expand()\ndisplay(quadratic_form)\n\n\\(\\displaystyle 2 x^{2} - 4 x y - 2 x z + 4 y^{2} + 2 y z + 6 z^{2}\\)\n\n\nNow find a symmetric matric\n\n# find a symmetric matrix\nA = sp.nsimplify(1/2*(M + M.T),rational=True)\ndisplay(A)\n# check that the quadratic form is the same\ndisplay((xx.T*A*xx)[0].expand())\n\n\\(\\displaystyle \\left[\\begin{matrix}2 & -2 & -1\\\\-2 & 4 & 1\\\\-1 & 1 & 6\\end{matrix}\\right]\\)\n\n\n\\(\\displaystyle 2 x^{2} - 4 x y - 2 x z + 4 y^{2} + 2 y z + 6 z^{2}\\)\n\n\nNow find the eigenvalues and eigenvectors of the symmetric matrix:\n\neigvals, eigvecs = A.eigenvals(), A.eigenvects()\n\nDiagonalize:\n\nP, D = A.diagonalize()\n\n\n3 simplify, and get rid of very small nonzero imaginary parts\n\ndd= sp.N(sp.nsimplify(sp.N(D),rational=True))\ndisplay(dd)\n\n\\(\\displaystyle \\left[\\begin{matrix}4.20080975647311 & 0 & 0\\\\0 & 7.05708725655633 & 0\\\\0 & 0 & 0.742102986970565\\end{matrix}\\right]\\)\n\n\n\n# define xp, yp and zp as symbol\nxp,yp,zp=sp.symbols('xp, yp, zp')\n\n\nxxprime = sp.Matrix([xp,yp,zp])\nnew_quadratic_form=sp.simplify(xxprime.T*dd*xxprime)\n\nMake a plot:\n\nfrom sympy.solvers import solve\nzeqn = solve(new_quadratic_form-sp.Matrix([1]),zp)\nlimit = 0.5\nfrom sympy.plotting import plot3d as plot3d\nplot3d(zeqn[1],(xp,-limit, limit),(yp,-limit, limit))\nplot3d(zeqn[0],(xp,-limit, limit),(yp,-limit, limit))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the original coordinates:\n\norig_z_eqn = solve(quadratic_form-1,z)\nlimit=2\nplot3d(orig_z_eqn[1],(x,-limit, limit),(y,-limit, limit))\nplot3d(orig_z_eqn[0],(x,-limit,limit),(y,-limit, limit))"
  },
  {
    "objectID": "HW/Projects3.sol.html#management-of-sheep-populations",
    "href": "HW/Projects3.sol.html#management-of-sheep-populations",
    "title": "Homework 5 Solutions",
    "section": "4.1 Management of Sheep Populations",
    "text": "4.1 Management of Sheep Populations\nDescription of the problem: You are working for the New Zealand Department of Agriculture on a project for sheep farmers. The species of sheep that these shepherds raise have a life span of 12 years. Of course, some live longer, but they are sufficiently few in number and their reproductive rate is so low that they may be ignored in your population study. Accordingly, you divide sheep into 12 age classes, namely those in the first year of life, etc. An extensive survey of the demographics of this species of sheep results in the following approximations for the demographic parameters \\(f_{i}\\) and \\(s_{i}\\), where \\(f_{i}\\) is the percapita reproductive rate for sheep in the \\(i\\) th age class and \\(s_{i}\\) is the survival rate for sheep in that age class, i.e., the fraction of sheep in that age class that survive to the \\((i+1)\\) th class. (As a matter of fact, this table is related to real data. The interested reader might consult the article [7] in the bibliography.)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(i\\)\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n\n\\(f_{i}\\)\n.000\n.023\n.145\n.236\n.242\n.273\n.271\n.251\n.234\n.229\n.216\n.210\n\n\n\\(s_{i}\\)\n.845\n.975\n.965\n.950\n.926\n.895\n.850\n.786\n.691\n.561\n.370\n-\n\n\n\nThe problem is as follows: in order to maintain a constant population of sheep, shepherds will harvest a certain number of sheep each year. Harvesting need not mean slaughter; it simply means removing sheep from the population (e.g., selling animals to other shepherds). Denote the fraction of sheep that are removed from the \\(i\\) th age group at the end of each growth period (a year in our case) by \\(h_{i}\\). If these numbers are constant from year to year, they constitute a harvesting policy. If, moreover, the yield of each harvest, i.e., total number of animals harvested each year, is a constant and the age distribution of the remaining populace is essentially constant after each harvest, then the harvesting policy is called sustainable. If all the \\(h_{i}\\) ’s are the same, say \\(h\\), then the harvesting policy is called uniform. Uniform policies are simple to implement: One selects the sheep to be harvested at random.\nYour problem: Find a uniform sustainable harvesting policy to recommend to shepherds, and find the resulting distribution of sheep that they can expect with this policy. Shepherds who raise sheep for sale to markets are also interested in a sustainable policy that gives a maximum yield. If you can find such a policy that has a larger annual yield than the uniform policy, then recommend it. On the other hand, shepherds who raise sheep for their wool may prefer to minimize the annual yield. If you can find a sustainable policy whose yield is smaller than that of the uniform policy, make a recommendation accordingly. In each case find the expected distribution of your harvesting policies. Do you think that there might be other economic factors that should be taken into account in this model? Organize your results for a report to be read by your supervisor and an informed public.\nProcedure: Express this problem as a discrete linear dynamical system \\(\\mathbf{x}^{(k+1)}=L \\mathbf{x}^{(k)}\\), where \\(L\\) is a so-called Leslie matrix of the form\n\\[\nL=\\left[\\begin{array}{cccccc}\nf_{1} & f_{2} & f_{3} & \\cdots & f_{n-1} & f_{n} \\\\\ns_{1} & 0 & 0 & \\cdots & 0 & 0 \\\\\n0 & s_{2} & 0 & \\cdots & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & s_{n-1} & 0\n\\end{array}\\right]\n\\]\nIt is understood that \\(0&lt;s_{i} \\leq 1,0 \\leq f_{i}\\), and at least one \\(f_{i}\\) is nonzero.\nIn regard to harvesting, let \\(H\\) be a diagonal matrix with the harvest fractions \\(h_{i}\\) down the diagonal. (Here \\(0 \\leq h_{i} \\leq 1\\).) Then the population that results from this harvesting at the end of each period is given by \\(\\mathbf{x}^{k+1}=L \\mathbf{x}^{k}-H L \\mathbf{x}^{k}=(I-H) L \\mathbf{x}^{k}\\). There are other theoretical tools, but all you need to do is to find a matrix \\(H\\) such that 1 is the dominant eigenvalue of \\((I-H) L\\). You can do this by trial and error, a method that is applicable to any harvesting policy, uniform or not. However, in the case of uniform policies it’s simpler to note that \\((I-H) L=(1-h) L\\), where \\(h\\) is the diagonal entry of \\(H\\). Find an eigenvector corresponding to this eigenvalue and scale this vector by dividing it by the sum of its components to obtains a probability distribution vector that could be used for any population."
  },
  {
    "objectID": "HW/projects_3.sol.html",
    "href": "HW/projects_3.sol.html",
    "title": "Projects 3 Solutions",
    "section": "",
    "text": "Instructions:\nChoose two of the following projects to complete.\nPlease submit a zip file containing your code (in .ipynb or .qmd format), any data files you load in, and if possible a pdf of your notebook. (A link to a GitHub repository or Google CoLab notebook would also work, so long as they are publically accessible.)\n\n1 Revisiting a previous project\nRevisit one of the previous projects using new concepts which we have learned since you first did the project. (The “Image Compression” project on page 389 of your textbook would be one example of this, but you may take this in any direction you like.)\n\n\n2 Quadratic Forms\nIn order to classify quadratic equations in \\(x\\) and \\(y\\) one goes through roughly three steps. First, perform a rotation transformation of coordinates to get rid of mixed terms such as \\(2 x y\\) in the quadratic equation \\(x^{2}+2 x y-y^{2}+x-3 y=4\\). Second, do a translation of coordinates to put the equation in a “standard form.” Third, identify the curve by your knowledge of the shape of a curve in that standard form. Standard forms are equations like \\(x^{2} / 4+y^{2} / 2=1\\), an ellipse with its axes along the \\(x\\) - and \\(y\\)-axes. It is the second-degree terms \\(\\left(x^{2}, 2 x y\\right.\\), and \\(\\left.y^{2}\\right)\\) alone that determine the nature of a quadratic.\nNow you’re ready for the rest of the story. Just as with curves in \\(x\\) and \\(y\\), the basic shape of the surface of a quadric equation in \\(x, y\\), and \\(z\\) is determined by the second-degree terms. So we will focus on an example with no first-degree terms, namely,\n\\[\nQ(x, y, z)=2 x^{2}+4 y^{2}+6 z^{2}-4 x y-2 x z+2 y z=1 .\n\\]\nThe problem is this: find a change of coordinates that will make it clear what standard forms is represented by this surface – is it a sphere, a spheroid, or a tri-axial ellipsoid?\nFirst you must express the so-called quadratic form \\(Q(x, y, z)\\) in matrix form as \\(Q(x, y, z)=\\) \\([x, y, z] A[x, y, z]^{T}\\).\nNext, replace \\(A\\) by the equivalent matrix \\(\\left(A+A^{T}\\right) / 2\\). (Check that if \\(A\\) specifies the quadratic form \\(Q\\), then so will \\(A^\\prime = \\left(A+A^{T}\\right) / 2\\).)\nNext, diagonalize your symmetric matrix. You can use the command diagonalize in sympy to do this, but check that the results are what you’d expect given the eigenvalues and eigenvectors of your symmetric matrix. The result is a diagonal matrix \\(D\\) and an orthogonal matrix \\(P\\) such that \\(P^{T} A^\\prime P=D\\).\nNext, make the linear change of variables \\([x, y, z]^{T}=P\\left[x^{\\prime}, y^{\\prime}, z^{\\prime}\\right]^{T}\\) and deduce that \\(Q(x, y, z)=\\) \\(\\left[x^{\\prime}, y^{\\prime}, z^{\\prime}\\right] P^{T} A^\\prime P\\left[x^{\\prime}, y^{\\prime}, z^{\\prime}\\right]^{T}\\). If \\(P^{T} A^\\prime P\\) is diagonal, we end up with squares of \\(x^{\\prime}, y^{\\prime}\\) and \\(z^{\\prime}\\), and no mixed terms.\nFrom this data alone you should be able to classify the surface represented by the above equation.\nConfirm your results by plotting the surface in the new coordinates.\nAn outstanding reference on this topic and many others relating to matrix analysis is the recently republished textbook [3] by Richard Bellman, widely considered to be a classic in the field.\n(Hint: when I did this problem, I encountered very small imagainary part at certain points. I used the sp.nsimplify(sp.N(f),rational=True) function to get rid of these.)\n\nFirst write the quadratic form in matrix form\n\nimport sympy as sp\nfrom sympy.abc import a, b, c, d, e, f, g, h, i, x, y, z\n\nM = sp.Matrix([[2, 0, 0], [-4, 4, 0], [-2, 2, 6]])\nxx = sp.Matrix([x, y, z])\nquadratic_form = xx.T*M*xx\n# simply pulling all quadratic terms together\nquadratic_form = quadratic_form[0].expand()\ndisplay(quadratic_form)\n\n\\(\\displaystyle 2 x^{2} - 4 x y - 2 x z + 4 y^{2} + 2 y z + 6 z^{2}\\)\n\n\nNow find a symmetric matric\n\n# find a symmetric matrix\nA = sp.nsimplify(1/2*(M + M.T),rational=True)\ndisplay(A)\n# check that the quadratic form is the same\ndisplay((xx.T*A*xx)[0].expand())\n\n\\(\\displaystyle \\left[\\begin{matrix}2 & -2 & -1\\\\-2 & 4 & 1\\\\-1 & 1 & 6\\end{matrix}\\right]\\)\n\n\n\\(\\displaystyle 2 x^{2} - 4 x y - 2 x z + 4 y^{2} + 2 y z + 6 z^{2}\\)\n\n\nNow find the eigenvalues and eigenvectors of the symmetric matrix:\n\neigvals, eigvecs = A.eigenvals(), A.eigenvects()\n\nDiagonalize:\n\nP, D = A.diagonalize()\n\n\n3 simplify, and get rid of very small nonzero imaginary parts\n\ndd= sp.N(sp.nsimplify(sp.N(D),rational=True))\ndisplay(dd)\n\n\\(\\displaystyle \\left[\\begin{matrix}4.20080975647311 & 0 & 0\\\\0 & 7.05708725655633 & 0\\\\0 & 0 & 0.742102986970565\\end{matrix}\\right]\\)\n\n\n\n# define xp, yp and zp as symbol\nxp,yp,zp=sp.symbols('xp, yp, zp')\n\n\nxxprime = sp.Matrix([xp,yp,zp])\nnew_quadratic_form=sp.simplify(xxprime.T*dd*xxprime)\n\nMake a plot:\n\nfrom sympy.solvers import solve\nzeqn = solve(new_quadratic_form-sp.Matrix([1]),zp)\nlimit = 0.5\nfrom sympy.plotting import plot3d as plot3d\nplot3d(zeqn[1],(xp,-limit, limit),(yp,-limit, limit))\nplot3d(zeqn[0],(xp,-limit, limit),(yp,-limit, limit))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the original coordinates:\n\norig_z_eqn = solve(quadratic_form-1,z)\nlimit=2\nplot3d(orig_z_eqn[1],(x,-limit, limit),(y,-limit, limit))\nplot3d(orig_z_eqn[0],(x,-limit,limit),(y,-limit, limit))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4 Management of Sheep Populations\nDescription of the problem: You are working for the New Zealand Department of Agriculture on a project for sheep farmers. The species of sheep that these shepherds raise have a life span of 12 years. Of course, some live longer, but they are sufficiently few in number and their reproductive rate is so low that they may be ignored in your population study. Accordingly, you divide sheep into 12 age classes, namely those in the first year of life, etc. An extensive survey of the demographics of this species of sheep results in the following approximations for the demographic parameters \\(f_{i}\\) and \\(s_{i}\\), where \\(f_{i}\\) is the percapita reproductive rate for sheep in the \\(i\\) th age class and \\(s_{i}\\) is the survival rate for sheep in that age class, i.e., the fraction of sheep in that age class that survive to the \\((i+1)\\) th class. (As a matter of fact, this table is related to real data. The interested reader might consult the article [7] in the bibliography.)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(i\\)\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n\n\\(f_{i}\\)\n.000\n.023\n.145\n.236\n.242\n.273\n.271\n.251\n.234\n.229\n.216\n.210\n\n\n\\(s_{i}\\)\n.845\n.975\n.965\n.950\n.926\n.895\n.850\n.786\n.691\n.561\n.370\n-\n\n\n\nThe problem is as follows: in order to maintain a constant population of sheep, shepherds will harvest a certain number of sheep each year. Harvesting need not mean slaughter; it simply means removing sheep from the population (e.g., selling animals to other shepherds). Denote the fraction of sheep that are removed from the \\(i\\) th age group at the end of each growth period (a year in our case) by \\(h_{i}\\). If these numbers are constant from year to year, they constitute a harvesting policy. If, moreover, the yield of each harvest, i.e., total number of animals harvested each year, is a constant and the age distribution of the remaining populace is essentially constant after each harvest, then the harvesting policy is called sustainable. If all the \\(h_{i}\\) ’s are the same, say \\(h\\), then the harvesting policy is called uniform. Uniform policies are simple to implement: One selects the sheep to be harvested at random.\nYour problem: Find a uniform sustainable harvesting policy to recommend to shepherds, and find the resulting distribution of sheep that they can expect with this policy. Shepherds who raise sheep for sale to markets are also interested in a sustainable policy that gives a maximum yield. If you can find such a policy that has a larger annual yield than the uniform policy, then recommend it. On the other hand, shepherds who raise sheep for their wool may prefer to minimize the annual yield. If you can find a sustainable policy whose yield is smaller than that of the uniform policy, make a recommendation accordingly. In each case find the expected distribution of your harvesting policies. Do you think that there might be other economic factors that should be taken into account in this model? Organize your results for a report to be read by your supervisor and an informed public.\nProcedure: Express this problem as a discrete linear dynamical system \\(\\mathbf{x}^{(k+1)}=L \\mathbf{x}^{(k)}\\), where \\(L\\) is a so-called Leslie matrix of the form\n\\[\nL=\\left[\\begin{array}{cccccc}\nf_{1} & f_{2} & f_{3} & \\cdots & f_{n-1} & f_{n} \\\\\ns_{1} & 0 & 0 & \\cdots & 0 & 0 \\\\\n0 & s_{2} & 0 & \\cdots & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & s_{n-1} & 0\n\\end{array}\\right]\n\\]\nIt is understood that \\(0&lt;s_{i} \\leq 1,0 \\leq f_{i}\\), and at least one \\(f_{i}\\) is nonzero.\nIn regard to harvesting, let \\(H\\) be a diagonal matrix with the harvest fractions \\(h_{i}\\) down the diagonal. (Here \\(0 \\leq h_{i} \\leq 1\\).) Then the population that results from this harvesting at the end of each period is given by \\(\\mathbf{x}^{k+1}=L \\mathbf{x}^{k}-H L \\mathbf{x}^{k}=(I-H) L \\mathbf{x}^{k}\\). There are other theoretical tools, but all you need to do is to find a matrix \\(H\\) such that 1 is the dominant eigenvalue of \\((I-H) L\\). You can do this by trial and error, a method that is applicable to any harvesting policy, uniform or not. However, in the case of uniform policies it’s simpler to note that \\((I-H) L=(1-h) L\\), where \\(h\\) is the diagonal entry of \\(H\\). Find an eigenvector corresponding to this eigenvalue and scale this vector by dividing it by the sum of its components to obtains a probability distribution vector that could be used for any population."
  },
  {
    "objectID": "HW/projects_3.sol.html#revisiting-a-previous-project",
    "href": "HW/projects_3.sol.html#revisiting-a-previous-project",
    "title": "Projects 3 Solutions",
    "section": "1.1 Revisiting a previous project",
    "text": "1.1 Revisiting a previous project\nRevisit one of the previous projects using new concepts which we have learned since you first did the project. (The “Image Compression” project on page 389 of your textbook would be one example of this, but you may take this in any direction you like.)"
  },
  {
    "objectID": "HW/projects_3.sol.html#quadratic-forms",
    "href": "HW/projects_3.sol.html#quadratic-forms",
    "title": "Projects 3 Solutions",
    "section": "2.1 Quadratic Forms",
    "text": "2.1 Quadratic Forms\nIn order to classify quadratic equations in \\(x\\) and \\(y\\) one goes through roughly three steps. First, perform a rotation transformation of coordinates to get rid of mixed terms such as \\(2 x y\\) in the quadratic equation \\(x^{2}+2 x y-y^{2}+x-3 y=4\\). Second, do a translation of coordinates to put the equation in a “standard form.” Third, identify the curve by your knowledge of the shape of a curve in that standard form. Standard forms are equations like \\(x^{2} / 4+y^{2} / 2=1\\), an ellipse with its axes along the \\(x\\) - and \\(y\\)-axes. It is the second-degree terms \\(\\left(x^{2}, 2 x y\\right.\\), and \\(\\left.y^{2}\\right)\\) alone that determine the nature of a quadratic.\nNow you’re ready for the rest of the story. Just as with curves in \\(x\\) and \\(y\\), the basic shape of the surface of a quadric equation in \\(x, y\\), and \\(z\\) is determined by the second-degree terms. So we will focus on an example with no first-degree terms, namely,\n\\[\nQ(x, y, z)=2 x^{2}+4 y^{2}+6 z^{2}-4 x y-2 x z+2 y z=1 .\n\\]\nThe problem is this: find a change of coordinates that will make it clear what standard forms is represented by this surface – is it a sphere, a spheroid, or a tri-axial ellipsoid?\nFirst you must express the so-called quadratic form \\(Q(x, y, z)\\) in matrix form as \\(Q(x, y, z)=\\) \\([x, y, z] A[x, y, z]^{T}\\).\nNext, replace \\(A\\) by the equivalent matrix \\(\\left(A+A^{T}\\right) / 2\\). (Check that if \\(A\\) specifies the quadratic form \\(Q\\), then so will \\(A^\\prime = \\left(A+A^{T}\\right) / 2\\).)\nNext, diagonalize your symmetric matrix. You can use the command diagonalize in sympy to do this, but check that the results are what you’d expect given the eigenvalues and eigenvectors of your symmetric matrix. The result is a diagonal matrix \\(D\\) and an orthogonal matrix \\(P\\) such that \\(P^{T} A^\\prime P=D\\).\nNext, make the linear change of variables \\([x, y, z]^{T}=P\\left[x^{\\prime}, y^{\\prime}, z^{\\prime}\\right]^{T}\\) and deduce that \\(Q(x, y, z)=\\) \\(\\left[x^{\\prime}, y^{\\prime}, z^{\\prime}\\right] P^{T} A^\\prime P\\left[x^{\\prime}, y^{\\prime}, z^{\\prime}\\right]^{T}\\). If \\(P^{T} A^\\prime P\\) is diagonal, we end up with squares of \\(x^{\\prime}, y^{\\prime}\\) and \\(z^{\\prime}\\), and no mixed terms.\nFrom this data alone you should be able to classify the surface represented by the above equation.\nConfirm your results by plotting the surface in the new coordinates.\nAn outstanding reference on this topic and many others relating to matrix analysis is the recently republished textbook [3] by Richard Bellman, widely considered to be a classic in the field.\n(Hint: when I did this problem, I encountered very small imagainary part at certain points. I used the sp.nsimplify(sp.N(f),rational=True) function to get rid of these.)\n\nFirst write the quadratic form in matrix form\n\nimport sympy as sp\nfrom sympy.abc import a, b, c, d, e, f, g, h, i, x, y, z\n\nM = sp.Matrix([[2, 0, 0], [-4, 4, 0], [-2, 2, 6]])\nxx = sp.Matrix([x, y, z])\nquadratic_form = xx.T*M*xx\n# simply pulling all quadratic terms together\nquadratic_form = quadratic_form[0].expand()\ndisplay(quadratic_form)\n\n\\(\\displaystyle 2 x^{2} - 4 x y - 2 x z + 4 y^{2} + 2 y z + 6 z^{2}\\)\n\n\nNow find a symmetric matric\n\n# find a symmetric matrix\nA = sp.nsimplify(1/2*(M + M.T),rational=True)\ndisplay(A)\n# check that the quadratic form is the same\ndisplay((xx.T*A*xx)[0].expand())\n\n\\(\\displaystyle \\left[\\begin{matrix}2 & -2 & -1\\\\-2 & 4 & 1\\\\-1 & 1 & 6\\end{matrix}\\right]\\)\n\n\n\\(\\displaystyle 2 x^{2} - 4 x y - 2 x z + 4 y^{2} + 2 y z + 6 z^{2}\\)\n\n\nNow find the eigenvalues and eigenvectors of the symmetric matrix:\n\neigvals, eigvecs = A.eigenvals(), A.eigenvects()\n\nDiagonalize:\n\nP, D = A.diagonalize()\n\n\n3 simplify, and get rid of very small nonzero imaginary parts\n\ndd= sp.N(sp.nsimplify(sp.N(D),rational=True))\ndisplay(dd)\n\n\\(\\displaystyle \\left[\\begin{matrix}4.20080975647311 & 0 & 0\\\\0 & 7.05708725655633 & 0\\\\0 & 0 & 0.742102986970565\\end{matrix}\\right]\\)\n\n\n\n# define xp, yp and zp as symbol\nxp,yp,zp=sp.symbols('xp, yp, zp')\n\n\nxxprime = sp.Matrix([xp,yp,zp])\nnew_quadratic_form=sp.simplify(xxprime.T*dd*xxprime)\n\nMake a plot:\n\nfrom sympy.solvers import solve\nzeqn = solve(new_quadratic_form-sp.Matrix([1]),zp)\nlimit = 0.5\nfrom sympy.plotting import plot3d as plot3d\nplot3d(zeqn[1],(xp,-limit, limit),(yp,-limit, limit))\nplot3d(zeqn[0],(xp,-limit, limit),(yp,-limit, limit))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the original coordinates:\n\norig_z_eqn = solve(quadratic_form-1,z)\nlimit=2\nplot3d(orig_z_eqn[1],(x,-limit, limit),(y,-limit, limit))\nplot3d(orig_z_eqn[0],(x,-limit,limit),(y,-limit, limit))"
  },
  {
    "objectID": "HW/projects_3.sol.html#management-of-sheep-populations",
    "href": "HW/projects_3.sol.html#management-of-sheep-populations",
    "title": "Projects 3 Solutions",
    "section": "4.1 Management of Sheep Populations",
    "text": "4.1 Management of Sheep Populations\nDescription of the problem: You are working for the New Zealand Department of Agriculture on a project for sheep farmers. The species of sheep that these shepherds raise have a life span of 12 years. Of course, some live longer, but they are sufficiently few in number and their reproductive rate is so low that they may be ignored in your population study. Accordingly, you divide sheep into 12 age classes, namely those in the first year of life, etc. An extensive survey of the demographics of this species of sheep results in the following approximations for the demographic parameters \\(f_{i}\\) and \\(s_{i}\\), where \\(f_{i}\\) is the percapita reproductive rate for sheep in the \\(i\\) th age class and \\(s_{i}\\) is the survival rate for sheep in that age class, i.e., the fraction of sheep in that age class that survive to the \\((i+1)\\) th class. (As a matter of fact, this table is related to real data. The interested reader might consult the article [7] in the bibliography.)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(i\\)\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n\n\\(f_{i}\\)\n.000\n.023\n.145\n.236\n.242\n.273\n.271\n.251\n.234\n.229\n.216\n.210\n\n\n\\(s_{i}\\)\n.845\n.975\n.965\n.950\n.926\n.895\n.850\n.786\n.691\n.561\n.370\n-\n\n\n\nThe problem is as follows: in order to maintain a constant population of sheep, shepherds will harvest a certain number of sheep each year. Harvesting need not mean slaughter; it simply means removing sheep from the population (e.g., selling animals to other shepherds). Denote the fraction of sheep that are removed from the \\(i\\) th age group at the end of each growth period (a year in our case) by \\(h_{i}\\). If these numbers are constant from year to year, they constitute a harvesting policy. If, moreover, the yield of each harvest, i.e., total number of animals harvested each year, is a constant and the age distribution of the remaining populace is essentially constant after each harvest, then the harvesting policy is called sustainable. If all the \\(h_{i}\\) ’s are the same, say \\(h\\), then the harvesting policy is called uniform. Uniform policies are simple to implement: One selects the sheep to be harvested at random.\nYour problem: Find a uniform sustainable harvesting policy to recommend to shepherds, and find the resulting distribution of sheep that they can expect with this policy. Shepherds who raise sheep for sale to markets are also interested in a sustainable policy that gives a maximum yield. If you can find such a policy that has a larger annual yield than the uniform policy, then recommend it. On the other hand, shepherds who raise sheep for their wool may prefer to minimize the annual yield. If you can find a sustainable policy whose yield is smaller than that of the uniform policy, make a recommendation accordingly. In each case find the expected distribution of your harvesting policies. Do you think that there might be other economic factors that should be taken into account in this model? Organize your results for a report to be read by your supervisor and an informed public.\nProcedure: Express this problem as a discrete linear dynamical system \\(\\mathbf{x}^{(k+1)}=L \\mathbf{x}^{(k)}\\), where \\(L\\) is a so-called Leslie matrix of the form\n\\[\nL=\\left[\\begin{array}{cccccc}\nf_{1} & f_{2} & f_{3} & \\cdots & f_{n-1} & f_{n} \\\\\ns_{1} & 0 & 0 & \\cdots & 0 & 0 \\\\\n0 & s_{2} & 0 & \\cdots & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & s_{n-1} & 0\n\\end{array}\\right]\n\\]\nIt is understood that \\(0&lt;s_{i} \\leq 1,0 \\leq f_{i}\\), and at least one \\(f_{i}\\) is nonzero.\nIn regard to harvesting, let \\(H\\) be a diagonal matrix with the harvest fractions \\(h_{i}\\) down the diagonal. (Here \\(0 \\leq h_{i} \\leq 1\\).) Then the population that results from this harvesting at the end of each period is given by \\(\\mathbf{x}^{k+1}=L \\mathbf{x}^{k}-H L \\mathbf{x}^{k}=(I-H) L \\mathbf{x}^{k}\\). There are other theoretical tools, but all you need to do is to find a matrix \\(H\\) such that 1 is the dominant eigenvalue of \\((I-H) L\\). You can do this by trial and error, a method that is applicable to any harvesting policy, uniform or not. However, in the case of uniform policies it’s simpler to note that \\((I-H) L=(1-h) L\\), where \\(h\\) is the diagonal entry of \\(H\\). Find an eigenvector corresponding to this eigenvalue and scale this vector by dividing it by the sum of its components to obtains a probability distribution vector that could be used for any population."
  },
  {
    "objectID": "HW/projects_3.html",
    "href": "HW/projects_3.html",
    "title": "Projects 3",
    "section": "",
    "text": "Instructions:\nChoose two of the following projects to complete.\nPlease submit a zip file containing your code (in .ipynb or .qmd format), any data files you load in, and if possible a pdf of your notebook. (A link to a GitHub repository or Google CoLab notebook would also work, so long as they are publically accessible.)\n\n1 Revisiting a previous project\nRevisit one of the previous projects using new concepts which we have learned since you first did the project. (The “Image Compression” project on page 389 of your textbook would be one example of this, but you may take this in any direction you like.)\n\n\n2 Quadratic Forms\nIn order to classify quadratic equations in \\(x\\) and \\(y\\) one goes through roughly three steps. First, perform a rotation transformation of coordinates to get rid of mixed terms such as \\(2 x y\\) in the quadratic equation \\(x^{2}+2 x y-y^{2}+x-3 y=4\\). Second, do a translation of coordinates to put the equation in a “standard form.” Third, identify the curve by your knowledge of the shape of a curve in that standard form. Standard forms are equations like \\(x^{2} / 4+y^{2} / 2=1\\), an ellipse with its axes along the \\(x\\) - and \\(y\\)-axes. It is the second-degree terms \\(\\left(x^{2}, 2 x y\\right.\\), and \\(\\left.y^{2}\\right)\\) alone that determine the nature of a quadratic.\nNow you’re ready for the rest of the story. Just as with curves in \\(x\\) and \\(y\\), the basic shape of the surface of a quadric equation in \\(x, y\\), and \\(z\\) is determined by the second-degree terms. So we will focus on an example with no first-degree terms, namely,\n\\[\nQ(x, y, z)=2 x^{2}+4 y^{2}+6 z^{2}-4 x y-2 x z+2 y z=1 .\n\\]\nThe problem is this: find a change of coordinates that will make it clear what standard forms is represented by this surface – is it a sphere, a spheroid, or a tri-axial ellipsoid?\nFirst you must express the so-called quadratic form \\(Q(x, y, z)\\) in matrix form as \\(Q(x, y, z)=\\) \\([x, y, z] A[x, y, z]^{T}\\).\nNext, replace \\(A\\) by the equivalent matrix \\(\\left(A+A^{T}\\right) / 2\\). (Check that if \\(A\\) specifies the quadratic form \\(Q\\), then so will \\(A^\\prime = \\left(A+A^{T}\\right) / 2\\).)\nNext, diagonalize your symmetric matrix. You can use the command diagonalize in sympy to do this, but check that the results are what you’d expect given the eigenvalues and eigenvectors of your symmetric matrix. The result is a diagonal matrix \\(D\\) and an orthogonal matrix \\(P\\) such that \\(P^{T} A^\\prime P=D\\).\nNext, make the linear change of variables \\([x, y, z]^{T}=P\\left[x^{\\prime}, y^{\\prime}, z^{\\prime}\\right]^{T}\\) and deduce that \\(Q(x, y, z)=\\) \\(\\left[x^{\\prime}, y^{\\prime}, z^{\\prime}\\right] P^{T} A^\\prime P\\left[x^{\\prime}, y^{\\prime}, z^{\\prime}\\right]^{T}\\). If \\(P^{T} A^\\prime P\\) is diagonal, we end up with squares of \\(x^{\\prime}, y^{\\prime}\\) and \\(z^{\\prime}\\), and no mixed terms.\nFrom this data alone you should be able to classify the surface represented by the above equation.\nConfirm your results by plotting the surface in the new coordinates.\nAn outstanding reference on this topic and many others relating to matrix analysis is the recently republished textbook [3] by Richard Bellman, widely considered to be a classic in the field.\n(Hint: when I did this problem, I encountered very small imagainary part at certain points. I used the sp.nsimplify(sp.N(f),rational=True) function to get rid of these.)\n\n\n3 Management of Sheep Populations\nDescription of the problem: You are working for the New Zealand Department of Agriculture on a project for sheep farmers. The species of sheep that these shepherds raise have a life span of 12 years. Of course, some live longer, but they are sufficiently few in number and their reproductive rate is so low that they may be ignored in your population study. Accordingly, you divide sheep into 12 age classes, namely those in the first year of life, etc. An extensive survey of the demographics of this species of sheep results in the following approximations for the demographic parameters \\(f_{i}\\) and \\(s_{i}\\), where \\(f_{i}\\) is the percapita reproductive rate for sheep in the \\(i\\) th age class and \\(s_{i}\\) is the survival rate for sheep in that age class, i.e., the fraction of sheep in that age class that survive to the \\((i+1)\\) th class. (As a matter of fact, this table is related to real data. The interested reader might consult the article [7] in the bibliography.)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(i\\)\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n\n\\(f_{i}\\)\n.000\n.023\n.145\n.236\n.242\n.273\n.271\n.251\n.234\n.229\n.216\n.210\n\n\n\\(s_{i}\\)\n.845\n.975\n.965\n.950\n.926\n.895\n.850\n.786\n.691\n.561\n.370\n-\n\n\n\nThe problem is as follows: in order to maintain a constant population of sheep, shepherds will harvest a certain number of sheep each year. Harvesting need not mean slaughter; it simply means removing sheep from the population (e.g., selling animals to other shepherds). Denote the fraction of sheep that are removed from the \\(i\\) th age group at the end of each growth period (a year in our case) by \\(h_{i}\\). If these numbers are constant from year to year, they constitute a harvesting policy. If, moreover, the yield of each harvest, i.e., total number of animals harvested each year, is a constant and the age distribution of the remaining populace is essentially constant after each harvest, then the harvesting policy is called sustainable. If all the \\(h_{i}\\) ’s are the same, say \\(h\\), then the harvesting policy is called uniform. Uniform policies are simple to implement: One selects the sheep to be harvested at random.\nYour problem: Find a uniform sustainable harvesting policy to recommend to shepherds, and find the resulting distribution of sheep that they can expect with this policy. Shepherds who raise sheep for sale to markets are also interested in a sustainable policy that gives a maximum yield. If you can find such a policy that has a larger annual yield than the uniform policy, then recommend it. On the other hand, shepherds who raise sheep for their wool may prefer to minimize the annual yield. If you can find a sustainable policy whose yield is smaller than that of the uniform policy, make a recommendation accordingly. In each case find the expected distribution of your harvesting policies. Do you think that there might be other economic factors that should be taken into account in this model? Organize your results for a report to be read by your supervisor and an informed public.\nProcedure: Express this problem as a discrete linear dynamical system \\(\\mathbf{x}^{(k+1)}=L \\mathbf{x}^{(k)}\\), where \\(L\\) is a so-called Leslie matrix of the form\n\\[\nL=\\left[\\begin{array}{cccccc}\nf_{1} & f_{2} & f_{3} & \\cdots & f_{n-1} & f_{n} \\\\\ns_{1} & 0 & 0 & \\cdots & 0 & 0 \\\\\n0 & s_{2} & 0 & \\cdots & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & s_{n-1} & 0\n\\end{array}\\right]\n\\]\nIt is understood that \\(0&lt;s_{i} \\leq 1,0 \\leq f_{i}\\), and at least one \\(f_{i}\\) is nonzero.\nIn regard to harvesting, let \\(H\\) be a diagonal matrix with the harvest fractions \\(h_{i}\\) down the diagonal. (Here \\(0 \\leq h_{i} \\leq 1\\).) Then the population that results from this harvesting at the end of each period is given by \\(\\mathbf{x}^{k+1}=L \\mathbf{x}^{k}-H L \\mathbf{x}^{k}=(I-H) L \\mathbf{x}^{k}\\). There are other theoretical tools, but all you need to do is to find a matrix \\(H\\) such that 1 is the dominant eigenvalue of \\((I-H) L\\). You can do this by trial and error, a method that is applicable to any harvesting policy, uniform or not. However, in the case of uniform policies it’s simpler to note that \\((I-H) L=(1-h) L\\), where \\(h\\) is the diagonal entry of \\(H\\). Find an eigenvector corresponding to this eigenvalue and scale this vector by dividing it by the sum of its components to obtains a probability distribution vector that could be used for any population."
  },
  {
    "objectID": "HW/projects_3.html#revisiting-a-previous-project",
    "href": "HW/projects_3.html#revisiting-a-previous-project",
    "title": "Projects 3",
    "section": "1.1 Revisiting a previous project",
    "text": "1.1 Revisiting a previous project\nRevisit one of the previous projects using new concepts which we have learned since you first did the project. (The “Image Compression” project on page 389 of your textbook would be one example of this, but you may take this in any direction you like.)"
  },
  {
    "objectID": "HW/projects_3.html#quadratic-forms",
    "href": "HW/projects_3.html#quadratic-forms",
    "title": "Projects 3",
    "section": "2.1 Quadratic Forms",
    "text": "2.1 Quadratic Forms\nIn order to classify quadratic equations in \\(x\\) and \\(y\\) one goes through roughly three steps. First, perform a rotation transformation of coordinates to get rid of mixed terms such as \\(2 x y\\) in the quadratic equation \\(x^{2}+2 x y-y^{2}+x-3 y=4\\). Second, do a translation of coordinates to put the equation in a “standard form.” Third, identify the curve by your knowledge of the shape of a curve in that standard form. Standard forms are equations like \\(x^{2} / 4+y^{2} / 2=1\\), an ellipse with its axes along the \\(x\\) - and \\(y\\)-axes. It is the second-degree terms \\(\\left(x^{2}, 2 x y\\right.\\), and \\(\\left.y^{2}\\right)\\) alone that determine the nature of a quadratic.\nNow you’re ready for the rest of the story. Just as with curves in \\(x\\) and \\(y\\), the basic shape of the surface of a quadric equation in \\(x, y\\), and \\(z\\) is determined by the second-degree terms. So we will focus on an example with no first-degree terms, namely,\n\\[\nQ(x, y, z)=2 x^{2}+4 y^{2}+6 z^{2}-4 x y-2 x z+2 y z=1 .\n\\]\nThe problem is this: find a change of coordinates that will make it clear what standard forms is represented by this surface – is it a sphere, a spheroid, or a tri-axial ellipsoid?\nFirst you must express the so-called quadratic form \\(Q(x, y, z)\\) in matrix form as \\(Q(x, y, z)=\\) \\([x, y, z] A[x, y, z]^{T}\\).\nNext, replace \\(A\\) by the equivalent matrix \\(\\left(A+A^{T}\\right) / 2\\). (Check that if \\(A\\) specifies the quadratic form \\(Q\\), then so will \\(A^\\prime = \\left(A+A^{T}\\right) / 2\\).)\nNext, diagonalize your symmetric matrix. You can use the command diagonalize in sympy to do this, but check that the results are what you’d expect given the eigenvalues and eigenvectors of your symmetric matrix. The result is a diagonal matrix \\(D\\) and an orthogonal matrix \\(P\\) such that \\(P^{T} A^\\prime P=D\\).\nNext, make the linear change of variables \\([x, y, z]^{T}=P\\left[x^{\\prime}, y^{\\prime}, z^{\\prime}\\right]^{T}\\) and deduce that \\(Q(x, y, z)=\\) \\(\\left[x^{\\prime}, y^{\\prime}, z^{\\prime}\\right] P^{T} A^\\prime P\\left[x^{\\prime}, y^{\\prime}, z^{\\prime}\\right]^{T}\\). If \\(P^{T} A^\\prime P\\) is diagonal, we end up with squares of \\(x^{\\prime}, y^{\\prime}\\) and \\(z^{\\prime}\\), and no mixed terms.\nFrom this data alone you should be able to classify the surface represented by the above equation.\nConfirm your results by plotting the surface in the new coordinates.\nAn outstanding reference on this topic and many others relating to matrix analysis is the recently republished textbook [3] by Richard Bellman, widely considered to be a classic in the field.\n(Hint: when I did this problem, I encountered very small imagainary part at certain points. I used the sp.nsimplify(sp.N(f),rational=True) function to get rid of these.)"
  },
  {
    "objectID": "HW/projects_3.html#management-of-sheep-populations",
    "href": "HW/projects_3.html#management-of-sheep-populations",
    "title": "Projects 3",
    "section": "3.1 Management of Sheep Populations",
    "text": "3.1 Management of Sheep Populations\nDescription of the problem: You are working for the New Zealand Department of Agriculture on a project for sheep farmers. The species of sheep that these shepherds raise have a life span of 12 years. Of course, some live longer, but they are sufficiently few in number and their reproductive rate is so low that they may be ignored in your population study. Accordingly, you divide sheep into 12 age classes, namely those in the first year of life, etc. An extensive survey of the demographics of this species of sheep results in the following approximations for the demographic parameters \\(f_{i}\\) and \\(s_{i}\\), where \\(f_{i}\\) is the percapita reproductive rate for sheep in the \\(i\\) th age class and \\(s_{i}\\) is the survival rate for sheep in that age class, i.e., the fraction of sheep in that age class that survive to the \\((i+1)\\) th class. (As a matter of fact, this table is related to real data. The interested reader might consult the article [7] in the bibliography.)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(i\\)\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n\n\\(f_{i}\\)\n.000\n.023\n.145\n.236\n.242\n.273\n.271\n.251\n.234\n.229\n.216\n.210\n\n\n\\(s_{i}\\)\n.845\n.975\n.965\n.950\n.926\n.895\n.850\n.786\n.691\n.561\n.370\n-\n\n\n\nThe problem is as follows: in order to maintain a constant population of sheep, shepherds will harvest a certain number of sheep each year. Harvesting need not mean slaughter; it simply means removing sheep from the population (e.g., selling animals to other shepherds). Denote the fraction of sheep that are removed from the \\(i\\) th age group at the end of each growth period (a year in our case) by \\(h_{i}\\). If these numbers are constant from year to year, they constitute a harvesting policy. If, moreover, the yield of each harvest, i.e., total number of animals harvested each year, is a constant and the age distribution of the remaining populace is essentially constant after each harvest, then the harvesting policy is called sustainable. If all the \\(h_{i}\\) ’s are the same, say \\(h\\), then the harvesting policy is called uniform. Uniform policies are simple to implement: One selects the sheep to be harvested at random.\nYour problem: Find a uniform sustainable harvesting policy to recommend to shepherds, and find the resulting distribution of sheep that they can expect with this policy. Shepherds who raise sheep for sale to markets are also interested in a sustainable policy that gives a maximum yield. If you can find such a policy that has a larger annual yield than the uniform policy, then recommend it. On the other hand, shepherds who raise sheep for their wool may prefer to minimize the annual yield. If you can find a sustainable policy whose yield is smaller than that of the uniform policy, make a recommendation accordingly. In each case find the expected distribution of your harvesting policies. Do you think that there might be other economic factors that should be taken into account in this model? Organize your results for a report to be read by your supervisor and an informed public.\nProcedure: Express this problem as a discrete linear dynamical system \\(\\mathbf{x}^{(k+1)}=L \\mathbf{x}^{(k)}\\), where \\(L\\) is a so-called Leslie matrix of the form\n\\[\nL=\\left[\\begin{array}{cccccc}\nf_{1} & f_{2} & f_{3} & \\cdots & f_{n-1} & f_{n} \\\\\ns_{1} & 0 & 0 & \\cdots & 0 & 0 \\\\\n0 & s_{2} & 0 & \\cdots & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & s_{n-1} & 0\n\\end{array}\\right]\n\\]\nIt is understood that \\(0&lt;s_{i} \\leq 1,0 \\leq f_{i}\\), and at least one \\(f_{i}\\) is nonzero.\nIn regard to harvesting, let \\(H\\) be a diagonal matrix with the harvest fractions \\(h_{i}\\) down the diagonal. (Here \\(0 \\leq h_{i} \\leq 1\\).) Then the population that results from this harvesting at the end of each period is given by \\(\\mathbf{x}^{k+1}=L \\mathbf{x}^{k}-H L \\mathbf{x}^{k}=(I-H) L \\mathbf{x}^{k}\\). There are other theoretical tools, but all you need to do is to find a matrix \\(H\\) such that 1 is the dominant eigenvalue of \\((I-H) L\\). You can do this by trial and error, a method that is applicable to any harvesting policy, uniform or not. However, in the case of uniform policies it’s simpler to note that \\((I-H) L=(1-h) L\\), where \\(h\\) is the diagonal entry of \\(H\\). Find an eigenvector corresponding to this eigenvalue and scale this vector by dividing it by the sum of its components to obtains a probability distribution vector that could be used for any population."
  },
  {
    "objectID": "HW/projects_3.sol.html#instructions",
    "href": "HW/projects_3.sol.html#instructions",
    "title": "Projects 3 Solutions",
    "section": "",
    "text": "Choose two of the following projects to complete.\nPlease submit a zip file containing your code (in .ipynb or .qmd format), any data files you load in, and if possible a pdf of your notebook. (A link to a GitHub repository or Google CoLab notebook would also work, so long as they are publically accessible.)"
  },
  {
    "objectID": "HW/projects_3.html#instructions",
    "href": "HW/projects_3.html#instructions",
    "title": "Projects 3",
    "section": "",
    "text": "Choose two of the following projects to complete.\nPlease submit a zip file containing your code (in .ipynb or .qmd format), any data files you load in, and if possible a pdf of your notebook. (A link to a GitHub repository or Google CoLab notebook would also work, so long as they are publically accessible.)"
  },
  {
    "objectID": "HW/HW6.sol.html",
    "href": "HW/HW6.sol.html",
    "title": "Homework 6 Solutions",
    "section": "",
    "text": "1 \nShow that if the real matrix \\(A\\) is orthogonally diagonalizable, then \\(A\\) is symmetric.\n\n5.4.13\n\n\n\n2 \n\n5.6.3\n\nUse a technology tool to compute an orthonormal basis for the null space and column space of the following matrices with the SVD and Corollary 5.7. You will have to decide which nearly-zero terms are really zero.\n\n\\(\\left[\\begin{array}{rrr}1 & 1 & 3 \\\\ 0 & -1 & 0 \\\\ 1 & -2 & 2 \\\\ 3 & 0 & 2\\end{array}\\right]\\)\n\\(\\left[\\begin{array}{rrr}3 & 1 & 2 \\\\ 4 & 0 & 1 \\\\ -1 & 1 & 1\\end{array}\\right]\\)\n\\(\\left[\\begin{array}{ccccr}1 & 0 & 1 & 0 & -3 \\\\ 1 & 2 & 1 & -5 & 2 \\\\ 0 & 1 & 0 & -3 & 1 \\\\ 0 & 2 & -3 & 1 & 4\\end{array}\\right]\\)\n\n\n\n3 \n\n5.6.4\n\nUse the pseudoinverse to find a least squares solution \\(A \\mathbf{x}=\\mathbf{b}\\), where \\(A\\) is a matrix from Exercise 3 with corresponding right-hand side below.\n\n\\((2,2,6,5)\\)\n\\((2,3,1)\\)\n\\((4,1,2,3)\\)\n\n\n\n4 \nUse a technology tool to construct a \\(3 \\times 10\\) table whose \\(j\\) th column is \\(A^{j} \\mathbf{x}\\), where \\(\\mathbf{x}=(1,1,1)\\) and \\(A=\\left[\\begin{array}{rrr}10 & 17 & 8 \\\\ -8 & -13 & -6 \\\\ 4 & 7 & 4\\end{array}\\right]\\). What can you deduce about the eigenvalues of \\(A\\) based on inspection of this table? Give reasons. Check your claims by finding the eigenvalues of \\(A\\).\n\n5.3.14\n\nimport sympy as sp\n\nA = sp.Matrix([[10, 17, 8], [-8, -13, -6], [4, 7, 4]])\ndisplay(A)\nx = sp.Matrix([1, 1, 1])\nev1 = A.eigenvects()[0][2][0]\nev2 = A.eigenvects()[1][2][0]\nev3 = A.eigenvects()[2][2][0]\nev1, ev2, ev3\nmytable = sp.Matrix([(A**j * x).T for j in range(1, 11)]).T\nmytable\n\n\\(\\displaystyle \\left[\\begin{matrix}10 & 17 & 8\\\\-8 & -13 & -6\\\\4 & 7 & 4\\end{matrix}\\right]\\)\n\n\n\\(\\displaystyle \\left[\\begin{matrix}35 & 11 & -125 & -29 & 515 & 131 & -2045 & -509 & 8195 & 2051\\\\-27 & -19 & 93 & 61 & -387 & -259 & 1533 & 1021 & -6147 & -4099\\\\15 & 11 & -45 & -29 & 195 & 131 & -765 & -509 & 3075 & 2051\\end{matrix}\\right]\\)\n\n\nLargest eigenvalue must be greater than one because the numbers grow, and complex because they take two iterations to change sign. But the complex part probably comes in pairs which cancel each other.\n\n\n\n5 \n\n\n\n\n\n\nNote\n\n\n\nPlease note that in definition 6.20 in your book, there is an apparent error. The formula should be \\(H(\\zeta)=\\sum_{n=-\\infty}^{\\infty} h_n e^{-i \\zeta n}\\). Note the \\(n\\) in the exponent!\nThe sequence in the first paragraph of page 438 makes this clear, where it says “Here \\(H(\\zeta) = h_0 + h_1 e^{i \\zeta} + \\dots + h_L e^{i L \\zeta}\\).”\n\n\nCompute the DTFT for the FIR filter \\(\\mathbf{h}=\\left\\{\\frac{1}{2}, \\frac{1}{2}\\right\\}\\) and confirm that \\(\\mathbf{h}\\) is a lowpass filter.\n\n6.6.1\n\n\n\n6 \nCompute the DTFT for the FIR filter \\(\\mathbf{h}=\\left\\{-\\frac{1}{2}, \\frac{1}{2}\\right\\}\\) and confirm that \\(\\mathbf{h}\\) is a highpass filter.\n\n6.6.2\n\n\n\n7 \nApply the filters of the previous two exercises to the sampling problem of Example 6.24 and graph the results as in Figure 2.\n\n6.6.5 6.6.6\n\n\n\n8 \nCompute the Fourier series for \\(x(t) \\in C_{P W}^{1}[-\\pi, \\pi]\\), where \\(x(t)=\\) \\(t^{2} / \\pi,-\\pi \\leq t \\leq \\pi\\) and graph \\(x(t)\\) and the partial Fourier sums with \\(N=3,6\\).\n\n6.6.3"
  },
  {
    "objectID": "HW/HW6.html",
    "href": "HW/HW6.html",
    "title": "Homework 6",
    "section": "",
    "text": "1 \nShow that if the real matrix \\(A\\) is orthogonally diagonalizable, then \\(A\\) is symmetric.\n\n\n2 \nUse a technology tool to compute an orthonormal basis for the null space and column space of the following matrices with the SVD and Corollary 5.7. You will have to decide which nearly-zero terms are really zero.\n\n\\(\\left[\\begin{array}{rrr}1 & 1 & 3 \\\\ 0 & -1 & 0 \\\\ 1 & -2 & 2 \\\\ 3 & 0 & 2\\end{array}\\right]\\)\n\\(\\left[\\begin{array}{rrr}3 & 1 & 2 \\\\ 4 & 0 & 1 \\\\ -1 & 1 & 1\\end{array}\\right]\\)\n\\(\\left[\\begin{array}{ccccr}1 & 0 & 1 & 0 & -3 \\\\ 1 & 2 & 1 & -5 & 2 \\\\ 0 & 1 & 0 & -3 & 1 \\\\ 0 & 2 & -3 & 1 & 4\\end{array}\\right]\\)\n\n\n\n3 \nUse the pseudoinverse to find a least squares solution \\(A \\mathbf{x}=\\mathbf{b}\\), where \\(A\\) is a matrix from Exercise 3 with corresponding right-hand side below.\n\n\\((2,2,6,5)\\)\n\\((2,3,1)\\)\n\\((4,1,2,3)\\)\n\n\n\n4 \nUse a technology tool to construct a \\(3 \\times 10\\) table whose \\(j\\) th column is \\(A^{j} \\mathbf{x}\\), where \\(\\mathbf{x}=(1,1,1)\\) and \\(A=\\left[\\begin{array}{rrr}10 & 17 & 8 \\\\ -8 & -13 & -6 \\\\ 4 & 7 & 4\\end{array}\\right]\\). What can you deduce about the eigenvalues of \\(A\\) based on inspection of this table? Give reasons. Check your claims by finding the eigenvalues of \\(A\\).\n\n\n5 \n\n\n\n\n\n\nNote\n\n\n\nPlease note that in definition 6.20 in your book, there is an apparent error. The formula should be \\(H(\\zeta)=\\sum_{n=-\\infty}^{\\infty} h_n e^{-i \\zeta n}\\). Note the \\(n\\) in the exponent!\nThe sequence in the first paragraph of page 438 makes this clear, where it says “Here \\(H(\\zeta) = h_0 + h_1 e^{i \\zeta} + \\dots + h_L e^{i L \\zeta}\\).”\n\n\nCompute the DTFT for the FIR filter \\(\\mathbf{h}=\\left\\{\\frac{1}{2}, \\frac{1}{2}\\right\\}\\) and confirm that \\(\\mathbf{h}\\) is a lowpass filter.\n\n\n6 \nCompute the DTFT for the FIR filter \\(\\mathbf{h}=\\left\\{-\\frac{1}{2}, \\frac{1}{2}\\right\\}\\) and confirm that \\(\\mathbf{h}\\) is a highpass filter.\n\n\n7 \nApply the filters of the previous two exercises to the sampling problem of Example 6.24 and graph the results as in Figure 2.\n\n\n8 \nCompute the Fourier series for \\(x(t) \\in C_{P W}^{1}[-\\pi, \\pi]\\), where \\(x(t)=\\) \\(t^{2} / \\pi,-\\pi \\leq t \\leq \\pi\\) and graph \\(x(t)\\) and the partial Fourier sums with \\(N=3,6\\)."
  },
  {
    "objectID": "submitted/projects_2/proj 2/Haar.html",
    "href": "submitted/projects_2/proj 2/Haar.html",
    "title": "Stat 24320",
    "section": "",
    "text": "from PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nCode from chatGPT to load in and convert to Grayscale\n\ndef load_image_as_numpy_array(filepath):\n    # Open the image file\n    with Image.open(filepath) as img:\n        # Convert the image to grayscale\n        img_gray = img.convert('L')\n        # Convert the grayscale image to a numpy array\n        numpy_array = np.array(img_gray)\n    return numpy_array\n\nload the file in and take note of the filesize\n\nfilename = 'mypic.png'\nraw_matrix = load_image_as_numpy_array(filename)\nraw_matrix.shape\n\n(1318, 1079)\n\n\nSince Haar-Wavelet requires even dimensions, I will remove the last column\n\nmatrix = raw_matrix[:, :-1]\nmatrix.shape\n\n(1318, 1078)\n\n\nMore chatGPT code to display an image\n\ndef display_image_from_array(image_array):\n    # Display the numpy array as an image\n    plt.imshow(image_array, cmap='gray')  # 'gray' to specify grayscale display\n    plt.axis('off')  # Turn off axis numbers and ticks\n    plt.show()\n\nThis image was taken in my childhood at Tianamen Square not too long after the Tianamen Square Massacre. The railings that I am on did not exist prior to the Tiananmen Square Massacre. I hope you enjoy this snapshot in history.\n\nWow, thank you indeed for sharing this!\n\n\ndisplay_image_from_array(matrix)\n\n\n\n\n\n\n\n\nThe Haar Wavelet function matrix is created here. The loop iterates over half the dimension of the picture and in each iteration the sum and the difference of columns are created. Each iteration creates a row of the sum and difference. At the end the rows are all concatenated together to form a dim x dim matrix.\n\ndef make_haar_wavelet(dim):\n    hwRowsSum = []\n    hwRowsDiff = []\n    for i in range(int(dim/2)):\n        zeros = np.zeros(dim)\n        zeros[i*2] = 1\n        zeros[i*2+1] = 1\n        negs = np.copy(zeros)\n        negs[i*2] = -1\n        hwRowsSum.append(zeros)\n        hwRowsDiff.append(negs)\n    haar_wavem = np.sqrt(2)/2 * np.concatenate((hwRowsSum,hwRowsDiff))\n    print(haar_wavem.shape)\n    return haar_wavem\n\nThis function applies the Haar Wavelet transformation and returns the resulting matrix.\n\ndef haar_transform(mat):\n    haar_left = make_haar_wavelet(mat.shape[0])\n    haar_rite = make_haar_wavelet(mat.shape[1])\n    haar_mat = haar_left @ mat @ haar_rite.T\n    return haar_mat\n\nDisplays the image and crops the image to the blurred image\n\ndef show_haar_and_crop(hmat):\n    display_image_from_array(hmat)\n    upper_right = hmat.shape[0]//2\n    lower_left = hmat.shape[1]//2\n    cropped = hmat[:upper_right,:lower_left]\n    return cropped\n\nLoop over and apply the Haar-Wavelet transform, at each step I take the blurred picture and use that as the input to the next Haar Wavelet transformation. Iteration 4 is the last iteration where I can make out the finer details of the picture such as the bicycles in the background and the Tiananmen Square building. The image has been compressed to be 1/4^4 = 1/256 of the original size.\n\nimages = []\nimages.append(matrix)\nfull_images = [matrix]\ndisplay_image_from_array(matrix)\nfor i in range(6):\n    haar_matrix = haar_transform(images[-1])\n    full_images.append(haar_matrix)\n    print(f'iteration {i+1}')\n    images.append(show_haar_and_crop(haar_matrix))\n\n\n\n\n\n\n\n\n(1318, 1318)\n(1078, 1078)\niteration 1\n(658, 659)\n(538, 539)\niteration 2\n(328, 329)\n(268, 269)\niteration 3\n(164, 164)\n(134, 134)\niteration 4\n(82, 82)\n(66, 67)\niteration 5\n(40, 41)\n(32, 33)\niteration 6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere I try a lossy compression by rounding the blurred image to integers and then converting the least frequent 10 values into 0’s. The results weren’t that great as the image suffered in quality without that many values converted to 0’s. The rounding looks like it does save space without compromising the quality of the image by much.\n\ndef round_to_zero(image):\n    rounded = np.round(image).astype(np.uint8)\n    least = pd.Series(rounded.reshape(-1)).value_counts().iloc[-10:].index.values\n    to0 = np.isin(rounded,least)\n    num = to0.sum().sum()\n    print(f'number converted: {num}')\n    print(f'{num / image.size} converted to 0')\n    image[to0] = 0.0\n    return image\n\n\nimages2 = []\nimages2.append(matrix)\nfull_images2 = [matrix]\ndisplay_image_from_array(matrix)\nfor i in range(4):\n    haar_matrix = haar_transform(images2[-1])\n    full_images2.append(haar_matrix)\n    print(f'iteration {i+1}')\n    cropped = show_haar_and_crop(haar_matrix)\n    haar_rounded = round_to_zero(cropped)\n    images2.append(haar_rounded)\n\n\n\n\n\n\n\n\n(1318, 1318)\n(1078, 1078)\niteration 1\nnumber converted: 3895\n0.010965622281468803 converted to 0\n(658, 659)\n(538, 539)\niteration 2\nnumber converted: 1780\n0.020112767087377543 converted to 0\n(328, 329)\n(268, 269)\niteration 3\nnumber converted: 226\n0.01028394612304332 converted to 0\n(164, 164)\n(134, 134)\niteration 4\nnumber converted: 98\n0.017837641062977794 converted to 0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReverse Haar here for comparison between iteration with rounding and without\n\ndef reverse_haar(mat):\n    if mat.shape[0] %2 == 1:\n        mat = mat[:-1,:]\n    if mat.shape[1] % 2 == 1:\n        mat = mat[:,:-1]\n    haar_left = make_haar_wavelet(mat.shape[0])\n    haar_rite = make_haar_wavelet(mat.shape[1])\n    haar_mat = 2* haar_left.T @ mat @ haar_rite\n    return haar_mat\n\ndisplay_image_from_array(reverse_haar(full_images[4]))\n\n(164, 164)\n(134, 134)\n\n\n\n\n\n\n\n\n\n\ndisplay_image_from_array(reverse_haar(full_images2[4]))\n\n(164, 164)\n(134, 134)\n\n\n\n\n\n\n\n\n\nIt looks like it might be better to just do additional iterations rather than rounding too much.\nI will attempt to do edge detection from the example in the book. First do the Haar transformation, then 0 out the blurred image. Then do the inverse transformation\n\ndef edge_detection(mat):\n    haar_left = make_haar_wavelet(mat.shape[0])\n    haar_rite = make_haar_wavelet(mat.shape[1])\n    hmat = haar_left @ mat @ haar_rite.T\n    # zero out\n    upper_right = hmat.shape[0]//2\n    lower_left = hmat.shape[1]//2\n    hmat[:upper_right,:lower_left] = 0\n    # inverse transformation\n    Ae = 2 * haar_left.T @ hmat @ haar_rite\n    return Ae\n\n\nA_edges = edge_detection(matrix)\ndisplay_image_from_array(A_edges)\n\n(1318, 1318)\n(1078, 1078)\n\n\n\n\n\n\n\n\n\n\nYou’ve done the first part really nicely. The effects of the compression are very clear.\nYou came close with your edge detection, but then you gave up! I checked what was happening by plotting a histogram of the values in A_edges. I saw that many of them were negative, so then I tried making plots of the absolute value of A_edges. Using imshow directly, I could set the vmin and vmax values to show everything more clearly; also, I found it easier to see when I used cmap='hot'.\nAlso: when you do the rounding to 0, the idea is to do that on the portions of the image which contain the edge information, not on the blurred part of the image. That’s because the edge information parts are the ones which should have the most values that are close to zero… then once you have the zeros, you could try saving at least those parts of the matrix as sparse matrices, to see if that helps with the compression.\nGrade: M"
  },
  {
    "objectID": "submitted/projects_2/proj 2/LeastSquares.html",
    "href": "submitted/projects_2/proj 2/LeastSquares.html",
    "title": "find the correlation between y_test and predictions",
    "section": "",
    "text": "import sympy as sym\nimport numpy as np\nimport networkx as nx\nimport pandas as pd\n\nI am using NBA games from 2021 as the the training data and 2022 season as the test data\n\ngames_data = pd.read_csv('games.csv', usecols=['HOME_TEAM_ID', 'VISITOR_TEAM_ID', 'SEASON', 'PTS_home', 'PTS_away'] )\ngames_df = games_data.loc[games_data.SEASON.isin([2021, 2022]), :]\nprint(games_df.shape)\ndisplay(games_df.head())\n\n(1931, 5)\n\n\n\n\n\n\n\n\n\n\nHOME_TEAM_ID\nVISITOR_TEAM_ID\nSEASON\nPTS_home\nPTS_away\n\n\n\n\n0\n1610612740\n1610612759\n2022\n126.0\n117.0\n\n\n1\n1610612762\n1610612764\n2022\n120.0\n112.0\n\n\n2\n1610612739\n1610612749\n2022\n114.0\n106.0\n\n\n3\n1610612755\n1610612765\n2022\n113.0\n93.0\n\n\n4\n1610612737\n1610612741\n2022\n108.0\n110.0\n\n\n\n\n\n\n\n\nIn order to create the graph analytics I first transform the data so that the winners and losers are in the same columns. Then I average the scores since the same teams play each other multiple times.\n\ngames_df['margin'] = np.abs(games_df.PTS_home - games_df.PTS_away)\ngames_df['winner'] = np.where(games_df.PTS_home &gt; games_df.PTS_away, games_df.HOME_TEAM_ID, games_df.VISITOR_TEAM_ID)\ngames_df['loser'] = np.where(games_df.PTS_home &lt;= games_df.PTS_away, games_df.HOME_TEAM_ID, games_df.VISITOR_TEAM_ID)\ndf = games_df.groupby(['winner', 'loser', 'SEASON'],as_index=False)[['margin']].mean()\n\n/var/folders/nw/bgfbrcp92xxbt0l2nkylhyqm0000gq/T/ipykernel_13550/2485298653.py:1: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n/var/folders/nw/bgfbrcp92xxbt0l2nkylhyqm0000gq/T/ipykernel_13550/2485298653.py:2: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n/var/folders/nw/bgfbrcp92xxbt0l2nkylhyqm0000gq/T/ipykernel_13550/2485298653.py:3: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n# find all rows where the SEASON is equal to 2021\ntrain = df.SEASON == 2021\ndf_2021 = df.loc[train, :]\ndf_2021.shape\n\n(705, 4)\n\n\n\nnodes = df_2021.winner.unique()\n\nCreate a weighted directed graph like in the prompt\n\nG = nx.DiGraph()\nG.add_nodes_from(nodes)\nG.add_weighted_edges_from([(h, v, w) for h,v,w in zip(df_2021.winner, df_2021.loser, df_2021.margin)])\n\nDraw using networkx\n\nnx.draw(G, with_labels=False)\n\n\n\n\n\n\n\n\nIncidence Matrix\n\nThis is actually the adjacency matrix. (Which is what you want.)\n\n\nadj_mat = nx.to_numpy_array(G)\nadj_mat.shape\n\n(30, 30)\n\n\nCreate power ranking as well as sum of points to serve as features in the least squares model\n\npower_ranking = pd.Series((adj_mat + adj_mat**2).mean(axis=1), index=nodes, name='power')\nranking = pd.Series(adj_mat.mean(axis=1), index=nodes, name='ranking')\n\nJoin the games dataframe with the rankings for both HOME teams as well as VISITOR teams. Note that the rankings were created off of 2021 data ONLY\n\ndf = games_df[['HOME_TEAM_ID', 'VISITOR_TEAM_ID', 'SEASON', 'PTS_home', 'PTS_away']].set_index('HOME_TEAM_ID')\\\n.join(ranking, how='left').join(power_ranking, how='left').rename(columns={'ranking':'home_rank', 'power': 'home_power'})\\\n.reset_index().set_index('VISITOR_TEAM_ID').join(ranking, how='left').join(\n    power_ranking, how='left').reset_index()\ndf.head()\n\n\n\n\n\n\n\n\n\nlevel_0\nindex\nSEASON\nPTS_home\nPTS_away\nhome_rank\nhome_power\nranking\npower\n\n\n\n\n0\n1610612737\n1610612738\n2021\n107.0\n98.0\n14.746032\n327.494255\n11.052778\n180.632639\n\n\n1\n1610612737\n1610612738\n2021\n105.0\n95.0\n14.746032\n327.494255\n11.052778\n180.632639\n\n\n2\n1610612737\n1610612739\n2022\n114.0\n102.0\n9.697778\n174.550148\n11.052778\n180.632639\n\n\n3\n1610612737\n1610612739\n2022\n105.0\n99.0\n9.697778\n174.550148\n11.052778\n180.632639\n\n\n4\n1610612737\n1610612739\n2021\n101.0\n107.0\n9.697778\n174.550148\n11.052778\n180.632639\n\n\n\n\n\n\n\n\nCreate the dependy variable as well as train and test columns. Filter to relevant columns for the regression\n\ndf['points'] = df.PTS_home - df.PTS_away\nusecols= ['home_rank', 'home_power', 'ranking', 'power', 'points']\ntrain_df = df.loc[df.SEASON==2021, usecols]\ntest_df = df.loc[df.SEASON==2022, usecols]\n\n\ntrain_df.head()\n\n\n\n\n\n\n\n\n\nhome_rank\nhome_power\nranking\npower\npoints\n\n\n\n\n0\n14.746032\n327.494255\n11.052778\n180.632639\n9.0\n\n\n1\n14.746032\n327.494255\n11.052778\n180.632639\n10.0\n\n\n4\n9.697778\n174.550148\n11.052778\n180.632639\n-6.0\n\n\n5\n9.697778\n174.550148\n11.052778\n180.632639\n-3.0\n\n\n6\n9.697778\n174.550148\n11.052778\n180.632639\n6.0\n\n\n\n\n\n\n\n\nThe make_AY function creates the relevant A and y matrices\n\ndef make_AY(adf):\n    A = adf.drop(columns='points').values\n    y = adf.points.values\n    return sym.Matrix(A), sym.Matrix(y)\n\n\nA_train, y_train = make_AY(train_df)\n\nCreate A transpose A\n\nAtA = A_train.T * A_train\n\nSolve for the coefficients using the normal equations\n\nx = AtA.inv() * A_train.T * y_train\nx\n\n\\(\\displaystyle \\left[\\begin{matrix}1.83033612127952\\\\-0.0129332710629721\\\\-1.48309050999143\\\\0.00485672173277143\\end{matrix}\\right]\\)\n\n\nMake test data\n\nA_test, y_test = make_AY(test_df)\n\nProjection of coefficients onto A is the prediction\n\npredictions = A_test * x\n\nRMSE is root mean squared error. It measures on average the deviation from the actual scores\n\nRMSE = np.sqrt(float(np.mean(np.square(y_test - predictions))))\nRMSE\n\n13.93356013780096\n\n\n\nfind the correlation between y_test and predictions\n\nnp.corrcoef(np.array(y_test).astype(float).T, np.array(predictions).astype(float).T)\n\narray([[1.        , 0.19526585],\n       [0.19526585, 1.        ]])\n\n\n\n\ndo a linear regression on the training data using sklearn\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n\n\nlr = LinearRegression()\nlr.fit(train_df.drop(columns=['points']), train_df.points)\npredictions = lr.predict(test_df.drop(columns=['points']))\nRMSE = np.sqrt(mean_squared_error(test_df.points, predictions))\nRMSE\n\n13.93364052078264\n\n\n\n\nplot residuals etc for the fit\n\nimport matplotlib.pyplot as plt\nplt.scatter(test_df.points, predictions)\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\nplt.title('Actual vs Predicted')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nsummary statistics for the fit\n\nimport sklearn.metrics as metrics\ndef regression_results(y_true, y_pred):\n\n    # Regression metrics\n    explained_variance=metrics.explained_variance_score(y_true, y_pred)\n    mean_absolute_error=metrics.mean_absolute_error(y_true, y_pred) \n    mse=metrics.mean_squared_error(y_true, y_pred) \n #   mean_squared_log_error=metrics.mean_squared_log_error(y_true, y_pred)\n    median_absolute_error=metrics.median_absolute_error(y_true, y_pred)\n    r2=metrics.r2_score(y_true, y_pred)\n\n    print('explained_variance: ', round(explained_variance,4))    \n #   print('mean_squared_log_error: ', round(mean_squared_log_error,4))\n    print('r2: ', round(r2,4))\n    print('MAE: ', round(mean_absolute_error,4))\n    print('MSE: ', round(mse,4))\n  #  print('RMSE: ', round(np.sqrt(mse),4))\n\n\nregression_results(test_df.points, predictions)\n\nexplained_variance:  -0.039\nr2:  -0.0432\nMAE:  10.9417\nMSE:  194.1463\n\n\n\nThis is quite elegant when you know how to use Pandas well! The performance is very poor, though… I have just played around a little bit to see if I could improve it, but I couldn’t. I don’t think it’s an error, just that the model here isn’t a very good fit for the data.\nI tried reproducing your fit using sklearn. In R, we have a lot of tools for visualizing the results of least squares regressions (which is very similar to what you did with the least squares above.) But sklearn in Pythin doesn’t seem to have these tools. Ah well…\nGrade: M"
  },
  {
    "objectID": "submitted/projects_2/xw/Haar.html",
    "href": "submitted/projects_2/xw/Haar.html",
    "title": "Stat 24320",
    "section": "",
    "text": "from PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nCode from chatGPT to load in and convert to Grayscale\n\ndef load_image_as_numpy_array(filepath):\n    # Open the image file\n    with Image.open(filepath) as img:\n        # Convert the image to grayscale\n        img_gray = img.convert('L')\n        # Convert the grayscale image to a numpy array\n        numpy_array = np.array(img_gray)\n    return numpy_array\n\nload the file in and take note of the filesize\n\nfilename = 'mypic.png'\nraw_matrix = load_image_as_numpy_array(filename)\nraw_matrix.shape\n\n(1318, 1079)\n\n\nSince Haar-Wavelet requires even dimensions, I will remove the last column\n\nmatrix = raw_matrix[:, :-1]\nmatrix.shape\n\n(1318, 1078)\n\n\nMore chatGPT code to display an image\n\ndef display_image_from_array(image_array):\n    # Display the numpy array as an image\n    plt.imshow(image_array, cmap='gray')  # 'gray' to specify grayscale display\n    plt.axis('off')  # Turn off axis numbers and ticks\n    plt.show()\n\nThis image was taken in my childhood at Tianamen Square not too long after the Tianamen Square Massacre. The railings that I am on did not exist prior to the Tiananmen Square Massacre. I hope you enjoy this snapshot in history.\n\nWow, thank you indeed for sharing this!\n\n\ndisplay_image_from_array(matrix)\n\n\n\n\n\n\n\n\nThe Haar Wavelet function matrix is created here. The loop iterates over half the dimension of the picture and in each iteration the sum and the difference of columns are created. Each iteration creates a row of the sum and difference. At the end the rows are all concatenated together to form a dim x dim matrix.\n\ndef make_haar_wavelet(dim):\n    hwRowsSum = []\n    hwRowsDiff = []\n    for i in range(int(dim/2)):\n        zeros = np.zeros(dim)\n        zeros[i*2] = 1\n        zeros[i*2+1] = 1\n        negs = np.copy(zeros)\n        negs[i*2] = -1\n        hwRowsSum.append(zeros)\n        hwRowsDiff.append(negs)\n    haar_wavem = np.sqrt(2)/2 * np.concatenate((hwRowsSum,hwRowsDiff))\n    print(haar_wavem.shape)\n    return haar_wavem\n\nThis function applies the Haar Wavelet transformation and returns the resulting matrix.\n\ndef haar_transform(mat):\n    haar_left = make_haar_wavelet(mat.shape[0])\n    haar_rite = make_haar_wavelet(mat.shape[1])\n    haar_mat = haar_left @ mat @ haar_rite.T\n    return haar_mat\n\nDisplays the image and crops the image to the blurred image\n\ndef show_haar_and_crop(hmat):\n    display_image_from_array(hmat)\n    upper_right = hmat.shape[0]//2\n    lower_left = hmat.shape[1]//2\n    cropped = hmat[:upper_right,:lower_left]\n    return cropped\n\nLoop over and apply the Haar-Wavelet transform, at each step I take the blurred picture and use that as the input to the next Haar Wavelet transformation. Iteration 4 is the last iteration where I can make out the finer details of the picture such as the bicycles in the background and the Tiananmen Square building. The image has been compressed to be 1/4^4 = 1/256 of the original size.\n\nimages = []\nimages.append(matrix)\nfull_images = [matrix]\ndisplay_image_from_array(matrix)\nfor i in range(6):\n    haar_matrix = haar_transform(images[-1])\n    full_images.append(haar_matrix)\n    print(f'iteration {i+1}')\n    images.append(show_haar_and_crop(haar_matrix))\n\n\n\n\n\n\n\n\n(1318, 1318)\n(1078, 1078)\niteration 1\n(658, 659)\n(538, 539)\niteration 2\n(328, 329)\n(268, 269)\niteration 3\n(164, 164)\n(134, 134)\niteration 4\n(82, 82)\n(66, 67)\niteration 5\n(40, 41)\n(32, 33)\niteration 6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere I try a lossy compression by rounding the blurred image to integers and then converting the least frequent 10 values into 0’s. The results weren’t that great as the image suffered in quality without that many values converted to 0’s. The rounding looks like it does save space without compromising the quality of the image by much.\n\ndef round_to_zero(image):\n    rounded = np.round(image).astype(np.uint8)\n    least = pd.Series(rounded.reshape(-1)).value_counts().iloc[-10:].index.values\n    to0 = np.isin(rounded,least)\n    num = to0.sum().sum()\n    print(f'number converted: {num}')\n    print(f'{num / image.size} converted to 0')\n    image[to0] = 0.0\n    return image\n\n\nimages2 = []\nimages2.append(matrix)\nfull_images2 = [matrix]\ndisplay_image_from_array(matrix)\nfor i in range(4):\n    haar_matrix = haar_transform(images2[-1])\n    full_images2.append(haar_matrix)\n    print(f'iteration {i+1}')\n    cropped = show_haar_and_crop(haar_matrix)\n    haar_rounded = round_to_zero(cropped)\n    images2.append(haar_rounded)\n\n\n\n\n\n\n\n\n(1318, 1318)\n(1078, 1078)\niteration 1\nnumber converted: 3895\n0.010965622281468803 converted to 0\n(658, 659)\n(538, 539)\niteration 2\nnumber converted: 1780\n0.020112767087377543 converted to 0\n(328, 329)\n(268, 269)\niteration 3\nnumber converted: 226\n0.01028394612304332 converted to 0\n(164, 164)\n(134, 134)\niteration 4\nnumber converted: 98\n0.017837641062977794 converted to 0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReverse Haar here for comparison between iteration with rounding and without\n\ndef reverse_haar(mat):\n    if mat.shape[0] %2 == 1:\n        mat = mat[:-1,:]\n    if mat.shape[1] % 2 == 1:\n        mat = mat[:,:-1]\n    haar_left = make_haar_wavelet(mat.shape[0])\n    haar_rite = make_haar_wavelet(mat.shape[1])\n    haar_mat = 2* haar_left.T @ mat @ haar_rite\n    return haar_mat\n\ndisplay_image_from_array(reverse_haar(full_images[4]))\n\n(164, 164)\n(134, 134)\n\n\n\n\n\n\n\n\n\n\ndisplay_image_from_array(reverse_haar(full_images2[4]))\n\n(164, 164)\n(134, 134)\n\n\n\n\n\n\n\n\n\nIt looks like it might be better to just do additional iterations rather than rounding too much.\nI will attempt to do edge detection from the example in the book. First do the Haar transformation, then 0 out the blurred image. Then do the inverse transformation\n\ndef edge_detection(mat):\n    haar_left = make_haar_wavelet(mat.shape[0])\n    haar_rite = make_haar_wavelet(mat.shape[1])\n    hmat = haar_left @ mat @ haar_rite.T\n    # zero out\n    upper_right = hmat.shape[0]//2\n    lower_left = hmat.shape[1]//2\n    hmat[:upper_right,:lower_left] = 0\n    # inverse transformation\n    Ae = 2 * haar_left.T @ hmat @ haar_rite\n    return Ae\n\n\nA_edges = edge_detection(matrix)\ndisplay_image_from_array(A_edges)\n\n(1318, 1318)\n(1078, 1078)\n\n\n\n\n\n\n\n\n\n\nYou’ve done the first part really nicely. The effects of the compression are very clear.\nYou came close with your edge detection, but then you gave up! I checked what was happening by plotting a histogram of the values in A_edges. I saw that many of them were negative, so then I tried making plots of the absolute value of A_edges. Using imshow directly, I could set the vmin and vmax values to show everything more clearly; also, I found it easier to see when I used cmap='hot'.\nAlso: when you do the rounding to 0, the idea is to do that on the portions of the image which contain the edge information, not on the blurred part of the image. That’s because the edge information parts are the ones which should have the most values that are close to zero… then once you have the zeros, you could try saving at least those parts of the matrix as sparse matrices, to see if that helps with the compression.\nGrade: M"
  },
  {
    "objectID": "submitted/projects_2/xw/LeastSquares.html",
    "href": "submitted/projects_2/xw/LeastSquares.html",
    "title": "find the correlation between y_test and predictions",
    "section": "",
    "text": "import sympy as sym\nimport numpy as np\nimport networkx as nx\nimport pandas as pd\n\nI am using NBA games from 2021 as the the training data and 2022 season as the test data\n\ngames_data = pd.read_csv('games.csv', usecols=['HOME_TEAM_ID', 'VISITOR_TEAM_ID', 'SEASON', 'PTS_home', 'PTS_away'] )\ngames_df = games_data.loc[games_data.SEASON.isin([2021, 2022]), :]\nprint(games_df.shape)\ndisplay(games_df.head())\n\n(1931, 5)\n\n\n\n\n\n\n\n\n\n\nHOME_TEAM_ID\nVISITOR_TEAM_ID\nSEASON\nPTS_home\nPTS_away\n\n\n\n\n0\n1610612740\n1610612759\n2022\n126.0\n117.0\n\n\n1\n1610612762\n1610612764\n2022\n120.0\n112.0\n\n\n2\n1610612739\n1610612749\n2022\n114.0\n106.0\n\n\n3\n1610612755\n1610612765\n2022\n113.0\n93.0\n\n\n4\n1610612737\n1610612741\n2022\n108.0\n110.0\n\n\n\n\n\n\n\n\nIn order to create the graph analytics I first transform the data so that the winners and losers are in the same columns. Then I average the scores since the same teams play each other multiple times.\n\ngames_df['margin'] = np.abs(games_df.PTS_home - games_df.PTS_away)\ngames_df['winner'] = np.where(games_df.PTS_home &gt; games_df.PTS_away, games_df.HOME_TEAM_ID, games_df.VISITOR_TEAM_ID)\ngames_df['loser'] = np.where(games_df.PTS_home &lt;= games_df.PTS_away, games_df.HOME_TEAM_ID, games_df.VISITOR_TEAM_ID)\ndf = games_df.groupby(['winner', 'loser', 'SEASON'],as_index=False)[['margin']].mean()\n\n/var/folders/nw/bgfbrcp92xxbt0l2nkylhyqm0000gq/T/ipykernel_14392/2485298653.py:1: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n/var/folders/nw/bgfbrcp92xxbt0l2nkylhyqm0000gq/T/ipykernel_14392/2485298653.py:2: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n/var/folders/nw/bgfbrcp92xxbt0l2nkylhyqm0000gq/T/ipykernel_14392/2485298653.py:3: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n# find all rows where the SEASON is equal to 2021\ntrain = df.SEASON == 2021\ndf_2021 = df.loc[train, :]\ndf_2021.shape\n\n(705, 4)\n\n\n\nnodes = df_2021.winner.unique()\n\nCreate a weighted directed graph like in the prompt\n\nG = nx.DiGraph()\nG.add_nodes_from(nodes)\nG.add_weighted_edges_from([(h, v, w) for h,v,w in zip(df_2021.winner, df_2021.loser, df_2021.margin)])\n\nDraw using networkx\n\nnx.draw(G, with_labels=False)\n\n\n\n\n\n\n\n\nIncidence Matrix\n\nThis is actually the adjacency matrix. (Which is what you want.)\n\n\nadj_mat = nx.to_numpy_array(G)\nadj_mat.shape\n\n(30, 30)\n\n\nCreate power ranking as well as sum of points to serve as features in the least squares model\n\npower_ranking = pd.Series((adj_mat + adj_mat**2).mean(axis=1), index=nodes, name='power')\nranking = pd.Series(adj_mat.mean(axis=1), index=nodes, name='ranking')\n\nJoin the games dataframe with the rankings for both HOME teams as well as VISITOR teams. Note that the rankings were created off of 2021 data ONLY\n\ndf = games_df[['HOME_TEAM_ID', 'VISITOR_TEAM_ID', 'SEASON', 'PTS_home', 'PTS_away']].set_index('HOME_TEAM_ID')\\\n.join(ranking, how='left').join(power_ranking, how='left').rename(columns={'ranking':'home_rank', 'power': 'home_power'})\\\n.reset_index().set_index('VISITOR_TEAM_ID').join(ranking, how='left').join(\n    power_ranking, how='left').reset_index()\ndf.head()\n\n\n\n\n\n\n\n\n\nlevel_0\nindex\nSEASON\nPTS_home\nPTS_away\nhome_rank\nhome_power\nranking\npower\n\n\n\n\n0\n1610612737\n1610612738\n2021\n107.0\n98.0\n14.746032\n327.494255\n11.052778\n180.632639\n\n\n1\n1610612737\n1610612738\n2021\n105.0\n95.0\n14.746032\n327.494255\n11.052778\n180.632639\n\n\n2\n1610612737\n1610612739\n2022\n114.0\n102.0\n9.697778\n174.550148\n11.052778\n180.632639\n\n\n3\n1610612737\n1610612739\n2022\n105.0\n99.0\n9.697778\n174.550148\n11.052778\n180.632639\n\n\n4\n1610612737\n1610612739\n2021\n101.0\n107.0\n9.697778\n174.550148\n11.052778\n180.632639\n\n\n\n\n\n\n\n\nCreate the dependy variable as well as train and test columns. Filter to relevant columns for the regression\n\ndf['points'] = df.PTS_home - df.PTS_away\nusecols= ['home_rank', 'home_power', 'ranking', 'power', 'points']\ntrain_df = df.loc[df.SEASON==2021, usecols]\ntest_df = df.loc[df.SEASON==2022, usecols]\n\n\ntrain_df.head()\n\n\n\n\n\n\n\n\n\nhome_rank\nhome_power\nranking\npower\npoints\n\n\n\n\n0\n14.746032\n327.494255\n11.052778\n180.632639\n9.0\n\n\n1\n14.746032\n327.494255\n11.052778\n180.632639\n10.0\n\n\n4\n9.697778\n174.550148\n11.052778\n180.632639\n-6.0\n\n\n5\n9.697778\n174.550148\n11.052778\n180.632639\n-3.0\n\n\n6\n9.697778\n174.550148\n11.052778\n180.632639\n6.0\n\n\n\n\n\n\n\n\nThe make_AY function creates the relevant A and y matrices\n\ndef make_AY(adf):\n    A = adf.drop(columns='points').values\n    y = adf.points.values\n    return sym.Matrix(A), sym.Matrix(y)\n\n\nA_train, y_train = make_AY(train_df)\n\nCreate A transpose A\n\nAtA = A_train.T * A_train\n\nSolve for the coefficients using the normal equations\n\nx = AtA.inv() * A_train.T * y_train\nx\n\n\\(\\displaystyle \\left[\\begin{matrix}1.83033612127952\\\\-0.0129332710629721\\\\-1.48309050999143\\\\0.00485672173277143\\end{matrix}\\right]\\)\n\n\nMake test data\n\nA_test, y_test = make_AY(test_df)\n\nProjection of coefficients onto A is the prediction\n\npredictions = A_test * x\n\nRMSE is root mean squared error. It measures on average the deviation from the actual scores\n\nRMSE = np.sqrt(float(np.mean(np.square(y_test - predictions))))\nRMSE\n\n13.93356013780096\n\n\n\nfind the correlation between y_test and predictions\n\nnp.corrcoef(np.array(y_test).astype(float).T, np.array(predictions).astype(float).T)\n\narray([[1.        , 0.19526585],\n       [0.19526585, 1.        ]])\n\n\n\n\ndo a linear regression on the training data using sklearn\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n\n\nlr = LinearRegression()\nlr.fit(train_df.drop(columns=['points']), train_df.points)\npredictions = lr.predict(test_df.drop(columns=['points']))\nRMSE = np.sqrt(mean_squared_error(test_df.points, predictions))\nRMSE\n\n13.93364052078264\n\n\n\n\nplot residuals etc for the fit\n\nimport matplotlib.pyplot as plt\nplt.scatter(test_df.points, predictions)\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\nplt.title('Actual vs Predicted')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nsummary statistics for the fit\n\nimport sklearn.metrics as metrics\ndef regression_results(y_true, y_pred):\n\n    # Regression metrics\n    explained_variance=metrics.explained_variance_score(y_true, y_pred)\n    mean_absolute_error=metrics.mean_absolute_error(y_true, y_pred) \n    mse=metrics.mean_squared_error(y_true, y_pred) \n #   mean_squared_log_error=metrics.mean_squared_log_error(y_true, y_pred)\n    median_absolute_error=metrics.median_absolute_error(y_true, y_pred)\n    r2=metrics.r2_score(y_true, y_pred)\n\n    print('explained_variance: ', round(explained_variance,4))    \n #   print('mean_squared_log_error: ', round(mean_squared_log_error,4))\n    print('r2: ', round(r2,4))\n    print('MAE: ', round(mean_absolute_error,4))\n    print('MSE: ', round(mse,4))\n  #  print('RMSE: ', round(np.sqrt(mse),4))\n\n\nregression_results(test_df.points, predictions)\n\nexplained_variance:  -0.039\nr2:  -0.0432\nMAE:  10.9417\nMSE:  194.1463\n\n\n\nThis is quite elegant when you know how to use Pandas well! The performance is very poor, though… I have just played around a little bit to see if I could improve it, but I couldn’t. I don’t think it’s an error, just that the model here isn’t a very good fit for the data.\nI tried reproducing your fit using sklearn. In R, we have a lot of tools for visualizing the results of least squares regressions (which is very similar to what you did with the least squares above.) But sklearn in Pythin doesn’t seem to have these tools. Ah well…\nGrade: M"
  },
  {
    "objectID": "lectures/ChNone.html#steps",
    "href": "lectures/ChNone.html#steps",
    "title": "Stat 24320",
    "section": "Steps",
    "text": "Steps\n\nRecap of the Fourier series per Strang"
  },
  {
    "objectID": "lectures/ChNone.html#section",
    "href": "lectures/ChNone.html#section",
    "title": "The Fourier Series",
    "section": "",
    "text": "\\[\n\\int_{0}^{2 \\pi} f(x) \\sin x d x=a_{0} \\int_{0}^{2 \\pi} \\sin x d x+a_{1} \\int_{0}^{2 \\pi} \\cos x \\sin x d x+b_{1} \\int_{0}^{2 \\pi}(\\sin x)^{2} d x+\\cdots\n\\]\n\\[\nb_{1}=\\frac{\\int_{0}^{2 \\pi} f(x) \\sin x d x}{\\int_{0}^{2 \\pi}(\\sin x)^{2} d x}=\\frac{(f, \\sin x)}{(\\sin x, \\sin x)}\n\\]"
  },
  {
    "objectID": "lectures/ChNone.html#section-1",
    "href": "lectures/ChNone.html#section-1",
    "title": "The Fourier Series",
    "section": "",
    "text": "Make a table. Note that \\(i^2 = i^6 = -1\\), \\(i^3 = i^7 = -i\\), and \\(i^4 = 1\\).\nFor n = 0:\n\n\n\nk\n\\(i 2\\pi k \\times 0\\)\n\\(e^{i 2\\pi k \\times 0}\\)\n\n\n\n\n0\n0\n1\n\n\n1\n0\n1\n\n\n2\n0\n1\n\n\n3\n0\n1\n\n\n\n\nFor n = 1:\n\n\n\nk\n\\(i 2\\pi k  \\frac{1}{4}\\)\n\\(e^{i 2\\pi k \\frac{1}{4}}\\)\n\n\n\n\n0\n0\n1\n\n\n1\n\\(i\\pi/2\\)\n\\(i\\)\n\n\n2\n\\(i\\pi\\)\n\\(-1=i^2\\)\n\n\n3\n\\(3i\\pi/2\\)\n\\(-i=i^3\\)"
  },
  {
    "objectID": "lectures/ChNone.html#analogy-with-projections",
    "href": "lectures/ChNone.html#analogy-with-projections",
    "title": "The Fourier Series",
    "section": "Analogy with projections",
    "text": "Analogy with projections\nThe component of the vector \\(b\\) along the line spanned by \\(a\\) is \\(b^{\\mathrm{T}} a / a^{\\mathrm{T}} a\\).\n\nA Fourier series is projecting \\(f(x)\\) onto \\(\\sin x\\). Its component \\(p\\) in this direction is exactly \\(b_{1} \\sin x\\)."
  },
  {
    "objectID": "lectures/ChNone.html#eulers-formula",
    "href": "lectures/ChNone.html#eulers-formula",
    "title": "The Fourier Series",
    "section": "Euler’s formula",
    "text": "Euler’s formula\nThe complex exponential \\(e^{i x}\\) is a combination of \\(\\cos x\\) and \\(\\sin x\\):\n\\[\ne^{i x}=\\cos x+i \\sin x\n\\]\n\nSo we can rewrite the Fourier series using complex exponentials:\n\\[\nf(x)=c_{0}+c_{1} e^{i x}+c_{2} e^{2 i x}+\\cdots = \\sum_k c_k\\cdot e^{i k x}\n\\]\npause\n\nThe coefficients \\(c_{0}, c_{1}, c_{2}, \\ldots\\) are related to the \\(a\\)’s and \\(b\\)’s.\nFor example, \\(c_{1}=a_{1}-i b_{1}\\).\n\n\n\nThe formula for finding the coefficients \\(c_{k}\\) is the same as before, but now we use the complex exponential functions \\(e^{i k x}\\) instead of the sines and cosines:\n\\[\nc_{k}=\\int_{0}^{2 \\pi} f(x) e^{-i k x} d x\n\\]"
  },
  {
    "objectID": "lectures/ChNone.html#discrete-fourier-series",
    "href": "lectures/ChNone.html#discrete-fourier-series",
    "title": "The Fourier Series",
    "section": "Discrete Fourier Series",
    "text": "Discrete Fourier Series\n\nWhat if we have only a series of N discrete values \\(y_{0}, y_{1}, y_{2}, \\ldots\\), instead of a continuous function \\(f(x)\\)?\n. . .\nFirst, we imagine that the values are periodic, with period N. So it’s as if they are repeating beyond the N values we have.\n. . .\nThen we can identify a natural period \\(N\\), and rewrite \\(x=2 \\pi n / N\\), so the discrete Fourier series becomes\n\\[\n\\begin{align}\ny[n] = \\sum_k c[k]\\cdot e^{i 2\\pi k \\frac{n}{N}}\n\\end{align}\n\\]\n. . .\nIn matrix form, \\(\\mathbf{y} = F \\mathbf{c}\\)\nwhere\n\\[\n\\begin{align}\n\\begin{bmatrix} y_0 \\\\ y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_{N-1} \\end{bmatrix}\n= F c = \\begin{bmatrix}\n1 & 1 & 1 & \\ldots & 1 \\\\\n1 & e^{-i 2\\pi/N} & e^{-i 4\\pi/N} & \\ldots & e^{-i 2\\pi (N-1)/N} \\\\\n1 & e^{-i 4\\pi/N} & e^{-i 8\\pi/N} & \\ldots & e^{-i 4\\pi (N-1)/N} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & e^{-i 2\\pi(N-1)/N} & e^{-i 4\\pi(N-1)/N} & \\ldots & e^{-i 2\\pi(N-1)(N-1)/N}\n\\end{bmatrix} \\begin{bmatrix}c_0 \\\\ c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_{N-1}\\end{bmatrix}\n\\end{align}\n\\]"
  },
  {
    "objectID": "lectures/ChNone.html#section-2",
    "href": "lectures/ChNone.html#section-2",
    "title": "The Fourier Series",
    "section": "",
    "text": "For n = 2:\n\n\n\nk\n\\(i 2\\pi k  \\frac{2}{4}\\)\n\\(e^{i 2\\pi k \\frac{2}{4}}\\)\n\n\n\n\n0\n0\n1\n\n\n1\n\\(i\\pi\\)\n\\(-1=i^2\\)\n\n\n2\n\\(2i\\pi\\)\n\\(1=i^4\\)\n\n\n3\n\\(3i\\pi\\)\n\\(-1=i^6\\)\n\n\n\n\nFor n = 3:\n\n\n\nk\n\\(i 2\\pi k  \\frac{3}{4}\\)\n\\(e^{i 2\\pi k \\frac{3}{4}}\\)\n\n\n\n\n0\n0\n1\n\n\n1\n\\(3i\\pi/2\\)\n\\(-i=i^3\\)\n\n\n2\n\\(3i\\pi\\)\n\\(-1 = i^6\\)\n\n\n3\n\\(9i\\pi/2\\)\n\\(i = i^9\\)"
  },
  {
    "objectID": "lectures/ChNone.html#finding-the-fourier-coefficients",
    "href": "lectures/ChNone.html#finding-the-fourier-coefficients",
    "title": "The Fourier Series",
    "section": "Finding the Fourier coefficients",
    "text": "Finding the Fourier coefficients\n\nHow can we find the Fourier coefficients? . . .\nBy analogy with the continuous case,\n\\[\nc_{k}=\\int_{0}^{2 \\pi} f(x) e^{-i k x} d x\n\\]\nWe will replace the integral by a sum over the discrete values \\(y_{0}, y_{1}, y_{2}, \\ldots\\).\n\\[\nc[k]= \\frac{1}{N} \\sum_{n=0}^{N-1} y[n] e^{-i 2 \\pi k n / N}, \\quad \\text{for } k=0,1,2, \\ldots, N-1\n\\]\nThis defines a matrix \\(F^\\prime\\) given as\n\\[\nF^\\prime = \\frac{1}{N}\\begin{bmatrix}\n1 & 1 & 1 & \\ldots & 1 \\\\\n1 & e^{i 2\\pi/N} & e^{i 4\\pi/N} & \\ldots & e^{i 2\\pi (N-1)/N} \\\\\n1 & e^{i 4\\pi/N} & e^{i 8\\pi/N} & \\ldots & e^{i 4\\pi (N-1)/N} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & e^{i 2\\pi(N-1)/N} & e^{i 4\\pi(N-1)/N} & \\ldots & e^{i 2\\pi(N-1)(N-1)/N}\n\\end{bmatrix}\n\\]\nNote: this is \\(\\frac{1}{N}\\) times the complex conjugate of the matrix \\(F\\): \\(F^\\prime = \\frac{1}{N} F^*\\)."
  },
  {
    "objectID": "lectures/ChNone.html#inverse-dft",
    "href": "lectures/ChNone.html#inverse-dft",
    "title": "The Fourier Series",
    "section": "Inverse DFT",
    "text": "Inverse DFT\nWe have just shown two things:\n\nThe DFT can be written as a matrix-vector product \\(\\mathbf{y} = F \\mathbf{c}\\).\nThe coefficients \\(\\mathbf{c}\\) can be found by \\(\\mathbf{c} = F^\\prime \\mathbf{y}\\).\nBut of course it is also true from 1 that if \\(F\\) is invertible, that \\(\\mathbf{c} = F^{-1} \\mathbf{y}\\). Therefore, \\(F^{-1} = F^\\prime=\\frac{1}{N} F^*\\).\nThis means that \\(F\\) is unitary. Its rows and columns are orthogonal."
  },
  {
    "objectID": "lectures/ChNone.html#a-concrete-example",
    "href": "lectures/ChNone.html#a-concrete-example",
    "title": "The Fourier Series",
    "section": "A concrete example",
    "text": "A concrete example\nLet’s take an example with \\(N=4\\) and \\(y=(2, 4, 6, 8)\\).\n\n. . .\nWe want to find coefficients \\(c_0, c_1, c_2, c_3\\) such that our Fourier series correctly reproduces the input values at each of the four points.\n. . .\n(We can’t find more than 4 coefficients because we only have 4 points of data.)\n. . .\n\\[\n\\begin{align}\ny[n] = \\sum_k c[k]\\cdot e^{i 2\\pi k \\frac{n}{N}}\n\\end{align}\n\\]\n. . .\n\\[\n\\begin{aligned}\nc_{0}+c_{1} e^{i x}+c_{2} e^{2 i x}+c_{3} e^{3 i x}=\\left\\{\\begin{array}{lll}\n2 & \\text { at } & x=0 \\\\\n4 & \\text { at } & x=\\pi / 2 \\\\\n6 & \\text { at } & x=\\pi \\\\\n8 & \\text { at } & x=3 \\pi / 2\n\\end{array}\\right)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lectures/ChNone.html#section-3",
    "href": "lectures/ChNone.html#section-3",
    "title": "The Fourier Series",
    "section": "",
    "text": "Check that \\(F F^\\prime = I\\):\n\\[\\begin{aligned}\nF F^\\prime =\\frac{1}{4}\\left[\\begin{array}{cccc}\n1 & 1 & 1 & 1  \\\\\n1 & i & i^{2} & i^{3} \\\\\n1 & i^{2} & i^{4} & i^{6} \\\\\n1 & i^{3} & i^{6} & i^{9}\n\\end{array}\\right]\\left[\\begin{array}{cccc}\n1 & 1 & 1 & 1 \\\\\n1 & (-i) & (-i)^{2} & (-i)^{3} \\\\\n1 & (-i)^{2} & (-i)^{4} & (-i)^{6} \\\\\n1 & (-i)^{3} & (-i)^{6} & (-i)^{9}\n\\end{array}\\right]=I\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lectures/ChNone.html#solving-for-the-coefficients",
    "href": "lectures/ChNone.html#solving-for-the-coefficients",
    "title": "The Fourier Series",
    "section": "Solving for the coefficients",
    "text": "Solving for the coefficients\n\\[\n\\begin{align*}\n& c_{0}+c_{1}+c_{2}+c_{3}=2 \\\\\n& c_{0}+i c_{1}+i^{2} c_{2}+i^{3} c_{3}=4 \\\\\n& c_{0}+i^{2} c_{1}+i^{4} c_{2}+i^{6} c_{3}=6  \\\\\n& c_{0}+i^{3} c_{1}+i^{6} c_{2}+i^{9} c_{3}=8\n\\end{align*}\n\\]\n\nIn other words, \\(F c = y\\) for\n\\[\nF=\\left[\\begin{array}{cccc}\n1 & 1 & 1 & 1 \\\\\n1 & i & i^{2} & i^{3} \\\\\n1 & i^{2} & i^{4} & i^{6} \\\\\n1 & i^{3} & i^{6} & i^{9}\n\\end{array}\\right]\n\\]"
  },
  {
    "objectID": "lectures/ChNone.html#section-4",
    "href": "lectures/ChNone.html#section-4",
    "title": "The Fourier Series",
    "section": "",
    "text": "Check in Sympy:\n\n\nF:\nF prime:\nF F prime:\n\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & 1 & 1 & 1\\\\1 & i & -1 & - i\\\\1 & -1 & 1 & -1\\\\1 & - i & -1 & i\\end{matrix}\\right]\\)\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0.25 & 0.25 & 0.25 & 0.25\\\\0.25 & - 0.25 i & -0.25 & 0.25 i\\\\0.25 & -0.25 & 0.25 & -0.25\\\\0.25 & 0.25 i & -0.25 & - 0.25 i\\end{matrix}\\right]\\)\n\n\n\\(\\displaystyle \\left[\\begin{matrix}1.0 & 0 & 0 & 0\\\\0 & 1.0 & 0 & 0\\\\0 & 0 & 1.0 & 0\\\\0 & 0 & 0 & 1.0\\end{matrix}\\right]\\)"
  },
  {
    "objectID": "lectures/ChNone.html#section-5",
    "href": "lectures/ChNone.html#section-5",
    "title": "The Fourier Series",
    "section": "",
    "text": "What’s the DFT of this signal?\n\nfrom scipy.fft import fft, fftshift\nyf = fft(y)\nyfalone = fft(yalone)\n# make a non connected plot of yf\n\nplt.plot(np.fft.fftfreq(200),np.abs(yf), color='red')\nplt.plot(np.fft.fftfreq(200),np.abs(yfalone), color='blue')\nplt.xlim(-0.25, 0.25)\nplt.legend(['Full signal', 'low frequency only'])\nplt.show()"
  },
  {
    "objectID": "lectures/ChNone.html#a-simplification",
    "href": "lectures/ChNone.html#a-simplification",
    "title": "The Fourier Series",
    "section": "A simplification",
    "text": "A simplification\n\nNotice that our matrix F had many terms which were exponents of \\(i\\).\nIn general, we can simplify the matrix \\(F\\) by defining \\(w = e^{i 2\\pi/N}\\). This is the \\(N\\)th “primitive root of unity”, so that \\(w^N = 1\\).\n. . .\nFor N = 4, \\(w = e^{i \\pi/2} = i\\).\n. . .\nWrite the first few terms of this: \\[\n\\left[\\begin{array}{ccccc}\n1 & 1 & 1 & \\cdot & 1  \\\\\n1 & w & w^{2} & \\cdot & w^{n-1} \\\\\n1 & w^{2} & w^{4} & \\cdot & w^{2(n-1)} \\\\\n\\cdot & \\cdot & \\cdot & \\cdot & \\cdot \\\\\n1 & w^{n-1} & w^{2(n-1)} & \\cdot & w^{(n-1)^{2}}\n\\end{array}\\right]\\left[\\begin{array}{c}\nc_{0} \\\\\nc_{1} \\\\\nc_{2} \\\\\n\\cdot \\\\\nc_{n-1}\n\\end{array}\\right]=\\left[\\begin{array}{c}\ny_{0} \\\\\ny_{1} \\\\\ny_{2} \\\\\n\\cdot \\\\\ny_{n-1}\n\\end{array}\\right], \\quad w = e^{i 2\\pi/n}\n\\]"
  },
  {
    "objectID": "lectures/ChNone.html#terminology",
    "href": "lectures/ChNone.html#terminology",
    "title": "The Fourier Series",
    "section": "Terminology",
    "text": "Terminology\nWe call the matrix \\(F\\) the DFT (Discrete Fourier Transform) matrix. It is a square unitary matrix of size \\(N \\times N\\).\n\nThe matrix \\(F^\\prime\\) is the inverse DFT matrix. It is the complex conjugate of \\(F\\) divided by \\(N\\)."
  },
  {
    "objectID": "lectures/ChNone.html#summary",
    "href": "lectures/ChNone.html#summary",
    "title": "The Fourier Series",
    "section": "Summary",
    "text": "Summary\n\nThe DFT can be written as a matrix-vector product \\(\\mathbf{y} = F \\mathbf{c}\\).\nThe coefficients \\(\\mathbf{c}\\) can be found by \\(\\mathbf{c} = F^\\prime \\mathbf{y}\\).\nF is easy to compute, with a simple formula for each entry.\n(There’s actually a really really fast way to compute the DFT using the FFT algorithm.)"
  },
  {
    "objectID": "lectures/ChNone.html#filtering",
    "href": "lectures/ChNone.html#filtering",
    "title": "The Fourier Series",
    "section": "Filtering",
    "text": "Filtering\nThe DFT is used in many applications, but one of the most common is filtering.\n\nFor example, suppose we have a signal that is a sum of two sinusoids:\n\\[\nf(x) = \\sin(2\\pi x) + 0.5 \\sin(20\\pi x)\n\\]"
  },
  {
    "objectID": "lectures/ChNone.html#section-6",
    "href": "lectures/ChNone.html#section-6",
    "title": "The Fourier Series",
    "section": "",
    "text": "What’s with the double peaks?"
  },
  {
    "objectID": "lectures/ChNone.html#section-7",
    "href": "lectures/ChNone.html#section-7",
    "title": "The Fourier Series",
    "section": "",
    "text": "Compute the inverse DFT:"
  },
  {
    "objectID": "lectures/ChNone.html#section-8",
    "href": "lectures/ChNone.html#section-8",
    "title": "The Fourier Series",
    "section": "",
    "text": "It’s pretty good!"
  },
  {
    "objectID": "lectures/ChNone.html#reconstruct-the-signal",
    "href": "lectures/ChNone.html#reconstruct-the-signal",
    "title": "The Fourier Series",
    "section": "Reconstruct the signal",
    "text": "Reconstruct the signal\nWe can reconstruct the signal by taking the inverse DFT."
  },
  {
    "objectID": "lectures/ChNone.html#section-9",
    "href": "lectures/ChNone.html#section-9",
    "title": "The Fourier Series",
    "section": "",
    "text": "Try one more thing. Can we describe the change in the DFT as a multiplicative vector? We sure can.\n\n\n\n\n\n\n\n\n\nSo we can describe the new DVD as the old DFT multiplied by a vector of 1s and 0s."
  },
  {
    "objectID": "lectures/ChNone.html#section-10",
    "href": "lectures/ChNone.html#section-10",
    "title": "The Fourier Series",
    "section": "",
    "text": "The total process went like this:\n\nTake the DFT of the signal.\nMultiply the DFT by a vector of 1s and 0s to filter out the high frequency.\nTake the inverse DFT to get the filtered signal."
  },
  {
    "objectID": "lectures/ChNone.html#section-11",
    "href": "lectures/ChNone.html#section-11",
    "title": "The Fourier Series",
    "section": "",
    "text": "Then we can write this as:\n\\[\nf_{\\text{filtered}}[n] = \\sum_{m=0}^{n} f[n-m] h[m]\n\\]\nThis operation is called convolution.\n\nIt’s messy to compute the convolution directly. But we can do it in the Fourier domain…"
  },
  {
    "objectID": "lectures/ChNone.html#section-12",
    "href": "lectures/ChNone.html#section-12",
    "title": "The Fourier Series",
    "section": "",
    "text": "Now try it again using the convolution theorem.\nSteps:\n\nTake the DFT of the signal.\nPad the filter with zeros to make it the same length as the signal.\nTake the DFT of the filter.\nMultiply the two DFTs.\nTake the inverse DFT to get the filtered signal.\nDone!"
  },
  {
    "objectID": "lectures/ChNone.html#section-13",
    "href": "lectures/ChNone.html#section-13",
    "title": "The Fourier Series",
    "section": "",
    "text": "yf = fft(y)\nhf = fft(h, N) # pad h with zeros\ny_filtered = ifft(yf*hf)\nplt.plot(x, y, label='original')\nplt.plot(x, y_filtered, label='filtered')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nThen we can take the inverse DFT to get the filtered signal."
  },
  {
    "objectID": "lectures/ChNone.html#filtering-as-convolution",
    "href": "lectures/ChNone.html#filtering-as-convolution",
    "title": "The Fourier Series",
    "section": "Filtering as convolution",
    "text": "Filtering as convolution\nSuppose we have a signal and we’d like to try to filter out the high frequencies, but we don’t know which ones they are.\nWe could try with a simple filter like \\([1, 1, 1, 1, 1]/5\\). This is a simple moving average filter.\nThat means that we replace each point in the signal with the average of the 5 points before it.\nMathematically, this is:\n\\[\nf_{\\text{filtered}}[n] = \\sum_{m=0}^{4} f[n-m]/5\n\\]\nOK. We note that this can also be written as a convolution, between the signal and the vector \\(h = [1, 1, 1, 1, 1, 0, 0, 0, ...]/5\\)., where we have padded the vector with zeros so that it is the same length as the signal vector.\nThat is, \\(h_0 = 1/5\\), \\(h_1 = 1/5\\), \\(h_2 = 1/5\\), \\(h_3 = 1/5\\), \\(h_4 = 1/5\\), and \\(h_5 = 0\\), all the way to \\(h_n=0\\)."
  },
  {
    "objectID": "lectures/ChNone.html#the-convolution-theorem",
    "href": "lectures/ChNone.html#the-convolution-theorem",
    "title": "The Fourier Series",
    "section": "The convolution theorem",
    "text": "The convolution theorem\n\nThe convolution of two signals \\(f\\) and \\(h\\) is the inverse DFT of the product of the DFTs of \\(f\\) and \\(h\\).\n\n\nIn other words, if \\(f = \\text{ifft}(F)\\) and \\(h = \\text{ifft}(H)\\), then the convolution of \\(f\\) and \\(h\\) is \\(\\text{ifft}(F \\cdot H)\\).\n\n\nThat’s much easier to calculate – only a dot product!"
  },
  {
    "objectID": "lectures/ChNone.html#proof",
    "href": "lectures/ChNone.html#proof",
    "title": "The Fourier Series",
    "section": "Proof",
    "text": "Proof\n(fill in if I have time)"
  },
  {
    "objectID": "lectures/ChNone.html#example",
    "href": "lectures/ChNone.html#example",
    "title": "The Fourier Series",
    "section": "Example",
    "text": "Example\nLet’s take the signal from before, \\(f(x) = \\sin(2\\pi x) + 0.5 \\sin(20\\pi x)\\), and filter it with the moving average filter \\(h = [1, 1, 1, 1, 1]/5\\).\n\nfrom scipy.signal import convolve\nN = 200\nx = np.linspace(0, 1, N)\ny = np.sin(2*np.pi*x) + 10*np.sin(3.3*np.pi*x+.2)+ 10*np.random.normal(0, 0.1, N)\nh = np.array([1, 1,1,1,1])/5\ny_filtered = convolve(y, h, mode='same')\nplt.plot(x, y, label='original')\nplt.plot(x, y_filtered, label='filtered')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "lectures/ChNone.html#section-14",
    "href": "lectures/ChNone.html#section-14",
    "title": "The Fourier Series",
    "section": "",
    "text": "If we use our filter on our signal, we get:\n\\[\ny_{n}=h_{0} x_{n}+h_{1} x_{n-1}+\\cdots+h_{L} x_{n-L}, \\quad n \\in \\mathbb{Z}\n\\]\n. . .\nTo understand the effect of the filter, examine its action on a single Fourier mode of the signal, with a possible phase shift \\(\\phi\\):\n\\(x_{m}(t)=c_{m} e^{i m(\\omega t+\\phi)}\\)\n. . .\nAt individual points in time, this gives\n\\(x_{m, n}=c_{m} e^{i m \\omega\\left(n T_{s}+\\phi\\right)}\\)"
  },
  {
    "objectID": "lectures/ChNone.html#convolution",
    "href": "lectures/ChNone.html#convolution",
    "title": "The Fourier Series",
    "section": "Convolution",
    "text": "Convolution\n\nIntroduce the DFT per Strang with plot from Wiki page on DFT\nIntroduce the idea of filtering as convolution – https://www.cs.uoi.gr/~cnikou/Courses/Digital_Image_Processing/\nCircular convolution\nCircular convolution as matrix multiplication by a circulant matrix\nDiagonalization of circulant matrices as the DFT\nConvolution theorem\nExample for a simple discrete filter\nMoving to continuous frequency per Shores"
  },
  {
    "objectID": "lectures/ChNone.html#the-fourier-series",
    "href": "lectures/ChNone.html#the-fourier-series",
    "title": "The Fourier Series",
    "section": "The Fourier series",
    "text": "The Fourier series"
  },
  {
    "objectID": "lectures/ChNone.html#dft-matrix",
    "href": "lectures/ChNone.html#dft-matrix",
    "title": "The Fourier Series",
    "section": "DFT matrix",
    "text": "DFT matrix\n\\[\n\\begin{align}\n\\begin{bmatrix} y_0 \\\\ y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_{N-1} \\end{bmatrix}\n= F c = \\begin{bmatrix}\n1 & 1 & 1 & \\ldots & 1 \\\\\n1 & e^{-i 2\\pi/N} & e^{-i 4\\pi/N} & \\ldots & e^{-i 2\\pi (N-1)/N} \\\\\n1 & e^{-i 4\\pi/N} & e^{-i 8\\pi/N} & \\ldots & e^{-i 4\\pi (N-1)/N} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & e^{-i 2\\pi(N-1)/N} & e^{-i 4\\pi(N-1)/N} & \\ldots & e^{-i 2\\pi(N-1)(N-1)/N}\n\\end{bmatrix} \\begin{bmatrix}c_0 \\\\ c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_{N-1}\\end{bmatrix}\n\\end{align}\n\\]"
  },
  {
    "objectID": "lectures/ChNone.html#visualization",
    "href": "lectures/ChNone.html#visualization",
    "title": "The Fourier Series",
    "section": "Visualization",
    "text": "Visualization\n\n\n\nFrom Wikipedia, Original upload by en:User:Glogger, CC BY-SA 3.0 http://creativecommons.org/licenses/by-sa/3.0/, via Wikimedia Commons"
  },
  {
    "objectID": "lectures/ChNone.html#simplest-form-of-fourier-matrix",
    "href": "lectures/ChNone.html#simplest-form-of-fourier-matrix",
    "title": "The Fourier Series",
    "section": "Simplest form of Fourier matrix",
    "text": "Simplest form of Fourier matrix\nThen we can write the Fourier equation as\n\\[\n\\left[\\begin{array}{ccccc}\n1 & 1 & 1 & \\cdot & 1  \\\\\n1 & w & w^{2} & \\cdot & w^{n-1} \\\\\n1 & w^{2} & w^{4} & \\cdot & w^{2(n-1)} \\\\\n\\cdot & \\cdot & \\cdot & \\cdot & \\cdot \\\\\n1 & w^{n-1} & w^{2(n-1)} & \\cdot & w^{(n-1)^{2}}\n\\end{array}\\right]\\left[\\begin{array}{c}\nc_{0} \\\\\nc_{1} \\\\\nc_{2} \\\\\n\\cdot \\\\\nc_{n-1}\n\\end{array}\\right]=\\left[\\begin{array}{c}\ny_{0} \\\\\ny_{1} \\\\\ny_{2} \\\\\n\\cdot \\\\\ny_{n-1}\n\\end{array}\\right], \\quad w = e^{i 2\\pi/n}\n\\]"
  },
  {
    "objectID": "lectures/ChNone.html#low-pass-filtering",
    "href": "lectures/ChNone.html#low-pass-filtering",
    "title": "The Fourier Series",
    "section": "Low-pass filtering",
    "text": "Low-pass filtering\nWhat if we just zero out the coefficient corresponding to the second sinusoid:\n\n\nyf_new = yf.copy()\nyf_new[190] = 0\nyf_new[10] = 0\nplt.plot(np.abs(fftshift(yf_new)))\nplt.title('Filtered DFT')\nplt.show()"
  },
  {
    "objectID": "lectures/ChNone.html#discrete-time-filters-in-continuous-frequency-space",
    "href": "lectures/ChNone.html#discrete-time-filters-in-continuous-frequency-space",
    "title": "The Fourier Series",
    "section": "Discrete-time filters in continuous frequency space",
    "text": "Discrete-time filters in continuous frequency space\nBack to your textbook, in Chapter 6: ::: notes Here we will work with discrete signals (using time as the index) and discrete filters, but be interested in their continuous frequency representations.\n\nImagine we have a signal \\(x(t)\\) which is defined for \\(t\\) is the interval \\([0, T]\\)\n\n\nWe will imagine that \\(x(t)\\) which is defined over all \\(t\\) but periodic of period \\(T\\). So the function has frequency \\(f=1 / T\\) and angular frequency \\(\\omega=2 \\pi / T\\)\n\n\nA discrete filter is a sequence of complex numbers \\(\\mathbf{h}=\\left\\{h_{n}\\right\\}_{n=-\\infty}^{\\infty}\\).\n\n\nThis will be a finite impulse response (FIR) filter if \\(h_n\\) is 0 for \\(n&lt;0\\) or \\(n&gt;L\\) for some L.\nIn this case we say \\(\\mathbf{h}\\) has length \\(L\\) and express it in the form \\(\\mathbf{h}=\\left\\{h_{n}\\right\\}_{n=0}^{L}\\).\n\n\nThen we can take its discrete time Fourier transform (DTFT) as the Fourier series \\(X(\\mathbf{h})(\\zeta)=\\sum_{n=-\\infty}^{\\infty} h_{n} e^{i \\zeta}, \\zeta \\in \\mathbb{R}\\).\n\n\nSuppose we have a discrete signal \\(x(t)\\) in sampling periods \\(T_{s}\\), i.e., at \\(t_{n}=n T_{s}\\). Define \\(x_{n}=x\\left(t_{n}\\right)\\)\n:::"
  },
  {
    "objectID": "lectures/ChNone.html#gain",
    "href": "lectures/ChNone.html#gain",
    "title": "The Fourier Series",
    "section": "Gain",
    "text": "Gain\n\\(\\left|y_{m, n}\\right| \\leq\\left|H\\left(-m \\omega T_{s}\\right)\\right|\\left|x_{m, n}\\right|\\)\nSo we define gain or attenuation of this transformation as \\(G(\\zeta)=|H(\\zeta)|\\)\n\nWe define phase rotation as \\(\\Theta(\\zeta)=\\theta\\), where \\(H(\\zeta)=|H(\\zeta)| e^{i \\theta}\\)\n\n\nThe filter \\(h\\) will attenuate the signal at frequencies where \\(|H(\\zeta)|&lt;1\\) and amplify the signal at frequencies where \\(|H(\\zeta)|&gt;1\\). It will phase shift the signal by \\(\\Theta(\\zeta)\\)."
  },
  {
    "objectID": "lectures/ChNone.html#high-pass-and-low-pass-filters",
    "href": "lectures/ChNone.html#high-pass-and-low-pass-filters",
    "title": "The Fourier Series",
    "section": "High-pass and low-pass filters",
    "text": "High-pass and low-pass filters\nThe FIR filter \\(\\mathbf{h}=\\left\\{h_{k}\\right\\}_{k=0}^{L}\\) with discrete time Fourier transform \\(H(\\zeta)\\) is a lowpass filter if \\(|H(0)|=1\\) and \\(|H(\\pi)|=0\\); \\(\\mathbf{h}\\) is a highpass filter if \\(|H(0)|=0\\) and \\(|H(\\pi)|=1\\)."
  },
  {
    "objectID": "lectures/ChNone.html#section-15",
    "href": "lectures/ChNone.html#section-15",
    "title": "The Fourier Series",
    "section": "",
    "text": "Thus for fixed \\(m\\) and arbitrary \\(n\\), we have\n\n\\[\n\\begin{aligned}\ny_{m, n} & =h_{0} c_{m} e^{i m \\omega\\left(n T_{s}+\\phi\\right)}+h_{1} c_{m} e^{i m \\omega\\left((n-1) T_{s}+\\phi\\right)}+\\cdots+h_{L} c_{m} e^{i m \\omega\\left((n-L) T_{s}+\\phi\\right)} \\\\\n& =\\left(h_{0} \\cdot 1+h_{1} e^{-i m \\omega T_{s}}+\\cdots+h_{L} e^{-i L m \\omega T_{s}}\\right) c_{m} e^{i m \\omega\\left(n T_{s}+\\phi\\right)} \\\\\n& =H\\left(-m \\omega T_{s}\\right) x_{m, n}\n\\end{aligned}\n\\]\nwhere\n\\(H(\\zeta)=h_{0}+h_{1} e^{i \\zeta}+\\cdots+h_{L} e^{i L \\zeta}, \\quad \\zeta \\in \\mathbb{R}\\) is the DTFT of \\(\\mathbf{h}\\)."
  },
  {
    "objectID": "HW/FinalExamQuestions.html",
    "href": "HW/FinalExamQuestions.html",
    "title": "Final Exam Questions",
    "section": "",
    "text": "1 \nConvert this difference equation into matrix–vector form.\n\\(2y_{k+3} + 2y_{k+1} - 3y_{k} = 0\\)\n\nSolution. We solve the equation above for \\(y_{k+3}\\) to get \\(y_{k+3} = \\frac{3y_{k} - 2y_{k+1}}{2}\\).\nMake a vector of the unknowns: \\(x_k = [y_{k}, y_{k+1}, y_{k+2}]^T\\). Then we have that \\(x_{k+1}=[y_{k+1},y_{k+2},y_{k+3}]^T\\).\nWe can now write three equations in terms of the vector \\(x_k\\) and \\(x_{k+1}\\):\n\\[\n\\begin{aligned}\nx_{k+1}[1] &= x_{k}[2] \\\\\nx_{k+1}[2] &= x_{k}[3] \\\\\nx_{k+1}[3] &= \\frac{3x_{k}[1] - 2x_{k}[2]}{2}\n\\end{aligned}\n\\]\nThen the equation can be written as in the form \\(Ax_{k} = x_{k+1}\\) where \\(A\\) is a matrix given by\n\\[\nA = \\begin{bmatrix}\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n\\frac{-3}{2} & -1 & 0\n\\end{bmatrix}\n\\]\n\n\n\n2 \nConvert this difference equation into matrix–vector form.\n\\(y_{k+2} + y_{k+1} - 2y_{k} = 3\\)\n\nSolution. Give on practice exam.\nWe solve the equation above for \\(y_{k+2}\\) to get \\(y_{k+2} = 2y_{k} - y_{k+1} + 3\\).\nMake a vector of the unknowns: \\(x_k = [y_{k}, y_{k+1},1]^T\\). Then we have that \\(x_{k+1}=[y_{k+1},y_{k+2},1]^T\\).\nWe can now write three equations in terms of the vector \\(x_k\\) and \\(x_{k+1}\\):\n\\[\n\\begin{aligned}\nx_{k+1}[1] &= x_{k}[2] \\\\\nx_{k+1}[2] &= 2x_{k}[1] - x_{k}[2] + 3x_{k}[3] \\\\\nx_{k+1}[3] &= x_{k}[3]\n\\end{aligned}\n\\]\nThen the equation can be written as in the form \\(Ax_{k} = x_{k+1}\\) where \\(A\\) is a matrix given by\n\\[\nA = \\begin{bmatrix}\n0 & 1 & 0 \\\\\n2 & -1 & 3 \\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\]\nCheck using sympy:\n\nimport sympy as sp\n\nA = sp.Matrix([[0, 1, 0], [2, -1, 3], [0, 0, 1]])\nx_k = sp.Matrix([*sp.symbols('x_k0:2'), 1])\nx_k1 = sp.Matrix([*sp.symbols('x_k1:3'),1])\n\neq = A*x_k - x_k1\neq\n\n\\(\\displaystyle \\left[\\begin{matrix}0\\\\2 x_{k0} - x_{k1} - x_{k2} + 3\\\\0\\end{matrix}\\right]\\)\n\n\nIt works!\n\n\n\n3 \nA population is modeled with three stages: larva, pupa and adult, and the resulting structured population model transition matrix is\n\\[\nA = \\begin{bmatrix}\n0 & 0 & 0.6 \\\\\n0.5 & 0 & 0 \\\\\n0 & 0.9 & 0\n\\end{bmatrix}\n\\]\n\nimport sympy as sp\n\nA = sp.Matrix([[0, 0, 0.6], [0.5, 0, 0], [0, 0.9, 0]])\nA.eigenvals()\nA.eigenvects()\n\n[(0.646330407009565 + 9.57696933338458e-33*I,\n  1,\n  [Matrix([\n   [-0.211957169909255 - 0.563510300230596*I],\n   [-0.163969672175827 - 0.435930519529353*I],\n   [ -0.22832393982674 - 0.607023069503539*I]])]),\n (-0.323165203504783 - 0.559738551708619*I,\n  1,\n  [Matrix([\n   [-0.446848502976125 + 0.418540422762337*I],\n   [-0.107562931920626 - 0.461259721138817*I],\n   [ 0.631131829114318 + 0.191434388320088*I]])]),\n (-0.323165203504783 + 0.559738551708619*I,\n  1,\n  [Matrix([\n   [-0.277204808242613 - 0.561858355489268*I],\n   [-0.269197612619668 + 0.403041213970164*I],\n   [0.673461217391963 + 0.0440180865002761*I]])])]\n\n\n\nExplain what this model says about the three states.\nStarting with a population of (0, 30, 100), calculate the population in the next two time steps.\nThe eigenvalues of the matrix are \\(\\lambda_1 = 0.646, \\lambda_2 = -0.323 - 0.560i, \\lambda_3 = -0.323 + 0.560i\\). What do these eigenvalues tell you about the long-term behavior of the population?\n\n\nSolution. \n\nThe model says that the adult has a 60% chance of creating a larva (but will die every cycle), the larva has a 50% chance of becoming a pupa, and the pupa has a 90% chance of becoming an adult.\nWe can calculate the population in the next two time steps by multiplying the matrix \\(A\\) with the vector \\(x_k = [0, 30, 100]^T\\).\n\n\nimport numpy as np\n\nA = np.array([[0, 0, 0.6], [0.5, 0, 0], [0, 0.9, 0]])\nx_k = np.array([0, 30, 100])\n\nx_k1 = A.dot(x_k)\nx_k2 = A.dot(x_k1)\n\nx_k1, x_k2\n\n(array([60.,  0., 27.]), array([16.2, 30. ,  0. ]))\n\n\n\nThe largest eigenvalue is less than one. This matrix will not have a stable equilibrium point.\n\n\n\n\n4 \nA transition matrix A consists of only real entries. It has one real eigenvalue \\(\\lambda_1 = a_1\\) and two complex eigenvalues \\(\\lambda_2 = a_2 + b_2 \\text{i}, \\lambda_3 = a_2 - b_2 \\text{i}\\), so that \\(\\lambda_2\\) is the complex conjugate of \\(\\lambda_3\\): \\(\\lambda_2=\\overline{\\lambda_3}\\). It has corresponding eigenvectors \\(\\mathbf{v_1}, \\mathbf{v_2}, \\mathbf{v_3}\\), where the entries in \\(\\mathbf{v_1}\\) are real and the entries in \\(\\mathbf{v_2}\\) are the complex conjugates of the entries of \\(\\mathbf{v_3}\\).\n(For matrices with all real entries, complex eigenvalues will always come in complex conjugate pairs, and their eigenvectors will always also be complex conguate pairs.)\n\nSuppose the initial population is given by \\(\\mathbf{x}=\\begin{bmatrix} x_{1} \\\\ x_2 \\\\ x_3 \\end{bmatrix}\\), where \\(x_1,x_2,x_3\\) are real numbers. The projection of the initial population onto the eigenvectors is given by \\(\\mathbf{x}=c_1 \\mathbf{v_1} + c_2 \\mathbf{v_2} + c_3 \\mathbf{v_3}\\).\n\nUsing the fact that \\(v_2\\) and \\(v_3\\) are complex conjugates, show that \\(c_2\\) and \\(c_3\\) are also complex conjugates.\n\nIn terms of the eigenvalues and eigenvectors, what will be the population distribution after \\(k\\) iterations?\nUsing your results from the previous two parts, show that the population at time \\(k\\) will always have real entries.\n\n(You may find it helpful to know that for complex scalars \\(d,g\\), \\(\\overline{dg}=\\overline{d}\\times\\overline{g}\\), and \\(\\overline{d^n}= \\overline{d}^n\\).)\n\nArgue that you could have gotten the same result as (c) but just using the fact that the matrix \\(A\\) has only real entries.\nSuppose \\(|\\lambda_1|&gt;|\\lambda_2|=|\\lambda_3|\\). Estimate \\(\\lim_{k\\to\\infty} \\mathbf{x_k}\\) for a random initial population \\(\\mathbf{x_0}\\), for the cases where \\(\\lambda_1&gt;1\\), \\(\\lambda_1=1\\), and \\(\\lambda_1&lt;1\\).\n\n\n\n5 \nThe graph G is given below.\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\nvertices = ['A', 'B', 'C', 'D']\nedges = [('A', 'B'), ('A', 'C'), ('B', 'C'), ('B', 'D'), ('C', 'D')]\nweights = [1, -1, 1, 1, 1]\nG = nx.DiGraph()\n\nG.add_nodes_from(vertices)\nG.add_edges_from(edges)\n#G.add_weighted_edges_from([(edges[i][0], edges[i][1], weights[i]) for i in range(len(edges))])\npos = nx.spring_layout(G)\n# draw the graph with arrows indicating the direction of the edges\nnx.draw_networkx_nodes(G, pos, cmap=plt.get_cmap('jet'), node_size = 500)\nnx.draw_networkx_labels(G, pos)\nnx.draw_networkx_edges(G, pos, arrows=True)\nplt.show()\n\n/Users/kendra/Library/Python/3.8/lib/python/site-packages/networkx/drawing/nx_pylab.py:433: UserWarning:\n\nNo data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n\n\n\n\n\n\n\n\n\n\n\nFind the adjacency matrix \\(A\\) and the incidence matrix \\(B\\) for the graph G.\nFrom \\(A\\) and \\(B\\), match the following descriptions to the corresponding quantities:\n\nthe number of outgoing edges from each vertex,\nthe power of each vertex (the number of paths of length 1 or 2 originating at each vertex)\nthe number of paths of length \\(k\\) between each pair of vertices\na basis for loops in the graph (don’t worry about the direction of the edges)\n\n\n\n(a) Sum of the entries in the corresponding row of $A$,\n(c) Entries in the matrix $A^k$,\n(d) Nullspace of the matrix $B^T$.\n(e) Entries in the matrix $B^k$.\n(b) Sum of the entries in the corresponding row of $A + A^2$,\n(f) Nullspace of the matrix $A^T$.\n\nSolution. \n\nThe adjacency matrix \\(A\\) is given by\n\n\nA=sp.Matrix(nx.adjacency_matrix(G).todense())\nA\n\n\\(\\displaystyle \\left[\\begin{matrix}0 & 1 & 1 & 0\\\\0 & 0 & 1 & 1\\\\0 & 0 & 0 & 1\\\\0 & 0 & 0 & 0\\end{matrix}\\right]\\)\n\n\nThe incidence matrix \\(B\\) is given by\n\nB=sp.nsimplify(sp.Matrix(nx.incidence_matrix(G, oriented=True).todense()),rational=True)\nB\n\n\\(\\displaystyle \\left[\\begin{matrix}-1 & -1 & 0 & 0 & 0\\\\1 & 0 & -1 & -1 & 0\\\\0 & 1 & 1 & 0 & -1\\\\0 & 0 & 0 & 1 & 1\\end{matrix}\\right]\\)\n\n\n\n\nThe degree of each vertex is given by the sum of the entries in the corresponding row of the adjacency matrix \\(A\\).\n\n\nThe power of each vertex is given by the sum of the entries in the corresponding row of the matrix \\(A + A^2\\).\nThe number of paths of length \\(k\\) between each pair of vertices is given by the entries in the matrix \\(A^k\\).\nA basis for loops in the graph is given by the nullspace of the matrix \\(B^T\\).\n\n\n\n\n6 \nA directed graph has vertices labeled \\(A\\), \\(B\\), \\(C\\), and \\(D\\). The edges are given by the following incidence matrix B:\n\nfrom IPython.display import display, Math\nvertices = ['A', 'B', 'C', 'D']\nedges = [('A', 'B'), ('A', 'C'), ('B', 'C'), ('B', 'D'), ('C', 'D'),('D','A')]\nG = nx.DiGraph()\nG.add_nodes_from(vertices)\nG.add_edges_from(edges)\n\nB = sp.nsimplify(sp.Matrix(nx.incidence_matrix(G, oriented=True).todense()),rational=True)\ndisplay(Math(f'B = {sp.latex(B)}'))\n\n\\(\\displaystyle B = \\left[\\begin{matrix}-1 & -1 & 0 & 0 & 0 & 1\\\\1 & 0 & -1 & -1 & 0 & 0\\\\0 & 1 & 1 & 0 & -1 & 0\\\\0 & 0 & 0 & 1 & 1 & -1\\end{matrix}\\right]\\)\n\n\n\nFind the adjacency matrix \\(A\\) for the graph.\nDraw the graph.\nDescribe how you could find the number of paths of length \\(k\\) between each pair of vertices using the adjacency matrix \\(A\\).\nIf you calculate the matrix \\(A\\) + \\(A^2\\), what does the sum of the entries in the corresponding row tell you about the graph?\nLet \\(\\mathbf{v}\\) be a row vector representing a subset of the edges in the graph. For example, the subset of edges given by the vector \\(\\mathbf{v} = [1, 0, 1, 0, 0, 1]\\) would correspond to the first, third, and sixth edges.\n\nWrite an equation which will be satisfied whenever the subset of edges represented by \\(\\mathbf{v}\\) forms a loop in the graph.\n\nDescribe two algorithms for ranking the importance of the vertices in a graph. (One or more of these algorithms may be based on prior parts of this problem.)\n\n\nSolution. \n\nA=sp.Matrix(nx.adjacency_matrix(G).todense())\nA\n\n\\(\\displaystyle \\left[\\begin{matrix}0 & 1 & 1 & 0\\\\0 & 0 & 1 & 1\\\\0 & 0 & 0 & 1\\\\1 & 0 & 0 & 0\\end{matrix}\\right]\\)\n\n\n\npos = nx.spring_layout(G)\n# draw the graph with arrows indicating the direction of the edges\nnx.draw_networkx_nodes(G, pos, cmap=plt.get_cmap('jet'), node_size = 500)\nnx.draw_networkx_labels(G, pos)\nnx.draw_networkx_edges(G, pos, arrows=True)\nplt.show()\n\n/Users/kendra/Library/Python/3.8/lib/python/site-packages/networkx/drawing/nx_pylab.py:433: UserWarning:\n\nNo data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n\n\n\n\n\n\n\n\n\n\n\nThe number of paths of length \\(k\\) between each pair of vertices is given by the entries in the matrix \\(B^k\\).\nThe sum of the entries in the corresponding row of the matrix \\(A + A^2\\) gives the power of each vertex. In other words, it gives the number of paths of length 1 or 2 originating at each vertex.\nA loop in the graph is a subset of edges that form a closed path. A closed path is a path that starts and ends at the same vertex. Therefore, a subset of edges forms a loop if the multiple of the incidence matrix \\(B\\) and the vector \\(\\mathbf{v}\\) is equal to zero. In other words, the subset of edges represented by \\(\\mathbf{v}\\) forms a loop if \\(\\mathbf{v}B^T = 0\\), or equivalently if \\(B \\mathbf{v^T}=0\\). These definitions work if B has one column for each edge; if B has one row for each edge, then the equation is \\(B^T \\mathbf{v^T}=0\\).\nTwo algorithms for ranking the importance of the vertices in a graph are the PageRank algorithm the power method.\n\n\n\n\n7 \nFind a least squares solution to the following system:\n\\[\n\\begin{bmatrix}\n1 & 1 \\\\\n1 & 1 \\\\\n1 & 2 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}\n\\]\nIs the solution a genuine solution?\n\nSolution. First we find the least squares solution by solving the normal equations \\(A^TAx = A^Tb\\).\n\nA = sp.Matrix([[1, 1], [1, 1], [1, 2]])\nb = sp.Matrix([1, 2, 3])\n\nx = (A.T*A).inv()*A.T*b\nx\n\n\\(\\displaystyle \\left[\\begin{matrix}0\\\\\\frac{3}{2}\\end{matrix}\\right]\\)\n\n\nThe solution is \\(x = \\begin{bmatrix} 1.5 \\\\ 0.5 \\end{bmatrix}\\).\nTo check if this is a genuine solution, we substitute the values of \\(x\\) back into the original equation.\n\nA*x\n\n\\(\\displaystyle \\left[\\begin{matrix}\\frac{3}{2}\\\\\\frac{3}{2}\\\\3\\end{matrix}\\right]\\)\n\n\nThe result is \\(\\begin{bmatrix} 1.5 \\\\ 1.5 \\\\ 3.0 \\end{bmatrix}\\), which is not equal to \\(\\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}\\). Therefore, the solution is not a genuine solution.\n\n\n\n8 \n\n\nThe system of equations\n\n\n\\[\n\\begin{aligned}\nx_{1}+x_{2} & =2 \\\\\nx_{3}+x_{4} & =1 \\\\\n2 x_{1}+4 x_{2}+2 x_{4} & =6\n\\end{aligned}\n\\]\nhas coefficient matrix \\(A\\) and right hand side \\(\\mathbf{b}\\) such that the row-reduced echelon form of \\([A \\mid \\mathbf{b}]\\) is\n\\[\n\\left[\\begin{array}{ccccc:c}1 & 0 & 0 & -1 & \\mid & 1 \\\\ 0 & 1 & 0 & 1 & | & 1 \\\\ 0 & 0 & 1 & 1 & | & 1\\end{array}\\right]\n\\]\nUse this information to answer the following:\n\nFind a basis for the null space of \\(A\\).\nFind the form of a general solution of the system \\(A \\mathbf{x}=\\mathbf{b}\\).\nFind the rank of \\(A\\).\nNo matter what the right hand side of \\(\\mathbf{b}\\) is, this system has solutions. In terms of rank, why do we know this?\n\n::: solution\n\nTo find a basis for the null space of \\(A\\), we solve the equation \\(A\\mathbf{x} = \\mathbf{0}\\).\n\nWe know that \\(x_4\\) is a free variable, so we can write the solution in terms of \\(x_4\\):\n\\[\n\\begin{aligned}\nx_1 &= x_4 \\\\\nx_2 &= - x_4 \\\\\nx_3 &= - x_4 \\\\\nx_4 &= x_4\n\\end{aligned}\n\\]\nSo the basis for the nullspace of \\(A\\) is simply the vector \\(\\begin{bmatrix} 1 \\\\ -1 \\\\ -1 \\\\ 1 \\end{bmatrix}\\).\nChecking using sympy:\n\nA = sp.Matrix([[1,0,0,-1],[0,1,0,1],[0,0,1,1]])\nA.nullspace()\nb=sp.Matrix([1,1,1])\n\n\nThe general solution will be a particular solution plus a linear combination of the basis for the nullspace of \\(A\\).\n\nTo find a particular solution, we can set \\(x_4 = 0\\) and solve the system. Then we have that \\(x_1 = x_2 = x_3 = 1\\). Therefore, a particular solution is \\(\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\end{bmatrix}\\).\nThe general solution is then given by \\(\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\end{bmatrix} + x_4 \\begin{bmatrix} 1 \\\\ -1 \\\\ -1 \\\\ 1 \\end{bmatrix}\\).\n\nThe rank of \\(A\\) is the number of pivot columns in the row-reduced echelon form of \\(A\\). In this case, the rank of \\(A\\) is 3.\nThe rank of \\(A\\) is less than the number of columns in \\(A\\). Therefore, the system will always have solutions.\n\n\n\n9 \nLet \\(A=\\left[\\begin{array}{ccc}1 & 0 & 1 \\\\ -1 & 1 & 1 \\\\ 2 & 1 & 4\\end{array}\\right]\\).\n\nFind the reduced row-echelon form and rank of \\(A\\).\nFind a basis for the column space of \\(A\\).\nDetermine which of the following vectors is in the column space of \\(A\\) and, if so, express the vector as a linear combination of the columns of \\(A\\) :\n\n\\(b_{1}=[2,1,0]^{T}\\), in \\(b_{2}=[2,-3,3]^{T}\\).\nYou may find it helpful to use the following outputs from sympy:\n\nA = sp.Matrix([[1, 0, 1], [-1, 1, 1], [2, 1, 4]])\nb1 = sp.Matrix([2, 1, 0])\nb2 = sp.Matrix([2, -3, 3])\n\nA.gauss_jordan_solve(b1): ValueError: Linear system has no solution\nA.gauss_jordan_solve(b2): \\(\\begin{bmatrix} 2 - \\tau_0 \\\\ -2 \\tau_0 -1 \\\\ \\tau_0 \\end{bmatrix}\\)\n\nSolution. \n\nThe reduced row-echelon form of \\(A\\) is:\n\n\nA = sp.Matrix([[1, 0, 1], [-1, 1, 1], [2, 1, 4]])\ndisplay(A.rref()[0])\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & 0 & 1\\\\0 & 1 & 2\\\\0 & 0 & 0\\end{matrix}\\right]\\)\n\n\n\nThe basis for the column space of A is given by the pivot columns of the row-reduced echelon form of A. In this case, the basis is given by the first two columns of A.\n\n\ncs=A.columnspace()\nc1=cs[0]\nc2=cs[1]\ndisplay(c1)\ndisplay(c2)\n\n\\(\\displaystyle \\left[\\begin{matrix}1\\\\-1\\\\2\\end{matrix}\\right]\\)\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0\\\\1\\\\1\\end{matrix}\\right]\\)\n\n\n\nTo determine if \\(b_1\\) is in the column space of A, we solve the equation \\(A\\mathbf{x} = b_1\\). and \\(b_2\\). The first has no solution, so it’s not in the column space. The second has a solution, so it is in the column space. The solution is \\(\\begin{bmatrix} 2 - \\tau_0 \\\\ -2 \\tau_0 -1 \\\\ \\tau_0 \\end{bmatrix}\\).\n\nWe pick \\(\\tau_0 = 0\\) to get that we can combine twice the first column and -1 times the second column to get \\(b_2\\).\n\nA.gauss_jordan_solve(b2)\n\n(Matrix([\n [   2 - tau0],\n [-2*tau0 - 1],\n [       tau0]]),\n Matrix([[tau0]]))\n\n\n\n\n\n10 \nYou are given a matrix \\(A\\):\n\nfrom IPython.display import display, Math\nimport sympy as sp\nA = sp.Matrix([[0,1],[1,0],[1,1]])\nA\n\n\\(\\displaystyle \\left[\\begin{matrix}0 & 1\\\\1 & 0\\\\1 & 1\\end{matrix}\\right]\\)\n\n\nThe eigenvalues of \\(AA^T\\) are \\(\\lambda_1 = 3, \\lambda_2 = 1, \\lambda_3 = 0\\).\n\nFind the eigenvalues of \\(AA^T\\).\nFind the eigenvectors of \\(A^TA\\) and \\(AA^T\\). Use the facts below and explain your reasoning.\nFind the singular value decomposition of \\(A\\).\nUsing the SVD, calculate \\(A^+\\), the pseudoinverse of \\(A\\). Use this to solve the least squares problem \\(A\\mathbf{x} = \\mathbf{b}\\), where \\(\\mathbf{b} = [1, 4,2]^T\\).\nUsing the SVD, calculate \\(A^k \\mathbf{x}\\) for \\(k=3\\) and \\(\\mathbf{x} = [-2, 2, 0]^T\\).\n\nHere are some facts which may be helpful:\n\nata=A.T*A\naat=A*A.T\nv1 = aat.eigenvects()[0][2][0]\nv2 = aat.eigenvects()[1][2][0]\nv3 = aat.eigenvects()[2][2][0]\nu1 = ata.eigenvects()[0][2][0]\nu2 = ata.eigenvects()[1][2][0]\n\ndisplay(Math(f'{sp.latex(aat)}{sp.latex(v1)}={sp.latex(aat*v1)}'))\ndisplay(Math(f'{sp.latex(aat-3*sp.eye(3))}{sp.latex(v3)}={sp.latex((aat-3*sp.eye(3))*v3)}'))\ndisplay(Math(f' {sp.latex(aat-sp.eye(3))}{sp.latex(v2)}={sp.latex((aat-sp.eye(3))*v2)}'))\n\ndisplay(Math(f'{sp.latex(ata-sp.eye(2))}{sp.latex(u1)}={sp.latex((ata-sp.eye(2))*u1)}'))\ndisplay(Math(f'{sp.latex(ata-3*sp.eye(2))}{sp.latex(u2)}={sp.latex((ata-3*sp.eye(2))*u2)}'))\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & 0 & 1\\\\0 & 1 & 1\\\\1 & 1 & 2\\end{matrix}\\right]\\left[\\begin{matrix}-1\\\\-1\\\\1\\end{matrix}\\right]=\\left[\\begin{matrix}0\\\\0\\\\0\\end{matrix}\\right]\\)\n\n\n\\(\\displaystyle \\left[\\begin{matrix}-2 & 0 & 1\\\\0 & -2 & 1\\\\1 & 1 & -1\\end{matrix}\\right]\\left[\\begin{matrix}\\frac{1}{2}\\\\\\frac{1}{2}\\\\1\\end{matrix}\\right]=\\left[\\begin{matrix}0\\\\0\\\\0\\end{matrix}\\right]\\)\n\n\n\\(\\displaystyle  \\left[\\begin{matrix}0 & 0 & 1\\\\0 & 0 & 1\\\\1 & 1 & 1\\end{matrix}\\right]\\left[\\begin{matrix}-1\\\\1\\\\0\\end{matrix}\\right]=\\left[\\begin{matrix}0\\\\0\\\\0\\end{matrix}\\right]\\)\n\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & 1\\\\1 & 1\\end{matrix}\\right]\\left[\\begin{matrix}-1\\\\1\\end{matrix}\\right]=\\left[\\begin{matrix}0\\\\0\\end{matrix}\\right]\\)\n\n\n\\(\\displaystyle \\left[\\begin{matrix}-1 & 1\\\\1 & -1\\end{matrix}\\right]\\left[\\begin{matrix}1\\\\1\\end{matrix}\\right]=\\left[\\begin{matrix}0\\\\0\\end{matrix}\\right]\\)\n\n\n::: solution\n\nWe first find the characteristic polynomial of \\(AA^T\\): we find \\(\\text{det}(AA^T - \\lambda I)\\) and set it equal to zero.\n\n\nimport io\n\ndef print_to_string(*args, **kwargs):\n    output = io.StringIO()\n    print(*args, file=output, **kwargs)\n    contents = output.getvalue()\n    output.close()\n    return contents\ndisplay(Math(f'{sp.latex(ata.charpoly().as_expr())}=0'))\n\n\\(\\displaystyle \\lambda^{2} - 4 \\lambda + 3=0\\)\n\n\nWe try the three eigenvalues given in the question: 0, 1, and 3. We find that 0 is not an eigenvalue, but that 1 and 3 satisfy the equation. Therefore, the eigenvalues of \\(AA^T\\) are \\(\\lambda_1 = 3, \\lambda_2 = 1\\).\n\nThe eigenvectors of \\(AA^T\\) are given by the nullspace of \\(AA^T - \\lambda I\\). We can find the eigenvectors by solving the equation \\((AA^T - \\lambda I)v = 0\\). These are what are given to us in the question, so the eigenvectors are \\[\\\\\\\\\\\\\\\\left\\[\\\\\\\\\\\\\\\\begin\\{smallmatrix\\}\\-1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\-1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\1\\\\\\\\\\\\\\\\end\\{smallmatrix\\}\\\\\\\\\\\\\\\\right\\]\\] and \\\\\\\\left[\\\\\\\\begin{matrix}-1\\\\\\\\\\\\\\\\1\\\\\\\\\\\\\\\\0\\\\\\\\end{matrix}\\\\\\\\right].\n\n7\\\\n #"
  },
  {
    "objectID": "HW/FinalExamQuestions.sol.html",
    "href": "HW/FinalExamQuestions.sol.html",
    "title": "Final Exam Questions Solutions",
    "section": "",
    "text": "1 \nConvert this difference equation into matrix–vector form.\n\\(2y_{k+3} + 2y_{k+1} - 3y_{k} = 0\\)\n\nSolution. We solve the equation above for \\(y_{k+3}\\) to get \\(y_{k+3} = \\frac{3y_{k} - 2y_{k+1}}{2}\\).\nMake a vector of the unknowns: \\(x_k = [y_{k}, y_{k+1}, y_{k+2}]^T\\). Then we have that \\(x_{k+1}=[y_{k+1},y_{k+2},y_{k+3}]^T\\).\nWe can now write three equations in terms of the vector \\(x_k\\) and \\(x_{k+1}\\):\n\\[\n\\begin{aligned}\nx_{k+1}[1] &= x_{k}[2] \\\\\nx_{k+1}[2] &= x_{k}[3] \\\\\nx_{k+1}[3] &= \\frac{3x_{k}[1] - 2x_{k}[2]}{2}\n\\end{aligned}\n\\]\nThen the equation can be written as in the form \\(Ax_{k} = x_{k+1}\\) where \\(A\\) is a matrix given by\n\\[\nA = \\begin{bmatrix}\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n\\frac{-3}{2} & -1 & 0\n\\end{bmatrix}\n\\]\n\n\n\n2 \nConvert this difference equation into matrix–vector form.\n\\(y_{k+2} + y_{k+1} - 2y_{k} = 3\\)\n\nSolution. Give on practice exam.\nWe solve the equation above for \\(y_{k+2}\\) to get \\(y_{k+2} = 2y_{k} - y_{k+1} + 3\\).\nMake a vector of the unknowns: \\(x_k = [y_{k}, y_{k+1},1]^T\\). Then we have that \\(x_{k+1}=[y_{k+1},y_{k+2},1]^T\\).\nWe can now write three equations in terms of the vector \\(x_k\\) and \\(x_{k+1}\\):\n\\[\n\\begin{aligned}\nx_{k+1}[1] &= x_{k}[2] \\\\\nx_{k+1}[2] &= 2x_{k}[1] - x_{k}[2] + 3x_{k}[3] \\\\\nx_{k+1}[3] &= x_{k}[3]\n\\end{aligned}\n\\]\nThen the equation can be written as in the form \\(Ax_{k} = x_{k+1}\\) where \\(A\\) is a matrix given by\n\\[\nA = \\begin{bmatrix}\n0 & 1 & 0 \\\\\n2 & -1 & 3 \\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\]\nCheck using sympy:\n\nimport sympy as sp\n\nA = sp.Matrix([[0, 1, 0], [2, -1, 3], [0, 0, 1]])\nx_k = sp.Matrix([*sp.symbols('x_k0:2'), 1])\nx_k1 = sp.Matrix([*sp.symbols('x_k1:3'),1])\n\neq = A*x_k - x_k1\neq\n\n\\(\\displaystyle \\left[\\begin{matrix}0\\\\2 x_{k0} - x_{k1} - x_{k2} + 3\\\\0\\end{matrix}\\right]\\)\n\n\nIt works!\n\n\n\n3 \nA population is modeled with three stages: larva, pupa and adult, and the resulting structured population model transition matrix is\n\\[\nA = \\begin{bmatrix}\n0 & 0 & 0.6 \\\\\n0.5 & 0 & 0 \\\\\n0 & 0.9 & 0\n\\end{bmatrix}\n\\]\n\nimport sympy as sp\n\nA = sp.Matrix([[0, 0, 0.6], [0.5, 0, 0], [0, 0.9, 0]])\nA.eigenvals()\nA.eigenvects()\n\n[(0.646330407009565 + 9.57696933338458e-33*I,\n  1,\n  [Matrix([\n   [-0.211957169909255 - 0.563510300230596*I],\n   [-0.163969672175827 - 0.435930519529353*I],\n   [ -0.22832393982674 - 0.607023069503539*I]])]),\n (-0.323165203504783 - 0.559738551708619*I,\n  1,\n  [Matrix([\n   [-0.446848502976125 + 0.418540422762337*I],\n   [-0.107562931920626 - 0.461259721138817*I],\n   [ 0.631131829114318 + 0.191434388320088*I]])]),\n (-0.323165203504783 + 0.559738551708619*I,\n  1,\n  [Matrix([\n   [-0.277204808242613 - 0.561858355489268*I],\n   [-0.269197612619668 + 0.403041213970164*I],\n   [0.673461217391963 + 0.0440180865002761*I]])])]\n\n\n\nExplain what this model says about the three states.\nStarting with a population of (0, 30, 100), calculate the population in the next two time steps.\nThe eigenvalues of the matrix are \\(\\lambda_1 = 0.646, \\lambda_2 = -0.323 - 0.560i, \\lambda_3 = -0.323 + 0.560i\\). What do these eigenvalues tell you about the long-term behavior of the population?\n\n\nSolution. \n\nThe model says that the adult has a 60% chance of creating a larva (but will die every cycle), the larva has a 50% chance of becoming a pupa, and the pupa has a 90% chance of becoming an adult.\nWe can calculate the population in the next two time steps by multiplying the matrix \\(A\\) with the vector \\(x_k = [0, 30, 100]^T\\).\n\n\nimport numpy as np\n\nA = np.array([[0, 0, 0.6], [0.5, 0, 0], [0, 0.9, 0]])\nx_k = np.array([0, 30, 100])\n\nx_k1 = A.dot(x_k)\nx_k2 = A.dot(x_k1)\n\nx_k1, x_k2\n\n(array([60.,  0., 27.]), array([16.2, 30. ,  0. ]))\n\n\n\nThe largest eigenvalue is less than one. This matrix will not have a stable equilibrium point.\n\n\n\n\n4 \nA transition matrix A consists of only real entries. It has one real eigenvalue \\(\\lambda_1 = a_1\\) and two complex eigenvalues \\(\\lambda_2 = a_2 + b_2 \\text{i}, \\lambda_3 = a_2 - b_2 \\text{i}\\), so that \\(\\lambda_2\\) is the complex conjugate of \\(\\lambda_3\\): \\(\\lambda_2=\\overline{\\lambda_3}\\). It has corresponding eigenvectors \\(\\mathbf{v_1}, \\mathbf{v_2}, \\mathbf{v_3}\\), where the entries in \\(\\mathbf{v_1}\\) are real and the entries in \\(\\mathbf{v_2}\\) are the complex conjugates of the entries of \\(\\mathbf{v_3}\\).\n(For matrices with all real entries, complex eigenvalues will always come in complex conjugate pairs, and their eigenvectors will always also be complex conguate pairs.)\n\nSuppose the initial population is given by \\(\\mathbf{x}=\\begin{bmatrix} x_{1} \\\\ x_2 \\\\ x_3 \\end{bmatrix}\\), where \\(x_1,x_2,x_3\\) are real numbers. The projection of the initial population onto the eigenvectors is given by \\(\\mathbf{x}=c_1 \\mathbf{v_1} + c_2 \\mathbf{v_2} + c_3 \\mathbf{v_3}\\).\n\nUsing the fact that \\(v_2\\) and \\(v_3\\) are complex conjugates, show that \\(c_2\\) and \\(c_3\\) are also complex conjugates.\n\nIn terms of the eigenvalues and eigenvectors, what will be the population distribution after \\(k\\) iterations?\nUsing your results from the previous two parts, show that the population at time \\(k\\) will always have real entries.\n\n(You may find it helpful to know that for complex scalars \\(d,g\\), \\(\\overline{dg}=\\overline{d}\\times\\overline{g}\\), and \\(\\overline{d^n}= \\overline{d}^n\\).)\n\nArgue that you could have gotten the same result as (c) but just using the fact that the matrix \\(A\\) has only real entries.\nSuppose \\(|\\lambda_1|&gt;|\\lambda_2|=|\\lambda_3|\\). Estimate \\(\\lim_{k\\to\\infty} \\mathbf{x_k}\\) for a random initial population \\(\\mathbf{x_0}\\), for the cases where \\(\\lambda_1&gt;1\\), \\(\\lambda_1=1\\), and \\(\\lambda_1&lt;1\\).\n\n\n\nThe projection onto \\(\\mathbf{v_2}\\) is given by \\(\\mathbf{x} \\cdot \\mathbf{v_2} = x[1]v_2[1] + x[2]v_2[2] + x[3]v_2[3]\\). Since \\(v_2\\) is a complex conjugate of \\(v_3\\), we have that \\(v_2[2] = \\overline{v_3[2]}\\) and \\(v_2[3] = \\overline{v_3[3]}\\).\n\nThen the projection onto \\(\\mathbf{v_2}\\) is given by \\(c_2=x[1]v_2[1] + x[2]v_2[2] + x[3]v_2[3] = x[1]\\overline{v_3[1]} + x[2]\\overline{v_3[2}] + x[3]\\overline{v_3[3]} = \\overline{c_3}\\).\n\nThe population at time \\(k\\) is given by \\(\\mathbf{x_k}=A^k x_0 = c_1 \\lambda_1^k \\mathbf{v_1} + c_2 \\lambda_2^k \\mathbf{v_2} + c_3 \\lambda_3^k \\mathbf{v_3}\\).\nThe first entry of \\(\\mathbf{x_k}\\) is given by\n\n\\[\n\\begin{aligned}\nx_k[1]&=c_1 \\lambda_1^k v_1[1] + c_2 \\lambda_2^k v_2[1] + c_3 \\lambda_3^k v_3[1] \\\\\n&=c_1 \\lambda_1^k v_1[1] + c_2 \\lambda_2^k v_2[1] + \\overline{c_2} \\overline{\\lambda_2^k} \\overline{v_2[1]} \\\\\n&=c_1 \\lambda_1^k v_1[1] + c_2 \\lambda_2^k v_2[1] + \\overline{c_2\\lambda_2^k v_2[1]} \\\\\n\\end{aligned}\n\\]\nA number plus its complex conjugate is always real. Therefore, \\(x_k[1]\\) is real. The same logic holds for \\(x_k[2]\\) and \\(x_k[3]\\), so the population at time \\(k\\) will always have real entries.\n\nSince the matrix \\(A\\) has only real entries, all of its powers will also have only real entries. Therefore, the population at time \\(k\\) will always have real entries.\nIf \\(|\\lambda_1|&gt;|\\lambda_2|=|\\lambda_3|\\), then the population at time \\(k\\) will be dominated by the term \\(c_1 \\lambda_1^k \\mathbf{v_1}\\). If \\(\\lambda_1&gt;1\\), then the population will grow exponentially. If \\(\\lambda_1=1\\), then the population will stay constant and will approach \\(c_1 \\mathbf{v_1}\\). If \\(\\lambda_1&lt;1\\), then the population will decay exponentially.\n\n\n\n\n5 \nThe graph G is given below.\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\nvertices = ['A', 'B', 'C', 'D']\nedges = [('A', 'B'), ('A', 'C'), ('B', 'C'), ('B', 'D'), ('C', 'D')]\nweights = [1, -1, 1, 1, 1]\nG = nx.DiGraph()\n\nG.add_nodes_from(vertices)\nG.add_edges_from(edges)\n#G.add_weighted_edges_from([(edges[i][0], edges[i][1], weights[i]) for i in range(len(edges))])\npos = nx.spring_layout(G)\n# draw the graph with arrows indicating the direction of the edges\nnx.draw_networkx_nodes(G, pos, cmap=plt.get_cmap('jet'), node_size = 500)\nnx.draw_networkx_labels(G, pos)\nnx.draw_networkx_edges(G, pos, arrows=True)\nplt.show()\n\n\n\n\n\n\n\n\n\nFind the adjacency matrix \\(A\\) and the incidence matrix \\(B\\) for the graph G.\nFrom \\(A\\) and \\(B\\), match the following descriptions to the corresponding quantities:\n\nthe number of outgoing edges from each vertex,\nthe power of each vertex (the number of paths of length 1 or 2 originating at each vertex)\nthe number of paths of length \\(k\\) between each pair of vertices\na basis for loops in the graph (don’t worry about the direction of the edges)\n\n\n\n(a) Sum of the entries in the corresponding row of $A$,\n(c) Entries in the matrix $A^k$,\n(d) Nullspace of the matrix $B^T$.\n(e) Entries in the matrix $B^k$.\n(b) Sum of the entries in the corresponding row of $A + A^2$,\n(f) Nullspace of the matrix $A^T$.\n\nSolution. \n\nThe adjacency matrix \\(A\\) is given by\n\n\nA=sp.Matrix(nx.adjacency_matrix(G).todense())\nA\n\n\\(\\displaystyle \\left[\\begin{matrix}0 & 1 & 1 & 0\\\\0 & 0 & 1 & 1\\\\0 & 0 & 0 & 1\\\\0 & 0 & 0 & 0\\end{matrix}\\right]\\)\n\n\nThe incidence matrix \\(B\\) is given by\n\nB=sp.nsimplify(sp.Matrix(nx.incidence_matrix(G, oriented=True).todense()),rational=True)\nB\n\n\\(\\displaystyle \\left[\\begin{matrix}-1 & -1 & 0 & 0 & 0\\\\1 & 0 & -1 & -1 & 0\\\\0 & 1 & 1 & 0 & -1\\\\0 & 0 & 0 & 1 & 1\\end{matrix}\\right]\\)\n\n\n\n\nThe degree of each vertex is given by the sum of the entries in the corresponding row of the adjacency matrix \\(A\\).\n\n\nThe power of each vertex is given by the sum of the entries in the corresponding row of the matrix \\(A + A^2\\).\nThe number of paths of length \\(k\\) between each pair of vertices is given by the entries in the matrix \\(A^k\\).\nA basis for loops in the graph is given by the nullspace of the matrix \\(B^T\\).\n\n\n\n\n6 \nA directed graph has vertices labeled \\(A\\), \\(B\\), \\(C\\), and \\(D\\). The edges are given by the following incidence matrix B:\n\nfrom IPython.display import display, Math\nvertices = ['A', 'B', 'C', 'D']\nedges = [('A', 'B'), ('A', 'C'), ('B', 'C'), ('B', 'D'), ('C', 'D'),('D','A')]\nG = nx.DiGraph()\nG.add_nodes_from(vertices)\nG.add_edges_from(edges)\n\nB = sp.nsimplify(sp.Matrix(nx.incidence_matrix(G, oriented=True).todense()),rational=True)\ndisplay(Math(f'B = {sp.latex(B)}'))\n\n\\(\\displaystyle B = \\left[\\begin{matrix}-1 & -1 & 0 & 0 & 0 & 1\\\\1 & 0 & -1 & -1 & 0 & 0\\\\0 & 1 & 1 & 0 & -1 & 0\\\\0 & 0 & 0 & 1 & 1 & -1\\end{matrix}\\right]\\)\n\n\n\nFind the adjacency matrix \\(A\\) for the graph.\nDraw the graph.\nDescribe how you could find the number of paths of length \\(k\\) between each pair of vertices using the adjacency matrix \\(A\\).\nIf you calculate the matrix \\(A\\) + \\(A^2\\), what does the sum of the entries in the corresponding row tell you about the graph?\nLet \\(\\mathbf{v}\\) be a row vector representing a subset of the edges in the graph. For example, the subset of edges given by the vector \\(\\mathbf{v} = [1, 0, 1, 0, 0, 1]\\) would correspond to the first, third, and sixth edges.\n\nWrite an equation which will be satisfied whenever the subset of edges represented by \\(\\mathbf{v}\\) forms a loop in the graph.\n\nDescribe two algorithms for ranking the importance of the vertices in a graph. (One or more of these algorithms may be based on prior parts of this problem.)\n\n\nSolution. \n\nA=sp.Matrix(nx.adjacency_matrix(G).todense())\nA\n\n\\(\\displaystyle \\left[\\begin{matrix}0 & 1 & 1 & 0\\\\0 & 0 & 1 & 1\\\\0 & 0 & 0 & 1\\\\1 & 0 & 0 & 0\\end{matrix}\\right]\\)\n\n\n\npos = nx.spring_layout(G)\n# draw the graph with arrows indicating the direction of the edges\nnx.draw_networkx_nodes(G, pos, cmap=plt.get_cmap('jet'), node_size = 500)\nnx.draw_networkx_labels(G, pos)\nnx.draw_networkx_edges(G, pos, arrows=True)\nplt.show()\n\n\n\n\n\n\n\n\n\nThe number of paths of length \\(k\\) between each pair of vertices is given by the entries in the matrix \\(B^k\\).\nThe sum of the entries in the corresponding row of the matrix \\(A + A^2\\) gives the power of each vertex. In other words, it gives the number of paths of length 1 or 2 originating at each vertex.\nA loop in the graph is a subset of edges that form a closed path. A closed path is a path that starts and ends at the same vertex. Therefore, a subset of edges forms a loop if the multiple of the incidence matrix \\(B\\) and the vector \\(\\mathbf{v}\\) is equal to zero. In other words, the subset of edges represented by \\(\\mathbf{v}\\) forms a loop if \\(\\mathbf{v}B^T = 0\\), or equivalently if \\(B \\mathbf{v^T}=0\\). These definitions work if B has one column for each edge; if B has one row for each edge, then the equation is \\(B^T \\mathbf{v^T}=0\\).\nTwo algorithms for ranking the importance of the vertices in a graph are the PageRank algorithm the power method.\n\n\n\n\n7 \nFind a least squares solution to the following system:\n\\[\n\\begin{bmatrix}\n1 & 1 \\\\\n1 & 1 \\\\\n1 & 2 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}\n\\]\nIs the solution a genuine solution?\n\nSolution. First we find the least squares solution by solving the normal equations \\(A^TAx = A^Tb\\).\n\nA = sp.Matrix([[1, 1], [1, 1], [1, 2]])\nb = sp.Matrix([1, 2, 3])\n\nx = (A.T*A).inv()*A.T*b\nx\n\n\\(\\displaystyle \\left[\\begin{matrix}0\\\\\\frac{3}{2}\\end{matrix}\\right]\\)\n\n\nThe solution is \\(x = \\begin{bmatrix} 1.5 \\\\ 0.5 \\end{bmatrix}\\).\nTo check if this is a genuine solution, we substitute the values of \\(x\\) back into the original equation.\n\nA*x\n\n\\(\\displaystyle \\left[\\begin{matrix}\\frac{3}{2}\\\\\\frac{3}{2}\\\\3\\end{matrix}\\right]\\)\n\n\nThe result is \\(\\begin{bmatrix} 1.5 \\\\ 1.5 \\\\ 3.0 \\end{bmatrix}\\), which is not equal to \\(\\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}\\). Therefore, the solution is not a genuine solution.\n\n\n\n8 \n\n\nThe system of equations\n\n\n\\[\n\\begin{aligned}\nx_{1}+x_{2} & =2 \\\\\nx_{3}+x_{4} & =1 \\\\\n2 x_{1}+4 x_{2}+2 x_{4} & =6\n\\end{aligned}\n\\]\nhas coefficient matrix \\(A\\) and right hand side \\(\\mathbf{b}\\) such that the row-reduced echelon form of \\([A \\mid \\mathbf{b}]\\) is\n\\[\n\\left[\\begin{array}{ccccc:c}1 & 0 & 0 & -1 & \\mid & 1 \\\\ 0 & 1 & 0 & 1 & | & 1 \\\\ 0 & 0 & 1 & 1 & | & 1\\end{array}\\right]\n\\]\nUse this information to answer the following:\n\nFind a basis for the null space of \\(A\\).\nFind the form of a general solution of the system \\(A \\mathbf{x}=\\mathbf{b}\\).\nFind the rank of \\(A\\).\nNo matter what the right hand side of \\(\\mathbf{b}\\) is, this system has solutions. In terms of rank, why do we know this?\n\n::: solution\n\nTo find a basis for the null space of \\(A\\), we solve the equation \\(A\\mathbf{x} = \\mathbf{0}\\).\n\nWe know that \\(x_4\\) is a free variable, so we can write the solution in terms of \\(x_4\\):\n\\[\n\\begin{aligned}\nx_1 &= x_4 \\\\\nx_2 &= - x_4 \\\\\nx_3 &= - x_4 \\\\\nx_4 &= x_4\n\\end{aligned}\n\\]\nSo the basis for the nullspace of \\(A\\) is simply the vector \\(\\begin{bmatrix} 1 \\\\ -1 \\\\ -1 \\\\ 1 \\end{bmatrix}\\).\nChecking using sympy:\n\nA = sp.Matrix([[1,0,0,-1],[0,1,0,1],[0,0,1,1]])\nA.nullspace()\nb=sp.Matrix([1,1,1])\n\n\nThe general solution will be a particular solution plus a linear combination of the basis for the nullspace of \\(A\\).\n\nTo find a particular solution, we can set \\(x_4 = 0\\) and solve the system. Then we have that \\(x_1 = x_2 = x_3 = 1\\). Therefore, a particular solution is \\(\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\end{bmatrix}\\).\nThe general solution is then given by \\(\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\end{bmatrix} + x_4 \\begin{bmatrix} 1 \\\\ -1 \\\\ -1 \\\\ 1 \\end{bmatrix}\\).\n\nThe rank of \\(A\\) is the number of pivot columns in the row-reduced echelon form of \\(A\\). In this case, the rank of \\(A\\) is 3.\nThe rank of \\(A\\) is less than the number of columns in \\(A\\). Therefore, the system will always have solutions.\n\n\n\n9 \nLet \\(A=\\left[\\begin{array}{ccc}1 & 0 & 1 \\\\ -1 & 1 & 1 \\\\ 2 & 1 & 4\\end{array}\\right]\\).\n\nFind the reduced row-echelon form and rank of \\(A\\).\nFind a basis for the column space of \\(A\\).\nDetermine which of the following vectors is in the column space of \\(A\\) and, if so, express the vector as a linear combination of the columns of \\(A\\) :\n\n\\(b_{1}=[2,1,0]^{T}\\), in \\(b_{2}=[2,-3,3]^{T}\\).\nYou may find it helpful to use the following outputs from sympy:\n\nA = sp.Matrix([[1, 0, 1], [-1, 1, 1], [2, 1, 4]])\nb1 = sp.Matrix([2, 1, 0])\nb2 = sp.Matrix([2, -3, 3])\n\nA.gauss_jordan_solve(b1): ValueError: Linear system has no solution\nA.gauss_jordan_solve(b2): \\(\\begin{bmatrix} 2 - \\tau_0 \\\\ -2 \\tau_0 -1 \\\\ \\tau_0 \\end{bmatrix}\\)\n\nSolution. \n\nThe reduced row-echelon form of \\(A\\) is:\n\n\nA = sp.Matrix([[1, 0, 1], [-1, 1, 1], [2, 1, 4]])\ndisplay(A.rref()[0])\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & 0 & 1\\\\0 & 1 & 2\\\\0 & 0 & 0\\end{matrix}\\right]\\)\n\n\n\nThe basis for the column space of A is given by the pivot columns of the row-reduced echelon form of A. In this case, the basis is given by the first two columns of A.\n\n\ncs=A.columnspace()\nc1=cs[0]\nc2=cs[1]\ndisplay(c1)\ndisplay(c2)\n\n\\(\\displaystyle \\left[\\begin{matrix}1\\\\-1\\\\2\\end{matrix}\\right]\\)\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0\\\\1\\\\1\\end{matrix}\\right]\\)\n\n\n\nTo determine if \\(b_1\\) is in the column space of A, we solve the equation \\(A\\mathbf{x} = b_1\\). and \\(b_2\\). The first has no solution, so it’s not in the column space. The second has a solution, so it is in the column space. The solution is \\(\\begin{bmatrix} 2 - \\tau_0 \\\\ -2 \\tau_0 -1 \\\\ \\tau_0 \\end{bmatrix}\\).\n\nWe pick \\(\\tau_0 = 0\\) to get that we can combine twice the first column and -1 times the second column to get \\(b_2\\).\n\nA.gauss_jordan_solve(b2)\n\n(Matrix([\n [   2 - tau0],\n [-2*tau0 - 1],\n [       tau0]]),\n Matrix([[tau0]]))\n\n\n\n\n\n10 \nYou are given a matrix \\(A\\):\n\nfrom IPython.display import display, Math\nimport sympy as sp\nA = sp.Matrix([[0,1],[1,0],[1,1]])\nA\n\n\\(\\displaystyle \\left[\\begin{matrix}0 & 1\\\\1 & 0\\\\1 & 1\\end{matrix}\\right]\\)\n\n\nThe eigenvalues of \\(AA^T\\) are \\(\\lambda_1 = 3, \\lambda_2 = 1, \\lambda_3 = 0\\).\n\nFind the eigenvalues of \\(AA^T\\).\nFind the eigenvectors of \\(A^TA\\) and \\(AA^T\\). Use the facts below and explain your reasoning.\nFind the singular value decomposition of \\(A\\).\nUsing the SVD, calculate \\(A^+\\), the pseudoinverse of \\(A\\). Use this to solve the least squares problem \\(A\\mathbf{x} = \\mathbf{b}\\), where \\(\\mathbf{b} = [1, 4,2]^T\\).\nUsing the SVD, calculate \\(A^k \\mathbf{x}\\) for \\(k=3\\) and \\(\\mathbf{x} = [-2, 2, 0]^T\\).\n\nHere are some facts which may be helpful:\n\nata=A.T*A\naat=A*A.T\nv1 = aat.eigenvects()[0][2][0]\nv2 = aat.eigenvects()[1][2][0]\nv3 = aat.eigenvects()[2][2][0]\nu1 = ata.eigenvects()[0][2][0]\nu2 = ata.eigenvects()[1][2][0]\n\ndisplay(Math(f'{sp.latex(aat)}{sp.latex(v1)}={sp.latex(aat*v1)}'))\ndisplay(Math(f'{sp.latex(aat-3*sp.eye(3))}{sp.latex(v3)}={sp.latex((aat-3*sp.eye(3))*v3)}'))\ndisplay(Math(f' {sp.latex(aat-sp.eye(3))}{sp.latex(v2)}={sp.latex((aat-sp.eye(3))*v2)}'))\n\ndisplay(Math(f'{sp.latex(ata-sp.eye(2))}{sp.latex(u1)}={sp.latex((ata-sp.eye(2))*u1)}'))\ndisplay(Math(f'{sp.latex(ata-3*sp.eye(2))}{sp.latex(u2)}={sp.latex((ata-3*sp.eye(2))*u2)}'))\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & 0 & 1\\\\0 & 1 & 1\\\\1 & 1 & 2\\end{matrix}\\right]\\left[\\begin{matrix}-1\\\\-1\\\\1\\end{matrix}\\right]=\\left[\\begin{matrix}0\\\\0\\\\0\\end{matrix}\\right]\\)\n\n\n\\(\\displaystyle \\left[\\begin{matrix}-2 & 0 & 1\\\\0 & -2 & 1\\\\1 & 1 & -1\\end{matrix}\\right]\\left[\\begin{matrix}\\frac{1}{2}\\\\\\frac{1}{2}\\\\1\\end{matrix}\\right]=\\left[\\begin{matrix}0\\\\0\\\\0\\end{matrix}\\right]\\)\n\n\n\\(\\displaystyle  \\left[\\begin{matrix}0 & 0 & 1\\\\0 & 0 & 1\\\\1 & 1 & 1\\end{matrix}\\right]\\left[\\begin{matrix}-1\\\\1\\\\0\\end{matrix}\\right]=\\left[\\begin{matrix}0\\\\0\\\\0\\end{matrix}\\right]\\)\n\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & 1\\\\1 & 1\\end{matrix}\\right]\\left[\\begin{matrix}-1\\\\1\\end{matrix}\\right]=\\left[\\begin{matrix}0\\\\0\\end{matrix}\\right]\\)\n\n\n\\(\\displaystyle \\left[\\begin{matrix}-1 & 1\\\\1 & -1\\end{matrix}\\right]\\left[\\begin{matrix}1\\\\1\\end{matrix}\\right]=\\left[\\begin{matrix}0\\\\0\\end{matrix}\\right]\\)\n\n\n::: solution\n\nWe first find the characteristic polynomial of \\(AA^T\\): we find \\(\\text{det}(AA^T - \\lambda I)\\) and set it equal to zero.\n\n\nimport io\n\ndef print_to_string(*args, **kwargs):\n    output = io.StringIO()\n    print(*args, file=output, **kwargs)\n    contents = output.getvalue()\n    output.close()\n    return contents\ndisplay(Math(f'{sp.latex(ata.charpoly().as_expr())}=0'))\n\n\\(\\displaystyle \\lambda^{2} - 4 \\lambda + 3=0\\)\n\n\nWe try the three eigenvalues given in the question: 0, 1, and 3. We find that 0 is not an eigenvalue, but that 1 and 3 satisfy the equation. Therefore, the eigenvalues of \\(AA^T\\) are \\(\\lambda_1 = 3, \\lambda_2 = 1\\).\n\nThe eigenvectors of \\(AA^T\\) are given by the nullspace of \\(AA^T - \\lambda I\\). We can find the eigenvectors by solving the equation \\((AA^T - \\lambda I)v = 0\\). These are what are given to us in the question, so the eigenvectors are \\[\\\\\\\\\\\\\\\\left\\[\\\\\\\\\\\\\\\\begin\\{smallmatrix\\}\\-1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\-1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\1\\\\\\\\\\\\\\\\end\\{smallmatrix\\}\\\\\\\\\\\\\\\\right\\]\\] and \\\\\\\\left[\\\\\\\\begin{matrix}-1\\\\\\\\\\\\\\\\1\\\\\\\\\\\\\\\\0\\\\\\\\end{matrix}\\\\\\\\right].\n\n7\\\\n #"
  },
  {
    "objectID": "lectures/Ch6Lecture1.html#linear-systems",
    "href": "lectures/Ch6Lecture1.html#linear-systems",
    "title": "Ch6 Lecture 1",
    "section": "Linear Systems",
    "text": "Linear Systems\nch1Lecture1b, ch1Lecture2\n\nApplications of linear systems\nPutting linear systems in matrix form\n*Gauss-Jordan to get to row echelon form\n*Solving linear systems with augmented matrices\nFree vs bound variables\nIll-conditioned systems & rounding errors"
  },
  {
    "objectID": "lectures/Ch6Lecture1.html#section",
    "href": "lectures/Ch6Lecture1.html#section",
    "title": "Ch6 Lecture 1",
    "section": "",
    "text": "Getting to row echelon form: \\[\n\\left[\\begin{array}{rrr}\n1 & 1 & 5 \\\\\n0 & -3 & -9\n\\end{array}\\right] \\overrightarrow{E_{2}(-1 / 3)}\\left[\\begin{array}{lll}\n1 & 1 & 5 \\\\\n0 & 1 & 3\n\\end{array}\\right] \\overrightarrow{E_{12}(-1)}\\left[\\begin{array}{lll}\n1 & 0 & 2 \\\\\n0 & 1 & 3\n\\end{array}\\right] .\n\\]"
  },
  {
    "objectID": "lectures/Ch6Lecture1.html#augmented-matrix-to-solve-linear-system",
    "href": "lectures/Ch6Lecture1.html#augmented-matrix-to-solve-linear-system",
    "title": "Ch6 Lecture 1",
    "section": "Augmented matrix to solve linear system:",
    "text": "Augmented matrix to solve linear system:\n\\[\n\\begin{aligned}\nz & =2 \\\\\nx+y+z & =2 \\\\\n2 x+2 y+4 z & =8\n\\end{aligned}\n\\]\n\nAugmented matrix:\n\\[\n\\left[\\begin{array}{llll}\n0 & 0 & 1 & 2 \\\\\n1 & 1 & 1 & 2 \\\\\n2 & 2 & 4 & 8\n\\end{array}\\right] \\stackrel{E_{12}}{\\longrightarrow}\\left[\\begin{array}{llll}\n(1 & 1 & 1 & 2 \\\\\n0 & 0 & 1 & 2 \\\\\n2 & 2 & 4 & 8\n\\end{array}\\right] \\xrightarrow[E_{31}(-2)]{\\longrightarrow}\\left[\\begin{array}{rlll}\n1 & 1 & 1 & 2 \\\\\n0 & 0 & 2 & 4 \\\\\n0 & 0 & 1 & 2\n\\end{array}\\right]\n\\]"
  },
  {
    "objectID": "lectures/Ch6Lecture1.html#section-1",
    "href": "lectures/Ch6Lecture1.html#section-1",
    "title": "Ch6 Lecture 1",
    "section": "",
    "text": "Now we are stuck! The first row we are going to use to solve for x. Neither the second or third row tell us anything about y…"
  },
  {
    "objectID": "lectures/Ch6Lecture1.html#section-2",
    "href": "lectures/Ch6Lecture1.html#section-2",
    "title": "Ch6 Lecture 1",
    "section": "",
    "text": "We keep on going… \\[\n\\begin{aligned}\n& {\\left[\\begin{array}{rrrr}\n(1) & 1 & 1 & 2 \\\\\n0 & 0 & 2 & 4 \\\\\n0 & 0 & 1 & 2\n\\end{array}\\right] \\xrightarrow[E_{2}(1 / 2)]{\\longrightarrow}\\left[\\begin{array}{rrrr}\n1 & 1 & 1 & 2 \\\\\n0 & 0 & 1 & 2 \\\\\n0 & 0 & 1 & 2\n\\end{array}\\right]} \\\\\n& \\overrightarrow{E_{32}(-1)}\\left[\\begin{array}{rrrr}\n(1) & 1 & 1 & 2 \\\\\n0 & 0 & 1 & 2 \\\\\n0 & 0 & 0 & 0\n\\end{array}\\right] \\xrightarrow[E_{12}(-1)]{\\longrightarrow}\\left[\\begin{array}{rrrr}\n(1) & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 2 \\\\\n0 & 0 & 0 & 0\n\\end{array}\\right] .\n\\end{aligned}\n\\]\nThere’s still no information on \\(y\\).\n\\[\n\\begin{aligned}\n& x=-y \\\\\n& z=2 \\\\\n& y \\text { is free. }\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lectures/Ch6Lecture1.html#matrix-multiplication",
    "href": "lectures/Ch6Lecture1.html#matrix-multiplication",
    "title": "Ch6 Lecture 1",
    "section": "Matrix multiplication",
    "text": "Matrix multiplication\nch2Lecture1\n\nMatrix-vector multiplication as a linear combination of columns\nMatrix multiplication as an operation\n*Scaling, rotation, shear"
  },
  {
    "objectID": "lectures/Ch6Lecture1.html#scaling-and-rotation",
    "href": "lectures/Ch6Lecture1.html#scaling-and-rotation",
    "title": "Ch6 Lecture 1",
    "section": "Scaling and rotation",
    "text": "Scaling and rotation\nTo rotate a vector by \\(\\theta\\):\n\\[\n=\\left[\\begin{array}{rr}\n\\cos \\theta &-\\sin \\theta \\\\\n\\sin \\theta & \\cos \\theta\n\\end{array}\\right]\\left[\\begin{array}{l}\nr \\cos \\phi \\\\\nr \\sin \\phi\n\\end{array}\\right]=\\left[\\begin{array}{rr}\n\\cos \\theta&-\\sin \\theta \\\\\n\\sin \\theta & \\cos \\theta\n\\end{array}\\right]\\left[\\begin{array}{l}\nx \\\\\ny\n\\end{array}\\right]\n\\]\n. . . Scaling:\n\\[ A = \\left[\\begin{array}{ll} z_1 & 0 \\\\ 0 & z_2 \\end{array}\\right] \\]\nShearing: adding a constant shear factor times one coordinate to another coordinate of the point. \\[\nA = \\left[\\begin{array}{ll} 1 & s_2 \\\\ s_1 & 1 \\end{array}\\right]\n\\]"
  },
  {
    "objectID": "lectures/Ch6Lecture1.html#graphs-and-directed-graphs",
    "href": "lectures/Ch6Lecture1.html#graphs-and-directed-graphs",
    "title": "Ch6 Lecture 1",
    "section": "Graphs and Directed Graphs",
    "text": "Graphs and Directed Graphs\nch2Lecture2\n\n*Adjacency and incidence matrices\nDegree of a vertex\n*Paths and cycles\nPageRank"
  },
  {
    "objectID": "lectures/Ch6Lecture1.html#adjacency-and-incidence-matrices",
    "href": "lectures/Ch6Lecture1.html#adjacency-and-incidence-matrices",
    "title": "Ch6 Lecture 1",
    "section": "Adjacency and incidence matrices",
    "text": "Adjacency and incidence matrices\nAdjacency matrix: A square matrix whose \\((i, j)\\) th entry is the number of edges going from vertex \\(i\\) to vertex \\(j\\)\nIncidence matrix: A matrix whose rows correspond to vertices and columns correspond to edges. The \\((i, j)\\) th entry is 1 if vertex \\(i\\) is the tail of edge \\(j\\), -1 if it is the head, and 0 otherwise."
  },
  {
    "objectID": "lectures/Ch6Lecture1.html#discrete-dynamical-systems",
    "href": "lectures/Ch6Lecture1.html#discrete-dynamical-systems",
    "title": "Ch6 Lecture 1",
    "section": "Discrete Dynamical Systems",
    "text": "Discrete Dynamical Systems\nCh2Lecture2\n\nTransition matrices\n*Markov chains"
  },
  {
    "objectID": "lectures/Ch6Lecture1.html#difference-equations",
    "href": "lectures/Ch6Lecture1.html#difference-equations",
    "title": "Ch6 Lecture 1",
    "section": "Difference Equations",
    "text": "Difference Equations\nCh2Lecture3\n\n*Difference equations in matrix form\n*Examples"
  },
  {
    "objectID": "lectures/Ch6Lecture1.html#mcmc",
    "href": "lectures/Ch6Lecture1.html#mcmc",
    "title": "Ch6 Lecture 1",
    "section": "MCMC",
    "text": "MCMC\n\nMCMC\n\nSimulate a distribution using a Markov chain\nSample from the simulated distribution\n\nRestricted Boltzmann Machines\n\n(Ch2 lecture 4) ## Inverses and determinants\n\nInverse of a matrix\n*Determinants\nSingularity\nLU factorization (Ch2 lecture 4)\n\n(Ch2 lecture 5)"
  },
  {
    "objectID": "lectures/Ch6Lecture1.html#inverses-and-determinants",
    "href": "lectures/Ch6Lecture1.html#inverses-and-determinants",
    "title": "Ch6 Lecture 1",
    "section": "Inverses and determinants",
    "text": "Inverses and determinants\n(Ch2 lecture 5)\n\nInverse of a matrix\n*Determinants\nSingularity\nLU factorization (Ch2 lecture 4)"
  },
  {
    "objectID": "lectures/Ch6Lecture1.html#spaces-of-matrices",
    "href": "lectures/Ch6Lecture1.html#spaces-of-matrices",
    "title": "Ch6 Lecture 1",
    "section": "Spaces of matrices",
    "text": "Spaces of matrices\n\nBasis\nFundamental subspaces:\n\nColumn space\nNull space\nRow space\n\nRank\nConsistency\n\n(ch3 lecture 1) ### Column and Row Spaces\n\nThe column space of the \\(m \\times n\\) matrix \\(A\\) is the subspace \\(\\mathcal{C}(A)\\) of \\(\\mathbb{R}^{m}\\) spanned by the columns of \\(A\\).\n\n\nThe row space of the \\(m \\times n\\) matrix \\(A\\) is the subspace \\(\\mathcal{R}(A)\\) of \\(\\mathbb{R}^{n}\\) spanned by the transposes of the rows of \\(A\\)\n\n\nA basis for the column space of \\(A\\) is the set of pivot columns of \\(A\\). (Find these by row reducing \\(A\\) and choosing the columns with leading 1s)"
  },
  {
    "objectID": "lectures/Ch6Lecture1.html#orthogonality",
    "href": "lectures/Ch6Lecture1.html#orthogonality",
    "title": "Ch6 Lecture 1",
    "section": "Orthogonality",
    "text": "Orthogonality\n\nGeometrical intuitions\n*Least Squares & Normal equations\nFinding orthogonal bases (Gram-Schmidt)\n\n(ch4 lecture 1 and 2) ### Least Squares and Normal Equations\nTo find the least squares solution to \\(A \\mathbf{x}=\\mathbf{b}\\), we minimize the squared error \\(\\left\\|A \\mathbf{x}-\\mathbf{b}\\right\\|^{2}\\) by solving the Normal Equations for \\(\\mathbf{x}\\): \\[\n\\begin{aligned}\n\\mathbf{A}^{T} \\mathbf{A} \\mathbf{x} &=\\mathbf{A}^{T} \\mathbf{b}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lectures/Ch6Lecture1.html#factorizations",
    "href": "lectures/Ch6Lecture1.html#factorizations",
    "title": "Ch6 Lecture 1",
    "section": "Factorizations",
    "text": "Factorizations\n\n*QR Factorization (Ch4 lecture 2)\n*Singular Value Decomposition (Ch5 lecture 2)"
  },
  {
    "objectID": "lectures/Ch6Lecture1.html#haar-wavelets",
    "href": "lectures/Ch6Lecture1.html#haar-wavelets",
    "title": "Ch6 Lecture 1",
    "section": "Haar Wavelets",
    "text": "Haar Wavelets\n(ch4 lecture 2)"
  },
  {
    "objectID": "lectures/Ch6Lecture1.html#eigenvalues-and-eigenvectors",
    "href": "lectures/Ch6Lecture1.html#eigenvalues-and-eigenvectors",
    "title": "Ch6 Lecture 1",
    "section": "Eigenvalues and Eigenvectors",
    "text": "Eigenvalues and Eigenvectors\n\nDefinition\n*How to find them\nSimilarity and Diagonalization\nApplications to dynamical systems\nSpectral radius\n\n(ch5 lecture 1) ### Finding Eigenvalues and Eigenvectors\n\nIf \\(A\\) is a square \\(n \\times n\\) matrix, the equation \\(\\operatorname{det}(\\lambda I-A)=0\\) is called the characteristic equation of \\(A\\)\n\nThe eigenvalues of \\(A\\) are the roots of the characteristic equation.\nFor each scalar \\(\\lambda\\) in (1), use the null space algorithm to find a basis of the eigenspace \\(\\mathcal{N}(\\lambda I-A)\\)."
  },
  {
    "objectID": "lectures/Ch6Lecture1.html#symmetric-matrices",
    "href": "lectures/Ch6Lecture1.html#symmetric-matrices",
    "title": "Ch6 Lecture 1",
    "section": "Symmetric matrices",
    "text": "Symmetric matrices\n\nProperties of symmetric matrices\nQuadratic forms\n\n(ch5 lecture 2) ## SVD\n\nDefinition\n*Psuedoinverse\nApplications to least squares\nImage compression\n\n(ch5 lecture 3, 4)"
  },
  {
    "objectID": "lectures/Ch6Lecture1.html#svd",
    "href": "lectures/Ch6Lecture1.html#svd",
    "title": "Ch6 Lecture 1",
    "section": "SVD",
    "text": "SVD\n(ch5 lecture 3, 4)\n\nDefinition\n*Psuedoinverse\nApplications to least squares\nImage compression"
  },
  {
    "objectID": "lectures/Ch6Lecture1.html#pca",
    "href": "lectures/Ch6Lecture1.html#pca",
    "title": "Ch6 Lecture 1",
    "section": "PCA",
    "text": "PCA\n\nDefinition\nApplications to data analysis\n\n(ch5 lecture 5)"
  },
  {
    "objectID": "lectures/Ch6Lecture1.html#fourier-transform",
    "href": "lectures/Ch6Lecture1.html#fourier-transform",
    "title": "Ch6 Lecture 1",
    "section": "Fourier Transform",
    "text": "Fourier Transform\n(ChNone, Ch6 lecture 1)"
  },
  {
    "objectID": "lectures/Ch6Lecture1.html#mcmc-ch2-lecture-4",
    "href": "lectures/Ch6Lecture1.html#mcmc-ch2-lecture-4",
    "title": "Ch6 Lecture 1",
    "section": "MCMC (Ch2 lecture 4)",
    "text": "MCMC (Ch2 lecture 4)\n\nMCMC\n\nSimulate a distribution using a Markov chain\nSample from the simulated distribution\n\nRestricted Boltzmann Machines"
  },
  {
    "objectID": "lectures/Ch6Lecture1.html#inverses-and-determinants-ch2-lecture-5",
    "href": "lectures/Ch6Lecture1.html#inverses-and-determinants-ch2-lecture-5",
    "title": "Ch6 Lecture 1",
    "section": "Inverses and determinants (Ch2 lecture 5)",
    "text": "Inverses and determinants (Ch2 lecture 5)\n\nInverse of a matrix\n*Determinants\nSingularity\nLU factorization (Ch2 lecture 4)"
  },
  {
    "objectID": "lectures/Ch6Lecture1.html#spaces-of-matrices-ch3-lecture-1",
    "href": "lectures/Ch6Lecture1.html#spaces-of-matrices-ch3-lecture-1",
    "title": "Ch6 Lecture 1",
    "section": "Spaces of matrices (ch3 lecture 1)",
    "text": "Spaces of matrices (ch3 lecture 1)\n\nBasis\nFundamental subspaces:\n\nColumn space\nNull space\nRow space\n\nRank\nConsistency"
  },
  {
    "objectID": "lectures/Ch6Lecture1.html#orthogonality-ch4-lecture-1-and-2",
    "href": "lectures/Ch6Lecture1.html#orthogonality-ch4-lecture-1-and-2",
    "title": "Ch6 Lecture 1",
    "section": "Orthogonality (ch4 lecture 1 and 2)",
    "text": "Orthogonality (ch4 lecture 1 and 2)\n\nGeometrical intuitions\n*Least Squares & Normal equations\nFinding orthogonal bases (Gram-Schmidt)"
  },
  {
    "objectID": "lectures/Ch6Lecture1.html#eigenvalues-and-eigenvectors-ch5-lecture-1",
    "href": "lectures/Ch6Lecture1.html#eigenvalues-and-eigenvectors-ch5-lecture-1",
    "title": "Ch6 Lecture 1",
    "section": "Eigenvalues and Eigenvectors (ch5 lecture 1)",
    "text": "Eigenvalues and Eigenvectors (ch5 lecture 1)\n\nDefinition\n*How to find them\nSimilarity and Diagonalization\nApplications to dynamical systems\nSpectral radius"
  },
  {
    "objectID": "lectures/Ch6Lecture1.html#symmetric-matrices-ch5-lecture-2",
    "href": "lectures/Ch6Lecture1.html#symmetric-matrices-ch5-lecture-2",
    "title": "Ch6 Lecture 1",
    "section": "Symmetric matrices (ch5 lecture 2)",
    "text": "Symmetric matrices (ch5 lecture 2)\n\nProperties of symmetric matrices\nQuadratic forms"
  },
  {
    "objectID": "lectures/Ch6Lecture1.html#svd-ch5-lecture-3-4",
    "href": "lectures/Ch6Lecture1.html#svd-ch5-lecture-3-4",
    "title": "Ch6 Lecture 1",
    "section": "SVD (ch5 lecture 3, 4)",
    "text": "SVD (ch5 lecture 3, 4)\n\nDefinition\n*Psuedoinverse\nApplications to least squares\nImage compression"
  },
  {
    "objectID": "lectures/Ch6Lecture1.html#pca-ch5-lecture-5",
    "href": "lectures/Ch6Lecture1.html#pca-ch5-lecture-5",
    "title": "Ch6 Lecture 1",
    "section": "PCA (ch5 lecture 5)",
    "text": "PCA (ch5 lecture 5)\n\nDefinition\nApplications to data analysis"
  },
  {
    "objectID": "lectures/Ch6Lecture1.html#fourier-transform-chnone-ch6-lecture-1",
    "href": "lectures/Ch6Lecture1.html#fourier-transform-chnone-ch6-lecture-1",
    "title": "Ch6 Lecture 1",
    "section": "Fourier Transform (ChNone, Ch6 lecture 1)",
    "text": "Fourier Transform (ChNone, Ch6 lecture 1)"
  },
  {
    "objectID": "lectures/Ch6Lecture1.html#linear-systems-ch1lecture1b-ch1lecture2",
    "href": "lectures/Ch6Lecture1.html#linear-systems-ch1lecture1b-ch1lecture2",
    "title": "Ch6 Lecture 1",
    "section": "Linear Systems (ch1Lecture1b, ch1Lecture2)",
    "text": "Linear Systems (ch1Lecture1b, ch1Lecture2)\n\nApplications of linear systems\nPutting linear systems in matrix form\n*Gauss-Jordan to get to row echelon form\n*Solving linear systems with augmented matrices\nFree vs bound variables\nIll-conditioned systems & rounding errors"
  },
  {
    "objectID": "lectures/Ch6Lecture1.html#matrix-multiplication-ch2lecture1",
    "href": "lectures/Ch6Lecture1.html#matrix-multiplication-ch2lecture1",
    "title": "Ch6 Lecture 1",
    "section": "Matrix multiplication (ch2Lecture1)",
    "text": "Matrix multiplication (ch2Lecture1)\n\nMatrix-vector multiplication as a linear combination of columns\nMatrix multiplication as an operation\n*Scaling, rotation, shear"
  },
  {
    "objectID": "lectures/Ch6Lecture1.html#graphs-and-directed-graphs-ch2lecture2",
    "href": "lectures/Ch6Lecture1.html#graphs-and-directed-graphs-ch2lecture2",
    "title": "Ch6 Lecture 1",
    "section": "Graphs and Directed Graphs (ch2Lecture2)",
    "text": "Graphs and Directed Graphs (ch2Lecture2)\n\n*Adjacency and incidence matrices\nDegree of a vertex\n*Paths and cycles\nPageRank"
  },
  {
    "objectID": "lectures/Ch6Lecture1.html#discrete-dynamical-systems-ch2lecture2",
    "href": "lectures/Ch6Lecture1.html#discrete-dynamical-systems-ch2lecture2",
    "title": "Ch6 Lecture 1",
    "section": "Discrete Dynamical Systems (Ch2Lecture2)",
    "text": "Discrete Dynamical Systems (Ch2Lecture2)\n\nTransition matrices\n*Markov chains"
  },
  {
    "objectID": "lectures/Ch6Lecture1.html#difference-equations-ch2lecture3",
    "href": "lectures/Ch6Lecture1.html#difference-equations-ch2lecture3",
    "title": "Ch6 Lecture 1",
    "section": "Difference Equations (Ch2Lecture3)",
    "text": "Difference Equations (Ch2Lecture3)\n\n*Difference equations in matrix form\n*Examples"
  },
  {
    "objectID": "lectures/Ch6Lecture1.html#inverses-and-determinants-ch2-lecture-4-5",
    "href": "lectures/Ch6Lecture1.html#inverses-and-determinants-ch2-lecture-4-5",
    "title": "Ch6 Lecture 1",
    "section": "Inverses and determinants (Ch2 lecture 4 & 5)",
    "text": "Inverses and determinants (Ch2 lecture 4 & 5)\n\nInverse of a matrix\n*Determinants\nSingularity\nLU factorization"
  },
  {
    "objectID": "lectures/Review.html#linear-systems-ch1lecture1b-ch1lecture2",
    "href": "lectures/Review.html#linear-systems-ch1lecture1b-ch1lecture2",
    "title": "Review",
    "section": "Linear Systems (ch1Lecture1b, ch1Lecture2)",
    "text": "Linear Systems (ch1Lecture1b, ch1Lecture2)\n\nApplications of linear systems\nPutting linear systems in matrix form\n*Gauss-Jordan to get to row echelon form\n*Solving linear systems with augmented matrices\nFree vs bound variables\nIll-conditioned systems & rounding errors"
  },
  {
    "objectID": "lectures/Review.html#section",
    "href": "lectures/Review.html#section",
    "title": "Review",
    "section": "",
    "text": "Getting to row echelon form: \\[\n\\left[\\begin{array}{rrr}\n1 & 1 & 5 \\\\\n0 & -3 & -9\n\\end{array}\\right] \\overrightarrow{E_{2}(-1 / 3)}\\left[\\begin{array}{lll}\n1 & 1 & 5 \\\\\n0 & 1 & 3\n\\end{array}\\right] \\overrightarrow{E_{12}(-1)}\\left[\\begin{array}{lll}\n1 & 0 & 2 \\\\\n0 & 1 & 3\n\\end{array}\\right] .\n\\]"
  },
  {
    "objectID": "lectures/Review.html#augmented-matrix-to-solve-linear-system",
    "href": "lectures/Review.html#augmented-matrix-to-solve-linear-system",
    "title": "Review",
    "section": "Augmented matrix to solve linear system:",
    "text": "Augmented matrix to solve linear system:\n\\[\n\\begin{aligned}\nz & =2 \\\\\nx+y+z & =2 \\\\\n2 x+2 y+4 z & =8\n\\end{aligned}\n\\]\n\nAugmented matrix:\n\\[\n\\left[\\begin{array}{llll}\n0 & 0 & 1 & 2 \\\\\n1 & 1 & 1 & 2 \\\\\n2 & 2 & 4 & 8\n\\end{array}\\right] \\stackrel{E_{12}}{\\longrightarrow}\\left[\\begin{array}{llll}\n(1 & 1 & 1 & 2 \\\\\n0 & 0 & 1 & 2 \\\\\n2 & 2 & 4 & 8\n\\end{array}\\right] \\xrightarrow[E_{31}(-2)]{\\longrightarrow}\\left[\\begin{array}{rlll}\n1 & 1 & 1 & 2 \\\\\n0 & 0 & 2 & 4 \\\\\n0 & 0 & 1 & 2\n\\end{array}\\right]\n\\]"
  },
  {
    "objectID": "lectures/Review.html#section-1",
    "href": "lectures/Review.html#section-1",
    "title": "Review",
    "section": "",
    "text": "Now we are stuck! The first row we are going to use to solve for x. Neither the second or third row tell us anything about y…"
  },
  {
    "objectID": "lectures/Review.html#section-2",
    "href": "lectures/Review.html#section-2",
    "title": "Review",
    "section": "",
    "text": "We keep on going… \\[\n\\begin{aligned}\n& {\\left[\\begin{array}{rrrr}\n(1) & 1 & 1 & 2 \\\\\n0 & 0 & 2 & 4 \\\\\n0 & 0 & 1 & 2\n\\end{array}\\right] \\xrightarrow[E_{2}(1 / 2)]{\\longrightarrow}\\left[\\begin{array}{rrrr}\n1 & 1 & 1 & 2 \\\\\n0 & 0 & 1 & 2 \\\\\n0 & 0 & 1 & 2\n\\end{array}\\right]} \\\\\n& \\overrightarrow{E_{32}(-1)}\\left[\\begin{array}{rrrr}\n(1) & 1 & 1 & 2 \\\\\n0 & 0 & 1 & 2 \\\\\n0 & 0 & 0 & 0\n\\end{array}\\right] \\xrightarrow[E_{12}(-1)]{\\longrightarrow}\\left[\\begin{array}{rrrr}\n(1) & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 2 \\\\\n0 & 0 & 0 & 0\n\\end{array}\\right] .\n\\end{aligned}\n\\]\nThere’s still no information on \\(y\\).\n\\[\n\\begin{aligned}\n& x=-y \\\\\n& z=2 \\\\\n& y \\text { is free. }\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lectures/Review.html#matrix-multiplication-ch2lecture1",
    "href": "lectures/Review.html#matrix-multiplication-ch2lecture1",
    "title": "Review",
    "section": "Matrix multiplication (ch2Lecture1)",
    "text": "Matrix multiplication (ch2Lecture1)\n\nMatrix-vector multiplication as a linear combination of columns\nMatrix multiplication as an operation\n*Scaling, rotation, shear"
  },
  {
    "objectID": "lectures/Review.html#graphs-and-directed-graphs-ch2lecture2",
    "href": "lectures/Review.html#graphs-and-directed-graphs-ch2lecture2",
    "title": "Review",
    "section": "Graphs and Directed Graphs (ch2Lecture2)",
    "text": "Graphs and Directed Graphs (ch2Lecture2)\n\n*Adjacency and incidence matrices\nDegree of a vertex\n*Paths and cycles\nPageRank"
  },
  {
    "objectID": "lectures/Review.html#discrete-dynamical-systems-ch2lecture2",
    "href": "lectures/Review.html#discrete-dynamical-systems-ch2lecture2",
    "title": "Review",
    "section": "Discrete Dynamical Systems (Ch2Lecture2)",
    "text": "Discrete Dynamical Systems (Ch2Lecture2)\n\nTransition matrices\n*Markov chains"
  },
  {
    "objectID": "lectures/Review.html#difference-equations-ch2lecture3",
    "href": "lectures/Review.html#difference-equations-ch2lecture3",
    "title": "Review",
    "section": "Difference Equations (Ch2Lecture3)",
    "text": "Difference Equations (Ch2Lecture3)\n\n*Difference equations in matrix form\n*Examples"
  },
  {
    "objectID": "lectures/Review.html#mcmc-ch2-lecture-4",
    "href": "lectures/Review.html#mcmc-ch2-lecture-4",
    "title": "Review",
    "section": "MCMC (Ch2 lecture 4)",
    "text": "MCMC (Ch2 lecture 4)\n\nMCMC\n\nSimulate a distribution using a Markov chain\nSample from the simulated distribution\n\nRestricted Boltzmann Machines"
  },
  {
    "objectID": "lectures/Review.html#inverses-and-determinants-ch2-lecture-4-5",
    "href": "lectures/Review.html#inverses-and-determinants-ch2-lecture-4-5",
    "title": "Review",
    "section": "Inverses and determinants (Ch2 lecture 4 & 5)",
    "text": "Inverses and determinants (Ch2 lecture 4 & 5)\n\nInverse of a matrix\n*Determinants\nSingularity\nLU factorization"
  },
  {
    "objectID": "lectures/Review.html#spaces-of-matrices-ch3-lecture-1",
    "href": "lectures/Review.html#spaces-of-matrices-ch3-lecture-1",
    "title": "Review",
    "section": "Spaces of matrices (ch3 lecture 1)",
    "text": "Spaces of matrices (ch3 lecture 1)\n\nBasis\nFundamental subspaces:\n\nColumn space\nNull space\nRow space\n\nRank\nConsistency"
  },
  {
    "objectID": "lectures/Review.html#orthogonality-ch4-lecture-1-and-2",
    "href": "lectures/Review.html#orthogonality-ch4-lecture-1-and-2",
    "title": "Review",
    "section": "Orthogonality (ch4 lecture 1 and 2)",
    "text": "Orthogonality (ch4 lecture 1 and 2)\n\nGeometrical intuitions\n*Least Squares & Normal equations\nFinding orthogonal bases (Gram-Schmidt)"
  },
  {
    "objectID": "lectures/Review.html#eigenvalues-and-eigenvectors-ch5-lecture-1",
    "href": "lectures/Review.html#eigenvalues-and-eigenvectors-ch5-lecture-1",
    "title": "Review",
    "section": "Eigenvalues and Eigenvectors (ch5 lecture 1)",
    "text": "Eigenvalues and Eigenvectors (ch5 lecture 1)\n\nDefinition\n*How to find them\nSimilarity and Diagonalization\nApplications to dynamical systems\nSpectral radius"
  },
  {
    "objectID": "lectures/Review.html#symmetric-matrices-ch5-lecture-2",
    "href": "lectures/Review.html#symmetric-matrices-ch5-lecture-2",
    "title": "Review",
    "section": "Symmetric matrices (ch5 lecture 2)",
    "text": "Symmetric matrices (ch5 lecture 2)\n\nProperties of symmetric matrices\nQuadratic forms"
  },
  {
    "objectID": "lectures/Review.html#svd-ch5-lecture-3-4",
    "href": "lectures/Review.html#svd-ch5-lecture-3-4",
    "title": "Review",
    "section": "SVD (ch5 lecture 3, 4)",
    "text": "SVD (ch5 lecture 3, 4)\n\nDefinition\n*Psuedoinverse\nApplications to least squares\nImage compression"
  },
  {
    "objectID": "lectures/Review.html#pca-ch5-lecture-5",
    "href": "lectures/Review.html#pca-ch5-lecture-5",
    "title": "Review",
    "section": "PCA (ch5 lecture 5)",
    "text": "PCA (ch5 lecture 5)\n\nDefinition\nApplications to data analysis"
  },
  {
    "objectID": "lectures/Review.html#fourier-transform-chnone-ch6-lecture-1",
    "href": "lectures/Review.html#fourier-transform-chnone-ch6-lecture-1",
    "title": "Review",
    "section": "Fourier Transform (ChNone, Ch6 lecture 1)",
    "text": "Fourier Transform (ChNone, Ch6 lecture 1)"
  },
  {
    "objectID": "lectures/Ch6_fourier.html#discrete-time-filters-in-continuous-frequency-space",
    "href": "lectures/Ch6_fourier.html#discrete-time-filters-in-continuous-frequency-space",
    "title": "Ch6 Fourier",
    "section": "Discrete-time filters in continuous frequency space",
    "text": "Discrete-time filters in continuous frequency space\nBack to your textbook, in Chapter 6:\n\nHere we will work with discrete signals (using time as the index) and discrete filters, but be interested in their continuous frequency representations.\n. . .\nImagine we have a signal \\(x(t)\\) which is defined for \\(t\\) is the interval \\([0, T]\\)\n. . .\nWe will imagine that \\(x(t)\\) which is defined over all \\(t\\) but periodic of period \\(T\\). So the function has frequency \\(f=1 / T\\) and angular frequency \\(\\omega=2 \\pi / T\\)\n. . .\n. . .\nA discrete filter is a sequence of complex numbers \\(\\mathbf{h}=\\left\\{h_{n}\\right\\}_{n=-\\infty}^{\\infty}\\).\n. . .\nThis will be a finite impulse response (FIR) filter if \\(h_n\\) is 0 for \\(n&lt;0\\) or \\(n&gt;L\\) for some L.\nIn this case we say \\(\\mathbf{h}\\) has length \\(L\\) and express it in the form \\(\\mathbf{h}=\\left\\{h_{n}\\right\\}_{n=0}^{L}\\).\n. . .\nThen we can take its discrete time Fourier transform (DTFT) as the Fourier series \\(X(\\mathbf{h})(\\zeta)=\\sum_{n=-\\infty}^{\\infty} h_{n} e^{i \\zeta}, \\zeta \\in \\mathbb{R}\\).\n. . .\nSuppose we have a discrete signal \\(x(t)\\) in sampling periods \\(T_{s}\\), i.e., at \\(t_{n}=n T_{s}\\). Define \\(x_{n}=x\\left(t_{n}\\right)\\)"
  },
  {
    "objectID": "lectures/Ch6_fourier.html#section",
    "href": "lectures/Ch6_fourier.html#section",
    "title": "Ch6 Fourier",
    "section": "",
    "text": "If we use our filter on our signal, we get:\n\\[\ny_{n}=h_{0} x_{n}+h_{1} x_{n-1}+\\cdots+h_{L} x_{n-L}, \\quad n \\in \\mathbb{Z}\n\\]\n. . .\nTo understand the effect of the filter, examine its action on a single Fourier mode of the signal, with a possible phase shift \\(\\phi\\):\n\\(x_{m}(t)=c_{m} e^{i m(\\omega t+\\phi)}\\)\n. . .\nAt individual points in time, this gives\n\\(x_{m, n}=c_{m} e^{i m \\omega\\left(n T_{s}+\\phi\\right)}\\)"
  },
  {
    "objectID": "lectures/Ch6_fourier.html#section-1",
    "href": "lectures/Ch6_fourier.html#section-1",
    "title": "Ch6 Fourier",
    "section": "",
    "text": "Thus for fixed \\(m\\) and arbitrary \\(n\\), we have\n\n\\[\n\\begin{aligned}\ny_{m, n} & =h_{0} c_{m} e^{i m \\omega\\left(n T_{s}+\\phi\\right)}+h_{1} c_{m} e^{i m \\omega\\left((n-1) T_{s}+\\phi\\right)}+\\cdots+h_{L} c_{m} e^{i m \\omega\\left((n-L) T_{s}+\\phi\\right)} \\\\\n& =\\left(h_{0} \\cdot 1+h_{1} e^{-i m \\omega T_{s}}+\\cdots+h_{L} e^{-i L m \\omega T_{s}}\\right) c_{m} e^{i m \\omega\\left(n T_{s}+\\phi\\right)} \\\\\n& =H\\left(-m \\omega T_{s}\\right) x_{m, n}\n\\end{aligned}\n\\]\nwhere\n\\(H(\\zeta)=h_{0}+h_{1} e^{i \\zeta}+\\cdots+h_{L} e^{i L \\zeta}, \\quad \\zeta \\in \\mathbb{R}\\) is the DTFT of \\(\\mathbf{h}\\).\nThe units of \\(\\zeta\\) are radians per sampling period. So when \\(\\zeta=1\\), our filter goes through half a cycle in one sampling period."
  },
  {
    "objectID": "lectures/Ch6_fourier.html#gain",
    "href": "lectures/Ch6_fourier.html#gain",
    "title": "Ch6 Fourier",
    "section": "Gain",
    "text": "Gain\n\\(\\left|y_{m, n}\\right| \\leq\\left|H\\left(-m \\omega T_{s}\\right)\\right|\\left|x_{m, n}\\right|\\)\nSo we define gain or attenuation of this transformation as \\(G(\\zeta)=|H(\\zeta)|\\)\n\nWe define phase rotation as \\(\\Theta(\\zeta)=\\theta\\), where \\(H(\\zeta)=|H(\\zeta)| e^{i \\theta}\\)\n\n\nThe filter \\(h\\) will attenuate the signal at frequencies where \\(|H(\\zeta)|&lt;1\\) and amplify the signal at frequencies where \\(|H(\\zeta)|&gt;1\\). It will phase shift the signal by \\(\\Theta(\\zeta)\\)."
  },
  {
    "objectID": "lectures/Ch6_fourier.html#high-pass-and-low-pass-filters",
    "href": "lectures/Ch6_fourier.html#high-pass-and-low-pass-filters",
    "title": "Ch6 Fourier",
    "section": "High-pass and low-pass filters",
    "text": "High-pass and low-pass filters\nThe FIR filter \\(\\mathbf{h}=\\left\\{h_{k}\\right\\}_{k=0}^{L}\\) with discrete time Fourier transform \\(H(\\zeta)\\) is a lowpass filter if \\(|H(0)|=1\\) and \\(|H(\\pi)|=0\\); \\(\\mathbf{h}\\) is a highpass filter if \\(|H(0)|=0\\) and \\(|H(\\pi)|=1\\)."
  },
  {
    "objectID": "Exams/FinalExamQuestions.html",
    "href": "Exams/FinalExamQuestions.html",
    "title": "Final Exam Questions",
    "section": "",
    "text": "Convert this difference equation into matrix–vector form.\n\\(2y_{k+3} + 2y_{k+1} - 3y_{k} = 0\\)\n\n\n\n\nConvert this difference equation into matrix–vector form.\n\\(y_{k+2} + y_{k+1} - 2y_{k} = 3\\)"
  },
  {
    "objectID": "Exams/FinalExamQuestions.html#section",
    "href": "Exams/FinalExamQuestions.html#section",
    "title": "Final Exam Questions",
    "section": "",
    "text": "Convert this difference equation into matrix–vector form.\n\\(2y_{k+3} + 2y_{k+1} - 3y_{k} = 0\\)"
  },
  {
    "objectID": "Exams/FinalExamQuestions.html#section-1",
    "href": "Exams/FinalExamQuestions.html#section-1",
    "title": "Final Exam Questions",
    "section": "",
    "text": "Convert this difference equation into matrix–vector form.\n\\(y_{k+2} + y_{k+1} - 2y_{k} = 3\\)"
  },
  {
    "objectID": "Exams/FinalExamQuestions.html#section-2",
    "href": "Exams/FinalExamQuestions.html#section-2",
    "title": "Final Exam Questions",
    "section": "2.1 ",
    "text": "2.1 \nA population is modeled with three stages: larva, pupa and adult, and the resulting structured population model transition matrix is\n\\[\nA = \\begin{bmatrix}\n0 & 0 & 0.6 \\\\\n0.5 & 0 & 0 \\\\\n0 & 0.9 & 0\n\\end{bmatrix}\n\\]\n\n\n[(0.646330407009565 + 9.57696933338458e-33*I,\n  1,\n  [Matrix([\n   [-0.211957169909255 - 0.563510300230596*I],\n   [-0.163969672175827 - 0.435930519529353*I],\n   [ -0.22832393982674 - 0.607023069503539*I]])]),\n (-0.323165203504783 - 0.559738551708619*I,\n  1,\n  [Matrix([\n   [-0.446848502976125 + 0.418540422762337*I],\n   [-0.107562931920626 - 0.461259721138817*I],\n   [ 0.631131829114318 + 0.191434388320088*I]])]),\n (-0.323165203504783 + 0.559738551708619*I,\n  1,\n  [Matrix([\n   [-0.277204808242613 - 0.561858355489268*I],\n   [-0.269197612619668 + 0.403041213970164*I],\n   [0.673461217391963 + 0.0440180865002761*I]])])]\n\n\n\nExplain what this model says about the three states.\nStarting with a population of (0, 30, 100), calculate the population in the next two time steps.\nThe eigenvalues of the matrix are \\(\\lambda_1 = 0.646, \\lambda_2 = -0.323 - 0.560i, \\lambda_3 = -0.323 + 0.560i\\). What do these eigenvalues tell you about the long-term behavior of the population?"
  },
  {
    "objectID": "Exams/FinalExamQuestions.html#section-3",
    "href": "Exams/FinalExamQuestions.html#section-3",
    "title": "Final Exam Questions",
    "section": "2.2 ",
    "text": "2.2 \nA transition matrix A consists of only real entries. It has one real eigenvalue \\(\\lambda_1 = a_1\\) and two complex eigenvalues \\(\\lambda_2 = a_2 + b_2 \\text{i}, \\lambda_3 = a_2 - b_2 \\text{i}\\), so that \\(\\lambda_2\\) is the complex conjugate of \\(\\lambda_3\\): \\(\\lambda_2=\\overline{\\lambda_3}\\). It has corresponding eigenvectors \\(\\mathbf{v_1}, \\mathbf{v_2}, \\mathbf{v_3}\\), where the entries in \\(\\mathbf{v_1}\\) are real and the entries in \\(\\mathbf{v_2}\\) are the complex conjugates of the entries of \\(\\mathbf{v_3}\\).\n(For matrices with all real entries, complex eigenvalues will always come in complex conjugate pairs, and their eigenvectors will always also be complex conguate pairs.)\n\nSuppose the initial population is given by \\(\\mathbf{x}=\\begin{bmatrix} x_{1} \\\\ x_2 \\\\ x_3 \\end{bmatrix}\\), where \\(x_1,x_2,x_3\\) are real numbers. The projection of the initial population onto the eigenvectors is given by \\(\\mathbf{x}=c_1 \\mathbf{v_1} + c_2 \\mathbf{v_2} + c_3 \\mathbf{v_3}\\).\n\nUsing the fact that \\(v_2\\) and \\(v_3\\) are complex conjugates, show that \\(c_2\\) and \\(c_3\\) are also complex conjugates.\n\nIn terms of the eigenvalues and eigenvectors, what will be the population distribution after \\(k\\) iterations?\nUsing your results from the previous two parts, show that the population at time \\(k\\) will always have real entries.\n\n(You may find it helpful to know that for complex scalars \\(d,g\\), \\(\\overline{dg}=\\overline{d}\\times\\overline{g}\\), and \\(\\overline{d^n}= \\overline{d}^n\\).)\n\nArgue that you could have gotten the same result as (c) but just using the fact that the matrix \\(A\\) has only real entries.\nSuppose \\(|\\lambda_1|&gt;|\\lambda_2|=|\\lambda_3|\\). Estimate \\(\\lim_{k\\to\\infty} \\mathbf{x_k}\\) for a random initial population \\(\\mathbf{x_0}\\), for the cases where \\(\\lambda_1&gt;1\\), \\(\\lambda_1=1\\), and \\(\\lambda_1&lt;1\\)."
  },
  {
    "objectID": "Exams/FinalExamQuestions.html#section-4",
    "href": "Exams/FinalExamQuestions.html#section-4",
    "title": "Final Exam Questions",
    "section": "3.1 ",
    "text": "3.1 \nThe graph G is given below.\n\n\n/Users/kendra/Library/Python/3.8/lib/python/site-packages/networkx/drawing/nx_pylab.py:433: UserWarning:\n\nNo data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n\n\n\n\n\n\n\n\n\n\n\nFind the adjacency matrix \\(A\\) and the incidence matrix \\(B\\) for the graph G.\nFrom \\(A\\) and \\(B\\), match the following descriptions to the corresponding quantities:\n\nthe number of outgoing edges from each vertex,\nthe power of each vertex (the number of paths of length 1 or 2 originating at each vertex)\nthe number of paths of length \\(k\\) between each pair of vertices\na basis for loops in the graph (don’t worry about the direction of the edges)\n\n\n\n(a) Sum of the entries in the corresponding row of $A$,\n(c) Entries in the matrix $A^k$,\n(d) Nullspace of the matrix $B^T$.\n(e) Entries in the matrix $B^k$.\n(b) Sum of the entries in the corresponding row of $A + A^2$,\n(f) Nullspace of the matrix $A^T$."
  },
  {
    "objectID": "Exams/FinalExamQuestions.html#section-5",
    "href": "Exams/FinalExamQuestions.html#section-5",
    "title": "Final Exam Questions",
    "section": "3.2 ",
    "text": "3.2 \nA directed graph has vertices labeled \\(A\\), \\(B\\), \\(C\\), and \\(D\\). The edges are given by the following incidence matrix B:\n\n\n\\(\\displaystyle B = \\left[\\begin{matrix}-1 & -1 & 0 & 0 & 0 & 1\\\\1 & 0 & -1 & -1 & 0 & 0\\\\0 & 1 & 1 & 0 & -1 & 0\\\\0 & 0 & 0 & 1 & 1 & -1\\end{matrix}\\right]\\)\n\n\n\nFind the adjacency matrix \\(A\\) for the graph.\nDraw the graph.\nDescribe how you could find the number of paths of length \\(k\\) between each pair of vertices using the adjacency matrix \\(A\\).\nIf you calculate the matrix \\(A\\) + \\(A^2\\), what does the sum of the entries in the corresponding row tell you about the graph?\nLet \\(\\mathbf{v}\\) be a row vector representing a subset of the edges in the graph. For example, the subset of edges given by the vector \\(\\mathbf{v} = [1, 0, 1, 0, 0, 1]\\) would correspond to the first, third, and sixth edges.\n\nWrite an equation which will be satisfied whenever the subset of edges represented by \\(\\mathbf{v}\\) forms a loop in the graph.\n\nDescribe two algorithms for ranking the importance of the vertices in a graph. (One or more of these algorithms may be based on prior parts of this problem.)"
  },
  {
    "objectID": "Exams/FinalExamQuestions.html#section-6",
    "href": "Exams/FinalExamQuestions.html#section-6",
    "title": "Final Exam Questions",
    "section": "4.1 ",
    "text": "4.1 \nFind a least squares solution to the following system:\n\\[\n\\begin{bmatrix}\n1 & 1 \\\\\n1 & 1 \\\\\n1 & 2 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}\n\\]\nIs the solution a genuine solution?"
  },
  {
    "objectID": "Exams/FinalExamQuestions.html#section-7",
    "href": "Exams/FinalExamQuestions.html#section-7",
    "title": "Final Exam Questions",
    "section": "4.2 ",
    "text": "4.2 \nThe system of equations\n\\[\n\\begin{aligned}\nx_{1}+x_{2} & =2 \\\\\nx_{3}+x_{4} & =1 \\\\\n2 x_{1}+4 x_{2}+2 x_{4} & =6\n\\end{aligned}\n\\]\nhas coefficient matrix \\(A\\) and right hand side \\(\\mathbf{b}\\) such that the row-reduced echelon form of \\([A \\mid \\mathbf{b}]\\) is\n\\[\n\\left[\\begin{array}{ccccc:c}1 & 0 & 0 & -1 & \\mid & 1 \\\\ 0 & 1 & 0 & 1 & | & 1 \\\\ 0 & 0 & 1 & 1 & | & 1\\end{array}\\right]\n\\]\nUse this information to answer the following:\n\nFind a basis for the null space of \\(A\\).\nFind the form of a general solution of the system \\(A \\mathbf{x}=\\mathbf{b}\\).\nFind the rank of \\(A\\).\nNo matter what the right hand side of \\(\\mathbf{b}\\) is, this system has solutions. In terms of rank, why do we know this?\n\n::: solutions\n\nTo find a basis for the null space of \\(A\\), we solve the equation \\(A\\mathbf{x} = \\mathbf{0}\\).\n\nWe know that \\(x_4\\) is a free variable, so we can write the solution in terms of \\(x_4\\):\n\\[\n\\begin{aligned}\nx_1 &= x_4 \\\\\nx_2 &= - x_4 \\\\\nx_3 &= - x_4 \\\\\nx_4 &= x_4\n\\end{aligned}\n\\]\nSo the basis for the nullspace of \\(A\\) is simply the vector \\(\\begin{bmatrix} 1 \\\\ -1 \\\\ -1 \\\\ 1 \\end{bmatrix}\\).\nChecking using sympy:\n\nThe general solution will be a particular solution plus a linear combination of the basis for the nullspace of \\(A\\).\n\nTo find a particular solution, we can set \\(x_4 = 0\\) and solve the system. Then we have that \\(x_1 = x_2 = x_3 = 1\\). Therefore, a particular solution is \\(\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\end{bmatrix}\\).\nThe general solution is then given by \\(\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\end{bmatrix} + x_4 \\begin{bmatrix} 1 \\\\ -1 \\\\ -1 \\\\ 1 \\end{bmatrix}\\).\n\nThe rank of \\(A\\) is the number of pivot columns in the row-reduced echelon form of \\(A\\). In this case, the rank of \\(A\\) is 3.\nThe rank of \\(A\\) is less than the number of columns in \\(A\\). Therefore, the system will always have solutions."
  },
  {
    "objectID": "Exams/FinalExamQuestions.html#section-8",
    "href": "Exams/FinalExamQuestions.html#section-8",
    "title": "Final Exam Questions",
    "section": "5.1 ",
    "text": "5.1 \nLet \\(A=\\left[\\begin{array}{ccc}1 & 0 & 1 \\\\ -1 & 1 & 1 \\\\ 2 & 1 & 4\\end{array}\\right]\\).\n\nFind the reduced row-echelon form and rank of \\(A\\).\nFind a basis for the column space of \\(A\\).\nDetermine which of the following vectors is in the column space of \\(A\\) and, if so, express the vector as a linear combination of the columns of \\(A\\) :\n\n\\(b_{1}=[2,1,0]^{T}\\), in \\(b_{2}=[2,-3,3]^{T}\\).\nYou may find it helpful to use the following outputs from sympy:\nA.gauss_jordan_solve(b1): ValueError: Linear system has no solution\nA.gauss_jordan_solve(b2): \\(\\begin{bmatrix} 2 - \\tau_0 \\\\ -2 \\tau_0 -1 \\\\ \\tau_0 \\end{bmatrix}\\)"
  },
  {
    "objectID": "Exams/FinalExamQuestions.html#section-9",
    "href": "Exams/FinalExamQuestions.html#section-9",
    "title": "Final Exam Questions",
    "section": "6.1 ",
    "text": "6.1 \nYou are given a matrix \\(A\\):\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0 & 1\\\\1 & 0\\\\1 & 1\\end{matrix}\\right]\\)\n\n\nThe eigenvalues of \\(AA^T\\) are \\(\\lambda_1 = 3, \\lambda_2 = 1, \\lambda_3 = 0\\).\n\nFind the eigenvalues of \\(AA^T\\).\nFind the eigenvectors of \\(A^TA\\) and \\(AA^T\\). Use the facts below and explain your reasoning.\nFind the singular value decomposition of \\(A\\).\nUsing the SVD, calculate \\(A^+\\), the pseudoinverse of \\(A\\). Use this to solve the least squares problem \\(A\\mathbf{x} = \\mathbf{b}\\), where \\(\\mathbf{b} = [1, 4,2]^T\\).\nUsing the SVD, calculate \\(A^k \\mathbf{x}\\) for \\(k=3\\) and \\(\\mathbf{x} = [-2, 2, 0]^T\\).\n\nHere are some facts which may be helpful:\n\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & 0 & 1\\\\0 & 1 & 1\\\\1 & 1 & 2\\end{matrix}\\right]\\left[\\begin{matrix}-1\\\\-1\\\\1\\end{matrix}\\right]=\\left[\\begin{matrix}0\\\\0\\\\0\\end{matrix}\\right]\\)\n\n\n\\(\\displaystyle \\left[\\begin{matrix}-2 & 0 & 1\\\\0 & -2 & 1\\\\1 & 1 & -1\\end{matrix}\\right]\\left[\\begin{matrix}\\frac{1}{2}\\\\\\frac{1}{2}\\\\1\\end{matrix}\\right]=\\left[\\begin{matrix}0\\\\0\\\\0\\end{matrix}\\right]\\)\n\n\n\\(\\displaystyle  \\left[\\begin{matrix}0 & 0 & 1\\\\0 & 0 & 1\\\\1 & 1 & 1\\end{matrix}\\right]\\left[\\begin{matrix}-1\\\\1\\\\0\\end{matrix}\\right]=\\left[\\begin{matrix}0\\\\0\\\\0\\end{matrix}\\right]\\)\n\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & 1\\\\1 & 1\\end{matrix}\\right]\\left[\\begin{matrix}-1\\\\1\\end{matrix}\\right]=\\left[\\begin{matrix}0\\\\0\\end{matrix}\\right]\\)\n\n\n\\(\\displaystyle \\left[\\begin{matrix}-1 & 1\\\\1 & -1\\end{matrix}\\right]\\left[\\begin{matrix}1\\\\1\\end{matrix}\\right]=\\left[\\begin{matrix}0\\\\0\\end{matrix}\\right]\\)\n\n\n::: solutions\n\nWe first find the characteristic polynomial of \\(AA^T\\): we find \\(\\text{det}(AA^T - \\lambda I)\\) and set it equal to zero.\n\n\n\n\\(\\displaystyle \\lambda^{2} - 4 \\lambda + 3=0\\)\n\n\nWe try the three eigenvalues given in the question: 0, 1, and 3. We find that 0 is not an eigenvalue, but that 1 and 3 satisfy the equation. Therefore, the eigenvalues of \\(AA^T\\) are \\(\\lambda_1 = 3, \\lambda_2 = 1\\).\n\nThe eigenvectors of \\(AA^T\\) are given by the nullspace of \\(AA^T - \\lambda I\\). We can find the eigenvectors by solving the equation \\((AA^T - \\lambda I)v = 0\\). These are what are given to us in the question, so the eigenvectors are\n\n\n\n\\(\\displaystyle \\left[\\begin{matrix}-1\\\\-1\\\\1\\end{matrix}\\right]\\)\n\n\n\\(\\displaystyle \\left[\\begin{matrix}-1\\\\1\\\\0\\end{matrix}\\right]\\)\n\n\n\\(\\displaystyle \\left[\\begin{matrix}\\frac{1}{2}\\\\\\frac{1}{2}\\\\1\\end{matrix}\\right]\\)\n\n\nThe eigenvectors of \\(A^TA\\) are given by the nullspace of \\(A^TA - \\lambda I\\). We can find the eigenvectors by solving the equation \\((A^TA - \\lambda I)u = 0\\). These are what are given to us in the question, so the eigenvectors are\n\n\n\\(\\displaystyle \\left[\\begin{matrix}-1\\\\1\\end{matrix}\\right]\\)\n\n\n\\(\\displaystyle \\left[\\begin{matrix}1\\\\1\\end{matrix}\\right]\\)\n\n\n\nThe singular value decomposition of \\(A\\) is given by \\(A = U\\Sigma V^T\\), where \\(U\\) is the matrix of eigenvectors of \\(A^TA\\), \\(V\\) is the matrix of eigenvectors of \\(AA^T\\), and \\(\\Sigma\\) is the diagonal matrix of the square roots of the eigenvalues of \\(A^TA\\).\nThe pseudoinverse of \\(A\\) is given by \\(A^+ = V\\Sigma^+ U^T\\), where \\(\\Sigma^+\\) is the diagonal matrix of the reciprocals of the non-zero elements of \\(\\Sigma\\). We can use this to solve the least squares problem \\(A\\mathbf{x} = \\mathbf{b}\\)."
  },
  {
    "objectID": "Exams/FinalExamQuestions.sol.html",
    "href": "Exams/FinalExamQuestions.sol.html",
    "title": "Final Exam Questions Solutions",
    "section": "",
    "text": "Convert this difference equation into matrix–vector form.\n\\(2y_{k+3} + 2y_{k+1} - 3y_{k} = 0\\)\n\nWe solve the equation above for \\(y_{k+3}\\) to get \\(y_{k+3} = \\frac{3y_{k} - 2y_{k+1}}{2}\\).\nMake a vector of the unknowns: \\(x_k = [y_{k}, y_{k+1}, y_{k+2}]^T\\). Then we have that \\(x_{k+1}=[y_{k+1},y_{k+2},y_{k+3}]^T\\).\nWe can now write three equations in terms of the vector \\(x_k\\) and \\(x_{k+1}\\):\n\\[\n\\begin{aligned}\nx_{k+1}[1] &= x_{k}[2] \\\\\nx_{k+1}[2] &= x_{k}[3] \\\\\nx_{k+1}[3] &= \\frac{3x_{k}[1] - 2x_{k}[2]}{2}\n\\end{aligned}\n\\]\nThen the equation can be written as in the form \\(Ax_{k} = x_{k+1}\\) where \\(A\\) is a matrix given by\n\\[\nA = \\begin{bmatrix}\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n\\frac{-3}{2} & -1 & 0\n\\end{bmatrix}\n\\]\n\n\n\n\n\nConvert this difference equation into matrix–vector form.\n\\(y_{k+2} + y_{k+1} - 2y_{k} = 3\\)\n\nGive on practice exam.\nWe solve the equation above for \\(y_{k+2}\\) to get \\(y_{k+2} = 2y_{k} - y_{k+1} + 3\\).\nMake a vector of the unknowns: \\(x_k = [y_{k}, y_{k+1},1]^T\\). Then we have that \\(x_{k+1}=[y_{k+1},y_{k+2},1]^T\\).\nWe can now write three equations in terms of the vector \\(x_k\\) and \\(x_{k+1}\\):\n\\[\n\\begin{aligned}\nx_{k+1}[1] &= x_{k}[2] \\\\\nx_{k+1}[2] &= 2x_{k}[1] - x_{k}[2] + 3x_{k}[3] \\\\\nx_{k+1}[3] &= x_{k}[3]\n\\end{aligned}\n\\]\nThen the equation can be written as in the form \\(Ax_{k} = x_{k+1}\\) where \\(A\\) is a matrix given by\n\\[\nA = \\begin{bmatrix}\n0 & 1 & 0 \\\\\n2 & -1 & 3 \\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\]\nCheck using sympy:\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0\\\\2 x_{k0} - x_{k1} - x_{k2} + 3\\\\0\\end{matrix}\\right]\\)\n\n\nIt works!"
  },
  {
    "objectID": "Exams/FinalExamQuestions.sol.html#section",
    "href": "Exams/FinalExamQuestions.sol.html#section",
    "title": "Final Exam Questions Solutions",
    "section": "",
    "text": "Convert this difference equation into matrix–vector form.\n\\(2y_{k+3} + 2y_{k+1} - 3y_{k} = 0\\)\n\nWe solve the equation above for \\(y_{k+3}\\) to get \\(y_{k+3} = \\frac{3y_{k} - 2y_{k+1}}{2}\\).\nMake a vector of the unknowns: \\(x_k = [y_{k}, y_{k+1}, y_{k+2}]^T\\). Then we have that \\(x_{k+1}=[y_{k+1},y_{k+2},y_{k+3}]^T\\).\nWe can now write three equations in terms of the vector \\(x_k\\) and \\(x_{k+1}\\):\n\\[\n\\begin{aligned}\nx_{k+1}[1] &= x_{k}[2] \\\\\nx_{k+1}[2] &= x_{k}[3] \\\\\nx_{k+1}[3] &= \\frac{3x_{k}[1] - 2x_{k}[2]}{2}\n\\end{aligned}\n\\]\nThen the equation can be written as in the form \\(Ax_{k} = x_{k+1}\\) where \\(A\\) is a matrix given by\n\\[\nA = \\begin{bmatrix}\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n\\frac{-3}{2} & -1 & 0\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "Exams/FinalExamQuestions.sol.html#section-1",
    "href": "Exams/FinalExamQuestions.sol.html#section-1",
    "title": "Final Exam Questions Solutions",
    "section": "",
    "text": "Convert this difference equation into matrix–vector form.\n\\(y_{k+2} + y_{k+1} - 2y_{k} = 3\\)\n\nGive on practice exam.\nWe solve the equation above for \\(y_{k+2}\\) to get \\(y_{k+2} = 2y_{k} - y_{k+1} + 3\\).\nMake a vector of the unknowns: \\(x_k = [y_{k}, y_{k+1},1]^T\\). Then we have that \\(x_{k+1}=[y_{k+1},y_{k+2},1]^T\\).\nWe can now write three equations in terms of the vector \\(x_k\\) and \\(x_{k+1}\\):\n\\[\n\\begin{aligned}\nx_{k+1}[1] &= x_{k}[2] \\\\\nx_{k+1}[2] &= 2x_{k}[1] - x_{k}[2] + 3x_{k}[3] \\\\\nx_{k+1}[3] &= x_{k}[3]\n\\end{aligned}\n\\]\nThen the equation can be written as in the form \\(Ax_{k} = x_{k+1}\\) where \\(A\\) is a matrix given by\n\\[\nA = \\begin{bmatrix}\n0 & 1 & 0 \\\\\n2 & -1 & 3 \\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\]\nCheck using sympy:\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0\\\\2 x_{k0} - x_{k1} - x_{k2} + 3\\\\0\\end{matrix}\\right]\\)\n\n\nIt works!"
  },
  {
    "objectID": "Exams/FinalExamQuestions.sol.html#section-2",
    "href": "Exams/FinalExamQuestions.sol.html#section-2",
    "title": "Final Exam Questions Solutions",
    "section": "2.1 ",
    "text": "2.1 \nA population is modeled with three stages: larva, pupa and adult, and the resulting structured population model transition matrix is\n\\[\nA = \\begin{bmatrix}\n0 & 0 & 0.6 \\\\\n0.5 & 0 & 0 \\\\\n0 & 0.9 & 0\n\\end{bmatrix}\n\\]\n\n\n[(0.646330407009565 + 9.57696933338458e-33*I,\n  1,\n  [Matrix([\n   [-0.211957169909255 - 0.563510300230596*I],\n   [-0.163969672175827 - 0.435930519529353*I],\n   [ -0.22832393982674 - 0.607023069503539*I]])]),\n (-0.323165203504783 - 0.559738551708619*I,\n  1,\n  [Matrix([\n   [-0.446848502976125 + 0.418540422762337*I],\n   [-0.107562931920626 - 0.461259721138817*I],\n   [ 0.631131829114318 + 0.191434388320088*I]])]),\n (-0.323165203504783 + 0.559738551708619*I,\n  1,\n  [Matrix([\n   [-0.277204808242613 - 0.561858355489268*I],\n   [-0.269197612619668 + 0.403041213970164*I],\n   [0.673461217391963 + 0.0440180865002761*I]])])]\n\n\n\nExplain what this model says about the three states.\nStarting with a population of (0, 30, 100), calculate the population in the next two time steps.\nThe eigenvalues of the matrix are \\(\\lambda_1 = 0.646, \\lambda_2 = -0.323 - 0.560i, \\lambda_3 = -0.323 + 0.560i\\). What do these eigenvalues tell you about the long-term behavior of the population?\n\n\n\nThe model says that the adult has a 60% chance of creating a larva (but will die every cycle), the larva has a 50% chance of becoming a pupa, and the pupa has a 90% chance of becoming an adult.\nWe can calculate the population in the next two time steps by multiplying the matrix \\(A\\) with the vector \\(x_k = [0, 30, 100]^T\\).\n\n\n\n(array([60.,  0., 27.]), array([16.2, 30. ,  0. ]))\n\n\n\nThe largest eigenvalue is less than one. This matrix will not have a stable equilibrium point."
  },
  {
    "objectID": "Exams/FinalExamQuestions.sol.html#section-3",
    "href": "Exams/FinalExamQuestions.sol.html#section-3",
    "title": "Final Exam Questions Solutions",
    "section": "2.2 ",
    "text": "2.2 \nA transition matrix A consists of only real entries. It has one real eigenvalue \\(\\lambda_1 = a_1\\) and two complex eigenvalues \\(\\lambda_2 = a_2 + b_2 \\text{i}, \\lambda_3 = a_2 - b_2 \\text{i}\\), so that \\(\\lambda_2\\) is the complex conjugate of \\(\\lambda_3\\): \\(\\lambda_2=\\overline{\\lambda_3}\\). It has corresponding eigenvectors \\(\\mathbf{v_1}, \\mathbf{v_2}, \\mathbf{v_3}\\), where the entries in \\(\\mathbf{v_1}\\) are real and the entries in \\(\\mathbf{v_2}\\) are the complex conjugates of the entries of \\(\\mathbf{v_3}\\).\n(For matrices with all real entries, complex eigenvalues will always come in complex conjugate pairs, and their eigenvectors will always also be complex conguate pairs.)\n\nSuppose the initial population is given by \\(\\mathbf{x}=\\begin{bmatrix} x_{1} \\\\ x_2 \\\\ x_3 \\end{bmatrix}\\), where \\(x_1,x_2,x_3\\) are real numbers. The projection of the initial population onto the eigenvectors is given by \\(\\mathbf{x}=c_1 \\mathbf{v_1} + c_2 \\mathbf{v_2} + c_3 \\mathbf{v_3}\\).\n\nUsing the fact that \\(v_2\\) and \\(v_3\\) are complex conjugates, show that \\(c_2\\) and \\(c_3\\) are also complex conjugates.\n\nIn terms of the eigenvalues and eigenvectors, what will be the population distribution after \\(k\\) iterations?\nUsing your results from the previous two parts, show that the population at time \\(k\\) will always have real entries.\n\n(You may find it helpful to know that for complex scalars \\(d,g\\), \\(\\overline{dg}=\\overline{d}\\times\\overline{g}\\), and \\(\\overline{d^n}= \\overline{d}^n\\).)\n\nArgue that you could have gotten the same result as (c) but just using the fact that the matrix \\(A\\) has only real entries.\nSuppose \\(|\\lambda_1|&gt;|\\lambda_2|=|\\lambda_3|\\). Estimate \\(\\lim_{k\\to\\infty} \\mathbf{x_k}\\) for a random initial population \\(\\mathbf{x_0}\\), for the cases where \\(\\lambda_1&gt;1\\), \\(\\lambda_1=1\\), and \\(\\lambda_1&lt;1\\).\n\n\n\nThe projection onto \\(\\mathbf{v_2}\\) is given by \\(\\mathbf{x} \\cdot \\mathbf{v_2} = x[1]v_2[1] + x[2]v_2[2] + x[3]v_2[3]\\). Since \\(v_2\\) is a complex conjugate of \\(v_3\\), we have that \\(v_2[2] = \\overline{v_3[2]}\\) and \\(v_2[3] = \\overline{v_3[3]}\\).\n\nThen the projection onto \\(\\mathbf{v_2}\\) is given by \\(c_2=x[1]v_2[1] + x[2]v_2[2] + x[3]v_2[3] = x[1]\\overline{v_3[1]} + x[2]\\overline{v_3[2}] + x[3]\\overline{v_3[3]} = \\overline{c_3}\\).\n\nThe population at time \\(k\\) is given by \\(\\mathbf{x_k}=A^k x_0 = c_1 \\lambda_1^k \\mathbf{v_1} + c_2 \\lambda_2^k \\mathbf{v_2} + c_3 \\lambda_3^k \\mathbf{v_3}\\).\nThe first entry of \\(\\mathbf{x_k}\\) is given by\n\n\\[\n\\begin{aligned}\nx_k[1]&=c_1 \\lambda_1^k v_1[1] + c_2 \\lambda_2^k v_2[1] + c_3 \\lambda_3^k v_3[1] \\\\\n&=c_1 \\lambda_1^k v_1[1] + c_2 \\lambda_2^k v_2[1] + \\overline{c_2} \\overline{\\lambda_2^k} \\overline{v_2[1]} \\\\\n&=c_1 \\lambda_1^k v_1[1] + c_2 \\lambda_2^k v_2[1] + \\overline{c_2\\lambda_2^k v_2[1]} \\\\\n\\end{aligned}\n\\]\nA number plus its complex conjugate is always real. Therefore, \\(x_k[1]\\) is real. The same logic holds for \\(x_k[2]\\) and \\(x_k[3]\\), so the population at time \\(k\\) will always have real entries.\n\nSince the matrix \\(A\\) has only real entries, all of its powers will also have only real entries. Therefore, the population at time \\(k\\) will always have real entries.\nIf \\(|\\lambda_1|&gt;|\\lambda_2|=|\\lambda_3|\\), then the population at time \\(k\\) will be dominated by the term \\(c_1 \\lambda_1^k \\mathbf{v_1}\\). If \\(\\lambda_1&gt;1\\), then the population will grow exponentially. If \\(\\lambda_1=1\\), then the population will stay constant and will approach \\(c_1 \\mathbf{v_1}\\). If \\(\\lambda_1&lt;1\\), then the population will decay exponentially."
  },
  {
    "objectID": "Exams/FinalExamQuestions.sol.html#section-4",
    "href": "Exams/FinalExamQuestions.sol.html#section-4",
    "title": "Final Exam Questions Solutions",
    "section": "3.1 ",
    "text": "3.1 \nThe graph G is given below.\n\n\n/Users/kendra/Library/Python/3.8/lib/python/site-packages/networkx/drawing/nx_pylab.py:433: UserWarning:\n\nNo data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n\n\n\n\n\n\n\n\n\n\n\nFind the adjacency matrix \\(A\\) and the incidence matrix \\(B\\) for the graph G.\nFrom \\(A\\) and \\(B\\), match the following descriptions to the corresponding quantities:\n\nthe number of outgoing edges from each vertex,\nthe power of each vertex (the number of paths of length 1 or 2 originating at each vertex)\nthe number of paths of length \\(k\\) between each pair of vertices\na basis for loops in the graph (don’t worry about the direction of the edges)\n\n\n\n(a) Sum of the entries in the corresponding row of $A$,\n(c) Entries in the matrix $A^k$,\n(d) Nullspace of the matrix $B^T$.\n(e) Entries in the matrix $B^k$.\n(b) Sum of the entries in the corresponding row of $A + A^2$,\n(f) Nullspace of the matrix $A^T$.\n\n\nThe adjacency matrix \\(A\\) is given by\n\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0 & 1 & 1 & 0\\\\0 & 0 & 1 & 1\\\\0 & 0 & 0 & 1\\\\0 & 0 & 0 & 0\\end{matrix}\\right]\\)\n\n\nThe incidence matrix \\(B\\) is given by\n\n\n\\(\\displaystyle \\left[\\begin{matrix}-1 & -1 & 0 & 0 & 0\\\\1 & 0 & -1 & -1 & 0\\\\0 & 1 & 1 & 0 & -1\\\\0 & 0 & 0 & 1 & 1\\end{matrix}\\right]\\)\n\n\n\n\nThe degree of each vertex is given by the sum of the entries in the corresponding row of the adjacency matrix \\(A\\).\n\n\nThe power of each vertex is given by the sum of the entries in the corresponding row of the matrix \\(A + A^2\\).\nThe number of paths of length \\(k\\) between each pair of vertices is given by the entries in the matrix \\(A^k\\).\nA basis for loops in the graph is given by the nullspace of the matrix \\(B^T\\)."
  },
  {
    "objectID": "Exams/FinalExamQuestions.sol.html#section-5",
    "href": "Exams/FinalExamQuestions.sol.html#section-5",
    "title": "Final Exam Questions Solutions",
    "section": "3.2 ",
    "text": "3.2 \nA directed graph has vertices labeled \\(A\\), \\(B\\), \\(C\\), and \\(D\\). The edges are given by the following incidence matrix B:\n\n\n\\(\\displaystyle B = \\left[\\begin{matrix}-1 & -1 & 0 & 0 & 0 & 1\\\\1 & 0 & -1 & -1 & 0 & 0\\\\0 & 1 & 1 & 0 & -1 & 0\\\\0 & 0 & 0 & 1 & 1 & -1\\end{matrix}\\right]\\)\n\n\n\nFind the adjacency matrix \\(A\\) for the graph.\nDraw the graph.\nDescribe how you could find the number of paths of length \\(k\\) between each pair of vertices using the adjacency matrix \\(A\\).\nIf you calculate the matrix \\(A\\) + \\(A^2\\), what does the sum of the entries in the corresponding row tell you about the graph?\nLet \\(\\mathbf{v}\\) be a row vector representing a subset of the edges in the graph. For example, the subset of edges given by the vector \\(\\mathbf{v} = [1, 0, 1, 0, 0, 1]\\) would correspond to the first, third, and sixth edges.\n\nWrite an equation which will be satisfied whenever the subset of edges represented by \\(\\mathbf{v}\\) forms a loop in the graph.\n\nDescribe two algorithms for ranking the importance of the vertices in a graph. (One or more of these algorithms may be based on prior parts of this problem.)\n\n\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0 & 1 & 1 & 0\\\\0 & 0 & 1 & 1\\\\0 & 0 & 0 & 1\\\\1 & 0 & 0 & 0\\end{matrix}\\right]\\)\n\n\n\n\n/Users/kendra/Library/Python/3.8/lib/python/site-packages/networkx/drawing/nx_pylab.py:433: UserWarning:\n\nNo data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n\n\n\n\n\n\n\n\n\n\n\nThe number of paths of length \\(k\\) between each pair of vertices is given by the entries in the matrix \\(B^k\\).\nThe sum of the entries in the corresponding row of the matrix \\(A + A^2\\) gives the power of each vertex. In other words, it gives the number of paths of length 1 or 2 originating at each vertex.\nA loop in the graph is a subset of edges that form a closed path. A closed path is a path that starts and ends at the same vertex. Therefore, a subset of edges forms a loop if the multiple of the incidence matrix \\(B\\) and the vector \\(\\mathbf{v}\\) is equal to zero. In other words, the subset of edges represented by \\(\\mathbf{v}\\) forms a loop if \\(\\mathbf{v}B^T = 0\\), or equivalently if \\(B \\mathbf{v^T}=0\\). These definitions work if B has one column for each edge; if B has one row for each edge, then the equation is \\(B^T \\mathbf{v^T}=0\\).\nTwo algorithms for ranking the importance of the vertices in a graph are the PageRank algorithm the power method."
  },
  {
    "objectID": "Exams/FinalExamQuestions.sol.html#section-6",
    "href": "Exams/FinalExamQuestions.sol.html#section-6",
    "title": "Final Exam Questions Solutions",
    "section": "4.1 ",
    "text": "4.1 \nFind a least squares solution to the following system:\n\\[\n\\begin{bmatrix}\n1 & 1 \\\\\n1 & 1 \\\\\n1 & 2 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}\n\\]\nIs the solution a genuine solution?\n\nFirst we find the least squares solution by solving the normal equations \\(A^TAx = A^Tb\\).\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0\\\\\\frac{3}{2}\\end{matrix}\\right]\\)\n\n\nThe solution is \\(x = \\begin{bmatrix} 1.5 \\\\ 0.5 \\end{bmatrix}\\).\nTo check if this is a genuine solution, we substitute the values of \\(x\\) back into the original equation.\n\n\n\\(\\displaystyle \\left[\\begin{matrix}\\frac{3}{2}\\\\\\frac{3}{2}\\\\3\\end{matrix}\\right]\\)\n\n\nThe result is \\(\\begin{bmatrix} 1.5 \\\\ 1.5 \\\\ 3.0 \\end{bmatrix}\\), which is not equal to \\(\\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}\\). Therefore, the solution is not a genuine solution."
  },
  {
    "objectID": "Exams/FinalExamQuestions.sol.html#section-7",
    "href": "Exams/FinalExamQuestions.sol.html#section-7",
    "title": "Final Exam Questions Solutions",
    "section": "4.2 ",
    "text": "4.2 \nThe system of equations\n\\[\n\\begin{aligned}\nx_{1}+x_{2} & =2 \\\\\nx_{3}+x_{4} & =1 \\\\\n2 x_{1}+4 x_{2}+2 x_{4} & =6\n\\end{aligned}\n\\]\nhas coefficient matrix \\(A\\) and right hand side \\(\\mathbf{b}\\) such that the row-reduced echelon form of \\([A \\mid \\mathbf{b}]\\) is\n\\[\n\\left[\\begin{array}{ccccc:c}1 & 0 & 0 & -1 & \\mid & 1 \\\\ 0 & 1 & 0 & 1 & | & 1 \\\\ 0 & 0 & 1 & 1 & | & 1\\end{array}\\right]\n\\]\nUse this information to answer the following:\n\nFind a basis for the null space of \\(A\\).\nFind the form of a general solution of the system \\(A \\mathbf{x}=\\mathbf{b}\\).\nFind the rank of \\(A\\).\nNo matter what the right hand side of \\(\\mathbf{b}\\) is, this system has solutions. In terms of rank, why do we know this?\n\n::: solutions\n\nTo find a basis for the null space of \\(A\\), we solve the equation \\(A\\mathbf{x} = \\mathbf{0}\\).\n\nWe know that \\(x_4\\) is a free variable, so we can write the solution in terms of \\(x_4\\):\n\\[\n\\begin{aligned}\nx_1 &= x_4 \\\\\nx_2 &= - x_4 \\\\\nx_3 &= - x_4 \\\\\nx_4 &= x_4\n\\end{aligned}\n\\]\nSo the basis for the nullspace of \\(A\\) is simply the vector \\(\\begin{bmatrix} 1 \\\\ -1 \\\\ -1 \\\\ 1 \\end{bmatrix}\\).\nChecking using sympy:\n\nThe general solution will be a particular solution plus a linear combination of the basis for the nullspace of \\(A\\).\n\nTo find a particular solution, we can set \\(x_4 = 0\\) and solve the system. Then we have that \\(x_1 = x_2 = x_3 = 1\\). Therefore, a particular solution is \\(\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\end{bmatrix}\\).\nThe general solution is then given by \\(\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\end{bmatrix} + x_4 \\begin{bmatrix} 1 \\\\ -1 \\\\ -1 \\\\ 1 \\end{bmatrix}\\).\n\nThe rank of \\(A\\) is the number of pivot columns in the row-reduced echelon form of \\(A\\). In this case, the rank of \\(A\\) is 3.\nThe rank of \\(A\\) is less than the number of columns in \\(A\\). Therefore, the system will always have solutions."
  },
  {
    "objectID": "Exams/FinalExamQuestions.sol.html#section-8",
    "href": "Exams/FinalExamQuestions.sol.html#section-8",
    "title": "Final Exam Questions Solutions",
    "section": "5.1 ",
    "text": "5.1 \nLet \\(A=\\left[\\begin{array}{ccc}1 & 0 & 1 \\\\ -1 & 1 & 1 \\\\ 2 & 1 & 4\\end{array}\\right]\\).\n\nFind the reduced row-echelon form and rank of \\(A\\).\nFind a basis for the column space of \\(A\\).\nDetermine which of the following vectors is in the column space of \\(A\\) and, if so, express the vector as a linear combination of the columns of \\(A\\) :\n\n\\(b_{1}=[2,1,0]^{T}\\), in \\(b_{2}=[2,-3,3]^{T}\\).\nYou may find it helpful to use the following outputs from sympy:\nA.gauss_jordan_solve(b1): ValueError: Linear system has no solution\nA.gauss_jordan_solve(b2): \\(\\begin{bmatrix} 2 - \\tau_0 \\\\ -2 \\tau_0 -1 \\\\ \\tau_0 \\end{bmatrix}\\)\n\n\nThe reduced row-echelon form of \\(A\\) is:\n\n\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & 0 & 1\\\\0 & 1 & 2\\\\0 & 0 & 0\\end{matrix}\\right]\\)\n\n\n\nThe basis for the column space of A is given by the pivot columns of the row-reduced echelon form of A. In this case, the basis is given by the first two columns of A.\n\n\n\n\\(\\displaystyle \\left[\\begin{matrix}1\\\\-1\\\\2\\end{matrix}\\right]\\)\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0\\\\1\\\\1\\end{matrix}\\right]\\)\n\n\n\nTo determine if \\(b_1\\) is in the column space of A, we solve the equation \\(A\\mathbf{x} = b_1\\). and \\(b_2\\). The first has no solution, so it’s not in the column space. The second has a solution, so it is in the column space. The solution is \\(\\begin{bmatrix} 2 - \\tau_0 \\\\ -2 \\tau_0 -1 \\\\ \\tau_0 \\end{bmatrix}\\).\n\nWe pick \\(\\tau_0 = 0\\) to get that we can combine twice the first column and -1 times the second column to get \\(b_2\\).\n\n\n(Matrix([\n [   2 - tau0],\n [-2*tau0 - 1],\n [       tau0]]),\n Matrix([[tau0]]))"
  },
  {
    "objectID": "Exams/FinalExamQuestions.sol.html#section-9",
    "href": "Exams/FinalExamQuestions.sol.html#section-9",
    "title": "Final Exam Questions Solutions",
    "section": "6.1 ",
    "text": "6.1 \nYou are given a matrix \\(A\\):\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0 & 1\\\\1 & 0\\\\1 & 1\\end{matrix}\\right]\\)\n\n\nThe eigenvalues of \\(AA^T\\) are \\(\\lambda_1 = 3, \\lambda_2 = 1, \\lambda_3 = 0\\).\n\nFind the eigenvalues of \\(AA^T\\).\nFind the eigenvectors of \\(A^TA\\) and \\(AA^T\\). Use the facts below and explain your reasoning.\nFind the singular value decomposition of \\(A\\).\nUsing the SVD, calculate \\(A^+\\), the pseudoinverse of \\(A\\). Use this to solve the least squares problem \\(A\\mathbf{x} = \\mathbf{b}\\), where \\(\\mathbf{b} = [1, 4,2]^T\\).\nUsing the SVD, calculate \\(A^k \\mathbf{x}\\) for \\(k=3\\) and \\(\\mathbf{x} = [-2, 2, 0]^T\\).\n\nHere are some facts which may be helpful:\n\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & 0 & 1\\\\0 & 1 & 1\\\\1 & 1 & 2\\end{matrix}\\right]\\left[\\begin{matrix}-1\\\\-1\\\\1\\end{matrix}\\right]=\\left[\\begin{matrix}0\\\\0\\\\0\\end{matrix}\\right]\\)\n\n\n\\(\\displaystyle \\left[\\begin{matrix}-2 & 0 & 1\\\\0 & -2 & 1\\\\1 & 1 & -1\\end{matrix}\\right]\\left[\\begin{matrix}\\frac{1}{2}\\\\\\frac{1}{2}\\\\1\\end{matrix}\\right]=\\left[\\begin{matrix}0\\\\0\\\\0\\end{matrix}\\right]\\)\n\n\n\\(\\displaystyle  \\left[\\begin{matrix}0 & 0 & 1\\\\0 & 0 & 1\\\\1 & 1 & 1\\end{matrix}\\right]\\left[\\begin{matrix}-1\\\\1\\\\0\\end{matrix}\\right]=\\left[\\begin{matrix}0\\\\0\\\\0\\end{matrix}\\right]\\)\n\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & 1\\\\1 & 1\\end{matrix}\\right]\\left[\\begin{matrix}-1\\\\1\\end{matrix}\\right]=\\left[\\begin{matrix}0\\\\0\\end{matrix}\\right]\\)\n\n\n\\(\\displaystyle \\left[\\begin{matrix}-1 & 1\\\\1 & -1\\end{matrix}\\right]\\left[\\begin{matrix}1\\\\1\\end{matrix}\\right]=\\left[\\begin{matrix}0\\\\0\\end{matrix}\\right]\\)\n\n\n::: solutions\n\nWe first find the characteristic polynomial of \\(AA^T\\): we find \\(\\text{det}(AA^T - \\lambda I)\\) and set it equal to zero.\n\n\n\n\\(\\displaystyle \\lambda^{2} - 4 \\lambda + 3=0\\)\n\n\nWe try the three eigenvalues given in the question: 0, 1, and 3. We find that 0 is not an eigenvalue, but that 1 and 3 satisfy the equation. Therefore, the eigenvalues of \\(AA^T\\) are \\(\\lambda_1 = 3, \\lambda_2 = 1\\).\n\nThe eigenvectors of \\(AA^T\\) are given by the nullspace of \\(AA^T - \\lambda I\\). We can find the eigenvectors by solving the equation \\((AA^T - \\lambda I)v = 0\\). These are what are given to us in the question, so the eigenvectors are\n\n\n\n\\(\\displaystyle \\left[\\begin{matrix}-1\\\\-1\\\\1\\end{matrix}\\right]\\)\n\n\n\\(\\displaystyle \\left[\\begin{matrix}-1\\\\1\\\\0\\end{matrix}\\right]\\)\n\n\n\\(\\displaystyle \\left[\\begin{matrix}\\frac{1}{2}\\\\\\frac{1}{2}\\\\1\\end{matrix}\\right]\\)\n\n\nThe eigenvectors of \\(A^TA\\) are given by the nullspace of \\(A^TA - \\lambda I\\). We can find the eigenvectors by solving the equation \\((A^TA - \\lambda I)u = 0\\). These are what are given to us in the question, so the eigenvectors are\n\n\n\\(\\displaystyle \\left[\\begin{matrix}-1\\\\1\\end{matrix}\\right]\\)\n\n\n\\(\\displaystyle \\left[\\begin{matrix}1\\\\1\\end{matrix}\\right]\\)\n\n\n\nThe singular value decomposition of \\(A\\) is given by \\(A = U\\Sigma V^T\\), where \\(U\\) is the matrix of eigenvectors of \\(A^TA\\), \\(V\\) is the matrix of eigenvectors of \\(AA^T\\), and \\(\\Sigma\\) is the diagonal matrix of the square roots of the eigenvalues of \\(A^TA\\).\nThe pseudoinverse of \\(A\\) is given by \\(A^+ = V\\Sigma^+ U^T\\), where \\(\\Sigma^+\\) is the diagonal matrix of the reciprocals of the non-zero elements of \\(\\Sigma\\). We can use this to solve the least squares problem \\(A\\mathbf{x} = \\mathbf{b}\\)."
  },
  {
    "objectID": "Exams/PracticeFinal.html",
    "href": "Exams/PracticeFinal.html",
    "title": "Practice Final Exam",
    "section": "",
    "text": "Instructions:\nYou’ll have two hours to do the real exam. You’ll be asked to do it without notes or the computer, but you may use a calculator.\nIf you have any questions, or if you think there is a mistake in the question, please let me know."
  },
  {
    "objectID": "Exams/PracticeFinal.html#section",
    "href": "Exams/PracticeFinal.html#section",
    "title": "Practice Final Exam",
    "section": "1 ",
    "text": "1 \nA transition matrix A consists of only real entries. It has one real eigenvalue \\(\\lambda_1 = a_1\\) and two complex eigenvalues \\(\\lambda_2 = a_2 + b_2 \\text{i}, \\lambda_3 = a_2 - b_2 \\text{i}\\), so that \\(\\lambda_2\\) is the complex conjugate of \\(\\lambda_3\\): \\(\\lambda_2=\\overline{\\lambda_3}\\). It has corresponding eigenvectors \\(\\mathbf{v_1}, \\mathbf{v_2}, \\mathbf{v_3}\\), where the entries in \\(\\mathbf{v_1}\\) are real and the entries in \\(\\mathbf{v_2}\\) are the complex conjugates of the entries of \\(\\mathbf{v_3}\\).\n(For matrices with all real entries, complex eigenvalues will always come in complex conjugate pairs, and their eigenvectors will always also be complex conguate pairs.)\n\nSuppose the initial population is given by \\(\\mathbf{x}=\\begin{bmatrix} x_{1} \\\\ x_2 \\\\ x_3 \\end{bmatrix}\\), where \\(x_1,x_2,x_3\\) are real numbers. The projection of the initial population onto the eigenvectors is given by \\(\\mathbf{x}=c_1 \\mathbf{v_1} + c_2 \\mathbf{v_2} + c_3 \\mathbf{v_3}\\).\n\nUsing the fact that \\(v_2\\) and \\(v_3\\) are complex conjugates, show that \\(c_2\\) and \\(c_3\\) are also complex conjugates.\n\nIn terms of the eigenvalues and eigenvectors, what will be the population distribution after \\(k\\) iterations?\nUsing your results from the previous two parts, show that the population at time \\(k\\) will always have real entries.\n\n(You may find it helpful to know that for complex scalars \\(d,g\\), \\(\\overline{dg}=\\overline{d}\\times\\overline{g}\\), and \\(\\overline{d^n}= \\overline{d}^n\\).)\n\nArgue that you could have gotten the same result as (c) but just using the fact that the matrix \\(A\\) has only real entries.\nSuppose \\(|\\lambda_1|&gt;|\\lambda_2|=|\\lambda_3|\\). Estimate \\(\\lim_{k\\to\\infty} \\mathbf{x_k}\\) for a random initial population \\(\\mathbf{x_0}\\), for the cases where \\(\\lambda_1&gt;1\\), \\(\\lambda_1=1\\), and \\(\\lambda_1&lt;1\\)."
  },
  {
    "objectID": "Exams/PracticeFinal.html#section-1",
    "href": "Exams/PracticeFinal.html#section-1",
    "title": "Practice Final Exam",
    "section": "2 ",
    "text": "2 \n\nConvert this difference equation into matrix–vector form.\n\n\\(y_{k+2} + y_{k+1} - 2y_{k} = 3\\)\n\nUsing the matrix, find \\(y_2\\) if it is given that \\(y_0 = 2\\) and \\(y_1 = 2\\)."
  },
  {
    "objectID": "Exams/PracticeFinal.html#section-2",
    "href": "Exams/PracticeFinal.html#section-2",
    "title": "Practice Final Exam",
    "section": "3 ",
    "text": "3 \nA directed graph has vertices labeled \\(A\\), \\(B\\), \\(C\\), and \\(D\\). The edges are given by the following incidence matrix B:\n\n\n\\(\\displaystyle B = \\left[\\begin{matrix}-1 & -1 & 0 & 0 & 0 & 1\\\\1 & 0 & -1 & -1 & 0 & 0\\\\0 & 1 & 1 & 0 & -1 & 0\\\\0 & 0 & 0 & 1 & 1 & -1\\end{matrix}\\right]\\)\n\n\n\nFind the adjacency matrix \\(A\\) for the graph.\nDraw the graph.\nDescribe how you could find the number of paths of length \\(k\\) between each pair of vertices using the adjacency matrix \\(A\\).\nIf you calculate the matrix \\(A\\) + \\(A^2\\), what does the sum of the entries in the corresponding row tell you about the graph?\nLet \\(\\mathbf{v}\\) be a row vector representing a subset of the edges in the graph. For example, the subset of edges given by the vector \\(\\mathbf{v} = [1, 0, 1, 0, 0, 1]\\) would correspond to the first, third, and sixth edges.\nWrite an equation which will be satisfied whenever the subset of edges represented by \\(\\mathbf{v}\\) forms a loop in the graph.\nDescribe two algorithms for ranking the importance of the vertices in a graph. (One or more of these algorithms may be based on prior parts of this problem.)"
  },
  {
    "objectID": "Exams/PracticeFinal.html#section-3",
    "href": "Exams/PracticeFinal.html#section-3",
    "title": "Practice Final Exam",
    "section": "4 ",
    "text": "4 \nThe system of equations\n\\[\n\\begin{aligned}\nx_{1}+x_{2} & =2 \\\\\nx_{3}+x_{4} & =1 \\\\\n2 x_{1}+4 x_{2}+2 x_{4} & =6\n\\end{aligned}\n\\]\nhas coefficient matrix \\(A\\) and right hand side \\(\\mathbf{b}\\) such that the row-reduced echelon form of \\([A \\mid \\mathbf{b}]\\) is\n\\[\n\\left[\\begin{array}{cccc|c}1 & 0 & 0 & -1 & 1 \\\\ 0 & 1 & 0 & 1  & 1 \\\\ 0 & 0 & 1 & 1  & 1\\end{array}\\right]\n\\]\nUse this information to answer the following:\n\nFind a basis for the null space of \\(A\\) (not \\(A|b\\)!).\nFind the form of a general solution of the system \\(A \\mathbf{x}=\\mathbf{b}\\).\nFind the rank of \\(A\\). Discuss the meaning of the rank in terms of the independence of the columns of \\(A\\)."
  },
  {
    "objectID": "Exams/PracticeFinal.html#section-4",
    "href": "Exams/PracticeFinal.html#section-4",
    "title": "Practice Final Exam",
    "section": "5 ",
    "text": "5 \nYou are given a matrix \\(A\\):\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0 & 1\\\\1 & 0\\\\1 & 1\\end{matrix}\\right]\\)\n\n\nThe eigenvalues of \\(AA^T\\) are \\(\\lambda_1 = 3, \\lambda_2 = 1, \\lambda_3 = 0\\).\n\nFind the eigenvalues of \\(A^TA\\).\nUsing the facts below, find the eigenvectors of \\(A^TA\\) and \\(AA^T\\), and explain your reasoning.\nFind the singular value decomposition of \\(A\\).\nUsing the SVD, calculate \\(A^+\\), the pseudoinverse of \\(A\\). Use this to solve the least squares problem \\(A\\mathbf{x} = \\mathbf{b}\\), where \\(\\mathbf{b} = [1, 4,2]^T\\).\nUsing the SVD, calculate \\(A^k \\mathbf{x}\\) for \\(k=3\\) and \\(\\mathbf{x} = [-2, 2, 0]^T\\).\n\nHere are some facts which may be helpful:\n\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & 0 & 1\\\\0 & 1 & 1\\\\1 & 1 & 2\\end{matrix}\\right]\\left[\\begin{matrix}-1\\\\-1\\\\1\\end{matrix}\\right]=\\left[\\begin{matrix}0\\\\0\\\\0\\end{matrix}\\right]\\)\n\n\n\\(\\displaystyle \\left[\\begin{matrix}-2 & 0 & 1\\\\0 & -2 & 1\\\\1 & 1 & -1\\end{matrix}\\right]\\left[\\begin{matrix}\\frac{1}{2}\\\\\\frac{1}{2}\\\\1\\end{matrix}\\right]=\\left[\\begin{matrix}0\\\\0\\\\0\\end{matrix}\\right]\\)\n\n\n\\(\\displaystyle  \\left[\\begin{matrix}0 & 0 & 1\\\\0 & 0 & 1\\\\1 & 1 & 1\\end{matrix}\\right]\\left[\\begin{matrix}-1\\\\1\\\\0\\end{matrix}\\right]=\\left[\\begin{matrix}0\\\\0\\\\0\\end{matrix}\\right]\\)\n\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & 1\\\\1 & 1\\end{matrix}\\right]\\left[\\begin{matrix}-1\\\\1\\end{matrix}\\right]=\\left[\\begin{matrix}0\\\\0\\end{matrix}\\right]\\)\n\n\n\\(\\displaystyle \\left[\\begin{matrix}-1 & 1\\\\1 & -1\\end{matrix}\\right]\\left[\\begin{matrix}1\\\\1\\end{matrix}\\right]=\\left[\\begin{matrix}0\\\\0\\end{matrix}\\right]\\)"
  },
  {
    "objectID": "Exams/PracticeFinal.html#section-5",
    "href": "Exams/PracticeFinal.html#section-5",
    "title": "Practice Final Exam",
    "section": "6 ",
    "text": "6 \n\nFor the matrix \\(A = \\begin{bmatrix} 9 & 8 \\\\ -6 & -5 \\end{bmatrix}\\), find the eigenvalues and eigenvectors.\nUsing these results, find the matrix \\(P\\) such that \\(P^{-1}AP\\) is a diagonal matrix.\n\n(Note: for a 2x2 matrix \\(\\begin{bmatrix}a&b\\\\c&d\\end{bmatrix}\\), you can calculate the inverse as \\(\\frac{1}{ad-bc}\\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}\\).)\n\nDiagonalize the matrix \\(A^4+2A^2-3I\\). (It’s fine to leave exponents in the answer, e.g. \\(5^{6}\\).)\nFind the nullspace of the matrix \\(A^4+2A^2-3I\\)."
  },
  {
    "objectID": "Exams/PracticeFinal.html#section-6",
    "href": "Exams/PracticeFinal.html#section-6",
    "title": "Practice Final Exam",
    "section": "7 ",
    "text": "7 \nThe Discrete Fourier Transform (DFT) of a sequence \\(x[n]\\) is given by \\(X[k] = \\sum_{n=0}^{N-1} x[n] e^{-i2\\pi kn/N}\\), where \\(N\\) is the length of the sequence, and where \\(k\\) runs from \\(0\\) to \\(N-1\\). The inverse DFT is given by \\(x[n] = \\frac{1}{N} \\sum_{k=0}^{N-1} X[k] e^{i2\\pi kn/N}\\).\n\nFind the DFT of the filter \\(h[n] = [\\frac{1}{3},-\\frac{1}{3},\\frac{1}{3},0]\\).\nUsing this DFT, find the circular convolution of the sequences \\(x[n] = [1,2,3,4]\\) and \\(h[n]\\). Note: the DFT of \\(x\\) is \\(X=[10,-2+2i,-2,-2-2i]\\).\n\n(Circular convolution is convolution where we assume the sequences are periodic, so that \\(x[n] = x[n+N]\\). It’s as if we had the long sequence \\(x[n] = [\\color{blue}1,2,3,4,\\color{black}1,2,3,4,\\color{blue}1,2,3,4,\\ldots]\\) to use when calculating the convolution. When we discussed convolution using the DFT in class, we were actually doing circular convolution. This matters if you want to check your results doing the convolution by hand without using the DFT.)\n\nFrom the plots below, match the signals to the correct DFTs. (Curve y3 is actually a quadratic. I have plotted only the absolute value of the DFTs. The complete DFT also includes negative frequencies, not shown here, which are the complex conjugates of the positive frequencies.)"
  },
  {
    "objectID": "Exams/PracticeFinal.sol.html",
    "href": "Exams/PracticeFinal.sol.html",
    "title": "Practice Final Exam Solutions",
    "section": "",
    "text": "Instructions:\nYou’ll have two hours to do the real exam. You’ll be asked to do it without notes or the computer, but you may use a calculator.\nIf you have any questions, or if you think there is a mistake in the question, please let me know."
  },
  {
    "objectID": "Exams/PracticeFinal.sol.html#section",
    "href": "Exams/PracticeFinal.sol.html#section",
    "title": "Practice Final Exam Solutions",
    "section": "1 ",
    "text": "1 \nA transition matrix A consists of only real entries. It has one real eigenvalue \\(\\lambda_1 = a_1\\) and two complex eigenvalues \\(\\lambda_2 = a_2 + b_2 \\text{i}, \\lambda_3 = a_2 - b_2 \\text{i}\\), so that \\(\\lambda_2\\) is the complex conjugate of \\(\\lambda_3\\): \\(\\lambda_2=\\overline{\\lambda_3}\\). It has corresponding eigenvectors \\(\\mathbf{v_1}, \\mathbf{v_2}, \\mathbf{v_3}\\), where the entries in \\(\\mathbf{v_1}\\) are real and the entries in \\(\\mathbf{v_2}\\) are the complex conjugates of the entries of \\(\\mathbf{v_3}\\).\n(For matrices with all real entries, complex eigenvalues will always come in complex conjugate pairs, and their eigenvectors will always also be complex conguate pairs.)\n\nSuppose the initial population is given by \\(\\mathbf{x}=\\begin{bmatrix} x_{1} \\\\ x_2 \\\\ x_3 \\end{bmatrix}\\), where \\(x_1,x_2,x_3\\) are real numbers. The projection of the initial population onto the eigenvectors is given by \\(\\mathbf{x}=c_1 \\mathbf{v_1} + c_2 \\mathbf{v_2} + c_3 \\mathbf{v_3}\\).\n\nUsing the fact that \\(v_2\\) and \\(v_3\\) are complex conjugates, show that \\(c_2\\) and \\(c_3\\) are also complex conjugates.\n\nIn terms of the eigenvalues and eigenvectors, what will be the population distribution after \\(k\\) iterations?\nUsing your results from the previous two parts, show that the population at time \\(k\\) will always have real entries.\n\n(You may find it helpful to know that for complex scalars \\(d,g\\), \\(\\overline{dg}=\\overline{d}\\times\\overline{g}\\), and \\(\\overline{d^n}= \\overline{d}^n\\).)\n\nArgue that you could have gotten the same result as (c) but just using the fact that the matrix \\(A\\) has only real entries.\nSuppose \\(|\\lambda_1|&gt;|\\lambda_2|=|\\lambda_3|\\). Estimate \\(\\lim_{k\\to\\infty} \\mathbf{x_k}\\) for a random initial population \\(\\mathbf{x_0}\\), for the cases where \\(\\lambda_1&gt;1\\), \\(\\lambda_1=1\\), and \\(\\lambda_1&lt;1\\).\n\n\n\nThe projection onto \\(\\mathbf{v_2}\\) is given by \\(\\mathbf{x} \\cdot \\mathbf{v_2} = x[1]v_2[1] + x[2]v_2[2] + x[3]v_2[3]\\). Since \\(v_2\\) is a complex conjugate of \\(v_3\\), we have that \\(v_2[2] = \\overline{v_3[2]}\\) and \\(v_2[3] = \\overline{v_3[3]}\\).\n\nThen the projection onto \\(\\mathbf{v_2}\\) is given by \\(c_2=x[1]v_2[1] + x[2]v_2[2] + x[3]v_2[3] = x[1]\\overline{v_3[1]} + x[2]\\overline{v_3[2}] + x[3]\\overline{v_3[3]} = \\overline{c_3}\\).\n\nThe population at time \\(k\\) is given by \\(\\mathbf{x_k}=A^k x_0 = c_1 \\lambda_1^k \\mathbf{v_1} + c_2 \\lambda_2^k \\mathbf{v_2} + c_3 \\lambda_3^k \\mathbf{v_3}\\).\nThe first entry of \\(\\mathbf{x_k}\\) is given by\n\n\\[\n\\begin{aligned}\nx_k[1]&=c_1 \\lambda_1^k v_1[1] + c_2 \\lambda_2^k v_2[1] + c_3 \\lambda_3^k v_3[1] \\\\\n&=c_1 \\lambda_1^k v_1[1] + c_2 \\lambda_2^k v_2[1] + \\overline{c_2} \\overline{\\lambda_2^k} \\overline{v_2[1]} \\\\\n&=c_1 \\lambda_1^k v_1[1] + c_2 \\lambda_2^k v_2[1] + \\overline{c_2\\lambda_2^k v_2[1]} \\\\\n\\end{aligned}\n\\]\nA number plus its complex conjugate is always real. Therefore, \\(x_k[1]\\) is real. The same logic holds for \\(x_k[2]\\) and \\(x_k[3]\\), so the population at time \\(k\\) will always have real entries.\n\nSince the matrix \\(A\\) has only real entries, all of its powers will also have only real entries. Therefore, the population at time \\(k\\) will always have real entries.\nIf \\(|\\lambda_1|&gt;|\\lambda_2|=|\\lambda_3|\\), then the population at time \\(k\\) will be dominated by the term \\(c_1 \\lambda_1^k \\mathbf{v_1}\\). If \\(\\lambda_1&gt;1\\), then the population will grow exponentially. If \\(\\lambda_1=1\\), then the population will stay constant and will approach \\(c_1 \\mathbf{v_1}\\). If \\(\\lambda_1&lt;1\\), then the population will decay exponentially."
  },
  {
    "objectID": "Exams/PracticeFinal.sol.html#section-1",
    "href": "Exams/PracticeFinal.sol.html#section-1",
    "title": "Practice Final Exam Solutions",
    "section": "2 ",
    "text": "2 \n\nConvert this difference equation into matrix–vector form.\n\n\\(y_{k+2} + y_{k+1} - 2y_{k} = 3\\)\n\nUsing the matrix, find \\(y_2\\) if it is given that \\(y_0 = 2\\) and \\(y_1 = 2\\).\n\n\nWe solve the equation above for \\(y_{k+2}\\) to get \\(y_{k+2} = 2y_{k} - y_{k+1} + 3\\).\nMake a vector of the unknowns: \\(x_k = [y_{k}, y_{k+1},1]^T\\). Then we have that \\(x_{k+1}=[y_{k+1},y_{k+2},1]^T\\).\nWe can now write three equations in terms of the vector \\(x_k\\) and \\(x_{k+1}\\):\n\\[\n\\begin{aligned}\nx_{k+1}[1] &= x_{k}[2] \\\\\nx_{k+1}[2] &= 2x_{k}[1] - x_{k}[2] + 3x_{k}[3] \\\\\nx_{k+1}[3] &= x_{k}[3]\n\\end{aligned}\n\\]\nThen the equation can be written as in the form \\(Ax_{k} = x_{k+1}\\) where \\(A\\) is a matrix given by\n\\[\nA = \\begin{bmatrix}\n0 & 1 & 0 \\\\\n2 & -1 & 3 \\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\]\nCheck using sympy:\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0\\\\2 x_{k0} - x_{k1} - x_{k2} + 3\\\\0\\end{matrix}\\right]\\)\n\n\nIt works!\nNow set up the initial conditions:\n\n\n\\(\\displaystyle \\left[\\begin{matrix}2\\\\5\\\\1\\end{matrix}\\right]\\)\n\n\nCheck this result: this tells us that \\(y_2 = 5\\) and that \\(y_1=2\\). We still have \\(y_0=2\\). Plugging this into the original equation,\n\\(y_{k+2} + y_{k+1} - 2y_{k} = 5+2-2(2)=3\\). It worked!"
  },
  {
    "objectID": "Exams/PracticeFinal.sol.html#section-2",
    "href": "Exams/PracticeFinal.sol.html#section-2",
    "title": "Practice Final Exam Solutions",
    "section": "3 ",
    "text": "3 \nA directed graph has vertices labeled \\(A\\), \\(B\\), \\(C\\), and \\(D\\). The edges are given by the following incidence matrix B:\n\n\n\\(\\displaystyle B = \\left[\\begin{matrix}-1 & -1 & 0 & 0 & 0 & 1\\\\1 & 0 & -1 & -1 & 0 & 0\\\\0 & 1 & 1 & 0 & -1 & 0\\\\0 & 0 & 0 & 1 & 1 & -1\\end{matrix}\\right]\\)\n\n\n\nFind the adjacency matrix \\(A\\) for the graph.\nDraw the graph.\nDescribe how you could find the number of paths of length \\(k\\) between each pair of vertices using the adjacency matrix \\(A\\).\nIf you calculate the matrix \\(A\\) + \\(A^2\\), what does the sum of the entries in the corresponding row tell you about the graph?\nLet \\(\\mathbf{v}\\) be a row vector representing a subset of the edges in the graph. For example, the subset of edges given by the vector \\(\\mathbf{v} = [1, 0, 1, 0, 0, 1]\\) would correspond to the first, third, and sixth edges.\nWrite an equation which will be satisfied whenever the subset of edges represented by \\(\\mathbf{v}\\) forms a loop in the graph.\nDescribe two algorithms for ranking the importance of the vertices in a graph. (One or more of these algorithms may be based on prior parts of this problem.)\n\n\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0 & 1 & 1 & 0\\\\0 & 0 & 1 & 1\\\\0 & 0 & 0 & 1\\\\1 & 0 & 0 & 0\\end{matrix}\\right]\\)\n\n\n\n\n/Users/kendra/Library/Python/3.8/lib/python/site-packages/networkx/drawing/nx_pylab.py:433: UserWarning:\n\nNo data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n\n\n\n\n\n\n\n\n\n\n\nThe number of paths of length \\(k\\) between each pair of vertices is given by the entries in the matrix \\(B^k\\).\nThe sum of the entries in the corresponding row of the matrix \\(A + A^2\\) gives the power of each vertex. In other words, it gives the number of paths of length 1 or 2 originating at each vertex.\nA loop in the graph is a subset of edges that form a closed path. A closed path is a path that starts and ends at the same vertex. Therefore, a subset of edges forms a loop if the multiple of the incidence matrix \\(B\\) and the vector \\(\\mathbf{v}\\) is equal to zero. In other words, the subset of edges represented by \\(\\mathbf{v}\\) forms a loop if \\(\\mathbf{v}B^T = 0\\), or equivalently if \\(B \\mathbf{v^T}=0\\). These definitions work if B has one column for each edge; if B has one row for each edge, then the equation is \\(B^T \\mathbf{v^T}=0\\).\nTwo algorithms for ranking the importance of the vertices in a graph are the PageRank algorithm the power method."
  },
  {
    "objectID": "Exams/PracticeFinal.sol.html#section-3",
    "href": "Exams/PracticeFinal.sol.html#section-3",
    "title": "Practice Final Exam Solutions",
    "section": "4 ",
    "text": "4 \nThe system of equations\n\\[\n\\begin{aligned}\nx_{1}+x_{2} & =2 \\\\\nx_{3}+x_{4} & =1 \\\\\n2 x_{1}+4 x_{2}+2 x_{4} & =6\n\\end{aligned}\n\\]\nhas coefficient matrix \\(A\\) and right hand side \\(\\mathbf{b}\\) such that the row-reduced echelon form of \\([A \\mid \\mathbf{b}]\\) is\n\\[\n\\left[\\begin{array}{cccc|c}1 & 0 & 0 & -1 & 1 \\\\ 0 & 1 & 0 & 1  & 1 \\\\ 0 & 0 & 1 & 1  & 1\\end{array}\\right]\n\\]\nUse this information to answer the following:\n\nFind a basis for the null space of \\(A\\) (not \\(A|b\\)!).\nFind the form of a general solution of the system \\(A \\mathbf{x}=\\mathbf{b}\\).\nFind the rank of \\(A\\). Discuss the meaning of the rank in terms of the independence of the columns of \\(A\\).\n\n\n\nTo find a basis for the null space of \\(A\\), we solve the equation \\(A\\mathbf{x} = \\mathbf{0}\\).\n\nWe know that \\(x_4\\) is a free variable, so we can write the solution in terms of \\(x_4\\):\n\\[\n\\begin{aligned}\nx_1 &= x_4 \\\\\nx_2 &= - x_4 \\\\\nx_3 &= - x_4 \\\\\nx_4 &= x_4\n\\end{aligned}\n\\]\nSo the basis for the nullspace of \\(A\\) is simply the vector \\(\\begin{bmatrix} 1 \\\\ -1 \\\\ -1 \\\\ 1 \\end{bmatrix}\\).\nChecking using sympy:\n\nA = sp.Matrix([[1,0,0,-1],[0,1,0,1],[0,0,1,1]])\nA.nullspace()\nb=sp.Matrix([1,1,1])\n\n\nThe general solution will be a particular solution plus a linear combination of the basis for the nullspace of \\(A\\).\n\nTo find a particular solution, we can set \\(x_4 = 0\\) and solve the system. Then we have that \\(x_1 = x_2 = x_3 = 1\\). Therefore, a particular solution is \\(\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\end{bmatrix}\\).\nThe general solution is then given by \\(\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\end{bmatrix} + x_4 \\begin{bmatrix} 1 \\\\ -1 \\\\ -1 \\\\ 1 \\end{bmatrix}\\).\n\nThe rank of \\(A\\) is the number of pivot columns in the row-reduced echelon form of \\(A\\). In this case, the rank of \\(A\\) is 3.\nThe rank of \\(A\\) is less than the number of columns in \\(A\\). This means that one of the columns of \\(A\\) is a linear combination of the other columns. In this case, the fourth column of \\(A\\) is a linear combination of the first three columns."
  },
  {
    "objectID": "Exams/PracticeFinal.sol.html#section-4",
    "href": "Exams/PracticeFinal.sol.html#section-4",
    "title": "Practice Final Exam Solutions",
    "section": "5 ",
    "text": "5 \nYou are given a matrix \\(A\\):\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0 & 1\\\\1 & 0\\\\1 & 1\\end{matrix}\\right]\\)\n\n\nThe eigenvalues of \\(AA^T\\) are \\(\\lambda_1 = 3, \\lambda_2 = 1, \\lambda_3 = 0\\).\n\nFind the eigenvalues of \\(A^TA\\).\nUsing the facts below, find the eigenvectors of \\(A^TA\\) and \\(AA^T\\), and explain your reasoning.\nFind the singular value decomposition of \\(A\\).\nUsing the SVD, calculate \\(A^+\\), the pseudoinverse of \\(A\\). Use this to solve the least squares problem \\(A\\mathbf{x} = \\mathbf{b}\\), where \\(\\mathbf{b} = [1, 4,2]^T\\).\nUsing the SVD, calculate \\(A^k \\mathbf{x}\\) for \\(k=3\\) and \\(\\mathbf{x} = [-2, 2, 0]^T\\).\n\nHere are some facts which may be helpful:\n\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & 0 & 1\\\\0 & 1 & 1\\\\1 & 1 & 2\\end{matrix}\\right]\\left[\\begin{matrix}-1\\\\-1\\\\1\\end{matrix}\\right]=\\left[\\begin{matrix}0\\\\0\\\\0\\end{matrix}\\right]\\)\n\n\n\\(\\displaystyle \\left[\\begin{matrix}-2 & 0 & 1\\\\0 & -2 & 1\\\\1 & 1 & -1\\end{matrix}\\right]\\left[\\begin{matrix}\\frac{1}{2}\\\\\\frac{1}{2}\\\\1\\end{matrix}\\right]=\\left[\\begin{matrix}0\\\\0\\\\0\\end{matrix}\\right]\\)\n\n\n\\(\\displaystyle  \\left[\\begin{matrix}0 & 0 & 1\\\\0 & 0 & 1\\\\1 & 1 & 1\\end{matrix}\\right]\\left[\\begin{matrix}-1\\\\1\\\\0\\end{matrix}\\right]=\\left[\\begin{matrix}0\\\\0\\\\0\\end{matrix}\\right]\\)\n\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & 1\\\\1 & 1\\end{matrix}\\right]\\left[\\begin{matrix}-1\\\\1\\end{matrix}\\right]=\\left[\\begin{matrix}0\\\\0\\end{matrix}\\right]\\)\n\n\n\\(\\displaystyle \\left[\\begin{matrix}-1 & 1\\\\1 & -1\\end{matrix}\\right]\\left[\\begin{matrix}1\\\\1\\end{matrix}\\right]=\\left[\\begin{matrix}0\\\\0\\end{matrix}\\right]\\)\n\n\n\n\nWe first find the characteristic polynomial of \\(AA^T\\): we find \\(\\text{det}(AA^T - \\lambda I)\\) and set it equal to zero.\n\n\n\n\\(\\displaystyle \\lambda^{2} - 4 \\lambda + 3=0\\)\n\n\nWe try the three eigenvalues given in the question: 0, 1, and 3. We find that 0 is not an eigenvalue, but that 1 and 3 satisfy the equation. Therefore, the eigenvalues of \\(AA^T\\) are \\(\\lambda_1 = 3, \\lambda_2 = 1\\).\n\nThe eigenvectors of \\(AA^T\\) are given by the nullspace of \\(AA^T - \\lambda I\\). We can find the eigenvectors by solving the equation \\((AA^T - \\lambda I)v = 0\\). These are what are given to us in the question, so the eigenvectors are\n\n\n\n\\(\\displaystyle \\left[\\begin{matrix}-1\\\\-1\\\\1\\end{matrix}\\right]\\)\n\n\n\\(\\displaystyle \\left[\\begin{matrix}-1\\\\1\\\\0\\end{matrix}\\right]\\)\n\n\n\\(\\displaystyle \\left[\\begin{matrix}\\frac{1}{2}\\\\\\frac{1}{2}\\\\1\\end{matrix}\\right]\\)\n\n\nThe eigenvectors of \\(A^TA\\) are given by the nullspace of \\(A^TA - \\lambda I\\). We can find the eigenvectors by solving the equation \\((A^TA - \\lambda I)u = 0\\). These are what are given to us in the question, so the eigenvectors are\n\n\n\\(\\displaystyle \\left[\\begin{matrix}-1\\\\1\\end{matrix}\\right]\\)\n\n\n\\(\\displaystyle \\left[\\begin{matrix}1\\\\1\\end{matrix}\\right]\\)\n\n\n\nThe singular value decomposition of \\(A\\) is given by \\(A = U\\Sigma V^T\\), where \\(U\\) is the matrix of eigenvectors of \\(A^TA\\), \\(V\\) is the matrix of eigenvectors of \\(AA^T\\), and \\(\\Sigma\\) is the diagonal matrix of the square roots of the eigenvalues of \\(A^TA\\).\nThe pseudoinverse of \\(A\\) is given by \\(A^+ = V\\Sigma^+ U^T\\), where \\(\\Sigma^+\\) is the diagonal matrix of the reciprocals of the non-zero elements of \\(\\Sigma\\). We can use this to solve the least squares problem \\(A\\mathbf{x} = \\mathbf{b}\\)."
  },
  {
    "objectID": "Exams/PracticeFinal.sol.html#section-5",
    "href": "Exams/PracticeFinal.sol.html#section-5",
    "title": "Practice Final Exam Solutions",
    "section": "6 ",
    "text": "6 \n\nFor the matrix \\(A = \\begin{bmatrix} 9 & 8 \\\\ -6 & -5 \\end{bmatrix}\\), find the eigenvalues and eigenvectors.\nUsing these results, find the matrix \\(P\\) such that \\(P^{-1}AP\\) is a diagonal matrix.\n\n(Note: for a 2x2 matrix \\(\\begin{bmatrix}a&b\\\\c&d\\end{bmatrix}\\), you can calculate the inverse as \\(\\frac{1}{ad-bc}\\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}\\).)\n\nDiagonalize the matrix \\(A^4+2A^2-3I\\). (It’s fine to leave exponents in the answer, e.g. \\(5^{6}\\).)\nFind the nullspace of the matrix \\(A^4+2A^2-3I\\).\n\n\n\n\n[(1,\n  1,\n  [Matrix([\n   [-1],\n   [ 1]])]),\n (3,\n  1,\n  [Matrix([\n   [-4/3],\n   [   1]])])]\n\n\nP = \nD = \n\n\n\\(\\displaystyle \\left[\\begin{matrix}-1 & -4\\\\1 & 3\\end{matrix}\\right]\\)\n\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & 0\\\\0 & 3\\end{matrix}\\right]\\)\n\n\nWe can diagonalize \\(A^4+2A^2-3I\\) by noting that we find \\(A^4\\) by taking the 4th power of the diagonal terms in \\(D\\), and similarly for \\(A^2\\).\nWe also need to write \\(I\\) in terms of \\(P\\) and \\(D\\). Note that \\(P I P^{-1} = I\\). So we can write the entire sum as \\(P \\left( D^4 + 2D^2 - 3I \\right) P^{-1}\\).\nChecking in sympy:\n\n\nTrue"
  },
  {
    "objectID": "Exams/PracticeFinal.sol.html#section-6",
    "href": "Exams/PracticeFinal.sol.html#section-6",
    "title": "Practice Final Exam Solutions",
    "section": "7 ",
    "text": "7 \nThe Discrete Fourier Transform (DFT) of a sequence \\(x[n]\\) is given by \\(X[k] = \\sum_{n=0}^{N-1} x[n] e^{-i2\\pi kn/N}\\), where \\(N\\) is the length of the sequence, and where \\(k\\) runs from \\(0\\) to \\(N-1\\). The inverse DFT is given by \\(x[n] = \\frac{1}{N} \\sum_{k=0}^{N-1} X[k] e^{i2\\pi kn/N}\\).\n\nFind the DFT of the filter \\(h[n] = [\\frac{1}{3},-\\frac{1}{3},\\frac{1}{3},0]\\).\nUsing this DFT, find the circular convolution of the sequences \\(x[n] = [1,2,3,4]\\) and \\(h[n]\\). Note: the DFT of \\(x\\) is \\(X=[10,-2+2i,-2,-2-2i]\\).\n\n(Circular convolution is convolution where we assume the sequences are periodic, so that \\(x[n] = x[n+N]\\). It’s as if we had the long sequence \\(x[n] = [\\color{blue}1,2,3,4,\\color{black}1,2,3,4,\\color{blue}1,2,3,4,\\ldots]\\) to use when calculating the convolution. When we discussed convolution using the DFT in class, we were actually doing circular convolution. This matters if you want to check your results doing the convolution by hand without using the DFT.)\n\nFrom the plots below, match the signals to the correct DFTs. (Curve y3 is actually a quadratic. I have plotted only the absolute value of the DFTs. The complete DFT also includes negative frequencies, not shown here, which are the complex conjugates of the positive frequencies.)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe DFT of the filter \\(h[n] = [\\frac{1}{3},-\\frac{1}{3},\\frac{1}{3},0]\\) is given by \\(H[k] = \\sum_{n=0}^{3} h[n] e^{-i2\\pi kn/4}\\) This equals \\(H=[1/3, i/3, 1, -i/3]\\).\nThe circular convolution of the sequences \\(x[n] = [1,2,3,4]\\) and \\(h[n]\\) is given by the inverse Fourier transform of the element-wise product \\(X[k]H[k]\\). This equals\n\n\\([10,-2+2i,-2,-2-2i]\\times[1/3, i/3, 1, -i/3]=[10/3, -2/3-2/3i, -2, -2/3+2/3i]\\).\nThe inverse transform is \\([0,5,2,3]/3\\).\n\nThe DFT of signal \\(y\\) is \\(b\\), the DFT of signal \\(y2\\) is \\(c\\), and the DFT of signal \\(y3\\) is \\(a\\). We can tell because \\(y2\\) is a simple sin curve, so its DFT is a single peak. \\(y\\) appears to be a sum of two sin curves, so its DFT has two peaks. \\(y3\\) has many frequency components, but they are low frequency, so the DFT has a peak near 0."
  },
  {
    "objectID": "Exams/FinalExam.html",
    "href": "Exams/FinalExam.html",
    "title": "Final Exam",
    "section": "",
    "text": "Convert this difference equation into matrix–vector form.\n\\(2y_{k+3} + 2y_{k+1} - 3y_{k} = 0\\)"
  },
  {
    "objectID": "Exams/FinalExam.html#section",
    "href": "Exams/FinalExam.html#section",
    "title": "Final Exam",
    "section": "",
    "text": "Convert this difference equation into matrix–vector form.\n\\(2y_{k+3} + 2y_{k+1} - 3y_{k} = 0\\)"
  },
  {
    "objectID": "Exams/FinalExam.html#section-1",
    "href": "Exams/FinalExam.html#section-1",
    "title": "Final Exam",
    "section": "2 ",
    "text": "2 \nA population is modeled with three stages: larva, pupa and adult, and the resulting structured population model transition matrix is\n\\[\nA = \\begin{bmatrix}\n0 & 0 & 0.6 \\\\\n0.5 & 0 & 0 \\\\\n0 & 0.9 & 0\n\\end{bmatrix}\n\\]\nPopulation changes are modeled as \\(\\mathbf{x_{k+1}} = A\\mathbf{x_k}\\), where \\(\\mathbf{x_k}=\\begin{bmatrix}\\text{larva}_k \\\\ \\text{pupa}_k \\\\ \\text{adult}_k \\end{bmatrix}\\) denotes the number of individuals at each state in time period \\(k\\).\n\nExplain in words what this model says about the three states - for instance, “Every year, X% of the larva become…”\nStarting with a population of \\(\\mathbf{x_0}=\\begin{bmatrix}0 \\\\ 30 \\\\ 100\\end{bmatrix}\\), calculate the population for \\(k=1\\) and \\(k=2\\).\nThe eigenvalues of \\(A\\) are \\(\\lambda_1 = 0.646, \\lambda_2 = -0.323 - 0.560i, \\lambda_3 = -0.323 + 0.560i\\). What do these eigenvalues tell you about the long-term behavior of the population?"
  },
  {
    "objectID": "Exams/FinalExam.html#section-2",
    "href": "Exams/FinalExam.html#section-2",
    "title": "Final Exam",
    "section": "3 ",
    "text": "3 \nThe graph G is given below.\n\n\n\n\n\n\n\n\n\n\nFind the adjacency matrix \\(A\\) and the incidence matrix \\(B\\) for the graph G.\nFrom \\(A\\) and \\(B\\), match the following descriptions to the corresponding quantities. (There are only four descriptions, so you will not use all of the quantities.)\n\nDescriptions: (i) the number of outgoing edges from each vertex, (ii) the power of each vertex (the number of paths of length 1 or 2 originating at each vertex) (iii) the number of paths of length \\(k\\) between each pair of vertices (iv) a basis for loops in the graph (don’t worry about the direction of the edges)\nQuantities: (a) Sum of the entries in the corresponding row of \\(A\\), (b) Entries in the matrix \\(A^k\\), (c) Nullspace of the matrix \\(B^T\\). (d) Entries in the matrix \\(B^k\\). (e) Sum of the entries in the corresponding row of \\(A + A^2\\), (f) Nullspace of the matrix \\(A^T\\)."
  },
  {
    "objectID": "Exams/FinalExam.html#section-3",
    "href": "Exams/FinalExam.html#section-3",
    "title": "Final Exam",
    "section": "4 ",
    "text": "4 \nFind a least squares solution to the following system:\n\\[\n\\begin{bmatrix}\n1 & 1 \\\\\n1 & 1 \\\\\n1 & 2 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}\n\\]\nIs the solution a genuine solution?"
  },
  {
    "objectID": "Exams/FinalExam.html#section-4",
    "href": "Exams/FinalExam.html#section-4",
    "title": "Final Exam",
    "section": "5 ",
    "text": "5 \nLet \\(A=\\left[\\begin{array}{ccc}1 & 0 & 1 \\\\ -1 & 1 & 1 \\\\ 2 & 1 & 4\\end{array}\\right]\\).\n\nFind the reduced row-echelon form and rank of \\(A\\).\nFind a basis for the column space of \\(A\\).\nDetermine which of the following vectors is in the column space of \\(A\\) and, if so, express the vector as a linear combination of the columns of \\(A\\) :\n\n\\(b_{1}=[2,1,0]^{T}\\), in \\(b_{2}=[2,-3,3]^{T}\\).\nYou may find it helpful to use the following outputs from sympy:\nA.gauss_jordan_solve(b1): ValueError: Linear system has no solution\nA.gauss_jordan_solve(b2): \\(\\begin{bmatrix} 2 - \\tau_0 \\\\ -2 \\tau_0 -1 \\\\ \\tau_0 \\end{bmatrix}\\)"
  },
  {
    "objectID": "Exams/FinalExam.html#section-5",
    "href": "Exams/FinalExam.html#section-5",
    "title": "Final Exam",
    "section": "6 ",
    "text": "6 \nYou have found the singular value decomposition of the real \\(m\\times n\\) matrix \\(A\\) to be \\(A = U\\Sigma V^T\\), where the nonzero elements of \\(\\Sigma\\) are \\(\\sigma_1, \\sigma_2, \\ldots  \\sigma_r\\). Let \\(\\mathbf{v_1}\\) be the first column of \\(V\\), \\(\\mathbf{u_1}\\) be the first column of \\(U\\), and so on.\n\nFind \\(A^T A \\mathbf{v_1}\\)\nFind \\(A \\mathbf{v_1}\\)\nFind \\(\\mathbf{v_1}^T \\mathbf{v_2}\\)\nFind \\(\\mathbf{u_1}^T A \\mathbf{v_2}\\)\nSuppose that \\(r = n \\le m\\); that is, the number of nonzero singular values is equal to the number of columns of \\(A\\), which is less or equal to the number of rows. Describe the solution set to the problem \\(A\\mathbf{x} = 0\\).\nFind the least-squares solution to the system \\(A\\mathbf{x} = \\mathbf{b}\\)."
  },
  {
    "objectID": "Exams/FinalExam.html#section-6",
    "href": "Exams/FinalExam.html#section-6",
    "title": "Final Exam",
    "section": "7 ",
    "text": "7 \nThe Discrete Fourier Transform (DFT) of a sequence \\(x[n]\\) is given by \\(X[k] = \\sum_{n=0}^{N-1} x[n] e^{-i2\\pi kn/N}\\), where \\(N\\) is the length of the sequence, and where \\(k\\) runs from \\(0\\) to \\(N-1\\).\n\nFind the DFT of the sequence \\(x[n] = [1,-1,1,-1]\\).\nAssume that \\(x[n]\\) is a sample from a continuous signal \\(x(t)\\), where samples were taken at times \\(t=[0,1,2,3]\\) seconds. Make a sketch which includes a plausible continuous signal \\(x(t)\\), including the values of \\(x(t)\\) at the points \\(t=0,1,2,3\\). Argue from this sketch why the DFT of \\(x[n]\\) has the form it does."
  },
  {
    "objectID": "Exams/FinalExam.sol.html",
    "href": "Exams/FinalExam.sol.html",
    "title": "Final Exam Solutions",
    "section": "",
    "text": "Convert this difference equation into matrix–vector form.\n\\(2y_{k+3} + 2y_{k+1} - 3y_{k} = 0\\)\n\nWe solve the equation above for \\(y_{k+3}\\) to get \\(y_{k+3} = \\frac{3y_{k} - 2y_{k+1}}{2}\\).\nMake a vector of the unknowns: \\(x_k = [y_{k}, y_{k+1}, y_{k+2}]^T\\). Then we have that \\(x_{k+1}=[y_{k+1},y_{k+2},y_{k+3}]^T\\).\nWe can now write three equations in terms of the vector \\(x_k\\) and \\(x_{k+1}\\):\n\\[\n\\begin{aligned}\nx_{k+1}[1] &= x_{k}[2] \\\\\nx_{k+1}[2] &= x_{k}[3] \\\\\nx_{k+1}[3] &= \\frac{3x_{k}[1] - 2x_{k}[2]}{2}\n\\end{aligned}\n\\]\nThen the equation can be written as in the form \\(Ax_{k} = x_{k+1}\\) where \\(A\\) is a matrix given by\n\\[\nA = \\begin{bmatrix}\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n\\frac{-3}{2} & -1 & 0\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "Exams/FinalExam.sol.html#section",
    "href": "Exams/FinalExam.sol.html#section",
    "title": "Final Exam Solutions",
    "section": "",
    "text": "Convert this difference equation into matrix–vector form.\n\\(2y_{k+3} + 2y_{k+1} - 3y_{k} = 0\\)\n\nWe solve the equation above for \\(y_{k+3}\\) to get \\(y_{k+3} = \\frac{3y_{k} - 2y_{k+1}}{2}\\).\nMake a vector of the unknowns: \\(x_k = [y_{k}, y_{k+1}, y_{k+2}]^T\\). Then we have that \\(x_{k+1}=[y_{k+1},y_{k+2},y_{k+3}]^T\\).\nWe can now write three equations in terms of the vector \\(x_k\\) and \\(x_{k+1}\\):\n\\[\n\\begin{aligned}\nx_{k+1}[1] &= x_{k}[2] \\\\\nx_{k+1}[2] &= x_{k}[3] \\\\\nx_{k+1}[3] &= \\frac{3x_{k}[1] - 2x_{k}[2]}{2}\n\\end{aligned}\n\\]\nThen the equation can be written as in the form \\(Ax_{k} = x_{k+1}\\) where \\(A\\) is a matrix given by\n\\[\nA = \\begin{bmatrix}\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n\\frac{-3}{2} & -1 & 0\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "Exams/FinalExam.sol.html#section-1",
    "href": "Exams/FinalExam.sol.html#section-1",
    "title": "Final Exam Solutions",
    "section": "2 ",
    "text": "2 \nA population is modeled with three stages: larva, pupa and adult, and the resulting structured population model transition matrix is\n\\[\nA = \\begin{bmatrix}\n0 & 0 & 0.6 \\\\\n0.5 & 0 & 0 \\\\\n0 & 0.9 & 0\n\\end{bmatrix}\n\\]\nPopulation changes are modeled as \\(\\mathbf{x_{k+1}} = A\\mathbf{x_k}\\), where \\(\\mathbf{x_k}=\\begin{bmatrix}\\text{larva}_k \\\\ \\text{pupa}_k \\\\ \\text{adult}_k \\end{bmatrix}\\) denotes the number of individuals at each state in time period \\(k\\).\n\nExplain in words what this model says about the three states - for instance, “Every year, X% of the larva become…”\nStarting with a population of \\(\\mathbf{x_0}=\\begin{bmatrix}0 \\\\ 30 \\\\ 100\\end{bmatrix}\\), calculate the population for \\(k=1\\) and \\(k=2\\).\nThe eigenvalues of \\(A\\) are \\(\\lambda_1 = 0.646, \\lambda_2 = -0.323 - 0.560i, \\lambda_3 = -0.323 + 0.560i\\). What do these eigenvalues tell you about the long-term behavior of the population?\n\n\n\nThe model says that the adult has a 60% chance of creating a larva (but will die every cycle), the larva has a 50% chance of becoming a pupa, and the pupa has a 90% chance of becoming an adult.\nWe can calculate the population in the next two time steps by multiplying the matrix \\(A\\) with the vector \\(x_k = [0, 30, 100]^T\\).\n\n\n\n(array([60.,  0., 27.]), array([16.2, 30. ,  0. ]))\n\n\n\nThe largest eigenvalue is less than one. This matrix will not have a stable equilibrium point."
  },
  {
    "objectID": "Exams/FinalExam.sol.html#section-2",
    "href": "Exams/FinalExam.sol.html#section-2",
    "title": "Final Exam Solutions",
    "section": "3 ",
    "text": "3 \nThe graph G is given below.\n\n\n\n\n\n\n\n\n\n\nFind the adjacency matrix \\(A\\) and the incidence matrix \\(B\\) for the graph G.\nFrom \\(A\\) and \\(B\\), match the following descriptions to the corresponding quantities. (There are only four descriptions, so you will not use all of the quantities.)\n\nDescriptions: (i) the number of outgoing edges from each vertex, (ii) the power of each vertex (the number of paths of length 1 or 2 originating at each vertex) (iii) the number of paths of length \\(k\\) between each pair of vertices (iv) a basis for loops in the graph (don’t worry about the direction of the edges)\nQuantities: (a) Sum of the entries in the corresponding row of \\(A\\), (b) Entries in the matrix \\(A^k\\), (c) Nullspace of the matrix \\(B^T\\). (d) Entries in the matrix \\(B^k\\). (e) Sum of the entries in the corresponding row of \\(A + A^2\\), (f) Nullspace of the matrix \\(A^T\\).\n\n\nThe adjacency matrix \\(A\\) is given by\n\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0 & 1 & 1 & 0\\\\0 & 0 & 1 & 1\\\\0 & 0 & 0 & 1\\\\0 & 0 & 0 & 0\\end{matrix}\\right]\\)\n\n\nThe incidence matrix \\(B\\) is given by\n\n\n\\(\\displaystyle \\left[\\begin{matrix}-1 & -1 & 0 & 0 & 0\\\\1 & 0 & -1 & -1 & 0\\\\0 & 1 & 1 & 0 & -1\\\\0 & 0 & 0 & 1 & 1\\end{matrix}\\right]\\)\n\n\n\n\nThe degree of each vertex is given by the sum of the entries in the corresponding row of the adjacency matrix \\(A\\).\n\n\nThe power of each vertex is given by the sum of the entries in the corresponding row of the matrix \\(A + A^2\\).\nThe number of paths of length \\(k\\) between each pair of vertices is given by the entries in the matrix \\(A^k\\).\nA basis for loops in the graph is given by the nullspace of the matrix \\(B^T\\)."
  },
  {
    "objectID": "Exams/FinalExam.sol.html#section-3",
    "href": "Exams/FinalExam.sol.html#section-3",
    "title": "Final Exam Solutions",
    "section": "4 ",
    "text": "4 \nFind a least squares solution to the following system:\n\\[\n\\begin{bmatrix}\n1 & 1 \\\\\n1 & 1 \\\\\n1 & 2 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}\n\\]\nIs the solution a genuine solution?\n\nFirst we find the least squares solution by solving the normal equations \\(A^TAx = A^Tb\\).\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0\\\\\\frac{3}{2}\\end{matrix}\\right]\\)\n\n\nThe solution is \\(x = \\begin{bmatrix} 1.5 \\\\ 0.5 \\end{bmatrix}\\).\nTo check if this is a genuine solution, we substitute the values of \\(x\\) back into the original equation.\n\n\n\\(\\displaystyle \\left[\\begin{matrix}\\frac{3}{2}\\\\\\frac{3}{2}\\\\3\\end{matrix}\\right]\\)\n\n\nThe result is \\(\\begin{bmatrix} 1.5 \\\\ 1.5 \\\\ 3.0 \\end{bmatrix}\\), which is not equal to \\(\\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}\\). Therefore, the solution is not a genuine solution."
  },
  {
    "objectID": "Exams/FinalExam.sol.html#section-4",
    "href": "Exams/FinalExam.sol.html#section-4",
    "title": "Final Exam Solutions",
    "section": "5 ",
    "text": "5 \nLet \\(A=\\left[\\begin{array}{ccc}1 & 0 & 1 \\\\ -1 & 1 & 1 \\\\ 2 & 1 & 4\\end{array}\\right]\\).\n\nFind the reduced row-echelon form and rank of \\(A\\).\nFind a basis for the column space of \\(A\\).\nDetermine which of the following vectors is in the column space of \\(A\\) and, if so, express the vector as a linear combination of the columns of \\(A\\) :\n\n\\(b_{1}=[2,1,0]^{T}\\), in \\(b_{2}=[2,-3,3]^{T}\\).\nYou may find it helpful to use the following outputs from sympy:\nA.gauss_jordan_solve(b1): ValueError: Linear system has no solution\nA.gauss_jordan_solve(b2): \\(\\begin{bmatrix} 2 - \\tau_0 \\\\ -2 \\tau_0 -1 \\\\ \\tau_0 \\end{bmatrix}\\)\n\n\nThe reduced row-echelon form of \\(A\\) is:\n\n\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & 0 & 1\\\\0 & 1 & 2\\\\0 & 0 & 0\\end{matrix}\\right]\\)\n\n\n\nThe basis for the column space of A is given by the pivot columns of the row-reduced echelon form of A. In this case, the basis is given by the first two columns of A.\n\n\n\n\\(\\displaystyle \\left[\\begin{matrix}1\\\\-1\\\\2\\end{matrix}\\right]\\)\n\n\n\\(\\displaystyle \\left[\\begin{matrix}0\\\\1\\\\1\\end{matrix}\\right]\\)\n\n\n\nTo determine if \\(b_1\\) is in the column space of A, we solve the equation \\(A\\mathbf{x} = b_1\\). and \\(b_2\\). The first has no solution, so it’s not in the column space. The second has a solution, so it is in the column space. The solution is \\(\\begin{bmatrix} 2 - \\tau_0 \\\\ -2 \\tau_0 -1 \\\\ \\tau_0 \\end{bmatrix}\\).\n\nWe pick \\(\\tau_0 = 0\\) to get that we can combine twice the first column and -1 times the second column to get \\(b_2\\).\n\n\n(Matrix([\n [   2 - tau0],\n [-2*tau0 - 1],\n [       tau0]]),\n Matrix([[tau0]]))"
  },
  {
    "objectID": "Exams/FinalExam.sol.html#section-5",
    "href": "Exams/FinalExam.sol.html#section-5",
    "title": "Final Exam Solutions",
    "section": "6 ",
    "text": "6 \nYou have found the singular value decomposition of the real \\(m\\times n\\) matrix \\(A\\) to be \\(A = U\\Sigma V^T\\), where the nonzero elements of \\(\\Sigma\\) are \\(\\sigma_1, \\sigma_2, \\ldots  \\sigma_r\\). Let \\(\\mathbf{v_1}\\) be the first column of \\(V\\), \\(\\mathbf{u_1}\\) be the first column of \\(U\\), and so on.\n\nFind \\(A^T A \\mathbf{v_1}\\)\nFind \\(A \\mathbf{v_1}\\)\nFind \\(\\mathbf{v_1}^T \\mathbf{v_2}\\)\nFind \\(\\mathbf{u_1}^T A \\mathbf{v_2}\\)\nSuppose that \\(r = n \\le m\\); that is, the number of nonzero singular values is equal to the number of columns of \\(A\\), which is less or equal to the number of rows. Describe the solution set to the problem \\(A\\mathbf{x} = 0\\).\nFind the least-squares solution to the system \\(A\\mathbf{x} = \\mathbf{b}\\).\n\n\nBecause the rank of \\(A\\) is \\(r\\), the nullspace of \\(A\\) is spanned by the vectors \\(\\mathbf{v_{r+1}}, \\ldots, \\mathbf{v_n}\\). But since \\(r = n\\), the nullspace of \\(A\\) is spanned by the zero vector. Therefore, the solution set to the problem \\(A\\mathbf{x} = 0\\) is the zero vector."
  },
  {
    "objectID": "Exams/FinalExam.sol.html#section-6",
    "href": "Exams/FinalExam.sol.html#section-6",
    "title": "Final Exam Solutions",
    "section": "7 ",
    "text": "7 \nThe Discrete Fourier Transform (DFT) of a sequence \\(x[n]\\) is given by \\(X[k] = \\sum_{n=0}^{N-1} x[n] e^{-i2\\pi kn/N}\\), where \\(N\\) is the length of the sequence, and where \\(k\\) runs from \\(0\\) to \\(N-1\\).\n\nFind the DFT of the sequence \\(x[n] = [1,-1,1,-1]\\).\nAssume that \\(x[n]\\) is a sample from a continuous signal \\(x(t)\\), where samples were taken at times \\(t=[0,1,2,3]\\) seconds. Make a sketch which includes a plausible continuous signal \\(x(t)\\), including the values of \\(x(t)\\) at the points \\(t=0,1,2,3\\). Argue from this sketch why the DFT of \\(x[n]\\) has the form it does.\n\n\n\nThe DFT of the sequence \\(x[n] = [1,-1,1,-1]\\) is given by \\(X[k] = \\sum_{n=0}^{3} x[n] e^{-i2\\pi kn/4}\\).\n\nFor \\(k\\)=0, this becomes \\(X[0]=1-1+1-1=0\\).\nFor \\(k\\)=1, this becomes \\(X[1]=1-e^{-2i\\pi/4}+e^{-4i\\pi/4}-e^{-6i\\pi/4}=1+0-1+0=0\\).\nFor \\(k\\)=2, this becomes \\(X[2]=1-e^{-4i\\pi/4}+e^{-8i\\pi/4}-e^{-12i\\pi/4}=1+1+1-1=4\\).\nFor \\(k\\)=3, this becomes \\(X[3]=1-e^{-6i\\pi/4}+e^{-12i\\pi}-e^{-18i\\pi}=1+0-1+0=0\\).\n\n\n\n\n\n\n\n\n\n\n\n\nThe sampled signal matches a cosine curve with a frequency of 0.5 Hz.\nThe DFT measures frequencies that are 0, 1, 2, and 3 times the frequency which would correspond to a complete cycle in the entire sampled period of 4s. One cycle in 4s is 0.25 Hz.\nTherefore, the DFT measures frequencies at 0 Hz, 0.25 Hz, 0.5 Hz, and 0.75 Hz.\nThe peak at 0.5 Hz is the largest because the signal is a cosine curve with a frequency of 0.5 Hz."
  }
]