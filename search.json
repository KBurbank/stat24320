[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stat 24320",
    "section": "",
    "text": "Title\n            \n                \n                    Lecture Day\n                \n                \n                \n                    Readings\n                \n                \n                    Colab Link\n        \n        \n                    \n                        \n                            Intro to linear systems\n                        \n                    \n\n                    \n                        \n                            1\n                        \n                        \n                        \n                            Ch. 1.1-1.3\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            More Systems of Linear Equations\n                        \n                    \n\n                    \n                        \n                            2\n                        \n                        \n                        \n                            Ch. 1.4-1.5\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch2 Lecture 1\n                        \n                    \n\n                    \n                        \n                            3\n                        \n                        \n                        \n                            2.1-2.3\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch2 Lecture 2\n                        \n                    \n\n                    \n                        \n                            4\n                        \n                        \n                        \n                            2.3-2.4\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch2 Lecture 3\n                        \n                    \n\n                    \n                        \n                            5\n                        \n                        \n                        \n                            2.3-2.8\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch2 Lecture 4\n                        \n                    \n\n                    \n                        \n                            5\n                        \n                        \n                        \n                            2.8\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n    \n\nNo matching items"
  },
  {
    "objectID": "index.html#lecture-notes",
    "href": "index.html#lecture-notes",
    "title": "Stat 24320",
    "section": "",
    "text": "Title\n            \n                \n                    Lecture Day\n                \n                \n                \n                    Readings\n                \n                \n                    Colab Link\n        \n        \n                    \n                        \n                            Intro to linear systems\n                        \n                    \n\n                    \n                        \n                            1\n                        \n                        \n                        \n                            Ch. 1.1-1.3\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            More Systems of Linear Equations\n                        \n                    \n\n                    \n                        \n                            2\n                        \n                        \n                        \n                            Ch. 1.4-1.5\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch2 Lecture 1\n                        \n                    \n\n                    \n                        \n                            3\n                        \n                        \n                        \n                            2.1-2.3\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch2 Lecture 2\n                        \n                    \n\n                    \n                        \n                            4\n                        \n                        \n                        \n                            2.3-2.4\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch2 Lecture 3\n                        \n                    \n\n                    \n                        \n                            5\n                        \n                        \n                        \n                            2.3-2.8\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch2 Lecture 4\n                        \n                    \n\n                    \n                        \n                            5\n                        \n                        \n                        \n                            2.8\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n    \n\nNo matching items"
  },
  {
    "objectID": "index.html#notebooks",
    "href": "index.html#notebooks",
    "title": "Stat 24320",
    "section": "Notebooks",
    "text": "Notebooks\n\n\n    \n            \n                \n        \n            Title\n            \n                \n                    Lecture Day\n                \n                \n                    Colab Link\n        \n        \n                    \n                        \n                            PageRank Tutorial\n                        \n                    \n\n                    \n                        \n                            1\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Turing Patterns\n                        \n                    \n\n                    \n                        \n                            4\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            MCMC Pagerank\n                        \n                    \n\n                    \n                        \n                            6\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n    \n\nNo matching items"
  },
  {
    "objectID": "index.html#homeworks",
    "href": "index.html#homeworks",
    "title": "Stat 24320",
    "section": "Homeworks",
    "text": "Homeworks\n\n\n            \n                \n        \n            Title\n            \n                \n                    Due Date\n                \n                \n                \n                    Textbook Chapters\n                \n                \n                    Solutions\n        \n        \n                    \n                        \n                            Homework 1\n                        \n                    \n\n                    \n                        \n                            Wednesday, Week 2\n                        \n                        \n                        \n                            1\n                        \n                        \n                            \n                           \n                            \n                                \n                                        Homework 1 Solutions\n                                    \n                            \n\n                \n\n                \n                \n                    \n                        \n                            Homework 2\n                        \n                    \n\n                    \n                        \n                            Wednesday, Week 3\n                        \n                        \n                        \n                            2\n                        \n                        \n                            \n                           \n                            \n                                \n                                        Homework 2 Solutions\n                                    \n                            \n\n                \n\n                \n                \n                    \n                        \n                            Homework 3\n                        \n                    \n\n                    \n                        \n                            Wednesday, Week 4\n                        \n                        \n                        \n                            2.4, 2.5, 2.8\n                        \n                        \n                            \n                           \n                            \n\n                \n\n                \n                \n                    \n                        \n                            Projects 1\n                        \n                    \n\n                    \n                        \n                            Friday, Week 4\n                        \n                        \n                        \n                            1, 2\n                        \n                        \n                            \n                           \n                            \n\n                \n\n                \n    \n\nNo matching items"
  },
  {
    "objectID": "lectures/ch2_lecture3.html#difference-equations-1",
    "href": "lectures/ch2_lecture3.html#difference-equations-1",
    "title": "Ch2 Lecture 3",
    "section": "Difference Equations",
    "text": "Difference Equations"
  },
  {
    "objectID": "lectures/ch2_lecture3.html#digital-filters",
    "href": "lectures/ch2_lecture3.html#digital-filters",
    "title": "Ch2 Lecture 3",
    "section": "Digital Filters",
    "text": "Digital Filters\n\nAnother use for difference equations:\n\\[\n\\begin{equation*}\ny_{k}=a_{0} x_{k}+a_{1} x_{k-1}+\\cdots+a_{m} x_{k-m}, \\quad k=m, m+1, m+2, \\ldots,\n\\end{equation*}\n\\]\n\nBefore, in the case of temperature in the metal rod, we were trying to find values of \\(x\\) that would satisfy the equation. Here, different context: imagine the \\(x\\)’s are known inputs, perhaps discrete samples of a continuous variable, and \\(y\\) are outputs that we want to calculate, such as a filter.\n\n\n\\(x\\) is a continuous variable – perhaps sound in time domain, or parts of an image in space.\n\\(y\\) is some filtered version of \\(x\\)."
  },
  {
    "objectID": "lectures/ch2_lecture3.html#example-cleaning-up-a-noisy-signal",
    "href": "lectures/ch2_lecture3.html#example-cleaning-up-a-noisy-signal",
    "title": "Ch2 Lecture 3",
    "section": "Example: Cleaning up a noisy signal",
    "text": "Example: Cleaning up a noisy signal\nTrue signal: \\(f(t)=\\cos (\\pi t),-1 \\leq t \\leq 1\\)\nSignal plus noise: \\(g(t)=\\cos (\\pi t)+\\) \\(\\frac{1}{5} \\sin (24 \\pi t)+\\frac{1}{4} \\cos (30 \\pi t)\\)\n\n\n\n\n\n\n\n\n\n\nSuppose the exact signal we want is the line in black, and we’d like to measure it. But all we have is the noisy set of measurements in red.\nNotice the low-frequency part of the measurement is the signal, and the high-frequency part is the noise.\n\nHow can we use this fact to build a good filter?"
  },
  {
    "objectID": "lectures/ch2_lecture3.html#filtered-data",
    "href": "lectures/ch2_lecture3.html#filtered-data",
    "title": "Ch2 Lecture 3",
    "section": "Filtered data",
    "text": "Filtered data\nIdea: we can sample nearby points and take a weighted average of them.\n\\[\ny_{k}=\\frac{1}{4} x_{k+1}+\\frac{1}{2} x_{k}+\\frac{1}{4} x_{k-1}, \\quad k=1,2, \\ldots, 63\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nThis captured the low-frequency part of the signal, and filtered out the high-frequency noise. That’s just what we wanted! This is called a low-pass filter."
  },
  {
    "objectID": "lectures/ch2_lecture3.html#section",
    "href": "lectures/ch2_lecture3.html#section",
    "title": "Ch2 Lecture 3",
    "section": "",
    "text": "We can zoom in on a few points to see the effect of the filter.\n\n\n\n\n\n\n\n\n\n\nWhat if we subtract the values at \\(x_{k+1}\\) and \\(x_{k-1}\\) instead of adding them?"
  },
  {
    "objectID": "lectures/ch2_lecture3.html#now-you-try",
    "href": "lectures/ch2_lecture3.html#now-you-try",
    "title": "Ch2 Lecture 3",
    "section": "Now you try",
    "text": "Now you try\nSee if you can make a figure which will just find the noisyness…\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/ch2_lecture3.html#a-different-filter",
    "href": "lectures/ch2_lecture3.html#a-different-filter",
    "title": "Ch2 Lecture 3",
    "section": "A different filter",
    "text": "A different filter\nWhat if we subtract the values at \\(x_{k+1}\\) and \\(x_{k-1}\\) instead of adding them?\n\\[\ny_{k}=-\\frac{1}{4} x_{k+1}+\\frac{1}{2} x_{k}-\\frac{1}{4} x_{k-1}, \\quad k=1,2, \\ldots, 63\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nThis is a high-pass filter. It captures the high-frequency noise, but filters out the low-frequency signal."
  },
  {
    "objectID": "lectures/ch2_lecture3.html#definition-of-inverse-matrix",
    "href": "lectures/ch2_lecture3.html#definition-of-inverse-matrix",
    "title": "Ch2 Lecture 3",
    "section": "Definition of inverse matrix",
    "text": "Definition of inverse matrix\nLet \\(A\\) be a square matrix.\nInverse for \\(A\\) is a square matrix \\(B\\) of the same size as \\(A\\)\n\nsuch that \\(A B=I=B A\\).\n\nIf such a \\(B\\) exists, then the matrix \\(A\\) is said to be invertible.\nAlso called “singular” (non-invertible), or “nonsingular” (invertible)"
  },
  {
    "objectID": "lectures/ch2_lecture3.html#conditions-for-invertibility",
    "href": "lectures/ch2_lecture3.html#conditions-for-invertibility",
    "title": "Ch2 Lecture 3",
    "section": "Conditions for invertibility",
    "text": "Conditions for invertibility\nConditions for Invertibility The following are equivalent conditions on the square \\(n \\times n\\) matrix \\(A\\) :\n\nThe matrix \\(A\\) is invertible.\nThere is a square matrix \\(B\\) such that \\(B A=I\\).\nThe linear system \\(A \\mathbf{x}=\\mathbf{b}\\) has a unique solution for every right-hand-side vector \\(\\mathbf{b}\\).\nThe linear system \\(A \\mathbf{x}=\\mathbf{b}\\) has a unique solution for some right-hand-side vector \\(\\mathbf{b}\\).\nThe linear system \\(A \\mathbf{x}=0\\) has only the trivial solution.\n\\(\\operatorname{rank} A=n\\).\nThe reduced row echelon form of \\(A\\) is \\(I_{n}\\).\nThe matrix \\(A\\) is a product of elementary matrices.\nThere is a square matrix \\(B\\) such that \\(A B=I\\).\n\n\nWe can prove this by starting with 1, which implies 2, etc, and then 9 implies 1…"
  },
  {
    "objectID": "lectures/ch2_lecture3.html#elementary-matrices",
    "href": "lectures/ch2_lecture3.html#elementary-matrices",
    "title": "Ch2 Lecture 3",
    "section": "Elementary matrices",
    "text": "Elementary matrices\n\nEach of the operations in row reduction can be represented by a matrix \\(E\\).\n\n\nRemember: - \\(E_{i j}\\) : The elementary operation of switching the ith and jth rows of the matrix. - \\(E_{i}(c)\\) : The elementary operation of multiplying the ith row by the nonzero constant \\(c\\). - \\(E_{i j}(d)\\) : The elementary operation of adding \\(d\\) times the jth row to the ith row.\n\n\nFind an elementary matrix of size \\(n\\) is by performing the corresponding elementary row operation on the identity matrix \\(I_{n}\\).\n\n\nExample: Find the elementary matrix for \\(E_{13}(-4)\\)\n\n\nAdd -4 times the 3rd row of \\(I_{3}\\) to its first row…\n\n\n\\[\nE_{13}(-4)=\\left[\\begin{array}{rrr}\n1 & 0 & -4 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{array}\\right]\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture3.html#section-1",
    "href": "lectures/ch2_lecture3.html#section-1",
    "title": "Ch2 Lecture 3",
    "section": "",
    "text": "Recall an example from the first week:\nSolve the simple system\n\\[\n\\begin{gather*}\n2 x-y=1 \\\\\n4 x+4 y=20 . \\tag{1.5}\n\\end{gather*}\n\\]\n\n\\[\n\\begin{aligned}\n& {\\left[\\begin{array}{rrr}\n2 & -1 & 1 \\\\\n4 & 4 & 20\n\\end{array}\\right] \\overrightarrow{E_{12}}\\left[\\begin{array}{rrr}\n4 & 4 & 20 \\\\\n2 & -1 & 1\n\\end{array}\\right] \\overrightarrow{E_{1}(1 / 4)}\\left[\\begin{array}{rrr}\n1 & 1 & 5 \\\\\n2 & -1 & 1\n\\end{array}\\right]} \\\\\n& \\overrightarrow{E_{21}(-2)}\\left[\\begin{array}{rrr}\n1 & 1 & 5 \\\\\n0 & -3 & -9\n\\end{array}\\right] \\overrightarrow{E_{2}(-1 / 3)}\\left[\\begin{array}{lll}\n1 & 1 & 5 \\\\\n0 & 1 & 3\n\\end{array}\\right] \\overrightarrow{E_{12}(-1)}\\left[\\begin{array}{lll}\n1 & 0 & 2 \\\\\n0 & 1 & 3\n\\end{array}\\right] .\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture3.html#section-2",
    "href": "lectures/ch2_lecture3.html#section-2",
    "title": "Ch2 Lecture 3",
    "section": "",
    "text": "Rewrite this using matrix multiplication: \\[\n\\left[\\begin{array}{lll}\n1 & 0 & 2 \\\\\n0 & 1 & 3\n\\end{array}\\right]=E_{12}(-1) E_{2}(-1 / 3) E_{21}(-2) E_{1}(1 / 4) E_{12}\\left[\\begin{array}{rrr}\n2 & -1 & 1 \\\\\n4 & 4 & 20\n\\end{array}\\right] \\text {. }\n\\]\n\nWe are rewriting the row operations as a product of matrices. Notice too that the reduced row echelon form is the identity matrix."
  },
  {
    "objectID": "lectures/ch2_lecture3.html#section-3",
    "href": "lectures/ch2_lecture3.html#section-3",
    "title": "Ch2 Lecture 3",
    "section": "",
    "text": "Remove the last columns in each matrix above. (They were the “augmented” part of the original problem.) All the operations still work:\n\\[\n\\left[\\begin{array}{ll}\n1 & 0 \\\\\n0 & 1\n\\end{array}\\right]=E_{12}(-1) E_{2}(-1 / 3) E_{21}(-2) E_{1}(1 / 4) E_{12}\\left[\\begin{array}{rr}\n2 & -1 \\\\\n4 & 4\n\\end{array}\\right] \\text {. }\n\\]\n\nAha! We now have a formula for the inverse of the original matrix:\n\\[\nA^{-1}=E_{12}(-1) E_{2}(-1 / 3) E_{21}(-2) E_{1}(1 / 4) E_{12}\n\\]\n\nHow can we generalize this? How can we keep track of what we are doing?"
  },
  {
    "objectID": "lectures/ch2_lecture3.html#superaugmented-matrix",
    "href": "lectures/ch2_lecture3.html#superaugmented-matrix",
    "title": "Ch2 Lecture 3",
    "section": "Superaugmented matrix",
    "text": "Superaugmented matrix\n\nForm the superaugmented matrix \\([A \\mid I]\\).\nIf we perform the elementary operation \\(E\\) on the superaugmented matrix, we get the matrix \\(E\\) in the augmented part:\n\n\\[\nE[A \\mid I]=[E A \\mid E I]=[E A \\mid E]\n\\]\n\nThis can help us keep track of our operations as we do row reduction\nThe augmented part is just the product of the elementary matrices that we have used so far.\nNow continue applying elementary row operations until the part of the matrix originally occupied by \\(A\\) is reduced to the reduced row echelon form of \\(A\\). . . . \\[\n[A \\mid I] \\overrightarrow{E_{1}, E_{2}, \\ldots, E_{k}}[I \\mid B]\n\\]\n\\(B=E_{k} E_{k-1} \\cdots E_{1}\\) is the product of the various elementary matrices we used."
  },
  {
    "objectID": "lectures/ch2_lecture3.html#inverse-algorithm",
    "href": "lectures/ch2_lecture3.html#inverse-algorithm",
    "title": "Ch2 Lecture 3",
    "section": "Inverse Algorithm",
    "text": "Inverse Algorithm\nGiven an \\(n \\times n\\) matrix \\(A\\), to compute \\(A^{-1}\\) :\n\nForm the superaugmented matrix \\(\\widetilde{A}=\\left[A \\mid I_{n}\\right]\\).\nReduce the first \\(n\\) columns of \\(\\tilde{A}\\) to reduced row echelon form by performing elementary operations on the matrix \\(\\widetilde{A}\\) resulting in the matrix \\([R \\mid B]\\).\nIf \\(R=I_{n}\\) then set \\(A^{-1}=B\\); otherwise, \\(A\\) is singular and \\(A^{-1}\\) does not exist."
  },
  {
    "objectID": "lectures/ch2_lecture3.html#x2-matrices",
    "href": "lectures/ch2_lecture3.html#x2-matrices",
    "title": "Ch2 Lecture 3",
    "section": "2x2 matrices",
    "text": "2x2 matrices\nSuppose we have the 2x2 matrix\n\\[\nA=\\left[\\begin{array}{ll}\na & b \\\\\nc & d\n\\end{array}\\right]\n\\]\n\nDo row reduction on the superaugmented matrix:\n\\[\n\\left[\\begin{array}{ll|ll}\na & b & 1 & 0 \\\\\nc & d & 0 & 1\n\\end{array}\\right]\n\\]\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/ch2_lecture3.html#section-4",
    "href": "lectures/ch2_lecture3.html#section-4",
    "title": "Ch2 Lecture 3",
    "section": "",
    "text": "\\[\nA^{-1}=\\frac{1}{D}\\left[\\begin{array}{rr}\nd & -b \\\\\n-c & a\n\\end{array}\\right]\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture3.html#back-to-pagerank",
    "href": "lectures/ch2_lecture3.html#back-to-pagerank",
    "title": "Ch2 Lecture 3",
    "section": "Back to PageRank",
    "text": "Back to PageRank\n\nDo page ranking as in the first week:\n\nfor page \\(j\\) let \\(n_{j}\\) be its total number of outgoing links on that page.\nThen the score for vertex \\(i\\) is the sum of the scores of all vertices \\(j\\) that link to \\(i\\), divided by the total number of outgoing links on page \\(j\\). . . . \\[\n\\begin{equation*}\nx_{i}=\\sum_{x_{j} \\in L_{i}} \\frac{x_{j}}{n_{j}} . \\tag{1.4}\n\\end{equation*}\n\\]\n\n\n\\[\n\\begin{aligned}\n& x_{1}=\\frac{x_{3}}{3} \\\\\n& x_{2}=\\frac{x_{1}}{2}+\\frac{x_{3}}{3} \\\\\n& x_{3}=\\frac{x_{1}}{2}+\\frac{x_{2}}{1} \\\\\n& x_{4}=\\frac{x_{3}}{3} \\\\\n& x_{5}=\\frac{x_{6}}{1} \\\\\n& x_{6}=\\frac{x_{5}}{1}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture3.html#pagerank-as-a-matrix-equation",
    "href": "lectures/ch2_lecture3.html#pagerank-as-a-matrix-equation",
    "title": "Ch2 Lecture 3",
    "section": "PageRank as a matrix equation",
    "text": "PageRank as a matrix equation\nDefine the matrix \\(Q\\) and vector \\(\\mathbf{x}\\) by\n\\[\nQ=\\left[\\begin{array}{llllll}\n0 & 0 & \\frac{1}{3} & 0 & 0 & 0 \\\\\n\\frac{1}{2} & 0 & \\frac{1}{3} & 0 & 0 & 0 \\\\\n\\frac{1}{2} & 1 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & \\frac{1}{3} & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 & 1 & 0\n\\end{array}\\right], \\mathbf{x}=\\left(x_{1}, x_{2}, x_{3}, x_{4}, x_{5}\\right)\n\\]\n\nThen the equation \\(x=Q x\\) is equivalent to the system of equations above.\n\n\n\\(\\mathbf{x}\\) is a stationary vector for the transition matrix \\(Q\\)."
  },
  {
    "objectID": "lectures/ch2_lecture3.html#connection-between-adjacency-matrix-and-transition-matrix",
    "href": "lectures/ch2_lecture3.html#connection-between-adjacency-matrix-and-transition-matrix",
    "title": "Ch2 Lecture 3",
    "section": "Connection between adjacency matrix and transition matrix",
    "text": "Connection between adjacency matrix and transition matrix\n\n\\(A\\): adjacency matrix of a graph or digraph\n\\(D\\): be a diagonal matrix whose \\(i\\) th entry is either:\n\nthe inverse of the sum of all entries in the \\(i\\) th row of \\(\\mathrm{A}\\), or\nzero if if this sum is zero.\n\nThen \\(Q=A^{T} D\\) is the transition matrix for the page ranking of this graph.\n\n. . . Check:\n\\[\nA=\\left[\\begin{array}{llllll}\n0 & 1 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & 0 \\\\\n1 & 1 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 & 1 & 0\n\\end{array}\\right]\n\\]\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/ch2_lecture3.html#pagerank-as-a-markov-chain",
    "href": "lectures/ch2_lecture3.html#pagerank-as-a-markov-chain",
    "title": "Ch2 Lecture 3",
    "section": "PageRank as a Markov chain",
    "text": "PageRank as a Markov chain\nThink of web surfing as a random process\n\neach page is a state\n\nprobabilities of moving from one state to another given by a stochastic transition matrix P.\n\nequal probability of moving to any of the outgoing links of a page\n\n\n\\[\nP\\stackrel{?}{=}\\left[\\begin{array}{llllll}\n0 & 0 & \\frac{1}{3} & 0 & 0 & 0 \\\\\n\\frac{1}{2} & 0 & \\frac{1}{3} & 0 & 0 & 0 \\\\\n\\frac{1}{2} & 1 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & \\frac{1}{3} & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 & 1 & 0\n\\end{array}\\right]\n\\]\nPause: Is \\(P\\) a stochastic matrix?\n\nIs P a stochastic matrix? What’s the problem? (One of the columns doesn’t sum to 1.)"
  },
  {
    "objectID": "lectures/ch2_lecture3.html#correction-vector",
    "href": "lectures/ch2_lecture3.html#correction-vector",
    "title": "Ch2 Lecture 3",
    "section": "Correction vector",
    "text": "Correction vector\nWe can introduce a correction vector, equivalent to adding links from the dangling node to every other node, or to all connecting nodes (via some path).\n\n\\[\nP=\\left[\\begin{array}{llllll}\n0 & 0 & \\frac{1}{3} & \\frac{1}{3} & 0 & 0 \\\\\n\\frac{1}{2} & 0 & \\frac{1}{3} & \\frac{1}{3} & 0 & 0 \\\\\n\\frac{1}{2} & 1 & 0 & \\frac{1}{3} & 0 & 0 \\\\\n0 & 0 & \\frac{1}{3} & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 & 1 & 0\n\\end{array}\\right]\n\\]\n\n\nThis is now a stochastic matrix “surfing matrix”\n\n\nFind the new transition matrix \\(Q\\)…\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/ch2_lecture3.html#teleportation",
    "href": "lectures/ch2_lecture3.html#teleportation",
    "title": "Ch2 Lecture 3",
    "section": "Teleportation",
    "text": "Teleportation\n\nThere may not be a single stationary vector for the surfing matrix \\(P\\).\n\nCan you think of two different vectors that can never move towards each other? That is, can you think of ways of getting “stuck”?\n\n\nSolution: Jump around. Every so often, instead of following a link, jump to a random page.\n\n\nHow can we modify the surfing matrix to include this?\nIntroduce a teleportation matrix \\(E\\), which is a transition matrix that allows the surfer to jump to any page with equal probability.\nPause: What must \\(E\\) be?\n\n\n\nLet \\(\\mathbf{v}\\) be the teleportation vector with entries 1/n -Let \\(\\mathbf{e}=\\) \\((1,1, \\ldots, 1)\\) be the vector of all ones.\nThen \\(\\mathbf{v e}^{T}\\) is a stochastic matrix whose columns are all equal to \\(\\mathbf{v}\\). This is \\(E\\)."
  },
  {
    "objectID": "lectures/ch2_lecture3.html#pagerank-matrix",
    "href": "lectures/ch2_lecture3.html#pagerank-matrix",
    "title": "Ch2 Lecture 3",
    "section": "PageRank Matrix",
    "text": "PageRank Matrix\nLet:\n\n\\(P\\) be a stochastic matrix,\n\\(\\mathbf{v}\\) a distribution vector of compatible size\n\\(\\alpha\\) a teleportation parameter with \\(0&lt;\\alpha&lt;1\\).\nThen \\(\\alpha P+(1-\\alpha) \\mathbf{v e}^{T}\\) and\n. . . our goal is to find stationary vectors \\[\n\\begin{equation*}\n\\left(\\alpha P+(1-\\alpha) \\mathbf{v e}^{T}\\right) \\mathbf{x}=\\mathbf{x} \\tag{2.4}\n\\end{equation*}\n\\]\n\n\nRearranging,\n\\[\n\\begin{equation*}\n(I-\\alpha P) \\mathbf{x}=(1-\\alpha) \\mathbf{v} \\tag{2.5}\n\\end{equation*}\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture3.html#now-solve-pagerank-for-our-example",
    "href": "lectures/ch2_lecture3.html#now-solve-pagerank-for-our-example",
    "title": "Ch2 Lecture 3",
    "section": "Now solve pagerank for our example",
    "text": "Now solve pagerank for our example\nUse M.solve(b)…\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/ch2_lecture2.html#definitions",
    "href": "lectures/ch2_lecture2.html#definitions",
    "title": "Ch2 Lecture 2",
    "section": "Definitions",
    "text": "Definitions\nDefinition: A discrete linear dynamical system is a sequence of vectors \\(\\mathbf{x}^{(k)}, k=0,1, \\ldots\\), called states, which is defined by an initial vector \\(\\mathbf{x}^{(0)}\\) and by the rule\n\\[\n\\mathbf{x}^{(k+1)}=A \\mathbf{x}^{(k)}+\\mathbf{b}_{k}, \\quad k=0,1, \\ldots\n\\]\n\n\n\\(A\\) is a fixed square matrix, called the transition matrix of the system\nvectors \\(\\mathbf{b}_{k}, k=0,1, \\ldots\\) are called the input vectors of the system.\nIf we don’t specify input vectors, assume that \\(\\mathbf{b}_{k}=\\mathbf{0}\\) for all \\(k\\). Then call the system a homogeneous dynamical system."
  },
  {
    "objectID": "lectures/ch2_lecture2.html#stability",
    "href": "lectures/ch2_lecture2.html#stability",
    "title": "Ch2 Lecture 2",
    "section": "Stability",
    "text": "Stability\nAn important question: is the system stable?\nDoes \\(\\mathbf{x}^{(k)}\\) tend towards a constant state \\(\\mathrm{x}\\)?\n\nIf system is homogeneous, then if a stable state is the initial state, it will equal all subsequent states.\nDefinition: A vector \\(\\mathbf{x}\\) satisfying \\(\\mathbf{x}=A \\mathbf{x}\\), for a square matrix \\(A\\), is called a stationary vector for \\(A\\).\n\n\nIf \\(A\\) is the transition matrix for a homogeneous discrete dynamical system, we also call such a vector a stationary state."
  },
  {
    "objectID": "lectures/ch2_lecture2.html#example",
    "href": "lectures/ch2_lecture2.html#example",
    "title": "Ch2 Lecture 2",
    "section": "Example",
    "text": "Example\n\nTwo toothpaste companies compete for customers in a fixed market\nEach customer uses either Brand A or Brand B.\nBuying habits. In each quarter:\n\n\\(30 \\%\\) of A users will switch to B, while the rest stay with A.\n\\(40 \\%\\) of B users will switch to A , while the the rest stay with B.\n\nThis is an example of a Markov chain model."
  },
  {
    "objectID": "lectures/ch2_lecture2.html#section",
    "href": "lectures/ch2_lecture2.html#section",
    "title": "Ch2 Lecture 2",
    "section": "",
    "text": "Let \\(a_1\\) be the fraction of customers using Brand A at the end of the first quarter, and \\(b_1\\) be the fraction using Brand B.\nThen we have the following system of equations: \\[\n\\begin{aligned}\na_{1} & =0.7 a_{0}+0.4 b_{0} \\\\\nb_{1} & =0.3 a_{0}+0.6 b_{0}\n\\end{aligned}\n\\]\n\nMore generally,\n\\[\n\\begin{aligned}\na_{k+1} & =0.7 a_{k}+0.4 b_{k} \\\\\nb_{k+1} & =0.3 a_{k}+0.6 b_{k} .\n\\end{aligned}\n\\]\n\n\nIn matrix form,\n\\[\n\\mathbf{x}^{(k)}=\\left[\\begin{array}{c}\na_{k} \\\\\nb_{k}\n\\end{array}\\right] \\text { and } A=\\left[\\begin{array}{ll}\n0.7 & 0.4 \\\\\n0.3 & 0.6\n\\end{array}\\right]\n\\]\nwith\n\\[\n\\mathbf{x}^{(k+1)}=A \\mathbf{x}^{(k)}\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture2.html#section-1",
    "href": "lectures/ch2_lecture2.html#section-1",
    "title": "Ch2 Lecture 2",
    "section": "",
    "text": "We can continue into future quarters by multiplying by \\(A\\) again: \\[\n\\mathbf{x}^{(2)}=A \\mathbf{x}^{(1)}=A \\cdot\\left(A \\mathbf{x}^{(0)}\\right)=A^{2} \\mathbf{x}^{(0)}\n\\]\n\nIn general,\n\\[\n\\mathbf{x}^{(k)}=A \\mathbf{x}^{(k-1)}=A^{2} \\mathbf{x}^{(k-2)}=\\cdots=A^{k} \\mathbf{x}^{(0)} .\n\\]\n\n\nThis is true of any homogeneous linear dynamical system!\nFor any positive integer \\(k\\) and discrete dynamical system with transition matrix \\(A\\) and initial state \\(\\mathbf{x}^{(0)}\\), the \\(k\\)-th state is given by\n\\[\n\\mathbf{x}^{(k)}=A^{k} \\mathbf{x}^{(0)}\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture2.html#distribution-vector-and-stochastic-matrix",
    "href": "lectures/ch2_lecture2.html#distribution-vector-and-stochastic-matrix",
    "title": "Ch2 Lecture 2",
    "section": "Distribution vector and stochastic matrix",
    "text": "Distribution vector and stochastic matrix\n\n\\(\\mathbf{x}^{(k)}\\) of the toothpaste example are column vectors with nonnegative coordinates that sum to 1.\nSuch vectors are called distribution vectors.\nAlso, each of the columns the matrix \\(A\\) is a distribution vector.\nA square matrix \\(A\\) whose columns are distribution vectors is called a stochastic matrix."
  },
  {
    "objectID": "lectures/ch2_lecture2.html#markov-chain-definition",
    "href": "lectures/ch2_lecture2.html#markov-chain-definition",
    "title": "Ch2 Lecture 2",
    "section": "Markov Chain definition",
    "text": "Markov Chain definition\nA Markov chain is a discrete dynamical system whose initial state \\(\\mathbf{x}^{(0)}\\) is a distribution vector and whose transition matrix \\(A\\) is stochastic, i.e., each column of \\(A\\) is a distribution vector.\n\nThink this through. - Suppose our current state is \\(\\mathbf{e}_{j}\\) - The system has selected \\(j\\) th event exclusively.\n- The next state is \\(\\mathbf{p}_{j}=P \\mathbf{e}_{j}\\), that is, the \\(j\\) th column of \\(P\\). - This implies that the entry \\(p_{i j}\\) is the probability that the \\(i\\) th event will occur, given that the \\(j\\) th event has just occurred. - Since events are mutually exclusive and some subsequent event must occur, the sum of these probabilities is 1 ."
  },
  {
    "objectID": "lectures/ch2_lecture2.html#checking-that-our-toothpase-example-is-a-markov-chain",
    "href": "lectures/ch2_lecture2.html#checking-that-our-toothpase-example-is-a-markov-chain",
    "title": "Ch2 Lecture 2",
    "section": "Checking that our toothpase example is a Markov chain",
    "text": "Checking that our toothpase example is a Markov chain\n\\[\n\\mathbf{x}^{(k)}=\\left[\\begin{array}{c}\na_{k} \\\\\nb_{k}\n\\end{array}\\right] \\text { and } A=\\left[\\begin{array}{ll}\n0.7 & 0.4 \\\\\n0.3 & 0.6\n\\end{array}\\right]\n\\]\n\n\nThe toothpaste example is a Markov chain:\n\nThe columns of \\(A\\) are distribution vectors.\nThe columns of \\(A\\) sum to 1.\nThe events in the example are mutually exclusive."
  },
  {
    "objectID": "lectures/ch2_lecture2.html#stochastic-matrix-inequality",
    "href": "lectures/ch2_lecture2.html#stochastic-matrix-inequality",
    "title": "Ch2 Lecture 2",
    "section": "Stochastic Matrix Inequality",
    "text": "Stochastic Matrix Inequality\n\nWe can define the 1-norm of a vector \\(\\mathbf{x}\\) as \\(\\|\\mathbf{x}\\|_{1}=\\sum_{i=1}^{n}\\left|x_{i}\\right|\\).\n\nIf all the elements of a vector are nonnegative, then \\(\\|\\mathbf{x}\\|_{1}\\) is the sum of the elements.\n\nCan show: For any stochastic matrix \\(P\\) and compatible vector \\(\\mathbf{x},\\|P \\mathbf{x}\\|_{1} \\leq\\|\\mathbf{x}\\|_{1}\\), with equality if the coordinates of \\(\\mathbf{x}\\) are all nonnegative.\n→ if a state in a Markov chain is a distribution vector (nonnegative entries and sums to 1), then the sum of the coordinates of the next state will also sum to one\n→ all subsequent states in a Markov chain are themselves Markov Chain State distribution vectors."
  },
  {
    "objectID": "lectures/ch2_lecture2.html#moving-into-the-future",
    "href": "lectures/ch2_lecture2.html#moving-into-the-future",
    "title": "Ch2 Lecture 2",
    "section": "Moving into the future",
    "text": "Moving into the future\nSuppose that initially Brand A has all the customers (i.e., Brand B is just entering the market). What are the market shares 2 quarters later? 20 quarters?\n\n\nInitial state vector is \\(\\mathbf{x}^{(0)}=(1,0)\\).\nNow do the arithmetic to find \\(\\mathbf{x}^{(2)}\\) : . . . \\[\n\\begin{aligned}\n{\\left[\\begin{array}{l}\na_{2} \\\\\nb_{2}\n\\end{array}\\right] } & =\\mathbf{x}^{(2)}=A^{2} \\mathbf{x}^{(0)}=\\left[\\begin{array}{ll}\n0.7 & 0.4 \\\\\n0.3 & 0.6\n\\end{array}\\right]\\left(\\left[\\begin{array}{ll}\n0.7 & 0.4 \\\\\n0.3 & 0.6\n\\end{array}\\right]\\left[\\begin{array}{l}\n1 \\\\\n0\n\\end{array}\\right]\\right) \\\\\n& =\\left[\\begin{array}{ll}\n0.7 & 0.4 \\\\\n0.3 & 0.6\n\\end{array}\\right]\\left[\\begin{array}{l}\n0.7 \\\\\n0.3\n\\end{array}\\right]=\\left[\\begin{array}{l}\n.61 \\\\\n.39\n\\end{array}\\right] .\n\\end{aligned}\n\\]\n\n\nBrand A will have \\(61 \\%\\) of the market and Brand B will have \\(39 \\%\\) of the market in the second quarter."
  },
  {
    "objectID": "lectures/ch2_lecture2.html#toothpaste-after-many-quarters",
    "href": "lectures/ch2_lecture2.html#toothpaste-after-many-quarters",
    "title": "Ch2 Lecture 2",
    "section": "Toothpaste after many quarters",
    "text": "Toothpaste after many quarters\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNow:\n\nfind the state after 2 quarters.\nfind the state after 20 quarters. Use matrix exponentiation: A**n\nGet specific numbers if toothpaste A has 100% of the market at the beginning. (Use sym.subs())\nNow what if toothpaste B has 100% at the beginning?\n\n\n\nOur calculation is reusable! Could go back after and then change the inital state.\nNotice that we also get the same result eventually, no matter where we started"
  },
  {
    "objectID": "lectures/ch2_lecture2.html#example-structured-population-model",
    "href": "lectures/ch2_lecture2.html#example-structured-population-model",
    "title": "Ch2 Lecture 2",
    "section": "Example: Structured Population Model",
    "text": "Example: Structured Population Model\n\n\n\ninsect life stages\n\n\n\nEvery week, 20% of the eggs die, and 60% move to the larva stage.\nAlso, 10% of larvae die, 60% become adults\n20% of adults die. Each adult produces 0.25 eggs.\nInitially we have 10k adults, 8k larvae, 6k eggs. How does the population evolve over time?\n\n\n\n\n\n\nSet up \\(\\mathbf{x}^{(k)}=\\left(a_{k}, b_{k}, c_{k}\\right)\\) \\(\\mathbf{x}^{(0)}=(10,8,6)\\) at week 0 Note that after first week, we have 20% of the initial eggs (20% died, 60% became larvae…) Transition matrix is: \\[\nA=\\left[\\begin{array}{ccc}\n0.2 & 0 & 0.25 \\\\\n0.6 & 0.3 & 0 \\\\\n0 & 0.6 & 0.8\n\\end{array}\\right]\n\\]\nWhat do we think will happen over time? How can we tell?"
  },
  {
    "objectID": "lectures/ch2_lecture2.html#section-2",
    "href": "lectures/ch2_lecture2.html#section-2",
    "title": "Ch2 Lecture 2",
    "section": "",
    "text": "class InsectEvolution(BaseStateSystem):\n    def __init__(self,transition_matrix=np.array([[.2,0,.25],[.6,.3,0],[0,.6,.8]]),max_y=10):\n        self.steps = 10;\n        self.t=0\n        self.transition_matrix = transition_matrix\n        self.max_y=max_y\n\n    def initialise(self):\n        self.x = np.array([6,8,10])\n        self.Ya = []\n        self.Yb = []\n        self.Yc = []\n        self.X = []\n\n    def update(self):\n        for _ in range(self.steps):\n            self.t += 1\n            self._update()\n\n    def _update(self):\n        self.x = self.transition_matrix.dot(self.x)\n\n    def draw(self, ax):\n        ax.clear()\n        self.X.append(self.t)\n        self.Ya.append(self.x[0])\n        self.Yb.append(self.x[1])\n        self.Yc.append([self.x[2]])\n        ax.plot(self.X,self.Ya, color=\"r\", label=\"Eggs\")\n        ax.plot(self.X,self.Yb, color=\"b\", label=\"Larvae\")\n        ax.plot(self.X,self.Yc, color=\"g\", label=\"Adults\")\n        ax.legend()\n\n        ax.set_ylim(0,self.max_y)\n        ax.set_xlim(0,200)\n        ax.set_title(\"t = {:.2f}\".format(self.t))"
  },
  {
    "objectID": "lectures/ch2_lecture2.html#section-3",
    "href": "lectures/ch2_lecture2.html#section-3",
    "title": "Ch2 Lecture 2",
    "section": "",
    "text": "t1=np.array([[.2,0,.25],[.6,.3,0],[0,.6,.8]])\ndying_insects = InsectEvolution(t1)\ndying_insects.plot_time_evolution(\"insects.gif\")\n\n\n\n\ndying insects\n\n\n\n\nt2=np.array([[.4,0,.45],[.5,.1,0],[0,.6,.8]])\nliving_insects = InsectEvolution(t2,200)\nliving_insects.plot_time_evolution(\"insects2.gif\")\n\n\n\n\nliving insects"
  },
  {
    "objectID": "lectures/ch2_lecture2.html#graph",
    "href": "lectures/ch2_lecture2.html#graph",
    "title": "Ch2 Lecture 2",
    "section": "Graph",
    "text": "Graph\n\nA graph is a set \\(V\\) of vertices (or nodes), together with a set or list \\(E\\) of unordered pairs with coordinates in \\(V\\), called edges.\n\nWhat are some things which can be represented with graphs?"
  },
  {
    "objectID": "lectures/ch2_lecture2.html#digraph",
    "href": "lectures/ch2_lecture2.html#digraph",
    "title": "Ch2 Lecture 2",
    "section": "Digraph",
    "text": "Digraph\n\nA directed graph (or “digraph”) has directed edges.\n\nWhat are some things which can be represented with digraphs?"
  },
  {
    "objectID": "lectures/ch2_lecture2.html#walks",
    "href": "lectures/ch2_lecture2.html#walks",
    "title": "Ch2 Lecture 2",
    "section": "Walks",
    "text": "Walks\n\nA walk is a sequence of edges \\(\\left\\{v_{0}, v_{1}\\right\\},\\left\\{v_{1}, v_{2}\\right\\}, \\ldots,\\left\\{v_{m-1}, v_{m}\\right\\}\\) that goes from vertex \\(v_{0}\\) to vertex \\(v_{m}\\).\nThe length of the walk is \\(m\\).\n\nA directed walk is a sequence of directed edges.\n\nWhat is an example of what a walk tells us? Work through from one of the examples of graph or digraph that came up.\nOne example: how many people are in your extended network. “Six degrees of separation”…"
  },
  {
    "objectID": "lectures/ch2_lecture2.html#winning-and-losing",
    "href": "lectures/ch2_lecture2.html#winning-and-losing",
    "title": "Ch2 Lecture 2",
    "section": "Winning and losing",
    "text": "Winning and losing\n\nSuppose this represents wins and losses by different teams. How can we rank the teams?\n\nIdea: a team which beats another team is good. Even better if the team they beat had beaten another team…\nIdea: count the number of walks of length 1 or 2 originating from each vertex.\nHave them do this…\nPower of 1 is 5, vertex 2 is 4, vertex 3 is 7, vertex 4 is 4, vertex 5 is 1, and the power of vertex 6 is 0."
  },
  {
    "objectID": "lectures/ch2_lecture2.html#power",
    "href": "lectures/ch2_lecture2.html#power",
    "title": "Ch2 Lecture 2",
    "section": "Power",
    "text": "Power\nVertex power: the number of walks of length 1 or 2 originating from a vertex.\n\nMakes most sense for graphs which don’t have any self-loops and at most one edge between nodes. These are called dominance directed graphs\n\n\nHow can we compute the number of walks for a given graph?"
  },
  {
    "objectID": "lectures/ch2_lecture2.html#adjacency-matrix",
    "href": "lectures/ch2_lecture2.html#adjacency-matrix",
    "title": "Ch2 Lecture 2",
    "section": "Adjacency matrix",
    "text": "Adjacency matrix\nAdjacency matrix: A square matrix whose \\((i, j)\\) th entry is the number of edges going from vertex \\(i\\) to vertex \\(j\\)\n\nEdges in non-directed graphs appear twice (at (i,j) and at (j,i))\nEdges in digraphs appear only once\n\n\nWhat is the adjacency matrix for this graph?\n\n\n\n\\[\nB=\\left[\\begin{array}{llllll}\n0 & 1 & 0 & 1 & 0 & 1 \\\\\n1 & 0 & 1 & 0 & 0 & 1 \\\\\n0 & 1 & 0 & 0 & 1 & 0 \\\\\n1 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 1 & 0 & 0 & 1 \\\\\n1 & 1 & 0 & 1 & 1 & 0\n\\end{array}\\right]\n\\]\n\nWe can reconstruct the graph entirely from this matrix! It must have all the information encapsulated in it."
  },
  {
    "objectID": "lectures/ch2_lecture2.html#section-4",
    "href": "lectures/ch2_lecture2.html#section-4",
    "title": "Ch2 Lecture 2",
    "section": "",
    "text": "What is the adjacency matrix for this graph?\n\n\n\\[\nA=\\left[\\begin{array}{llllll}\n0 & 1 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 1 & 0 & 1 \\\\\n0 & 1 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 & 0 & 0\n\\end{array}\\right]\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture2.html#computing-power-from-an-adjacencty-matrix",
    "href": "lectures/ch2_lecture2.html#computing-power-from-an-adjacencty-matrix",
    "title": "Ch2 Lecture 2",
    "section": "Computing power from an adjacencty matrix",
    "text": "Computing power from an adjacencty matrix\n\\[\nA=\\left[\\begin{array}{llllll}\n0 & 1 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 1 & 0 & 1 \\\\\n0 & 1 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 & 0 & 0\n\\end{array}\\right]\n\\]\nHow can we count the walks of length 1 emanating from vertex \\(i\\)?\n\nAnswer: Add up the elements of the \\(i\\) th row of \\(A\\)."
  },
  {
    "objectID": "lectures/ch2_lecture2.html#section-5",
    "href": "lectures/ch2_lecture2.html#section-5",
    "title": "Ch2 Lecture 2",
    "section": "",
    "text": "\\[\nA=\\left[\\begin{array}{llllll}\n0 & 1 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 1 & 0 & 1 \\\\\n0 & 1 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 & 0 & 0\n\\end{array}\\right]\n\\]\nWhat about walks of length 2?\n\nStart by finding number of walks of length 2 from vertex i to vertex j.\n\n\n\\[\na_{i 1} a_{1 j}+a_{i 2} a_{2 j}+\\cdots+a_{i n} a_{n j} .\n\\]\n\n\nThis is just the \\((i, j)\\) th entry of the matrix \\(A^{2}\\).\n\nWhen is there an edge from \\(i\\) to \\(k\\) and then from \\(k\\) to \\(j\\)?\nIf the adjacency matrix has 1’s in both \\(a_{i,k}\\) and \\(a_{k,j}\\).\nCan represent this by the product…\nSo number of paths is the sum over k’s…\n\\[\na_{i 1} a_{1 j}+a_{i 2} a_{2 j}+\\cdots+a_{i n} a_{n j} .\n\\]\nThis is just the \\((i, j)\\) th entry of the matrix \\(A^{2}\\)."
  },
  {
    "objectID": "lectures/ch2_lecture2.html#adjacency-matrix-and-power",
    "href": "lectures/ch2_lecture2.html#adjacency-matrix-and-power",
    "title": "Ch2 Lecture 2",
    "section": "Adjacency matrix and power",
    "text": "Adjacency matrix and power\nResult: The \\((i, j)\\) th entry of \\(A^{r}\\) gives the number of (directed) walks of length \\(r\\) starting at vertex \\(i\\) and ending at vertex \\(j\\).\n\nTherefore: power of the \\(i\\) th vertex is the sum of all entries in the \\(i\\) th row of the matrix \\(A+A^{2}\\)."
  },
  {
    "objectID": "lectures/ch2_lecture2.html#vertex-power-in-our-teams-example",
    "href": "lectures/ch2_lecture2.html#vertex-power-in-our-teams-example",
    "title": "Ch2 Lecture 2",
    "section": "Vertex power in our teams example",
    "text": "Vertex power in our teams example\n\\[\nA+A^{2}=\\left[\\begin{array}{llllll}\n0 & 1 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 1 & 0 & 1 \\\\\n0 & 1 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 & 0 & 0\n\\end{array}\\right]+\\left[\\begin{array}{llllll}\n0 & 1 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 1 & 0 & 1 \\\\\n0 & 1 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 & 0 & 0\n\\end{array}\\right]\n\\left[\\begin{array}{llllll}\n0 & 1 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 1 & 0 & 1 \\\\\n0 & 1 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 & 0 & 0\n\\end{array}\\right]\n\\]\n\\[=\\left[\\begin{array}{llllll}\n0 & 2 & 1 & 1 & 1 & 0 \\\\\n1 & 0 & 1 & 1 & 0 & 1 \\\\\n1 & 2 & 0 & 2 & 1 & 1 \\\\\n0 & 1 & 1 & 0 & 1 & 1 \\\\\n0 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 & 0 & 0\n\\end{array}\\right] .\n\\]\n\nThe sum of each row gives the vertex power.\n\nHow can you use multiplication to find the sum of each row?\nCan multiply by a column of ones…"
  },
  {
    "objectID": "lectures/ch2_lecture2.html#now-you-try",
    "href": "lectures/ch2_lecture2.html#now-you-try",
    "title": "Ch2 Lecture 2",
    "section": "Now you try",
    "text": "Now you try\nWorking together,\n\npick a topic (sports? elections?).\nLook up data and make a graph or digraph.\nCompute the adjacency matrix.\nFind the vertex powers.\n\nDo your results make sense in context?\nDoes it make sense?\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n \n\n\n \n\n\n\n\n \n\n\n\ninsect life stages\ndying insects\nliving insects"
  }
]