[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stat 24320",
    "section": "",
    "text": "Title\n            \n                \n                    Lecture Day\n                \n                \n                \n                    Readings\n                \n                \n                    Colab Link\n        \n        \n                    \n                        \n                            Intro to linear systems\n                        \n                    \n\n                    \n                        \n                            1\n                        \n                        \n                        \n                            Ch. 1.1-1.3\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            More Systems of Linear Equations\n                        \n                    \n\n                    \n                        \n                            2\n                        \n                        \n                        \n                            Ch. 1.4-1.5\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch2 Lecture 1\n                        \n                    \n\n                    \n                        \n                            3\n                        \n                        \n                        \n                            2.1-2.3\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch2 Lecture 2\n                        \n                    \n\n                    \n                        \n                            4\n                        \n                        \n                        \n                            2.3-2.4\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch2 Lecture 3\n                        \n                    \n\n                    \n                        \n                            5\n                        \n                        \n                        \n                            2.3-2.8\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch2 Lecture 4\n                        \n                    \n\n                    \n                        \n                            5\n                        \n                        \n                        \n                            2.8\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n    \n\nNo matching items"
  },
  {
    "objectID": "index.html#lecture-notes",
    "href": "index.html#lecture-notes",
    "title": "Stat 24320",
    "section": "",
    "text": "Title\n            \n                \n                    Lecture Day\n                \n                \n                \n                    Readings\n                \n                \n                    Colab Link\n        \n        \n                    \n                        \n                            Intro to linear systems\n                        \n                    \n\n                    \n                        \n                            1\n                        \n                        \n                        \n                            Ch. 1.1-1.3\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            More Systems of Linear Equations\n                        \n                    \n\n                    \n                        \n                            2\n                        \n                        \n                        \n                            Ch. 1.4-1.5\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch2 Lecture 1\n                        \n                    \n\n                    \n                        \n                            3\n                        \n                        \n                        \n                            2.1-2.3\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch2 Lecture 2\n                        \n                    \n\n                    \n                        \n                            4\n                        \n                        \n                        \n                            2.3-2.4\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch2 Lecture 3\n                        \n                    \n\n                    \n                        \n                            5\n                        \n                        \n                        \n                            2.3-2.8\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch2 Lecture 4\n                        \n                    \n\n                    \n                        \n                            5\n                        \n                        \n                        \n                            2.8\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n    \n\nNo matching items"
  },
  {
    "objectID": "index.html#notebooks",
    "href": "index.html#notebooks",
    "title": "Stat 24320",
    "section": "Notebooks",
    "text": "Notebooks\n\n\n    \n            \n                \n        \n            Title\n            \n                \n                    Lecture Day\n                \n                \n                    Colab Link\n        \n        \n                    \n                        \n                            PageRank Tutorial\n                        \n                    \n\n                    \n                        \n                            1\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Turing Patterns\n                        \n                    \n\n                    \n                        \n                            4\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            MCMC Pagerank\n                        \n                    \n\n                    \n                        \n                            6\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n    \n\nNo matching items"
  },
  {
    "objectID": "index.html#homeworks",
    "href": "index.html#homeworks",
    "title": "Stat 24320",
    "section": "Homeworks",
    "text": "Homeworks\n\n\n            \n                \n        \n            Title\n            \n                \n                    Due Date\n                \n                \n                \n                    Textbook Chapters\n                \n                \n                    Solutions\n        \n        \n                    \n                        \n                            Homework 1\n                        \n                    \n\n                    \n                        \n                            Wednesday, Week 2\n                        \n                        \n                        \n                            1\n                        \n                        \n                            \n                           \n                            \n                                \n                                        Homework 1 Solutions\n                                    \n                            \n\n                \n\n                \n                \n                    \n                        \n                            Homework 2\n                        \n                    \n\n                    \n                        \n                            Wednesday, Week 3\n                        \n                        \n                        \n                            2\n                        \n                        \n                            \n                           \n                            \n                                \n                                        Homework 2 Solutions\n                                    \n                            \n\n                \n\n                \n                \n                    \n                        \n                            Homework 3\n                        \n                    \n\n                    \n                        \n                            Wednesday, Week 4\n                        \n                        \n                        \n                            2.4, 2.5, 2.8\n                        \n                        \n                            \n                           \n                            \n\n                \n\n                \n                \n                    \n                        \n                            Projects 1\n                        \n                    \n\n                    \n                        \n                            Friday, Week 4\n                        \n                        \n                        \n                            1, 2\n                        \n                        \n                            \n                           \n                            \n\n                \n\n                \n    \n\nNo matching items"
  },
  {
    "objectID": "lectures/ch2_lecture4.html#idea-of-mcmc",
    "href": "lectures/ch2_lecture4.html#idea-of-mcmc",
    "title": "Ch2 Lecture 4",
    "section": "Idea of MCMC",
    "text": "Idea of MCMC\n\nSuppose there’s something we want to calculate – say the average web revenue for pages every day.\nStart out simple: suppose we have a revenue amount for each page, per user. How do we calculate the average revenue per page?\nWe can figure out how many users visit each page, and then multiply that by the revenue per user for that page.\nWe can do this by calculating the stationary distribution of the Markov chain that describes the transitions between pages.\nOr, we can do it by sampling.\nWork through the notebook to see how this works in practice.\nOK, now make it more complicated.\nWe have a formula for a given webpage that tells us how much revenue it generates given the user’s age, their gender, their location, etc.\nHow can we find the average revenue per page now?\nWe need to know the distribution of users and their characteristics. We can’t just use the stationary distribution of the Markov chain anymore.\nWe could try to calculate the distribution of users, but that’s hard.\nInstead, we can use a Markov Chain Monte Carlo method to sample from the distribution of users. We can then use these samples to estimate the average revenue per page.\nTo do Gibbs sampling, we will sample from the distribution of each variable given the other variables. This is easier than sampling from the joint distribution of all the variables.\nSo for instance, we might suppose that a user is male, 25 years old, and on the webpage for the Chicago Cubs. We can specify a distribution of locations for the user’s location given this information. Sample from that distribution.\nThen, we can say ok, we’ve got a user who is 25 years old, on the webpage for the Cubs, and in Chicago. What’s the probability that the user is male?\nGo through all the variables in turn, sampling from the distribution of each given the others, including for the webpages."
  },
  {
    "objectID": "lectures/ch2_lecture4.html#intro-to-the-idea-of-a-restricted-boltzmann-machine",
    "href": "lectures/ch2_lecture4.html#intro-to-the-idea-of-a-restricted-boltzmann-machine",
    "title": "Ch2 Lecture 4",
    "section": "Intro to the idea of a Restricted Boltzmann Machine",
    "text": "Intro to the idea of a Restricted Boltzmann Machine\n\nYou’d like to learn about what sort of objects exist in the world. You have a bunch of data, but you don’t know what the objects are. You’d like to learn about the objects and the features that describe them.\nOne way to do this is to start trying to assign labels to the objects you are seeing. You can start with random labels, and then try to improve them.\nYour goal can be that if you imagine objects given the labels, you’ll end up with a distribution that looks like the data you have.\n\n\n\nRBM schematic\n\n\nYour dreams should look like the real world.\nOK, but how do you measure how well your dreams match the real world?\nYou could calculate the probability of imagining each possible image… but that’s intractable.\nInstead, you can do a Markov Chain Monte Carlo method called Gibbs sampling. The basic idea of MCMC is that you can sample from a distribution by starting at some point and then moving around in a way that the distribution of your samples will eventually match the distribution you’re interested in.\nFor Gibbs sampling, you start with a random sample, and then you update each variable in turn, given the other variables. This may be much easier to compute than needing to know the full transition matrix for every possible pair of states. (We are in high dimensions here, because every node in our network is a variable.)\nIn the case of the RBM, you update each hidden node given the visible nodes, and then you update each visible node given the hidden nodes. You keep doing this for a while, and then you have a sample from the distribution you’re interested in.\nNow you compare this sample to your data, and you adjust the weights in your network to make the sample look more like the data."
  },
  {
    "objectID": "lectures/ch2_lecture4.html#math-of-the-rbm",
    "href": "lectures/ch2_lecture4.html#math-of-the-rbm",
    "title": "Ch2 Lecture 4",
    "section": "Math of the RBM",
    "text": "Math of the RBM\n\n\n\nStates are determined by an energy function \\(E(\\mathbf{v}, \\mathbf{h})\\). \\[\nE(\\mathbf{v}, \\mathbf{h})=-\\sum_{i} a_{i} v_{i}-\\sum_{j} b_{j} h_{j}-\\sum_{i j} v_{i} W_{i j} h_{j}\n\\]\n\n\nThen the probability distribution is given by the Boltzmann distribution:\n\\(P_{\\mathrm{rbm}}(\\mathbf{v}, \\mathbf{h})=\\frac{1}{Z} e^{-E(\\mathbf{v}, \\mathbf{h})}\\) where \\(Z=\\sum_{\\mathbf{v}, \\mathbf{h}} e^{-E(\\mathbf{v}, \\mathbf{h})}\\)\n\n\nFrom https://ml-lectures.org/docs/unsupervised_learning/ml_unsupervised-1.html"
  },
  {
    "objectID": "lectures/ch2_lecture4.html#section",
    "href": "lectures/ch2_lecture4.html#section",
    "title": "Ch2 Lecture 4",
    "section": "",
    "text": "The probability of a visible vector \\(\\mathbf{v}\\) is given by marginalizing over the hidden variables:\n\\[\n\\begin{equation*}\nP_{\\mathrm{rbm}}(\\mathbf{v})=\\sum_{\\mathbf{h}} P_{\\mathrm{rbm}}(\\mathbf{v}, \\mathbf{h})=\\frac{1}{Z} \\sum_{h} e^{-E(\\mathbf{v}, \\mathbf{h})}\n\\end{equation*}\n\\]\nConveniently, this gives each visible unit an independent probability of activation:\n\\[\nP_{\\mathrm{rbm}}\\left(v_{i}=1 | \\mathbf{h}\\right)=\\sigma\\left(a_{i}+\\sum_{j} W_{i j} h_{j}\\right), \\quad i=1, \\ldots, n_{\\mathrm{v}}\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture4.html#section-1",
    "href": "lectures/ch2_lecture4.html#section-1",
    "title": "Ch2 Lecture 4",
    "section": "",
    "text": "The same is true for hidden units, given the visible units:\n\\[\n\\begin{equation*}\nP_{\\mathrm{rbm}}\\left(h_{j}=1 | \\mathbf{v}\\right)=\\sigma\\left(b_{j}+\\sum_{i} v_{i} W_{i j}\\right) \\quad j=1, \\ldots, n_{\\mathrm{h}}\n\\end{equation*}\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture4.html#training",
    "href": "lectures/ch2_lecture4.html#training",
    "title": "Ch2 Lecture 4",
    "section": "Training",
    "text": "Training\nConsider a set of binary input data \\(\\mathbf{x}_{k}, k=1, \\ldots, M\\), drawn from a probability distribution \\(P_{\\text {data }}(\\mathbf{x})\\).\nGoal: tune the parameters \\(\\{\\mathbf{a}, \\mathbf{b}, W\\}\\) such that after training \\(P_{\\mathrm{rbm}}(\\mathbf{x}) \\approx P_{\\mathrm{data}}(\\mathbf{x})\\).\n\nTo do this, we need to be able to estimate \\(P_{\\mathrm{rbm}}\\)!\nUnfortunately, this is often intractable, because it requires calculating the partition function \\(Z\\)."
  },
  {
    "objectID": "lectures/ch2_lecture4.html#details-of-the-training",
    "href": "lectures/ch2_lecture4.html#details-of-the-training",
    "title": "Ch2 Lecture 4",
    "section": "Details of the training",
    "text": "Details of the training\nWe want to maximize the log-likelihood of the data under the model: \\[\nL(\\mathbf{a}, \\mathbf{b}, W)=-\\sum_{k=1}^{M} \\log P_{\\mathrm{rbm}}\\left(\\mathbf{x}_{k}\\right)\n\\]\n\nTake derivatives of this with respect to the parameters, and use gradient descent:\n\\[\n\\begin{equation*}\n\\frac{\\partial L(\\mathbf{a}, \\mathbf{b}, W)}{\\partial W_{i j}}=-\\sum_{k=1}^{M} \\frac{\\partial \\log P_{\\mathrm{rbm}}\\left(\\mathbf{x}_{k}\\right)}{\\partial W_{i j}}\n\\end{equation*}\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture4.html#section-2",
    "href": "lectures/ch2_lecture4.html#section-2",
    "title": "Ch2 Lecture 4",
    "section": "",
    "text": "The derivative has two terms: \\[\n\\begin{equation*}\n\\frac{\\partial \\log P_{\\mathrm{rbm}}(\\mathbf{x})}{\\partial W_{i j}}=x_{i} P_{\\mathrm{rbm}}\\left(h_{j}=1 |\\mathbf{x}\\right)-\\sum_{\\mathbf{v}} v_{i} P_{\\mathrm{rbm}}\\left(h_{j}=1 | \\mathbf{v}\\right) P_{\\mathrm{rbm}}(\\mathbf{v})\n\\end{equation*}\n\\]\n\nUse this to update the weights:\n\\[\nW_{i j} \\rightarrow W_{i j}-\\eta \\frac{\\partial L(a, b, W)}{\\partial W_{i j}}\n\\]\nProblem: the second term in the derivative is intractable! It has \\(2^{n_{\\mathrm{v}}}\\) terms:\n\\[\n\\sum_{\\mathbf{v}} v_{i} P_{\\mathrm{rbm}}\\left(h_{j}=1 | \\mathbf{v}\\right) P_{\\mathrm{rbm}}(\\mathbf{v})\n\\]\nInstead, we will use Gibbs sampling to estimate \\(P_{\\mathrm{rbm}}(\\mathbf{v})\\)."
  },
  {
    "objectID": "lectures/ch2_lecture4.html#gibbs-sampling-to-the-rescue",
    "href": "lectures/ch2_lecture4.html#gibbs-sampling-to-the-rescue",
    "title": "Ch2 Lecture 4",
    "section": "Gibbs sampling to the rescue",
    "text": "Gibbs sampling to the rescue\n\nInput: Any visible vector \\(\\mathbf{v}(0)\\)\nOutput: Visible vector \\(\\mathbf{v}(r)\\)\nfor: \\(n=1 \\backslash\\) dots \\(r\\)\n\\(\\operatorname{sample} \\mathbf{h}(n)\\) from \\(P_{\\mathrm{rbm}}(\\mathbf{h} \\mathbf{v}=\\mathbf{v}(n-1))\\)\nsample \\(\\mathbf{v}(n)\\) from \\(P_{\\mathrm{rbm}}(\\mathbf{v} \\mathbf{h}=\\mathbf{h}(n))\\) end"
  },
  {
    "objectID": "lectures/ch2_lecture4.html#using-an-rbm",
    "href": "lectures/ch2_lecture4.html#using-an-rbm",
    "title": "Ch2 Lecture 4",
    "section": "Using an RBM",
    "text": "Using an RBM"
  },
  {
    "objectID": "lectures/ch2_lecture4.html#section-3",
    "href": "lectures/ch2_lecture4.html#section-3",
    "title": "Ch2 Lecture 4",
    "section": "",
    "text": "Suppose we want to solve a nonsingular linear system \\(A x=b\\) repeatedly, with different choices of \\(b\\).\n\n\nE.g. Heat flow problem, where the right-hand side is determined by the heat source term \\(f(x)\\).\n\n\n\\[\n\\begin{equation*}\n-y_{i-1}+2 y_{i}-y_{i+1}=\\frac{h^{2}}{K} f\\left(x_{i}\\right), i=1,2, \\ldots, n\n\\end{equation*}\n\\]\n\n\nPerhaps you want to experiment with different functions for the heat source term.\n\n\nWhat do we do? Each time, we create the augmented matrix \\(\\widetilde{A}=[A \\mid b]\\), then get it into reduced row echelon form.\n\n\nEach time change \\(b\\), we have to redo all the work of Gaussian or Gauss-Jordan elimination !\n\n\nEspecially frustrating because the main part of our work is the same: putting the part of \\(\\widetilde{A}\\) corresponding to the coefficient matrix \\(A\\) into reduced row echelon form."
  },
  {
    "objectID": "lectures/ch2_lecture4.html#lu-factorization-saving-that-work",
    "href": "lectures/ch2_lecture4.html#lu-factorization-saving-that-work",
    "title": "Ch2 Lecture 4",
    "section": "LU Factorization: Saving that work",
    "text": "LU Factorization: Saving that work\nGoal: Find a way to record our work on \\(A\\), so that solving a new system involves very little additional work.\n\nLU Factorization: Let \\(A\\) be an \\(n \\times n\\) matrix. An LU factorization of \\(A\\) is a pair of \\(n \\times n\\) matrices \\(L, U\\) such that\n\n\\(L\\) is lower triangular.\n\\(U\\) is upper triangular.\n\\(A=L U\\).\n\n\n\nWhy is this so wonderful? Triangular systems \\(A \\mathbf{x}=\\mathbf{b}\\) are easy to solve.\nRemember: If \\(A\\) is upper triangular, we can solve for the last variable, then the next-to-last variable, etc."
  },
  {
    "objectID": "lectures/ch2_lecture4.html#solving-an-upper-triangular-system",
    "href": "lectures/ch2_lecture4.html#solving-an-upper-triangular-system",
    "title": "Ch2 Lecture 4",
    "section": "Solving an upper triangular system",
    "text": "Solving an upper triangular system\nLet’s say we have the following system:\n\\(A x = b\\) where A is the upper-triangular matrix \\(A = \\begin{bmatrix} 2 & 1 & 0 \\\\ 0 & 1 & -1 \\\\ 0 & 0 & -1 \\end{bmatrix}\\), and we want to solve for \\(b = \\begin{bmatrix} 1 \\\\ 1 \\\\ -2 \\end{bmatrix}\\).\n\nWe form the augmented matrix \\(\\widetilde{A} = [A | b] = \\begin{bmatrix} 2 & 1 & 0 & | & 1 \\\\ 0 & 1 & -1 & | & 1 \\\\ 0 & 0 & -1 & | & -2 \\end{bmatrix}\\).\n\n\nBack substitution:\n\nLast equation: \\(-x_3 = -2\\), so \\(x_3 = 2\\).\nSubstitute this value into the second equation, \\(x_2 - x_3 = 1\\), so \\(x_2 = 3\\).\nFinally, we substitute \\(x_2\\) and \\(x_3\\) into the first equation, \\(2x_1 + x_2 = 1\\), so \\(x_1 = -1\\)."
  },
  {
    "objectID": "lectures/ch2_lecture4.html#solving-a-lower-triangular-system",
    "href": "lectures/ch2_lecture4.html#solving-a-lower-triangular-system",
    "title": "Ch2 Lecture 4",
    "section": "Solving a lower triangular system",
    "text": "Solving a lower triangular system\nIf \\(A\\) is lower triangular, we can solve for the first variable, then the second variable, etc.\nLet’s say we have the following system:\n\\(A y = b\\) where A is the lower-triangular matrix \\(A = \\begin{bmatrix} 1 & 0 & 0 \\\\ -1 & 1 & 0 \\\\ 1 & 2 & 1 \\end{bmatrix}\\), and we want to solve for \\(b = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix}\\).\n\nWe form the augmented matrix \\(\\widetilde{A} = [A | b] = \\begin{bmatrix} 1 & 0 & 0 & | & 1 \\\\ -1 & 1 & 0 & | & 0 \\\\ 1 & 2 & 1 & | & 1 \\end{bmatrix}\\).\n\n\nForward substitution:\n\nFirst equation: \\(y_1 = 1\\).\nSubstitute this value into the second equation, \\(-y_1 + y_2 = 0\\), so \\(y_2 = 1\\).\nFinally, we substitute \\(y_1\\) and \\(y_2\\) into the third equation, \\(y_1 + 2y_2 + y_3 = 1\\), so \\(y_3 = -2\\).\n\n\n\nThis was just as easy as solving the upper triangular system!"
  },
  {
    "objectID": "lectures/ch2_lecture4.html#solving-a-x-b-with-lu-factorization",
    "href": "lectures/ch2_lecture4.html#solving-a-x-b-with-lu-factorization",
    "title": "Ch2 Lecture 4",
    "section": "Solving \\(A x = b\\) with LU factorization",
    "text": "Solving \\(A x = b\\) with LU factorization\nNow suppose we want to solve \\(A x = b\\) and we know that \\(A = L U\\). The original system becomes \\(L U x = b\\).\nIntroduce an intermediate variable \\(y = U x\\). Our system is now \\(L y = b\\). Now perform these steps:\n\nForward solve: Solve lower triangular system \\(L y = b\\) for the variable \\(y\\).\nBack solve: Solve upper triangular system \\(U x = y\\) for the variable \\(x\\).\nThis does it!\n\n\nOnce we have the matrices \\(L, U\\), the right-hand sides only come when solving the two triangular systems. Easy!"
  },
  {
    "objectID": "lectures/ch2_lecture4.html#example",
    "href": "lectures/ch2_lecture4.html#example",
    "title": "Ch2 Lecture 4",
    "section": "Example",
    "text": "Example\nYou are given that\n\\[\nA=\\left[\\begin{array}{rrr}\n2 & 1 & 0 \\\\\n-2 & 0 & -1 \\\\\n2 & 3 & -3\n\\end{array}\\right]=\\left[\\begin{array}{rrr}\n1 & 0 & 0 \\\\\n-1 & 1 & 0 \\\\\n1 & 2 & 1\n\\end{array}\\right]\\left[\\begin{array}{rrr}\n2 & 1 & 0 \\\\\n0 & 1 & -1 \\\\\n0 & 0 & -1\n\\end{array}\\right] .\n\\]\nSolve this system for \\(\\mathbf{b} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix}\\).\n\nSay that y = Ux.\n\n\n\n\n\nForward solve: \\[\n\\left[\\begin{array}{rrr}\n1 & 0 & 0 \\\\\n-1 & 1 & 0 \\\\\n1 & 2 & 1\n\\end{array}\\right]\\left[\\begin{array}{l}\ny_{1} \\\\\ny_{2} \\\\\ny_{3}\n\\end{array}\\right]=\\left[\\begin{array}{l}\n1 \\\\\n0 \\\\\n1\n\\end{array}\\right]\n\\]\n\\(y_{1}=1\\), then \\(y_{2}=0+1 y_{1}=1\\), then \\(y_{3}=1-1 y_{1}-2 y_{2}=-2\\).\n\n\n\nBack solve:\n\\[\n\\left[\\begin{array}{rrr}\n2 & 1 & 0 \\\\\n0 & 1 & -1 \\\\\n0 & 0 & -1\n\\end{array}\\right]\\left[\\begin{array}{l}\nx_{1} \\\\\nx_{2} \\\\\nx_{3}\n\\end{array}\\right]=\\left[\\begin{array}{r}\n1 \\\\\n1 \\\\\n-2\n\\end{array}\\right]\n\\]\n\\(x_{3}=-2 /(-1)=2\\), then \\(x_{2}=1+x_{3}=3\\), then \\(x_{1}=\\left(1-1 x_{2}\\right) / 2=-1\\)."
  },
  {
    "objectID": "lectures/ch2_lecture4.html#when-we-can-do-lu-factorization",
    "href": "lectures/ch2_lecture4.html#when-we-can-do-lu-factorization",
    "title": "Ch2 Lecture 4",
    "section": "When we can do LU factorization",
    "text": "When we can do LU factorization\n\nNot all square matrices have LU factorizations! This one doesn’t: \\(\\left[\\begin{array}{ll}0 & 1 \\\\ 1 & 0\\end{array}\\right]\\)\nIf Gaussian elimination can be performed on the matrix \\(A\\) without row exchanges, then the factorization exists\n\n(it’s really a by-product of Gaussian elimination.)\n\nIf row exchanges are needed, there is still a factorization that will work, but it’s a bit more complicated."
  },
  {
    "objectID": "lectures/ch2_lecture4.html#intuition-behind-lu-factorization",
    "href": "lectures/ch2_lecture4.html#intuition-behind-lu-factorization",
    "title": "Ch2 Lecture 4",
    "section": "Intuition behind LU factorization",
    "text": "Intuition behind LU factorization\n\nWhen we do Gaussian elimination, we are multiplying \\(A\\) by a series of elementary matrices to get it into upper triangular form.\nCall the product of all those elementary matrices \\(\\tilde L\\).\nSo our original system \\(A x = b\\) becomes \\(\\tilde L A x = \\tilde L b\\).\nWe recognize that \\(\\tilde L A\\) will be an upper triangular matrix, because that’s what we get when we do Gaussian elimination. This is our \\(U\\).\nBut what about the other part? We can multiply both sides by the inverse of \\(\\tilde L\\) to get\n\\[\n\\begin{aligned}\n\\tilde{L}^{-1} \\tilde{L} A x &= \\tilde{L}^{-1} \\tilde L b \\\\\n\\tilde{L}^{-1} U x &= b\n\\end{aligned}\n\\]\nNow the challenge is simply to find the inverse of \\(\\tilde L\\).\nRemember that \\(\\tilde L\\) is the product of the elementary matrices that we used when doing Gaussian elimination – call them \\(E_1\\), \\(E_2\\), etc.\nSo \\(\\tilde L = E_n E_{n-1} \\ldots E_1\\).\nThe inverse of a product of matrices is the product of the inverses in reverse order: $(E_n E_{n-1} E_1)^{-1} = E_1^{-1} E_2^{-1} E_n^{-1} $.\nFortunately, the inverse of an elementary matrix is easy to find: it’s just the same matrix with the opposite sign on the entry that was used to eliminate. So if we have the matrix which will add twice the first row to the second row, its inverse will be the matrix which will subtract twice the first row from the second row.\n\\[\nE = \\begin{bmatrix} 1 & 0 & 0 \\\\ 2 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\quad E^{-1} = \\begin{bmatrix} 1 & 0 & 0 \\\\ -2 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\n\\]\nAs it happens, when we follow the steps of Gaussian elimination (with no row swaps), the product of all these inverse matrices is just a lower triangular matrix, with each entry equal to the negative of the multiplier we used in doing the associated elimination step.\nThis is our \\(L\\)!\nSo all we need to do is keep track of these multipliers as we do Gaussian elimination, and we can use them to find the inverse of \\(\\tilde L\\). If we are doing things by hand, we can even write them into the lower part of our matrix as we go, since that part will be zeroed out anyways."
  },
  {
    "objectID": "lectures/ch2_lecture4.html#example-1",
    "href": "lectures/ch2_lecture4.html#example-1",
    "title": "Ch2 Lecture 4",
    "section": "Example",
    "text": "Example\nHere we do Gaussian elimination on the matrix \\(A = \\begin{bmatrix} 2 & 1 & 0 \\\\ -2 & 0 & -1 \\\\ 2 & 3 & -3 \\end{bmatrix}\\):\n\\(\\left[\\begin{smallmatrix}2 & 1 & 0\\\\-2 & 0 & -1\\\\2 & 3 & -3\\end{smallmatrix}\\right]\\) \\(\\xrightarrow[E_{31}(-1)]{E_{21}(1)}\\) \\(\\left[\\begin{smallmatrix}2 & 1 & 0\\\\0 & 1 & -1\\\\0 & 2 & -3\\end{smallmatrix}\\right]\\) \\(\\xrightarrow[E_{32}(-2)]{\\longrightarrow}\\) \\(\\left[\\begin{smallmatrix}2 & 1 & 0\\\\0 & 1 & -1\\\\0 & 0 & -1\\end{smallmatrix}\\right]\\)\n\n\n\nLet’s put those elementary row operations into matrix form. There were three of them:\n\n\\(E_{21}(1)\\) : \\(\\begin{bmatrix} 1 & 0 & 0 \\\\ 1 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\)\n\\(E_{31}(-1)\\): \\(\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ -1 & 0 & 1 \\end{bmatrix}\\)\n\\(E_{32}(-2)\\): \\(\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & -2 & 1 \\end{bmatrix}\\)\n\n\n\n\nThe inverses of these matrices are\n\n\\(\\begin{bmatrix} 1 & 0 & 0 \\\\ -1 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\), \\(\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 1 \\end{bmatrix}\\), and \\(\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 2 & 1 \\end{bmatrix}\\)."
  },
  {
    "objectID": "lectures/ch2_lecture4.html#section-4",
    "href": "lectures/ch2_lecture4.html#section-4",
    "title": "Ch2 Lecture 4",
    "section": "",
    "text": "The product of all these matrices is\n\\[\n\\left[\\begin{matrix}1 & 0 & 0\\\\0 & 1 & 0\\\\-1 & 0 & 1\\end{matrix}\\right]\\left[\\begin{matrix}1 & 0 & 0\\\\1 & 1 & 0\\\\0 & 0 & 1\\end{matrix}\\right]\\left[\\begin{matrix}1 & 0 & 0\\\\0 & 1 & 0\\\\0 & -2 & 1\\end{matrix}\\right]=\\left[\\begin{matrix}1 & 0 & 0\\\\1 & 1 & 0\\\\-1 & -2 & 1\\end{matrix}\\right]\n\\]\nThis is a lower triangular matrix, and it is the inverse of the matrix we used to do Gaussian elimination.\nWe can also see that the entries below the diagonal are the negatives of the multipliers we used in the elimination steps."
  },
  {
    "objectID": "lectures/ch2_lecture4.html#steps-to-lu-factorization",
    "href": "lectures/ch2_lecture4.html#steps-to-lu-factorization",
    "title": "Ch2 Lecture 4",
    "section": "Steps to LU factorization",
    "text": "Steps to LU factorization\nLet \\(\\left[a_{i j}^{(k)}\\right]\\) be the matrix obtained from \\(A\\) after using the \\(k\\) th pivot to clear out entries below it.\n\n(The original matrix is \\(A=\\left[a_{i j}^{(0)}\\right]\\))\n\n\nAll the row operations we will use include ratios \\(\\left(-a_{i j} / a_{j j}\\right)\\).\n\n\nThe row-adding elementary operations are of the form\n\\(E_{i j}\\left(-a_{i j}^{(k)} / a_{j j}^{(k)}\\right)\\)\n\n\nWe can give these ratios a name: multipliers.\n\\(m_{i j}=-a_{i j}^{(k)} / a_{j j}^{(k)}\\), where \\(i&gt;j\\)\n\n\n\nIf Gaussian elimination is used without row exchanges on the nonsingular matrix \\(A\\), resulting in the upper triangular matrix \\(U\\), and if \\(L\\) is the unit lower triangular matrix whose entries below the diagonal are the negatives of the multipliers \\(m_{i j}\\), then \\(A=L U\\)."
  },
  {
    "objectID": "lectures/ch2_lecture4.html#storing-the-multipliers-as-we-go",
    "href": "lectures/ch2_lecture4.html#storing-the-multipliers-as-we-go",
    "title": "Ch2 Lecture 4",
    "section": "Storing the multipliers as we go",
    "text": "Storing the multipliers as we go\nFor efficiency, we can just “store” the multipliers in the lower triangular part of the matrix on the left as we go along, since that will be zero anyways.\n\\[\n\\left[\\begin{array}{rrr}\n(2) & 1 & 0 \\\\\n-2 & 0 & -1 \\\\\n2 & 3 & -3\n\\end{array}\\right] \\xrightarrow[E_{31}(-1)]{E_{21}(1)}\\left[\\begin{array}{rrr}\n2 & 1 & 0 \\\\\n-1 & (1) & -1 \\\\\n1 & 2 & -3\n\\end{array}\\right] \\xrightarrow[E_{32}(-2)]{\\longrightarrow}\\left[\\begin{array}{rrr}\n2 & 1 & 0 \\\\\n-1 & 1 & -1 \\\\\n1 & 2 & -1\n\\end{array}\\right] .\n\\]\n\n\ncircle the multipliers as we go along\n\nNow we read off the results from the final matrix:\n\\[\nL=\\left[\\begin{array}{rrr}\n1 & 0 & 0 \\\\\n1 & 1 & 0 \\\\\n-1 & 2 & 1\n\\end{array}\\right] \\text { and } U=\\left[\\begin{array}{rrr}\n2 & 1 & 0 \\\\\n0 & 1 & -1 \\\\\n0 & 0 & -1\n\\end{array}\\right]\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture4.html#superaugmented-matrix",
    "href": "lectures/ch2_lecture4.html#superaugmented-matrix",
    "title": "Ch2 Lecture 4",
    "section": "Superaugmented matrix",
    "text": "Superaugmented matrix\nCould we just keep track by using the superaugmented matrix, like we did last lecture? What would that look like?\npause\n\n\\(\\left[\\begin{smallmatrix}2 & 1 & 0 & 1 & 0 & 0\\\\-2 & 0 & -1 & 0 & 1 & 0\\\\2 & 3 & -3 & 0 & 0 & 1\\end{smallmatrix}\\right]\\) \\(\\xrightarrow[E_{31}(-1)]{E_{21}(1)}\\) \\(\\left[\\begin{smallmatrix}2 & 1 & 0 & 1 & 0 & 0\\\\0 & 1 & -1 & 1 & 1 & 0\\\\0 & 2 & -3 & -1 & 0 & 1\\end{smallmatrix}\\right]\\) \\(\\xrightarrow[E_{32}(-2)]{\\longrightarrow}\\) \\(\\left[\\begin{smallmatrix}2 & 1 & 0 & 1 & 0 & 0\\\\0 & 1 & -1 & 1 & 1 & 0\\\\0 & 0 & -1 & -3 & -2 & 1\\end{smallmatrix}\\right]\\)\n\n\nOur superaugmented matrix does become an upper triangular matrix on the left and a lower triangular matrix on the right.\nUnfortunately, the lower triangular matrix on the right is \\(\\tilde{L}^{-1}\\), not \\(\\tilde{L}\\).\nSo we can’t just read off \\(L\\) and \\(U\\) from the superaugmented matrix."
  },
  {
    "objectID": "lectures/ch2_lecture4.html#plu-factorization",
    "href": "lectures/ch2_lecture4.html#plu-factorization",
    "title": "Ch2 Lecture 4",
    "section": "PLU factorization",
    "text": "PLU factorization\nWhat if we need row exchanges?\n\nWe could start off by doing all the row-exchanging elementary operations that we need, and store the product of these row-exchanging matrices as a matrix \\(P\\).\nThis product is called a permutation matrix\nApplying the correct permuatation matrix \\(P\\) to \\(A\\), we get a matrix for which Gaussian elimination will succeed without further row exchanges.\n\n\nNow we have a theorem that applies to all nonsingular matrices:\n\nIf \\(A\\) is a nonsingular matrix, then there exists a permutation matrix \\(P\\), upper triangular matrix \\(U\\), and unit lower triangular matrix \\(L\\) such that \\(P A=L U\\).\n\n\n\nSo, if you’ve got a nonsingular matrix \\(A\\), you can always find a permutation matrix \\(P\\), an upper triangular matrix \\(U\\), and a unit lower triangular matrix \\(L\\) that satisfy \\(P A=L U\\). Pretty neat, huh?\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n \n\n\n \n\n\n\n\n \n\n\n\nRBM schematic"
  }
]