[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stat 24320",
    "section": "",
    "text": "Title\n            \n                \n                    Lecture Day\n                \n                \n                \n                    Readings\n                \n                \n                    Colab Link\n        \n        \n                    \n                        \n                            Intro to linear systems\n                        \n                    \n\n                    \n                        \n                            1\n                        \n                        \n                        \n                            Ch. 1.1-1.3\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            More Systems of Linear Equations\n                        \n                    \n\n                    \n                        \n                            2\n                        \n                        \n                        \n                            Ch. 1.4-1.5\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch2 Lecture 1\n                        \n                    \n\n                    \n                        \n                            3\n                        \n                        \n                        \n                            2.1-2.3\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch2 Lecture 2\n                        \n                    \n\n                    \n                        \n                            4\n                        \n                        \n                        \n                            2.3-2.4\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch2 Lecture 3\n                        \n                    \n\n                    \n                        \n                            5\n                        \n                        \n                        \n                            2.3-2.8\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch2 Lecture 4\n                        \n                    \n\n                    \n                        \n                            6\n                        \n                        \n                        \n                            2.8\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch2 Lecture 5\n                        \n                    \n\n                    \n                        \n                            7\n                        \n                        \n                        \n                            2.4,2.5,2.6\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch3 Lecture 1\n                        \n                    \n\n                    \n                        \n                            8\n                        \n                        \n                        \n                            3.3, 3.4, 3.6\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n    \n\nNo matching items"
  },
  {
    "objectID": "index.html#lecture-notes",
    "href": "index.html#lecture-notes",
    "title": "Stat 24320",
    "section": "",
    "text": "Title\n            \n                \n                    Lecture Day\n                \n                \n                \n                    Readings\n                \n                \n                    Colab Link\n        \n        \n                    \n                        \n                            Intro to linear systems\n                        \n                    \n\n                    \n                        \n                            1\n                        \n                        \n                        \n                            Ch. 1.1-1.3\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            More Systems of Linear Equations\n                        \n                    \n\n                    \n                        \n                            2\n                        \n                        \n                        \n                            Ch. 1.4-1.5\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch2 Lecture 1\n                        \n                    \n\n                    \n                        \n                            3\n                        \n                        \n                        \n                            2.1-2.3\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch2 Lecture 2\n                        \n                    \n\n                    \n                        \n                            4\n                        \n                        \n                        \n                            2.3-2.4\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch2 Lecture 3\n                        \n                    \n\n                    \n                        \n                            5\n                        \n                        \n                        \n                            2.3-2.8\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch2 Lecture 4\n                        \n                    \n\n                    \n                        \n                            6\n                        \n                        \n                        \n                            2.8\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch2 Lecture 5\n                        \n                    \n\n                    \n                        \n                            7\n                        \n                        \n                        \n                            2.4,2.5,2.6\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Ch3 Lecture 1\n                        \n                    \n\n                    \n                        \n                            8\n                        \n                        \n                        \n                            3.3, 3.4, 3.6\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n    \n\nNo matching items"
  },
  {
    "objectID": "index.html#notebooks",
    "href": "index.html#notebooks",
    "title": "Stat 24320",
    "section": "Notebooks",
    "text": "Notebooks\n\n\n    \n            \n                \n        \n            Title\n            \n                \n                    Lecture Day\n                \n                \n                    Colab Link\n        \n        \n                    \n                        \n                            PageRank Tutorial\n                        \n                    \n\n                    \n                        \n                            1\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            Turing Patterns\n                        \n                    \n\n                    \n                        \n                            4\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n                \n                    \n                        \n                            MCMC Pagerank\n                        \n                    \n\n                    \n                        \n                            6\n                        \n                        \n                            \n\n                                \n                                        \n                                    \n\n                \n\n                \n    \n\nNo matching items"
  },
  {
    "objectID": "index.html#homeworks",
    "href": "index.html#homeworks",
    "title": "Stat 24320",
    "section": "Homeworks",
    "text": "Homeworks\n\n\n            \n                \n        \n            Title\n            \n                \n                    Due Date\n                \n                \n                \n                    Textbook Chapters\n                \n                \n                    Solutions\n        \n        \n                    \n                        \n                            Homework 1\n                        \n                    \n\n                    \n                        \n                            Wednesday, Week 2\n                        \n                        \n                        \n                            1\n                        \n                        \n                            \n                           \n                            \n                                \n                                        Homework 1 Solutions\n                                    \n                            \n\n                \n\n                \n                \n                    \n                        \n                            Homework 2\n                        \n                    \n\n                    \n                        \n                            Wednesday, Week 3\n                        \n                        \n                        \n                            2\n                        \n                        \n                            \n                           \n                            \n                                \n                                        Homework 2 Solutions\n                                    \n                            \n\n                \n\n                \n                \n                    \n                        \n                            Homework 3\n                        \n                    \n\n                    \n                        \n                            Wednesday, Week 4\n                        \n                        \n                        \n                            2.4, 2.5, 2.8\n                        \n                        \n                            \n                           \n                            \n\n                \n\n                \n                \n                    \n                        \n                            Projects 1\n                        \n                    \n\n                    \n                        \n                            Friday, Week 4\n                        \n                        \n                        \n                            1, 2\n                        \n                        \n                            \n                           \n                            \n\n                \n\n                \n    \n\nNo matching items"
  },
  {
    "objectID": "HW/HW1.html",
    "href": "HW/HW1.html",
    "title": "Homework 1",
    "section": "",
    "text": "1 \nBy solving a \\(3 \\times 3\\) system, find the coefficients in the equation of the parabola \\(y=\\alpha+\\beta x+\\gamma x^{2}\\) that passes through the points \\((1,1),(2,2)\\), and \\((3,0)\\).\n\n\n2 \nSuppose that 100 insects are distributed in an enclosure consisting of four chambers with passageways between them as shown below.\n\nAt the end of one minute, the insects have redistributed themselves. Assume that a minute is not enough time for an insect to visit more than one chamber and that at the end of a minute \\(40 \\%\\) of the insects in each chamber have not left the chamber they occupied at the beginning of the minute. The insects that leave a chamber disperse uniformly among the chambers that are directly accessible from the one they initially occupied-e.g., from \\(\\# 3\\), half move to \\(\\# 2\\) and half move to \\(\\# 4\\).\n\nIf at the end of one minute there are \\(12,25,26\\), and 37 insects in chambers \\(\\# 1, \\# 2, \\# 3\\), and \\(\\# 4\\), respectively, determine what the initial distribution had to be.\nIf the initial distribution is \\(20,20,20,40\\), what is the distribution at the end of one minute?\n\n\n\n3 \nSuppose that \\([\\mathbf{A} \\mid \\mathbf{b}]\\) is the augmented matrix associated with a linear system. You know that performing row operations on \\([\\mathbf{A} \\mid \\mathbf{b}]\\) does not change the solution of the system. However, no mention of column operations was ever made because column operations can alter the solution.\n\nDescribe the effect on the solution of a linear system when columns \\(\\mathbf{A}_{* j}\\) and \\(\\mathbf{A}_{* k}\\) are interchanged.\nDescribe the effect when column \\(\\mathbf{A}_{* j}\\) is replaced by \\(\\alpha \\mathbf{A}_{* j}\\) for \\(\\alpha \\neq 0\\).\nDescribe the effect when \\(\\mathbf{A}_{* j}\\) is replaced by \\(\\mathbf{A}_{* j}+\\alpha \\mathbf{A}_{* k}\\). Hint: Experiment with a \\(2 \\times 2\\) or \\(3 \\times 3\\) system.\n\n\n\n4 \nUse the Gauss-Jordan method to solve the following three systems at the same time.\n\\[\n\\begin{array}{rr|l|l}\n2 x_{1}-x_{2} & =1 & 0 & 0 \\\\\n-x_{1}+2 x_{2}-x_{3}&=0 & 1 & 0 \\\\\n-x_{2}+x_{3}&=0 & 0 & 1\n\\end{array}\n\\]\n\n\n5 \nConsider the following system:\n\\[\n\\begin{aligned}\n10^{-3} x-y & =1, \\\\\nx+y & =0 .\n\\end{aligned}\n\\]\n\nUse 3-digit arithmetic with no pivoting to solve this system.\nFind a system that is exactly satisfied by your solution from part (a), and note how close this system is to the original system.\nNow use partial pivoting and 3-digit arithmetic to solve the original system.\nFind a system that is exactly satisfied by your solution from part (c), and note how close this system is to the original system.\nUse exact arithmetic to obtain the solution to the original system, and compare the exact solution with the results of parts (a) and (c).\nRound the exact solution to three significant digits, and compare the result with those of parts (a) and (c).\n\n\n\n6 \nConsider the following well-scaled matrix:\n\\[\n\\mathbf{W}_{n}=\\left(\\begin{array}{rrrrrrr}\n1 & 0 & 0 & \\cdots & 0 & 0 & 1 \\\\\n-1 & 1 & 0 & \\cdots & 0 & 0 & 1 \\\\\n-1 & -1 & 1 & \\ddots & 0 & 0 & 1 \\\\\n\\vdots & \\vdots & \\ddots & \\ddots & \\ddots & \\vdots & \\vdots \\\\\n-1 & -1 & -1 & \\ddots & 1 & 0 & 1 \\\\\n-1 & -1 & -1 & \\cdots & -1 & 1 & 1 \\\\\n-1 & -1 & -1 & \\cdots & -1 & -1 & 1\n\\end{array}\\right)\n\\]\n\nReduce \\(\\mathbf{W}_{n}\\) to an upper-triangular form using Gaussian elimination with partial pivoting, and determine the element of maximal magnitude that emerges during the elimination procedure.\n\n\n\n7 \nAnswer True/False and explain your answers: 1. If a linear system is inconsistent, then the rank of the augmented matrix exceeds the number of unknowns. 2. Any homogeneous linear system is consistent. 3. A system of 3 linear equations in 4 unknowns has infinitely many solutions. 4. Every matrix can be reduced to only one matrix in row echelon form. 5. Any homogeneous linear system with more equations than unknowns has a nontrivial solution."
  },
  {
    "objectID": "HW/HW1.nosol.html",
    "href": "HW/HW1.nosol.html",
    "title": "Homework 1",
    "section": "",
    "text": "1 \nBy solving a \\(3 \\times 3\\) system, find the coefficients in the equation of the parabola \\(y=\\alpha+\\beta x+\\gamma x^{2}\\) that passes through the points \\((1,1),(2,2)\\), and \\((3,0)\\).\n\n\n2 \nSuppose that 100 insects are distributed in an enclosure consisting of four chambers with passageways between them as shown below.\n\nAt the end of one minute, the insects have redistributed themselves. Assume that a minute is not enough time for an insect to visit more than one chamber and that at the end of a minute \\(40 \\%\\) of the insects in each chamber have not left the chamber they occupied at the beginning of the minute. The insects that leave a chamber disperse uniformly among the chambers that are directly accessible from the one they initially occupied-e.g., from \\(\\# 3\\), half move to \\(\\# 2\\) and half move to \\(\\# 4\\).\n\nIf at the end of one minute there are \\(12,25,26\\), and 37 insects in chambers \\(\\# 1, \\# 2, \\# 3\\), and \\(\\# 4\\), respectively, determine what the initial distribution had to be.\nIf the initial distribution is \\(20,20,20,40\\), what is the distribution at the end of one minute?\n\n\n\n3 \nSuppose that \\([\\mathbf{A} \\mid \\mathbf{b}]\\) is the augmented matrix associated with a linear system. You know that performing row operations on \\([\\mathbf{A} \\mid \\mathbf{b}]\\) does not change the solution of the system. However, no mention of column operations was ever made because column operations can alter the solution.\n\nDescribe the effect on the solution of a linear system when columns \\(\\mathbf{A}_{* j}\\) and \\(\\mathbf{A}_{* k}\\) are interchanged.\nDescribe the effect when column \\(\\mathbf{A}_{* j}\\) is replaced by \\(\\alpha \\mathbf{A}_{* j}\\) for \\(\\alpha \\neq 0\\).\nDescribe the effect when \\(\\mathbf{A}_{* j}\\) is replaced by \\(\\mathbf{A}_{* j}+\\alpha \\mathbf{A}_{* k}\\). Hint: Experiment with a \\(2 \\times 2\\) or \\(3 \\times 3\\) system.\n\n\n\n4 \nUse the Gauss-Jordan method to solve the following three systems at the same time.\n\\[\n\\begin{array}{rr|l|l}\n2 x_{1}-x_{2} & =1 & 0 & 0 \\\\\n-x_{1}+2 x_{2}-x_{3}&=0 & 1 & 0 \\\\\n-x_{2}+x_{3}&=0 & 0 & 1\n\\end{array}\n\\]\n\n\n5 \nConsider the following system:\n\\[\n\\begin{aligned}\n10^{-3} x-y & =1, \\\\\nx+y & =0 .\n\\end{aligned}\n\\]\n\nUse 3-digit arithmetic with no pivoting to solve this system.\nFind a system that is exactly satisfied by your solution from part (a), and note how close this system is to the original system.\nNow use partial pivoting and 3-digit arithmetic to solve the original system.\nFind a system that is exactly satisfied by your solution from part (c), and note how close this system is to the original system.\nUse exact arithmetic to obtain the solution to the original system, and compare the exact solution with the results of parts (a) and (c).\nRound the exact solution to three significant digits, and compare the result with those of parts (a) and (c).\n\n\n\n6 \nConsider the following well-scaled matrix:\n\\[\n\\mathbf{W}_{n}=\\left(\\begin{array}{rrrrrrr}\n1 & 0 & 0 & \\cdots & 0 & 0 & 1 \\\\\n-1 & 1 & 0 & \\cdots & 0 & 0 & 1 \\\\\n-1 & -1 & 1 & \\ddots & 0 & 0 & 1 \\\\\n\\vdots & \\vdots & \\ddots & \\ddots & \\ddots & \\vdots & \\vdots \\\\\n-1 & -1 & -1 & \\ddots & 1 & 0 & 1 \\\\\n-1 & -1 & -1 & \\cdots & -1 & 1 & 1 \\\\\n-1 & -1 & -1 & \\cdots & -1 & -1 & 1\n\\end{array}\\right)\n\\]\n\nReduce \\(\\mathbf{W}_{n}\\) to an upper-triangular form using Gaussian elimination with partial pivoting, and determine the element of maximal magnitude that emerges during the elimination procedure.\n\n\n\n7 \nAnswer True/False and explain your answers: 1. If a linear system is inconsistent, then the rank of the augmented matrix exceeds the number of unknowns. 2. Any homogeneous linear system is consistent. 3. A system of 3 linear equations in 4 unknowns has infinitely many solutions. 4. Every matrix can be reduced to only one matrix in row echelon form. 5. Any homogeneous linear system with more equations than unknowns has a nontrivial solution."
  },
  {
    "objectID": "HW/HW1.sol.html",
    "href": "HW/HW1.sol.html",
    "title": "Homework 1 Solutions",
    "section": "",
    "text": "1 \nBy solving a \\(3 \\times 3\\) system, find the coefficients in the equation of the parabola \\(y=\\alpha+\\beta x+\\gamma x^{2}\\) that passes through the points \\((1,1),(2,2)\\), and \\((3,0)\\).\n\n(Meyer 1.2.10) \\(\\alpha=-3, \\beta=\\frac{11}{2}\\), and \\(\\gamma=-\\frac{3}{2}\\)\n\n\n\n2 \nSuppose that 100 insects are distributed in an enclosure consisting of four chambers with passageways between them as shown below.\n\nAt the end of one minute, the insects have redistributed themselves. Assume that a minute is not enough time for an insect to visit more than one chamber and that at the end of a minute \\(40 \\%\\) of the insects in each chamber have not left the chamber they occupied at the beginning of the minute. The insects that leave a chamber disperse uniformly among the chambers that are directly accessible from the one they initially occupied-e.g., from \\(\\# 3\\), half move to \\(\\# 2\\) and half move to \\(\\# 4\\).\n\nIf at the end of one minute there are \\(12,25,26\\), and 37 insects in chambers \\(\\# 1, \\# 2, \\# 3\\), and \\(\\# 4\\), respectively, determine what the initial distribution had to be.\nIf the initial distribution is \\(20,20,20,40\\), what is the distribution at the end of one minute?\n\n\n(Meyer 1.2.11) (a) If \\(x_{i}=\\) the number initially in chamber \\(\\# i\\), then\n\\[\n\\begin{aligned}\n.4 x_{1}+0 x_{2}+0 x_{3}+.2 x_{4} & =12 \\\\\n0 x_{1}+.4 x_{2}+.3 x_{3}+.2 x_{4} & =25 \\\\\n0 x_{1}+.3 x_{2}+.4 x_{3}+.2 x_{4} & =26 \\\\\n.6 x_{1}+.3 x_{2}+.3 x_{3}+.4 x_{4} & =37\n\\end{aligned}\n\\]\nand the solution is \\(x_{1}=10, x_{2}=20, x_{3}=30\\), and \\(x_{4}=40\\).\n\n\\(16,22,22,40\\)\n\n\n\n\n3 \nSuppose that \\([\\mathbf{A} \\mid \\mathbf{b}]\\) is the augmented matrix associated with a linear system. You know that performing row operations on \\([\\mathbf{A} \\mid \\mathbf{b}]\\) does not change the solution of the system. However, no mention of column operations was ever made because column operations can alter the solution.\n\nDescribe the effect on the solution of a linear system when columns \\(\\mathbf{A}_{* j}\\) and \\(\\mathbf{A}_{* k}\\) are interchanged.\nDescribe the effect when column \\(\\mathbf{A}_{* j}\\) is replaced by \\(\\alpha \\mathbf{A}_{* j}\\) for \\(\\alpha \\neq 0\\).\nDescribe the effect when \\(\\mathbf{A}_{* j}\\) is replaced by \\(\\mathbf{A}_{* j}+\\alpha \\mathbf{A}_{* k}\\). Hint: Experiment with a \\(2 \\times 2\\) or \\(3 \\times 3\\) system.\n\n\n(Meyer 1.2.13) a. This has the effect of interchanging the order of the unknowns \\(-x_{j}\\) and \\(x_{k}\\) are p b. The solution to the new system is the same as the solution to the old system except that the solution for the \\(j^{\\text {th }}\\) unknown of the new system is \\(\\hat{x}_{j}=\\frac{1}{\\alpha} x_{j}\\). This has the effect of “changing the units” of the \\(j^{\\text {th }}\\) unknown. c. The solution to the new system is the same as the solution for the old system except that the solution for the \\(k^{t h}\\) unknown in the new system is \\(\\hat{x}_{k}=x_{k}-\\alpha x_{j}\\).\n\n\n\n4 \nUse the Gauss-Jordan method to solve the following three systems at the same time.\n\\[\n\\begin{array}{rr|l|l}\n2 x_{1}-x_{2} & =1 & 0 & 0 \\\\\n-x_{1}+2 x_{2}-x_{3}&=0 & 1 & 0 \\\\\n-x_{2}+x_{3}&=0 & 0 & 1\n\\end{array}\n\\]\n\n(Meyer 1.3.3) \\[\n\\left(\\begin{array}{lll}1 & 1 & 1 \\\\ 1 & 2 & 2 \\\\ 1 & 2 & 3\\end{array}\\right)\n\\]\n\n\n\n5 \nConsider the following system:\n\\[\n\\begin{aligned}\n10^{-3} x-y & =1, \\\\\nx+y & =0 .\n\\end{aligned}\n\\]\n\nUse 3-digit arithmetic with no pivoting to solve this system.\nFind a system that is exactly satisfied by your solution from part (a), and note how close this system is to the original system.\nNow use partial pivoting and 3-digit arithmetic to solve the original system.\nFind a system that is exactly satisfied by your solution from part (c), and note how close this system is to the original system.\nUse exact arithmetic to obtain the solution to the original system, and compare the exact solution with the results of parts (a) and (c).\nRound the exact solution to three significant digits, and compare the result with those of parts (a) and (c).\n\n\n(Meyer 1.5.1) (a)\n\\(\\left[\\begin{smallmatrix}0.001 & -1 & 1\\\\1 & 1 & 0\\end{smallmatrix}\\right]\\) \\(\\overrightarrow{E_{2,1}(1000)}\\) \\(\\left[\\begin{smallmatrix}0.001 & -1.0 & 1.0\\\\0 & 1.0 \\cdot 10^{3} & -1.0 \\cdot 10^{3}\\end{smallmatrix}\\right]\\) \\(\\overrightarrow{E_{1}(1000)}\\) \\(\\left[\\begin{smallmatrix}1.00 & -1.00 \\cdot 10^{3} & 1.00 \\cdot 10^{3}\\\\0 & 1.00 \\cdot 10^{3} & -1.00 \\cdot 10^{3}\\end{smallmatrix}\\right]\\)\nThe solution is \\((0,-1)\\).\n\nOne example of a system whose exact solution is \\(0,-1\\) would be\n\n\\[\n\\begin{aligned}\n2x-y & =1, \\\\\nx+y & =-1 .\n\\end{aligned}\n\\]\nThis is not close at all to the original system!\n\n\n\n\\(\\left[\\begin{smallmatrix}0.001 & -1 & 1\\\\1 & 1 & 0\\end{smallmatrix}\\right]\\) \\(\\overrightarrow{E_{2,1}}\\) \\(\\left[\\begin{smallmatrix}1.0 & 1.0 & 0\\\\0.001 & -1.0 & 1.0\\end{smallmatrix}\\right]\\) \\(\\overrightarrow{E_{2,1}(1/1000)}\\) \\(\\left[\\begin{smallmatrix}1.00 & 1.00 & 0\\\\-7.25 \\cdot 10^{-8} & -1.00 & 1.00\\end{smallmatrix}\\right]\\)\nSolution is \\((1,-1)\\).\n\nOne example of a system whose exact solution is \\(1,-1\\) would be \\[\n\\begin{aligned}\n0 x-y & =1, \\\\\nx+y & =0 .\n\\end{aligned}\n\\]\n\nThis is very close to the original system!\n\n\\(\\left(\\frac{1}{1.001}, \\frac{-1}{1.001}\\right)\\)\n\nThis is close to our solution in (c).\n\n\n\n6 \nConsider the following well-scaled matrix:\n\\[\n\\mathbf{W}_{n}=\\left(\\begin{array}{rrrrrrr}\n1 & 0 & 0 & \\cdots & 0 & 0 & 1 \\\\\n-1 & 1 & 0 & \\cdots & 0 & 0 & 1 \\\\\n-1 & -1 & 1 & \\ddots & 0 & 0 & 1 \\\\\n\\vdots & \\vdots & \\ddots & \\ddots & \\ddots & \\vdots & \\vdots \\\\\n-1 & -1 & -1 & \\ddots & 1 & 0 & 1 \\\\\n-1 & -1 & -1 & \\cdots & -1 & 1 & 1 \\\\\n-1 & -1 & -1 & \\cdots & -1 & -1 & 1\n\\end{array}\\right)\n\\]\n\nReduce \\(\\mathbf{W}_{n}\\) to an upper-triangular form using Gaussian elimination with partial pivoting, and determine the element of maximal magnitude that emerges during the elimination procedure.\n\n\n(Meyer 1.5.7) \\(2^{n-1}\\)\n\n\n\n7 \nAnswer True/False and explain your answers: 1. If a linear system is inconsistent, then the rank of the augmented matrix exceeds the number of unknowns. 2. Any homogeneous linear system is consistent. 3. A system of 3 linear equations in 4 unknowns has infinitely many solutions. 4. Every matrix can be reduced to only one matrix in row echelon form. 5. Any homogeneous linear system with more equations than unknowns has a nontrivial solution.\n\n(Shores 1.4.19)\n\nFalse. One very simple counterexample: \\(0 x=1\\).\n\nAnother: \\(\\left[\\begin{array}{ll}1 & 1 \\\\ 1 & 1\\end{array}\\right]\\left[\\begin{array}{l}x_{1} \\\\ x_{2}\\end{array}\\right]=\\left[\\begin{array}{l}1 \\\\ 2\\end{array}\\right]\\). The rank of the augmented matrix is 2, but the system is inconsistent.\nWe know that \\(b\\) must not be a linear combination of the columns of \\(A\\) for the system to be inconsistent; however, there is no requirement that the columns of \\(A\\) be linearly independent. Therefore the rank of the augmented matrix could be less than the number of unknowns.\n\nTrue, a homogeneous system must have the trivial solution.\nFalse, it could be inconsistent (and therefore have zero solutions).\nFalse. Here is a counterexample. The matrix \\(\\left[\\begin{array}{ll}2 & 2 \\\\ 0 & 1\\end{array}\\right]\\) can be reduced to either \\(\\left[\\begin{array}{ll}2 & 0 \\\\ 0 & 1\\end{array}\\right]\\) or \\(\\left[\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right]\\). (Only the latter is in reduced row echelon form, which must be unique.)\nFalse, a counterexample is \\(\\left[\\begin{array}{l}1 \\\\ 2\\end{array}\\right] x=\\left[\\begin{array}{l}0 \\\\ 0\\end{array}\\right]\\)."
  },
  {
    "objectID": "HW/HW2.sol.html",
    "href": "HW/HW2.sol.html",
    "title": "Homework 2 Solutions",
    "section": "",
    "text": "1 \nGiven that a linear system in the unknowns \\(x_{1}, x_{2}, x_{3}, x_{4}\\) has general solution \\(\\left(x_{2}+3 x_{4}+4, x_{2}, 2-x_{4}, x_{4}\\right)\\) for free variables \\(x_{2}, x_{4}\\), find a minimal reduced row echelon for this system.\n\nExercise 2.1.13\nWe know that there are at least two equations in the system (since we have two constraints in the general solution.) We can write these two constraints as:\n\\(R=\\begin{bmatrix}1 & -1 & 0 & -3 & 4 \\\\ 0 & 0 & 1 & 1 & 2\\end{bmatrix}\\)\nWe could also have written\n\\(R=\\begin{bmatrix}1 & -1 & 0 & -3 & 4 \\\\ 0 & 0 & 1 & 1 & 2 \\\\ 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0\\end{bmatrix}\\), which has the same solutions.\nJust for kicks, checking in Sympy:\n\nfrom sympy import print_latex\ndef my_print(x, *args, **kwargs):\n    print_latex(x, itex=False, mode='equation', *args, **kwargs)\n\n\nfrom sympy import Matrix\nR=Matrix([[1, -1, 0, -3], [0, 0, 1, 1]])\nrhs = Matrix([4,2])\nR.gauss_jordan_solve(rhs)\nmy_print(R.gauss_jordan_solve(rhs))\n\n\\begin{equation}\\left( \\left[\\begin{matrix}\\tau_{0} + 3 \\tau_{1} + 4\\\\\\tau_{0}\\\\2 - \\tau_{1}\\\\\\tau_{1}\\end{matrix}\\right], \\  \\left[\\begin{matrix}\\tau_{0}\\\\\\tau_{1}\\end{matrix}\\right]\\right)\\end{equation}\n\n\nfrom sympy import Matrix, print_latex\nR=Matrix([[1, -1, 0, -3], [0, 0, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]])\nrhs = Matrix([4,2,0,0])\nmy_print(R.gauss_jordan_solve(rhs))\n\\[\\begin{equation}\\left( \\left[\\begin{matrix}\\tau_{0} + 3 \\tau_{1} + 4\\\\\\tau_{0}\\\\2 - \\tau_{1}\\\\\\tau_{1}\\end{matrix}\\right], \\  \\left[\\begin{matrix}\\tau_{0}\\\\\\tau_{1}\\end{matrix}\\right]\\right)\\end{equation}\\]\n\n\n\n2 \nUse the technique of Example 2.10 in your textbook to balance the following chemical equation:\n\\[\n\\mathrm{C}_{8} \\mathrm{H}_{18}+\\mathrm{O}_{2} \\rightarrow \\mathrm{CO}_{2}+\\mathrm{H}_{2} \\mathrm{O} .\n\\]\n\nExercise 2.2.23\nWith vectors indicating amount of \\(\\mathrm{C}, \\mathrm{H}\\) and \\(\\mathrm{O}\\), variables the number of molecules of each compound occurring, system represented is\n\\[\nx_{1}\\left[\\begin{array}{r}\n8 \\\\\n18 \\\\\n0\n\\end{array}\\right]+x_{2}\\left[\\begin{array}{l}\n0 \\\\\n0 \\\\\n2\n\\end{array}\\right]=x_{3}\\left[\\begin{array}{l}\n1 \\\\\n0 \\\\\n2\n\\end{array}\\right]+x_{4}\\left[\\begin{array}{l}\n0 \\\\\n2 \\\\\n1\n\\end{array}\\right]\n\\]\nresulting in coefficient matrix\n\\[\n\\left[\\begin{array}{cccc}\n8 & 0 & -1 & 0 \\\\\n18 & 0 & 0 & -2 \\\\\n0 & 2 & -2 & -1\n\\end{array}\\right] \\rightarrow\\left[\\begin{array}{cccc}\n1 & 0 & 0 & -\\frac{1}{9} \\\\\n0 & 1 & 0 & -\\frac{25}{18} \\\\\n0 & 0 & 1 & -\\frac{8}{9}\n\\end{array}\\right]\n\\]\nThe smallest integer solution is \\(x_{1}=2, x_{2}=23, x_{3}=16, x_{4}=18\\).\n\n\n\\begin{equation}\\left[\\begin{matrix}2 & 25 & 16 & 18\\end{matrix}\\right]\\end{equation}\n\\begin{equation}\\left[\\begin{matrix}1 & 0 & 0 & - \\frac{1}{9}\\\\0 & 1 & 0 & - \\frac{25}{18}\\\\0 & 0 & 1 & - \\frac{8}{9}\\end{matrix}\\right]\\end{equation}\n\n\n\n\n\n3 \nExpress the following functions, if linear, as matrix operators. (If not linear, explain why.)\n\n\\(T\\left(\\left(x_{1}, x_{2}\\right)\\right)=\\left(x_{1}+x_{2}, 2 x_{1}, 4 x_{2}-x_{1}\\right)\\)\n\\(T\\left(\\left(x_{1}, x_{2}\\right)\\right)=\\left(x_{1}+x_{2}, 2 x_{1} x_{2}\\right)\\)\n\\(T\\left(\\left(x_{1}, x_{2}, x_{3}\\right)\\right)=\\left(2 x_{3},-x_{1}\\right)\\)\n\\(T\\left(\\left(x_{1}, x_{2}, x_{3}\\right)\\right)=\\left(x_{2}-x_{1}, x_{3}, x_{2}+x_{3}\\right)\\)\n\n\nExercise 2.3.3\nOperator is \\(T_{A}\\) with:\n\n\\(A=\\left[\\begin{array}{rr}1 & 1 \\\\ 2 & 0 \\\\ 4 & -1\\end{array}\\right]\\)\nnonlinear\n\\(\\left[\\begin{array}{rrr}0 & 0 & 2 \\\\ -1 & 0 & 0\\end{array}\\right]\\left(\\right.\\)\n\n\n\\(A=\\left[\\begin{array}{rrr}-1 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 1 & 1\\end{array}\\right]\\)\n\n\n\n\n4 \nA fixed-point of a linear operator \\(T_{A}\\) is a vector \\(\\mathbf{x}\\) such that \\(T_{A}(\\mathbf{x})=\\mathbf{x}\\). Find all fixed points, if any, of the linear operators in the previous exercise.\n\nExercise 2.3.9\nWe require our matrix to be square (i.e., have the same number of rows and columns) to have a fixed point. For the matrices in the previous exercise, only the matrix in part (d) is square. The fixed points are the solutions to the system of equations\n\\[\n\\left[\\begin{array}{rrr}-1 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 1 & 1\\end{array}\\right]\\left[\\begin{array}{l}x_{1} \\\\ x_{2} \\\\ x_{3}\\end{array}\\right]=\\left[\\begin{array}{l}x_{1} \\\\ x_{2} \\\\ x_{3}\\end{array}\\right]\n\\]\nFirst, we subtract the right-hand side from the left-hand side to get\n\\[\n\\left[\\begin{array}{rrr}-1 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 1 & 1\\end{array}\\right]\\left[\\begin{array}{l}x_{1} \\\\ x_{2} \\\\ x_{3}\\end{array}\\right]-\\left[\\begin{array}{l}x_{1} \\\\ x_{2} \\\\ x_{3}\\end{array}\\right]=\\left[\\begin{array}{l}0 \\\\ 0 \\\\ 0\\end{array}\\right]\n\\]\nThis simplifies to\n\\[\n\\left[\\begin{array}{rrr}-2 & 1 & 0 \\\\ 0 & -1 & 1 \\\\ 0 & 1 & 0\\end{array}\\right]\\left[\\begin{array}{l}x_{1} \\\\ x_{2} \\\\ x_{3}\\end{array}\\right]=\\left[\\begin{array}{l}0 \\\\ 0 \\\\ 0\\end{array}\\right]\n\\]\nPutting our matrix into reduced row echelon form, we have\n\\[\n\\left[\\begin{array}{rrr}-2 & 1 & 0 \\\\ 0 & -1 & 1 \\\\ 0 & 1 & 0\\end{array}\\right] \\rightarrow\\left[\\begin{array}{rrr}1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1\\end{array}\\right]\n\\]\nThis is the identity matrix, which means that the system of equations is nonsingular and has a unique solution. Moreover, the right-hand side of the equation is the zero vector, which means that the fixed point is the zero vector: \\(\\left(x_{1}, x_{2}, x_{3}\\right)=\\left(0,0,0\\right)\\).\n\nfrom sympy import eye\n#| echo: false\n#| output: none\n\nA = Matrix([[-1, 1, 0], [0, 0, 1], [0, 1, 1]])\n(A-eye(3))\n# append a zero vector to the right of the matrix\nR = (A-eye(3)).row_join(Matrix([0,0,0]))\nR.rref()\n\n(Matrix([\n [1, 0, 0, 0],\n [0, 1, 0, 0],\n [0, 0, 1, 0]]),\n (0, 1, 2))\n\n\n\n\n\n5 \nA linear operator on \\(\\mathbb{R}^{2}\\) is defined by first applying a scaling operator with scale factors of 2 in the \\(x\\)-direction and 4 in the \\(y\\)-direction, followed by a counterclockwise rotation about the origin of \\(\\pi / 6\\) radians. Express this operator and the operator that results from reversing the order of the scaling and rotation as matrix operators.\n\nExercise 2.3.5 The scaling operator is given by the matrix\n\\[\n\\left[\\begin{array}{ll}\n2 & 0 \\\\\n0 & 4\n\\end{array}\\right]\n\\]\nTo create a counterclockwise rotation by \\(\\theta\\) radians, we use the matrix\n\\[\n\\left[\\begin{array}{ll}\n\\cos \\theta & -\\sin \\theta \\\\\n\\sin \\theta & \\cos \\theta\n\\end{array}\\right]\n\\]\nHere, we want to rotate by \\(\\pi / 6\\) radians, so we have\n\\[\n\\left[\\begin{array}{ll}\n\\cos \\frac{\\pi}{6} & -\\sin \\frac{\\pi}{6} \\\\\n\\sin \\frac{\\pi}{6} & \\cos \\frac{\\pi}{6}\n\\end{array}\\right]=\\left[\\begin{array}{ll}\n\\frac{\\sqrt{3}}{2} & -\\frac{1}{2} \\\\\n\\frac{1}{2} & \\frac{\\sqrt{3}}{2}\n\\end{array}\\right]\n\\]\nThe operator for the scaling followed by the rotation is the product of these two matrices:\n\\[T_{A}, A=\n\\left[\\begin{array}{ll}\n\\frac{\\sqrt{3}}{2} & -\\frac{1}{2} \\\\\n\\frac{1}{2} & \\frac{\\sqrt{3}}{2}\n\\end{array}\\right]\\left[\\begin{array}{ll}\n2 & 0 \\\\\n0 & 4\n\\end{array}\\right]=\\left[\\begin{array}{ll}\n\\sqrt{3} & -2 \\\\\n1 & 2 \\sqrt{3}\n\\end{array}\\right]\n\\]\nThe operator for the rotation followed by the scaling is the product of the matrices in reverse order:\n\\[T_{B}, B=\n\\left[\\begin{array}{ll}\n2 & 0 \\\\\n0 & 4\n\\end{array}\\right]\\left[\\begin{array}{ll}\n\\frac{\\sqrt{3}}{2} & -\\frac{1}{2} \\\\\n\\frac{1}{2} & \\frac{\\sqrt{3}}{2}\n\\end{array}\\right]=\\left[\\begin{array}{ll}\n\\sqrt{3} & -1 \\\\\n2 & 2 \\sqrt{3}\n\\end{array}\\right]\n\\]\n\n\n\n6 \nFind a scaling operator \\(S\\) and shearing operator \\(H\\) such that the concatenation \\(S \\circ H\\) maps the points \\((1,0)\\) to \\((2,0)\\) and \\((0,1)\\) to \\((4,3)\\).\n\nExercise 2.3.7\nWe know that the shearing operator must add a multiple of the \\(x_2\\) component to the \\(x_1\\) component. For now, we can write this as\n\\[\n\\left[\\begin{array}{ll}\n1 & \\alpha \\\\\n0 & 1\n\\end{array}\\right]\n\\]\nThen we can apply the shearing operator to our two points to get\n\\[\n\\left[\\begin{array}{ll}\n1 & \\alpha \\\\\n0 & 1\n\\end{array}\\right]\\left[\\begin{array}{l}\n1 \\\\\n0\n\\end{array}\\right]=\\left[\\begin{array}{l}\n1 \\\\\n0\n\\end{array}\\right]\n\\]\nand\n\\[\n\\left[\\begin{array}{ll}\n1 & \\alpha \\\\\n0 & 1\n\\end{array}\\right]\\left[\\begin{array}{l}\n0 \\\\\n1\n\\end{array}\\right]=\\left[\\begin{array}{l}\n\\alpha \\\\\n1\n\\end{array}\\right]\n\\]\nWe know our scaling operator will be of the form\n\\[\n\\left[\\begin{array}{ll}\ns & 0 \\\\\n0 & t\n\\end{array}\\right]\n\\]\nApplying this to our two points, we get\n\\[\n\\left[\\begin{array}{ll}\ns & 0 \\\\\n0 & t\n\\end{array}\\right]\\left[\\begin{array}{l}\n1 \\\\\n0\n\\end{array}\\right]=\\left[\\begin{array}{l}\ns \\\\\n0\n\\end{array}\\right]\n\\]\nand\n\\[\n\\left[\\begin{array}{ll}\ns & 0 \\\\\n0 & t\n\\end{array}\\right]\\left[\\begin{array}{l}\n\\alpha \\\\\n1\n\\end{array}\\right]=\\left[\\begin{array}{l}\ns \\alpha \\\\\nt\n\\end{array}\\right]\n\\]\nWe can now set up the equations\n\\[\n\\begin{aligned}\ns &=2 , 0=0\\\\\n\\alpha s &=4,\nt =3\n\\end{aligned}\n\\]\nIt is clear that the solutions to these equations are \\(s=2, t=3, \\alpha=2\\). Thus, our scaling operator is\n\\[\nS=\\left[\\begin{array}{ll}2 & 0 \\\\ 0 & 3\\end{array}\\right]\n\\]\nand our shearing operator is\n\\[\nH=\\left[\\begin{array}{ll}1 & 2 \\\\ 0 & 1\\end{array}\\right]\n\\]\nChecking:\n\nS = Matrix([[2, 0], [0, 3]])\nH = Matrix([[1, 2], [0, 1]])\nS*H*Matrix([1,0]), S*H*Matrix([0,1])\n\n(Matrix([\n [2],\n [0]]),\n Matrix([\n [4],\n [3]]))\n\n\n\n\n\n7 \nGiven transition matrices for discrete dynamical systems\n\n\\(\\left[\\begin{array}{rrr}.1 & .3 & 0 \\\\ 0 & .4 & 1 \\\\ .9 & .3 & 0\\end{array}\\right] \\quad\\) (b) \\(\\left[\\begin{array}{lll}0 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 0\\end{array}\\right] \\quad\\) (c) \\(\\left[\\begin{array}{rrr}.5 & .3 & 0 \\\\ 0 & .4 & 0 \\\\ .5 & .3 & 1\\end{array}\\right] \\quad\\) (d) \\(\\left[\\begin{array}{rrr}0 & 0 & 0.9 \\\\ 0.5 & 0 & 0 \\\\ 0 & 0.5 & 0.1\\end{array}\\right]\\) and initial state vector \\(\\mathbf{x}^{(0)}=\\frac{1}{2}(1,1,0)\\), calculate the first and second state vector for each system and determine whether it is a Markov chain.\n\n\nExercise 2.3.11\nWe know going in that (d) is not a stochastic matrix, because the neither the first or second column sums to 1. The others are stochastic.\nFirst and second states are (a) \\((0.2,0.2,0.6)\\), \\((0.08,0.68,0.24)\\) (b) \\(\\frac{1}{2}(0,1,1), \\frac{1}{2}(1,1,0)\\) (c) \\((0.4,0.3,0.4),(0.26,0.08,0.66)\\) (d) \\((0,0.25,0.25),(0.225,0,0.15)\\)\nAs expected, the states sum to 1 for a-c, but not for d. Therefore a-c are Markov chains.\n\n\n\n8 \nFor each of the dynamical systems of the previous exercise, determine by calculation whether the system tends to a limiting steady-state vector. If so, what is it?\n\nExercise 2.3.12\nWe can do this in Python by multiplying the initial state vector by each matrix a large number of times and seeing if the result converges.\n\nfrom sympy import Matrix\nA1 = Matrix([[0.1, 0.3, 0], [0, 0.4, 1], [0.9, 0.3, 0]])\nA2 = Matrix([[0, 0, 1], [0, 1, 0], [1, 0, 0]])\nA3 = Matrix([[0.5, 0.3, 0], [0, 0.4, 0], [0.5, 0.3, 1]])\nA4 = Matrix([[0, 0, 0.9], [0.5, 0, 0], [0, 0.5, 0.1]])\nx = Matrix([1/2, 1/2, 0])\n\ndef limit(A, x0, n):\n    print(x0)\n    for i in range(n):\n        x0 = A*x0\n        # This was initially confusing -- printing only every 10 iterations, I missed the fact that the chain alternates between two states.\n        if i % 10 == 0 or i % 10 == 1:\n            print(x0)\n\nprint(\"A1\")\nlimit(A1, x, 100)\nprint(\"A2\")\nlimit(A2, x, 100)\nprint(\"A3\")\nlimit(A3, x, 100)\nprint(\"A4\")\nlimit(A4, x, 100)\n\nA1\nMatrix([[0.500000000000000], [0.500000000000000], [0]])\nMatrix([[0.200000000000000], [0.200000000000000], [0.600000000000000]])\nMatrix([[0.0800000000000000], [0.680000000000000], [0.240000000000000]])\nMatrix([[0.172271278520000], [0.517519069520000], [0.310209651960000]])\nMatrix([[0.172482848708000], [0.517217279768000], [0.310299871524000]])\nMatrix([[0.172413911708915], [0.517241315268424], [0.310344773022661]])\nMatrix([[0.172413785751419], [0.517241299130031], [0.310344915118550]])\nMatrix([[0.172413793080544], [0.517241379195438], [0.310344827724019]])\nMatrix([[0.172413793066686], [0.517241379402194], [0.310344827531121]])\nMatrix([[0.172413793103395], [0.517241379310495], [0.310344827586110]])\nMatrix([[0.172413793103488], [0.517241379310308], [0.310344827586204]])\nMatrix([[0.172413793103448], [0.517241379310345], [0.310344827586207]])\nMatrix([[0.172413793103448], [0.517241379310345], [0.310344827586207]])\nMatrix([[0.172413793103448], [0.517241379310345], [0.310344827586207]])\nMatrix([[0.172413793103448], [0.517241379310345], [0.310344827586207]])\nMatrix([[0.172413793103448], [0.517241379310345], [0.310344827586207]])\nMatrix([[0.172413793103448], [0.517241379310345], [0.310344827586207]])\nMatrix([[0.172413793103448], [0.517241379310345], [0.310344827586207]])\nMatrix([[0.172413793103448], [0.517241379310345], [0.310344827586207]])\nMatrix([[0.172413793103448], [0.517241379310345], [0.310344827586207]])\nMatrix([[0.172413793103448], [0.517241379310345], [0.310344827586207]])\nA2\nMatrix([[0.500000000000000], [0.500000000000000], [0]])\nMatrix([[0], [0.500000000000000], [0.500000000000000]])\nMatrix([[0.500000000000000], [0.500000000000000], [0]])\nMatrix([[0], [0.500000000000000], [0.500000000000000]])\nMatrix([[0.500000000000000], [0.500000000000000], [0]])\nMatrix([[0], [0.500000000000000], [0.500000000000000]])\nMatrix([[0.500000000000000], [0.500000000000000], [0]])\nMatrix([[0], [0.500000000000000], [0.500000000000000]])\nMatrix([[0.500000000000000], [0.500000000000000], [0]])\nMatrix([[0], [0.500000000000000], [0.500000000000000]])\nMatrix([[0.500000000000000], [0.500000000000000], [0]])\nMatrix([[0], [0.500000000000000], [0.500000000000000]])\nMatrix([[0.500000000000000], [0.500000000000000], [0]])\nMatrix([[0], [0.500000000000000], [0.500000000000000]])\nMatrix([[0.500000000000000], [0.500000000000000], [0]])\nMatrix([[0], [0.500000000000000], [0.500000000000000]])\nMatrix([[0.500000000000000], [0.500000000000000], [0]])\nMatrix([[0], [0.500000000000000], [0.500000000000000]])\nMatrix([[0.500000000000000], [0.500000000000000], [0]])\nMatrix([[0], [0.500000000000000], [0.500000000000000]])\nMatrix([[0.500000000000000], [0.500000000000000], [0]])\nA3\nMatrix([[0.500000000000000], [0.500000000000000], [0]])\nMatrix([[0.400000000000000], [0.200000000000000], [0.400000000000000]])\nMatrix([[0.260000000000000], [0.0800000000000000], [0.660000000000000]])\nMatrix([[0.000913647940000000], [2.09715200000000e-5], [0.999065380540000]])\nMatrix([[0.000463115426000000], [8.38860800000000e-6], [0.999528495966000]])\nMatrix([[9.47077246639594e-7], [2.19902325555200e-9], [0.999999050723730]])\nMatrix([[4.74198330296463e-7], [8.79609302220801e-10], [0.999999524922060]])\nMatrix([[9.30630821712715e-10], [2.30584300921370e-13], [0.999999999069139]])\nMatrix([[4.65384586146634e-10], [9.22337203685479e-14], [0.999999999534523]])\nMatrix([[9.09422166223752e-13], [2.41785163922926e-17], [0.999999999999090]])\nMatrix([[4.54718336666794e-13], [9.67140655691706e-18], [0.999999999999545]])\nMatrix([[8.88170813796524e-16], [2.53530120045647e-21], [0.999999999999999]])\nMatrix([[4.44086167488622e-16], [1.01412048018259e-21], [0.999999999999999]])\nMatrix([[8.67360940451606e-19], [2.65845599156984e-25], [1.00000000000000]])\nMatrix([[4.33680549979483e-19], [1.06338239662794e-25], [1.00000000000000]])\nMatrix([[8.47032863626506e-22], [2.78759314981634e-29], [1.00000000000000]])\nMatrix([[4.23516440176032e-22], [1.11503725992654e-29], [1.00000000000000]])\nMatrix([[8.27180603784018e-25], [2.92300327466182e-33], [1.00000000000000]])\nMatrix([[4.13590302768910e-25], [1.16920130986473e-33], [1.00000000000000]])\nMatrix([[8.07793566026819e-28], [3.06499108173179e-37], [1.00000000000000]])\nMatrix([[4.03896783105359e-28], [1.22599643269272e-37], [1.00000000000000]])\nA4\nMatrix([[0.500000000000000], [0.500000000000000], [0]])\nMatrix([[0], [0.250000000000000], [0.250000000000000]])\nMatrix([[0.225000000000000], [0], [0.150000000000000]])\nMatrix([[0.00302039010000000], [0.00120386925000000], [0.00264539827500000]])\nMatrix([[0.00238085844750000], [0.00151019505000000], [0.000866474452500000]])\nMatrix([[4.03791461577412e-5], [2.52845782290541e-5], [2.17203569987220e-5]])\nMatrix([[1.95483212988498e-5], [2.01895730788706e-5], [1.48143248143993e-5]])\nMatrix([[4.06315437745616e-7], [3.50928578393455e-7], [2.81839630258029e-7]])\nMatrix([[2.53655667232226e-7], [2.03157718872808e-7], [2.03648252222531e-7]])\nMatrix([[4.89866853485548e-9], [3.95031105574765e-9], [3.70325613478943e-9]])\nMatrix([[3.33293052131049e-9], [2.44933426742774e-9], [2.34548114135277e-9]])\nMatrix([[6.19440844858878e-11], [4.70794262956052e-11], [4.43926066707180e-11]])\nMatrix([[3.99533460036462e-11], [3.09720422429439e-11], [2.79789738148744e-11]])\nMatrix([[7.53264744484970e-13], [5.82090739342239e-13], [5.32977001565985e-13]])\nMatrix([[4.79679301409387e-13], [3.76632372242485e-13], [3.44343069827718e-13]])\nMatrix([[9.09975559486833e-15], [7.10204455266123e-15], [6.50648903042589e-15]])\nMatrix([[5.85584012738330e-15], [4.54987779743417e-15], [4.20167117937320e-15]])\nMatrix([[1.10752324227116e-16], [8.61303873249460e-17], [7.93329748585034e-17]])\nMatrix([[7.13996773726531e-17], [5.53761621135579e-17], [5.09984911483233e-17]])\nMatrix([[1.34906603357799e-18], [1.04742159016943e-18], [9.64385211129635e-19]])\nMatrix([[8.67946690016671e-19], [6.74533016788995e-19], [6.20149316197677e-19]])\n\n\nAnswers:\n\nTends to steady state \\((0.172414,0.517241,0.310345)\\).\nNo steady state (alternates).\nYes, steady state \\((0,0,1)\\).\nYes, steady state \\((0,0,0)\\).\n\n\n\n\n9 \nA population is modeled with two states, immature and mature, and the resulting structured population model transition matrix is \\(\\left[\\begin{array}{cc}\\frac{1}{2} & 1 \\\\ \\frac{1}{2} & 0\\end{array}\\right]\\).\n\nExplain what this matrix says about the two states.\nStarting with a population of \\((30,100)\\), does the population stabilize, increase or decrease over time? If it stabilizes, to what distribution?\n\n\n\nfrom sympy import Matrix\nA = Matrix([[1/2, 1], [1/2, 0]])\nx = Matrix([30, 100])\nfor i in range(100):\n    x = A*x\n    if i % 10 == 0:\n        print(x)\n\nMatrix([[115.000000000000], [15.0000000000000]])\nMatrix([[86.6943359375000], [43.3056640625000]])\nMatrix([[86.6666936874390], [43.3333063125610]])\nMatrix([[86.6666666930541], [43.3333333069459]])\nMatrix([[86.6666666666924], [43.3333333333076]])\nMatrix([[86.6666666666667], [43.3333333333333]])\nMatrix([[86.6666666666667], [43.3333333333333]])\nMatrix([[86.6666666666667], [43.3333333333333]])\nMatrix([[86.6666666666667], [43.3333333333333]])\nMatrix([[86.6666666666667], [43.3333333333333]])\n\n\nExercise 2.3.13\nSolution. (a) The first column says that \\(50 \\%\\) of the immature become mature and \\(50 \\%\\) of the immature remain immature in one time period. The second column says that none of the mature survive, but each mature individual produces one immature in one time period. (b) The total populations after \\(0,3,6,9,18\\) time periods is a constant 130 , and populations tend to approximately \\((86.667,43.333)\\).\n\n\n\n10 \nA digraph \\(G\\) has vertex set \\(V=\\{1,2,3,4,5\\}\\) and edge set \\(E=\\) \\(\\{(2,1),(1,5),(2,5),(5,4),(4,2),(4,3),(3,2)\\}\\). Sketch a picture of the graph \\(G\\) and find its adjacency matrix. Use this to find the power of each vertex of the graph and determine whether this graph is dominance-directed.\n\nExercise 2.3.15\nPowers of vertices \\(1-5\\) are \\(2,4,3,5,3\\), respectively. Graph is dominance directed (there are no bi-directional edges between any pairs of vertices), adjacency matrix is \\(\\left[\\begin{array}{ccccc}0 & 0 & 0 & 0 & 1 \\\\ 1 & 0 & 0 & 0 & 1 \\\\ 0 & 1 & 0 & 0 & 0 \\\\ 0 & 1 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 & 0\\end{array}\\right]\\). Picture:\n\n\n\n\n11 \nConsider the linear difference \\(y_{k+2}-y_{k+1}-y_{k}=0\\).\n\nExpress this difference in matrix form.\nFind the first ten terms of the solution to this difference given the initial conditions \\(y_{0}=0, y_{1}=1\\). (This is the well-known Fibonacci sequence.)\n\n\nExercise 2.3.19\n\n\\(\\left[\\begin{array}{l}y_{k+1} \\\\ y_{k+2}\\end{array}\\right]=\\left[\\begin{array}{ll}0 & 1 \\\\ 1 & 1\\end{array}\\right]\\left[\\begin{array}{c}y_{k} \\\\ y_{k+1}\\end{array}\\right]\\)\nThe first ten terms are \\(0,1,1,2,3,5,8,13,21,34\\).\n\n\n\n\n12 \nSuppose that in Example 2.27 you invest \\(\\$ 1,000\\) initially (the zeroth year) and no further amounts. Make a table of the value of your investment for years 0 to 12. Also include a column that calculates the annual interest rate that your investment is earning each year, based on the current and previous year’s values. What conclusions do you draw? You will need a technology tool for this exercise.\n\nProblem 2.3.24 Solution.\nAfter a sizable third year earning of \\(24 \\%\\), the annual rate appears to settle down to \\(14.41 \\%\\). Partial table:\n\n\n\nYear\nValue\nInterest Rate (%)\n\n\n\n\n0\n1000\n-\n\n\n1\n1000\n0\n\n\n3\n1240\n24\n\n\n6\n1859\n14.1\n\n\n9\n2785\n14.41\n\n\n12\n4171\n14.41"
  },
  {
    "objectID": "HW/HW2.html",
    "href": "HW/HW2.html",
    "title": "Homework 2",
    "section": "",
    "text": "1 \nGiven that a linear system in the unknowns \\(x_{1}, x_{2}, x_{3}, x_{4}\\) has general solution \\(\\left(x_{2}+3 x_{4}+4, x_{2}, 2-x_{4}, x_{4}\\right)\\) for free variables \\(x_{2}, x_{4}\\), find a minimal reduced row echelon for this system.\n\n\n2 \nUse the technique of Example 2.10 in your textbook to balance the following chemical equation:\n\\[\n\\mathrm{C}_{8} \\mathrm{H}_{18}+\\mathrm{O}_{2} \\rightarrow \\mathrm{CO}_{2}+\\mathrm{H}_{2} \\mathrm{O} .\n\\]\n\n\n3 \nExpress the following functions, if linear, as matrix operators. (If not linear, explain why.)\n\n\\(T\\left(\\left(x_{1}, x_{2}\\right)\\right)=\\left(x_{1}+x_{2}, 2 x_{1}, 4 x_{2}-x_{1}\\right)\\)\n\\(T\\left(\\left(x_{1}, x_{2}\\right)\\right)=\\left(x_{1}+x_{2}, 2 x_{1} x_{2}\\right)\\)\n\\(T\\left(\\left(x_{1}, x_{2}, x_{3}\\right)\\right)=\\left(2 x_{3},-x_{1}\\right)\\)\n\\(T\\left(\\left(x_{1}, x_{2}, x_{3}\\right)\\right)=\\left(x_{2}-x_{1}, x_{3}, x_{2}+x_{3}\\right)\\)\n\n\n\n4 \nA fixed-point of a linear operator \\(T_{A}\\) is a vector \\(\\mathbf{x}\\) such that \\(T_{A}(\\mathbf{x})=\\mathbf{x}\\). Find all fixed points, if any, of the linear operators in the previous exercise.\n\n\n5 \nA linear operator on \\(\\mathbb{R}^{2}\\) is defined by first applying a scaling operator with scale factors of 2 in the \\(x\\)-direction and 4 in the \\(y\\)-direction, followed by a counterclockwise rotation about the origin of \\(\\pi / 6\\) radians. Express this operator and the operator that results from reversing the order of the scaling and rotation as matrix operators.\n\n\n6 \nFind a scaling operator \\(S\\) and shearing operator \\(H\\) such that the concatenation \\(S \\circ H\\) maps the points \\((1,0)\\) to \\((2,0)\\) and \\((0,1)\\) to \\((4,3)\\).\n\n\n7 \nGiven transition matrices for discrete dynamical systems\n\n\\(\\left[\\begin{array}{rrr}.1 & .3 & 0 \\\\ 0 & .4 & 1 \\\\ .9 & .3 & 0\\end{array}\\right] \\quad\\) (b) \\(\\left[\\begin{array}{lll}0 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 0\\end{array}\\right] \\quad\\) (c) \\(\\left[\\begin{array}{rrr}.5 & .3 & 0 \\\\ 0 & .4 & 0 \\\\ .5 & .3 & 1\\end{array}\\right] \\quad\\) (d) \\(\\left[\\begin{array}{rrr}0 & 0 & 0.9 \\\\ 0.5 & 0 & 0 \\\\ 0 & 0.5 & 0.1\\end{array}\\right]\\) and initial state vector \\(\\mathbf{x}^{(0)}=\\frac{1}{2}(1,1,0)\\), calculate the first and second state vector for each system and determine whether it is a Markov chain.\n\n\n\n8 \nFor each of the dynamical systems of the previous exercise, determine by calculation whether the system tends to a limiting steady-state vector. If so, what is it?\n\n\n9 \nA population is modeled with two states, immature and mature, and the resulting structured population model transition matrix is \\(\\left[\\begin{array}{cc}\\frac{1}{2} & 1 \\\\ \\frac{1}{2} & 0\\end{array}\\right]\\).\n\nExplain what this matrix says about the two states.\nStarting with a population of \\((30,100)\\), does the population stabilize, increase or decrease over time? If it stabilizes, to what distribution?\n\n\n\n10 \nA digraph \\(G\\) has vertex set \\(V=\\{1,2,3,4,5\\}\\) and edge set \\(E=\\) \\(\\{(2,1),(1,5),(2,5),(5,4),(4,2),(4,3),(3,2)\\}\\). Sketch a picture of the graph \\(G\\) and find its adjacency matrix. Use this to find the power of each vertex of the graph and determine whether this graph is dominance-directed.\n\n\n11 \nConsider the linear difference \\(y_{k+2}-y_{k+1}-y_{k}=0\\).\n\nExpress this difference in matrix form.\nFind the first ten terms of the solution to this difference given the initial conditions \\(y_{0}=0, y_{1}=1\\). (This is the well-known Fibonacci sequence.)\n\n\n\n12 \nSuppose that in Example 2.27 you invest \\(\\$ 1,000\\) initially (the zeroth year) and no further amounts. Make a table of the value of your investment for years 0 to 12. Also include a column that calculates the annual interest rate that your investment is earning each year, based on the current and previous year’s values. What conclusions do you draw? You will need a technology tool for this exercise."
  },
  {
    "objectID": "HW/projects_1.html",
    "href": "HW/projects_1.html",
    "title": "Projects 1",
    "section": "",
    "text": "Instructions\n\nPick three projects from the list below.\nPlease format each project as a Jupyter notebook or a Quarto document (or some similar format which allows me to see/run code.)\nYou are welcome to work with a partner on some or all of the projects, but please make sure you are each contributing to each part of the project – don’t just divide up the work.\n\nIf you work with a partner, you may choose to submit one report for both of you, or you may submit separate reports. Either way, please make sure to indicate who your partner was. (Please indicate very clearly what you are doing – which reports if any are joint, and with whom, etc, so I don’t get confused!)\n\nThis is the first time I’m assigning something like this, so please feel to reach out with questions or suggestions for improvement.\nProjects will be due on Friday night, 11:59pm.\n\nChapter 1 projects\nGas in a Tube\n(Shores 62)\nProblem Description: You are given a long tube of still dry air in which there are 7 sampling/insertion points equally spaced \\(1 / 6\\) meters apart from each other. The position of each point is measured by setting the leftmost point at 0.0 meters and rightmost at 1.0 meters. Initially, a small amount of a certain gas is inserted in the central insertion point. Subsequently, measurements of the concentration of the gas at each sampling/insertion point are taken at later times in seconds. The results of these measurements, which you may assume are accurate to about 2-3 digits, are specified in Table 1.1. Based on this information, your task is to determine the best estimate you can find for the true value of the diffusion coefficient \\(D\\) of this gas in a motionless air medium. Use this estimate and a marching method to calculate values of the material density function on the interval \\([0,1]\\) at times \\(t=210\\) and \\(t=300\\) and at the given spatial nodes.\nTable: Concentration data measurements of a gaseous material.\nProcedure: You should use equation (1.10) or some variant to move backward and forward in time. These will result in linear systems, which ALAMA calculator or another technology tool can solve. One way to proceed is simply to use trial and error until you think you’ve hit on a reasonable value of \\(D\\), that is, the one that gives the best approximation to \\(t=180\\) from the \\(t=360\\) values. Do not expect perfect matches - the data is relatively sparse. Then march backwards in time once more to get the initial values at \\(t=0\\). Finally, march forward in time to compute and plot the resulting approximate density function.\nOutput: Discus your results and provide a graph of profiles of the material density function at times in the data table along with your computed profiles.\nComments: This project introduces you to a very interesting area of mathematics called “inverse theory.” The idea is, rather than proceeding from problem (the governing equations for concentration values) to solution (concentration profiles), you are given the “solution,” namely the measured solution values at various points, and are to determine from this information the “problem,” i.e., the diffusion coefficient needed to define the governing equations.\nChapter 2 Projects\nLU Factorization\nWe didn’t yet get to the discussion of this in class. If you’re interested in doing this project, you can look at the second part of the Chapter 2 Lecture 4 slides.\n(Shores p. 177) Write a program module that implements Theorem 2.14 using partial pivoting and implicit row exchanges. This means that space is allocated for the \\(n \\times n\\) matrix \\(A=[a[i, j]]\\) and an array of row indices, say indx \\([i]\\). Initially, indx should consist of the integers \\(1,2, \\ldots, n\\). Whenever two rows need to be exchanged, say the first and third, then the indices indx[1] and indx[3] are exchanged. References to array elements throughout the Gaussian elimination process should be indirect: Refer to the \\((1,4)\\) th entry of \\(A\\) as the element \\(a[\\operatorname{indx}[1], 4]\\). This method of reference has the same effect as physically exchanging rows, but without the work. It also has the appealing feature that we can design the algorithm as though no row exchanges have taken place provided we replace the direct reference \\(a[i, j]\\) by the indirect reference \\(a[\\operatorname{indx}[i], j]\\). The module should return the lower/upper matrix in the format of Example 2.70 as well as the permuted array indx \\([i]\\). Effectively, this index array tells the user what the permutation matrix \\(P\\) is.\nUse this module to implement an LU system solver module that uses the \\(\\mathrm{LU}\\) factorization to solve a general linear system. Also write a module that finds the inverse of an \\(n \\times n\\) matrix \\(A\\) by first using the LU factorization module, then making repeated use of the LU system solver to solve \\(A \\mathbf{x}^{(i)}=\\mathbf{e}_{i}\\), where \\(\\mathbf{e}_{i}\\) is the \\(i\\) th column of the identity. Then we will have\n\\[\nA^{-1}=\\left[\\mathbf{x}^{(1)}, \\mathbf{x}^{(2)}, \\ldots, \\mathbf{x}^{(n)}\\right]\n\\]\nBe sure to document and test your code and report on the results.\nMarkov Chains\n(Shores p. 177)\nRefer to Example 2.19 and Section 2.3 for background. Three automobile insurance firms compete for a fixed market of customers. Annual premiums are sold to these customers. Label the companies A, B, and C. You work for Company A, and your team of market analysts has done a survey that draws the following conclusions: In each of the past three years, the number of A customers switching to B is \\(20 \\%\\), and to \\(\\mathrm{C}\\) is \\(30 \\%\\). The number of \\(\\mathrm{B}\\) customers switching to \\(\\mathrm{A}\\) is \\(20 \\%\\), and to \\(\\mathrm{C}\\) is \\(20 \\%\\). The number of \\(\\mathrm{C}\\) customers switching to \\(\\mathrm{A}\\) is \\(30 \\%\\), and to \\(\\mathrm{B}\\) is \\(10 \\%\\). Those who do not switch continue to use their current company’s insurance for the next year. Model this market as a Markov chain. Display the transition matrix for the model. Illustrate the workings of the model by showing what it would predict as the market shares three years from now if currently \\(\\mathrm{A}, \\mathrm{B}\\), and \\(\\mathrm{C}\\) owned equal shares of the market.\nThe next part of your problem is as follows: Your team has tested two advertising campaigns in some smaller test markets and are confident that the first campaign will convince \\(20 \\%\\) of the B customers who would otherwise stay with B in a given year to switch to A. The second advertising campaign would convince \\(20 \\%\\) of the \\(\\mathrm{C}\\) customers who would otherwise stay with C in a given year to switch to A. Both campaigns have about equal costs and would not change other customers’ habits. Make a recommendation, based on your experiments with various possible initial state vectors for the market. Will these campaigns actually improve your company’s market share? If so, which one do you recommend? Write up your recommendation in the form of a report, with supporting evidence. It’s a good idea to hedge on your bets a little by pointing out limitations to your model and claims, so devote a few sentences to those points.\nIt would be a plus to carry the analysis further (your manager might appreciate that). For instance, you could turn the additional market share from, say B customers, into a variable and plot the long-term gain for your company against this variable. A manager could use this data to decide whether it was worthwhile to attempt gaining more customers from B.\nSports Ranking\n(Shores p. 180)\nRefer to Example 2.24 and Section 2.3 for background. As a sports analyst you are given the following data about a league of seven teams numbered 1-7, where the pair \\((j, k)\\) represents a game in which team \\(j\\) defeated team \\(k\\) :\n\\[\n\\begin{aligned}\nE= & \\{(1,2),(7,3),(2,4),(4,5),(3,2),(5,1),(6,1),(3,1),(7,2),(2,6), \\\\\n& (3,4),(7,4),(5,7),(6,4),(3,5),(5,6),(7,1),(5,2),(7,6),(1,4),(6,3)\\}\n\\end{aligned}\n\\]\nBased on these data you are to rank the teams. To this end, begin with the simplest method, ranking by win/loss record. Next, treat the data as defining a digraph. Begin this analysis by constructing the adjacency matrix of this digraph and drawing a picture of the digraph either by hand or using some software. Then rank the teams by using the following methods: First use the method of Example 2.26 to find a power ranking of each team. Then use the reverse PageRank idea of Example 2.47 to rank the the teams.\nNext, suppose you are given additional information, namely, the game margins (winning score minus losing score) for each game. Following is a list of these margins matching the order of matches in the definition of \\(E\\) :\n\\[\nM=\\{4,8,7,3,7,7,23,15,6,18,13,14,7,13,7,18,45,10,19,14,13\\}\n\\]\nIn order to utilize these data examine your picture of the digraph and label each edge with the margin that matches it in \\(M\\). You are now dealing with a weighted graph and one can construct a different sort of “adjacency matrix” by entering this margin in the \\((i, j)\\) th entry according as team \\(i\\) defeated team \\(j\\) by that margin. Use this approach to calculate “power ranking”.\nIsoRank\nRead the discussion of IsoRank on pages 172-176.\nThe notion of embedding one graph into another is a useful idea for some scientific studies. In this report you will test the basic idea of network embedding by using the variant IsoRank of the PageRank technique on three relatively simple examples. This project requires a technology tool for these calculations and the resulting output should be interpreted as in the discussion of the IsoRank technique following Example 2.73 of Section 2.8.\n\n\n\nFigure 2.13\n\n\n\n\n\nFigure 2.14\n\n\nBy an isomorphism of graphs we mean a one-to-one edge preserving map of vertices from one graph onto another. One can think of an isomorphism as simply a relabeling of the vertices of a graph. The first test is to provide an example of how well IsoRank can recognize isomorphisms. Consider the graph of Figure 2.13. Let \\(G_{1}\\) be the graph with vertices \\(1,2,3,4,5\\) in that order and \\(G_{2}\\) the same graph with vertices \\(A, B, C, D, E\\) in that order. Apply IsoRank to these two graphs and discuss the validity of your results.\nThe next embedding test is to remove the edge connecting vertices \\(B\\) and \\(C\\) in Figure 2.12 and use IsoRank with teleportation vector \\(\\mathbf{v}=\\mathbf{e} / 15\\) and teleportation parameter \\(\\alpha=0.85\\) to find the best matchings of the graph \\(G_{1}\\) with the resulting graph \\(G_{2}\\). List all possible mappings that are calculated.\nThe last embedding test is to use IsoRank with teleportation vector \\(\\mathbf{v}=\\) \\(\\mathrm{e} / 15\\) and teleportation parameter \\(\\alpha=0.85\\), along with correction vector \\(\\mathbf{u}=\\mathbf{e} / 5\\) for \\(G_{2}\\), to find the best matchings of the digraph \\(G_{1}\\) with the digraph \\(G_{2}\\) in Figure 2.14. List all possible mappings and discuss your calculations."
  },
  {
    "objectID": "HW/projects_1.sol.html",
    "href": "HW/projects_1.sol.html",
    "title": "Projects 1 Solutions",
    "section": "",
    "text": "Instructions\n\nPick three projects from the list below.\nPlease format each project as a Jupyter notebook or a Quarto document (or some similar format which allows me to see/run code.)\nYou are welcome to work with a partner on some or all of the projects, but please make sure you are each contributing to each part of the project – don’t just divide up the work.\n\nIf you work with a partner, you may choose to submit one report for both of you, or you may submit separate reports. Either way, please make sure to indicate who your partner was. (Please indicate very clearly what you are doing – which reports if any are joint, and with whom, etc, so I don’t get confused!)\n\nThis is the first time I’m assigning something like this, so please feel to reach out with questions or suggestions for improvement.\nProjects will be due on Friday night, 11:59pm.\n\nChapter 1 projects\nGas in a Tube\n(Shores 62)\nProblem Description: You are given a long tube of still dry air in which there are 7 sampling/insertion points equally spaced \\(1 / 6\\) meters apart from each other. The position of each point is measured by setting the leftmost point at 0.0 meters and rightmost at 1.0 meters. Initially, a small amount of a certain gas is inserted in the central insertion point. Subsequently, measurements of the concentration of the gas at each sampling/insertion point are taken at later times in seconds. The results of these measurements, which you may assume are accurate to about 2-3 digits, are specified in Table 1.1. Based on this information, your task is to determine the best estimate you can find for the true value of the diffusion coefficient \\(D\\) of this gas in a motionless air medium. Use this estimate and a marching method to calculate values of the material density function on the interval \\([0,1]\\) at times \\(t=210\\) and \\(t=300\\) and at the given spatial nodes.\nTable: Concentration data measurements of a gaseous material.\nProcedure: You should use equation (1.10) or some variant to move backward and forward in time. These will result in linear systems, which ALAMA calculator or another technology tool can solve. One way to proceed is simply to use trial and error until you think you’ve hit on a reasonable value of \\(D\\), that is, the one that gives the best approximation to \\(t=180\\) from the \\(t=360\\) values. Do not expect perfect matches - the data is relatively sparse. Then march backwards in time once more to get the initial values at \\(t=0\\). Finally, march forward in time to compute and plot the resulting approximate density function.\nOutput: Discus your results and provide a graph of profiles of the material density function at times in the data table along with your computed profiles.\nComments: This project introduces you to a very interesting area of mathematics called “inverse theory.” The idea is, rather than proceeding from problem (the governing equations for concentration values) to solution (concentration profiles), you are given the “solution,” namely the measured solution values at various points, and are to determine from this information the “problem,” i.e., the diffusion coefficient needed to define the governing equations.\nChapter 2 Projects\nLU Factorization\nWe didn’t yet get to the discussion of this in class. If you’re interested in doing this project, you can look at the second part of the Chapter 2 Lecture 4 slides.\n(Shores p. 177) Write a program module that implements Theorem 2.14 using partial pivoting and implicit row exchanges. This means that space is allocated for the \\(n \\times n\\) matrix \\(A=[a[i, j]]\\) and an array of row indices, say indx \\([i]\\). Initially, indx should consist of the integers \\(1,2, \\ldots, n\\). Whenever two rows need to be exchanged, say the first and third, then the indices indx[1] and indx[3] are exchanged. References to array elements throughout the Gaussian elimination process should be indirect: Refer to the \\((1,4)\\) th entry of \\(A\\) as the element \\(a[\\operatorname{indx}[1], 4]\\). This method of reference has the same effect as physically exchanging rows, but without the work. It also has the appealing feature that we can design the algorithm as though no row exchanges have taken place provided we replace the direct reference \\(a[i, j]\\) by the indirect reference \\(a[\\operatorname{indx}[i], j]\\). The module should return the lower/upper matrix in the format of Example 2.70 as well as the permuted array indx \\([i]\\). Effectively, this index array tells the user what the permutation matrix \\(P\\) is.\nUse this module to implement an LU system solver module that uses the \\(\\mathrm{LU}\\) factorization to solve a general linear system. Also write a module that finds the inverse of an \\(n \\times n\\) matrix \\(A\\) by first using the LU factorization module, then making repeated use of the LU system solver to solve \\(A \\mathbf{x}^{(i)}=\\mathbf{e}_{i}\\), where \\(\\mathbf{e}_{i}\\) is the \\(i\\) th column of the identity. Then we will have\n\\[\nA^{-1}=\\left[\\mathbf{x}^{(1)}, \\mathbf{x}^{(2)}, \\ldots, \\mathbf{x}^{(n)}\\right]\n\\]\nBe sure to document and test your code and report on the results.\nMarkov Chains\n(Shores p. 177)\nRefer to Example 2.19 and Section 2.3 for background. Three automobile insurance firms compete for a fixed market of customers. Annual premiums are sold to these customers. Label the companies A, B, and C. You work for Company A, and your team of market analysts has done a survey that draws the following conclusions: In each of the past three years, the number of A customers switching to B is \\(20 \\%\\), and to \\(\\mathrm{C}\\) is \\(30 \\%\\). The number of \\(\\mathrm{B}\\) customers switching to \\(\\mathrm{A}\\) is \\(20 \\%\\), and to \\(\\mathrm{C}\\) is \\(20 \\%\\). The number of \\(\\mathrm{C}\\) customers switching to \\(\\mathrm{A}\\) is \\(30 \\%\\), and to \\(\\mathrm{B}\\) is \\(10 \\%\\). Those who do not switch continue to use their current company’s insurance for the next year. Model this market as a Markov chain. Display the transition matrix for the model. Illustrate the workings of the model by showing what it would predict as the market shares three years from now if currently \\(\\mathrm{A}, \\mathrm{B}\\), and \\(\\mathrm{C}\\) owned equal shares of the market.\nThe next part of your problem is as follows: Your team has tested two advertising campaigns in some smaller test markets and are confident that the first campaign will convince \\(20 \\%\\) of the B customers who would otherwise stay with B in a given year to switch to A. The second advertising campaign would convince \\(20 \\%\\) of the \\(\\mathrm{C}\\) customers who would otherwise stay with C in a given year to switch to A. Both campaigns have about equal costs and would not change other customers’ habits. Make a recommendation, based on your experiments with various possible initial state vectors for the market. Will these campaigns actually improve your company’s market share? If so, which one do you recommend? Write up your recommendation in the form of a report, with supporting evidence. It’s a good idea to hedge on your bets a little by pointing out limitations to your model and claims, so devote a few sentences to those points.\nIt would be a plus to carry the analysis further (your manager might appreciate that). For instance, you could turn the additional market share from, say B customers, into a variable and plot the long-term gain for your company against this variable. A manager could use this data to decide whether it was worthwhile to attempt gaining more customers from B.\nSports Ranking\n(Shores p. 180)\nRefer to Example 2.24 and Section 2.3 for background. As a sports analyst you are given the following data about a league of seven teams numbered 1-7, where the pair \\((j, k)\\) represents a game in which team \\(j\\) defeated team \\(k\\) :\n\\[\n\\begin{aligned}\nE= & \\{(1,2),(7,3),(2,4),(4,5),(3,2),(5,1),(6,1),(3,1),(7,2),(2,6), \\\\\n& (3,4),(7,4),(5,7),(6,4),(3,5),(5,6),(7,1),(5,2),(7,6),(1,4),(6,3)\\}\n\\end{aligned}\n\\]\nBased on these data you are to rank the teams. To this end, begin with the simplest method, ranking by win/loss record. Next, treat the data as defining a digraph. Begin this analysis by constructing the adjacency matrix of this digraph and drawing a picture of the digraph either by hand or using some software. Then rank the teams by using the following methods: First use the method of Example 2.26 to find a power ranking of each team. Then use the reverse PageRank idea of Example 2.47 to rank the the teams.\nNext, suppose you are given additional information, namely, the game margins (winning score minus losing score) for each game. Following is a list of these margins matching the order of matches in the definition of \\(E\\) :\n\\[\nM=\\{4,8,7,3,7,7,23,15,6,18,13,14,7,13,7,18,45,10,19,14,13\\}\n\\]\nIn order to utilize these data examine your picture of the digraph and label each edge with the margin that matches it in \\(M\\). You are now dealing with a weighted graph and one can construct a different sort of “adjacency matrix” by entering this margin in the \\((i, j)\\) th entry according as team \\(i\\) defeated team \\(j\\) by that margin. Use this approach to calculate “power ranking”.\nIsoRank\nRead the discussion of IsoRank on pages 172-176.\nThe notion of embedding one graph into another is a useful idea for some scientific studies. In this report you will test the basic idea of network embedding by using the variant IsoRank of the PageRank technique on three relatively simple examples. This project requires a technology tool for these calculations and the resulting output should be interpreted as in the discussion of the IsoRank technique following Example 2.73 of Section 2.8.\n\n\n\nFigure 2.13\n\n\n\n\n\nFigure 2.14\n\n\nBy an isomorphism of graphs we mean a one-to-one edge preserving map of vertices from one graph onto another. One can think of an isomorphism as simply a relabeling of the vertices of a graph. The first test is to provide an example of how well IsoRank can recognize isomorphisms. Consider the graph of Figure 2.13. Let \\(G_{1}\\) be the graph with vertices \\(1,2,3,4,5\\) in that order and \\(G_{2}\\) the same graph with vertices \\(A, B, C, D, E\\) in that order. Apply IsoRank to these two graphs and discuss the validity of your results.\nThe next embedding test is to remove the edge connecting vertices \\(B\\) and \\(C\\) in Figure 2.12 and use IsoRank with teleportation vector \\(\\mathbf{v}=\\mathbf{e} / 15\\) and teleportation parameter \\(\\alpha=0.85\\) to find the best matchings of the graph \\(G_{1}\\) with the resulting graph \\(G_{2}\\). List all possible mappings that are calculated.\nThe last embedding test is to use IsoRank with teleportation vector \\(\\mathbf{v}=\\) \\(\\mathrm{e} / 15\\) and teleportation parameter \\(\\alpha=0.85\\), along with correction vector \\(\\mathbf{u}=\\mathbf{e} / 5\\) for \\(G_{2}\\), to find the best matchings of the digraph \\(G_{1}\\) with the digraph \\(G_{2}\\) in Figure 2.14. List all possible mappings and discuss your calculations."
  },
  {
    "objectID": "HW/HW3.html",
    "href": "HW/HW3.html",
    "title": "Homework 3",
    "section": "",
    "text": "1 \n\n\n2.4.29\nShow that if \\(P\\) and \\(Q\\) are stochastic matrices of the same size, then \\(P Q\\) is also stochastic.\n\n\n2 \n\n\n2.4.36\nThe digraph \\(H\\) that results from reversing all the arrows in a digraph \\(G\\) is called reverse digraph of \\(G\\). Show that if \\(A\\) is the adjacency matrix for \\(G\\) then \\(A^{T}\\) is the adjacency matrix for the reverse digraph \\(H\\).\n\n\n3 \n\n\n2.5.21\nSolve the PageRank problem with \\(P\\) as in Example 2.46, teleportation vector \\(\\mathbf{v}=\\frac{1}{6} \\mathbf{e}\\) and teleportation parameter \\(\\alpha=0.8\\). (In this example, the correction vector was \\(\\frac{1}{3}(1,1,1,0,0,0)\\); that’s what you’ll use here.)\n\n\n4 \n\n\n2.5.22\nModify the surfing matrix \\(P\\) of Example 2.46 by using the correction vector \\(\\frac{1}{5}(1,1,1,0,1,1)\\) and solve the resulting PageRank problem with teleportation vector \\(\\mathbf{v}=\\frac{1}{6} \\mathbf{e}\\) and teleportation parameter \\(\\alpha=0.8\\).\n\n\n5 \n\n\n2.5.29\nShow that there is more than one stationary state for the Markov chain of Example 2.46.\n\n\n6 \n\n\n2.5.30\nRepair the dangling node problem of the graph of Figure 2.7 by creating a correction vector that makes transition to all nodes equally likely. (Note that this means all nodes, includes transitioning back to the dangling node.)\nNext, find all stationary states for the resulting Markov chain.\n\n\n7 \n\n\n2.5.25\nSolve the nonlinear system of equations of Example 2.48 by using nine iterations of the vector Newton formula (2.5), starting with the initial guess \\(\\mathbf{x}^{(0)}=(0,1)\\). Evaluate \\(F\\left(\\mathbf{x}^{(9)}\\right)\\).\n\n\n8 \n\n\n2.5.26\nFind the minimum value of the function \\(F(x, y)=\\left(x^{2}+y+1\\right)^{2}+\\) \\(x^{4}+y^{4}\\) by using the Newton method to find critical points of the function \\(F(x, y)\\), i.e., points where \\(f(x, y)=F_{x}(x, y)=0\\) and \\(g(x, y)=F_{y}(x, y)=0\\).\n\n\n9 \n\n\n2.8.9\nApply the following digital filter to the noisy data of Example 2.71 and graph the results. Does it appear to be a low pass filter?\n\\[\ny_{k}=\\frac{1}{2} x_{k}+\\frac{1}{2} x_{k-1}, \\quad k=1,2, \\ldots, 33\n\\]\n\n\n10 \n\n\n2.8.10\nApply the following digital filter to the noisy data of Example 2.71 and graph the results. Does it appear to be a high pass filter?\n\\[\ny_{k}=\\frac{1}{2} x_{k}-\\frac{1}{2} x_{k-1}, \\quad k=1,2, \\ldots, 33\n\\]\n\n\n11 \n\n\n2.8.1\n(I’ll talk about LU factorization in class on the Wednesday that this homework is due; you may want to hold off on the next few problems until then.)\nShow that \\(L=\\left[\\begin{array}{lll}1 & 0 & 0 \\\\ 1 & 1 & 0 \\\\ 2 & 1 & 1\\end{array}\\right]\\) and \\(U=\\left[\\begin{array}{rrr}2 & -1 & 1 \\\\ 0 & 4 & -3 \\\\ 0 & 0 & -1\\end{array}\\right]\\) is an LU factorization of \\(A=\\left[\\begin{array}{rrr}2 & -1 & 1 \\\\ 2 & 3 & -2 \\\\ 4 & 2 & -2\\end{array}\\right]\\).\n\n\n12 \nBy hand:\n\n\n2.8.5\nFind an LU factorization of the matrix \\(A=\\left[\\begin{array}{rrr}2 & 1 & 0 \\\\ -4 & -1 & -1 \\\\ 2 & 3 & -3\\end{array}\\right]\\).\n\n\n13 \nBy hand:\n\n\n2.8.6\nFind a PLU factorization of the matrix \\(A=\\left[\\begin{array}{rrr}2 & 1 & 3 \\\\ -4 & -2 & -1 \\\\ 2 & 3 & -3\\end{array}\\right]\\)."
  },
  {
    "objectID": "HW/HW3.sol.html",
    "href": "HW/HW3.sol.html",
    "title": "Homework 3 Solutions",
    "section": "",
    "text": "1 \n\n\n2.4.29\nShow that if \\(P\\) and \\(Q\\) are stochastic matrices of the same size, then \\(P Q\\) is also stochastic.\n\n\n2 \n\n\n2.4.36\nThe digraph \\(H\\) that results from reversing all the arrows in a digraph \\(G\\) is called reverse digraph of \\(G\\). Show that if \\(A\\) is the adjacency matrix for \\(G\\) then \\(A^{T}\\) is the adjacency matrix for the reverse digraph \\(H\\).\n\n\n3 \n\n\n2.5.21\nSolve the PageRank problem with \\(P\\) as in Example 2.46, teleportation vector \\(\\mathbf{v}=\\frac{1}{6} \\mathbf{e}\\) and teleportation parameter \\(\\alpha=0.8\\). (In this example, the correction vector was \\(\\frac{1}{3}(1,1,1,0,0,0)\\); that’s what you’ll use here.)\n\n\n4 \n\n\n2.5.22\nModify the surfing matrix \\(P\\) of Example 2.46 by using the correction vector \\(\\frac{1}{5}(1,1,1,0,1,1)\\) and solve the resulting PageRank problem with teleportation vector \\(\\mathbf{v}=\\frac{1}{6} \\mathbf{e}\\) and teleportation parameter \\(\\alpha=0.8\\).\n\n\n5 \n\n\n2.5.29\nShow that there is more than one stationary state for the Markov chain of Example 2.46.\n\n\n6 \n\n\n2.5.30\nRepair the dangling node problem of the graph of Figure 2.7 by creating a correction vector that makes transition to all nodes equally likely. (Note that this means all nodes, includes transitioning back to the dangling node.)\nNext, find all stationary states for the resulting Markov chain.\n\n\n7 \n\n\n2.5.25\nSolve the nonlinear system of equations of Example 2.48 by using nine iterations of the vector Newton formula (2.5), starting with the initial guess \\(\\mathbf{x}^{(0)}=(0,1)\\). Evaluate \\(F\\left(\\mathbf{x}^{(9)}\\right)\\).\n\n\n8 \n\n\n2.5.26\nFind the minimum value of the function \\(F(x, y)=\\left(x^{2}+y+1\\right)^{2}+\\) \\(x^{4}+y^{4}\\) by using the Newton method to find critical points of the function \\(F(x, y)\\), i.e., points where \\(f(x, y)=F_{x}(x, y)=0\\) and \\(g(x, y)=F_{y}(x, y)=0\\).\n\n\n9 \n\n\n2.8.9\nApply the following digital filter to the noisy data of Example 2.71 and graph the results. Does it appear to be a low pass filter?\n\\[\ny_{k}=\\frac{1}{2} x_{k}+\\frac{1}{2} x_{k-1}, \\quad k=1,2, \\ldots, 33\n\\]\n\n\n10 \n\n\n2.8.10\nApply the following digital filter to the noisy data of Example 2.71 and graph the results. Does it appear to be a high pass filter?\n\\[\ny_{k}=\\frac{1}{2} x_{k}-\\frac{1}{2} x_{k-1}, \\quad k=1,2, \\ldots, 33\n\\]\n\n\n11 \n\n\n2.8.1\n(I’ll talk about LU factorization in class on the Wednesday that this homework is due; you may want to hold off on the next few problems until then.)\nShow that \\(L=\\left[\\begin{array}{lll}1 & 0 & 0 \\\\ 1 & 1 & 0 \\\\ 2 & 1 & 1\\end{array}\\right]\\) and \\(U=\\left[\\begin{array}{rrr}2 & -1 & 1 \\\\ 0 & 4 & -3 \\\\ 0 & 0 & -1\\end{array}\\right]\\) is an LU factorization of \\(A=\\left[\\begin{array}{rrr}2 & -1 & 1 \\\\ 2 & 3 & -2 \\\\ 4 & 2 & -2\\end{array}\\right]\\).\n\n\n12 \nBy hand:\n\n\n2.8.5\nFind an LU factorization of the matrix \\(A=\\left[\\begin{array}{rrr}2 & 1 & 0 \\\\ -4 & -1 & -1 \\\\ 2 & 3 & -3\\end{array}\\right]\\).\n\n\n13 \nBy hand:\n\n\n2.8.6\nFind a PLU factorization of the matrix \\(A=\\left[\\begin{array}{rrr}2 & 1 & 3 \\\\ -4 & -2 & -1 \\\\ 2 & 3 & -3\\end{array}\\right]\\)."
  },
  {
    "objectID": "HW/errata.sol.html",
    "href": "HW/errata.sol.html",
    "title": "Exercise 2.2.23",
    "section": "",
    "text": "Exercise 2.2.23\nNumbers are wrong in the “resulting coefficient matrix”. Final answer is correct.\n\n\nPage 285\nIn equation (2), should be $ = w_1 + $, not $ = u_1 + $\n\n\nPage 245\n“Hence, \\(\\mathbf{w}_1\\) and … \\(\\mathbf{w}_1\\)” should be “Hence, \\(\\mathbf{w}_1\\) and … \\(\\mathbf{w}_2\\)”."
  },
  {
    "objectID": "HW/errata.html",
    "href": "HW/errata.html",
    "title": "Exercise 2.2.23",
    "section": "",
    "text": "Exercise 2.2.23\nNumbers are wrong in the “resulting coefficient matrix”. Final answer is correct.\n\n\nPage 285\nIn equation (2), should be $ = w_1 + $, not $ = u_1 + $\n\n\nPage 245\n“Hence, \\(\\mathbf{w}_1\\) and … \\(\\mathbf{w}_1\\)” should be “Hence, \\(\\mathbf{w}_1\\) and … \\(\\mathbf{w}_2\\)”."
  },
  {
    "objectID": "HW/projects_2.sol.html",
    "href": "HW/projects_2.sol.html",
    "title": "Projects 2 Solutions",
    "section": "",
    "text": "Project Descriptions: These projects introduce more applications of digraphs as mathematical modeling tools. You are given that the digraph \\(G\\) has vertex set \\(V=\\{1,2,3,4,5,6\\}\\) and edge set\n\\[\nE=\\{(1,2),(2,3),(3,4),(4,2),(1,4),(3,1),(3,6),(6,3),(4,5),(5,6)\\}\n\\]\nAddress the following points regarding \\(G\\).\n\n\nDraw a picture of this digraph. You may leave space in your report and draw this by hand, or if you prefer, you may use the computer drawing applications available to you on your system.\n\n\n\nExhibit the incidence matrix \\(A\\) of this digraph and find a basis for \\(\\mathcal{N}(A)\\) using its reduced row echelon form. Some of the basis elements may be algebraic but not directed loops. Use this basis to find a basis of directed loops (e.g., non-directed basis element \\(\\mathbf{c}_{1}\\) might be replaced by directed \\(\\mathbf{c}_{1}+\\mathbf{c}_{2}\\) ).\n\n\nThink of the digraph as representing an electrical circuit where an edge represents some electrical object like a resistor or capacitor. Each node represents the circuit space between these objects. and we can attach a potential value to each node, say the potentials are \\(x_{1}, \\ldots, x_{6}\\). The potential difference across an edge is the potential value of head minus tail. Kirchhoff’s second law of electrical circuits says that the sum of potential differences around a circuit loop must be zero. Assume and use the fact (p. 422) that \\(A \\mathbf{x}=\\mathbf{b}\\) implies that for all \\(\\mathbf{y} \\in \\mathcal{N}\\left(A^{T}\\right), \\mathbf{y}^{T} \\mathbf{b}=0\\) to find conditions that a vector \\(\\mathbf{b}\\) must satisfy in order for it to be a vector of potential differences for some potential distribution on the vertices.\n\n\n\n\n\nAssume that across each edge of a circuit a current flows. Thus, we can assign to each edge a “weight,” namely the current flow along the edge. This is an example of a weighted digraph. However, not just any set of current weights will do, since Kirchhoff’s first law of circuits says that the total flow of current in and out of any node should be 0 . Use this law to find a matrix condition that must be satisfied by the currents and solve it to exhibit some current flows.\nThink of the digraph as representing a directed communications network. Here loops determine which nodes have bidirectional communication since any two nodes of a loop can only communicate with each other by way of a loop. By examining only a basis of directed loops how could you determine which nodes in the network can communicate with each other?\n\n\n\n\n\nThink of vertices of the digraph as representing airports and edges representing flight connections between airports for Gamma Airlines. Suppose further that for each connection there is a maximum number of daily flights that will be allowed by the destination airport from an origin airport and that, in the order that the edges in \\(E\\) are listed above, these limits are\n\n\\[\nM=\\{4,3,8,7,2,6,7,10,5,8\\} .\n\\]\nNow suppose that Gamma wants to maximize the flow of flights into airport 1 and out of airport 6 . Count inflows into an airport as positive and outflows as negative. Assume that the net in/outflow of Gamma flights at each airport 1 to 5 is zero, while the net inflow of such flights into airport 1 matches the net outflow from 6 .\n\nDescribe the problem of maximizing this inflow to airport 1 as a linear programming problem and express it in a standard form (block matrices are helpful.) Note that the appropriate variables are all outflows from one airport to another, i.e., along edges, together with the net inflow into airport 1.\nSolve the problem of part (a). Also solve the reverse problem: Maximize inflow into airport 6 and matching outflow from 1. Explain and justify your answers.\n\n\nWith the same limits on allowable flights into airports as in item 5, suppose that Gamma Airlines wants to determine an allocation of planes that will maximize their profits, given the following constraints: (1) Airports 1 and 6 have repair facilities for their planes, so no limit is placed on the inflow or outflow of their planes other than the airport limits. (2) Flights through airports 2-5 of Gamma planes are pass through, i.e., inflow and outflow must match. (3) Gamma has 32 planes available for this network of airports. (4) The profits per flight in thousands are, in the order that the edges in \\(E\\) are listed above,\n\n\\[\nP=\\{5,6,7,9,10,8,9,5,6,10\\}\n\\] (a) Set this problem up as a linear programming problem in standard form. Clearly identify the variables and explain how the constraints follow.\n\nSolve this problem explicitly and specify the operations taken to do so. Example 3.56 is instructive for this problem, so be aware of it. Use a technology tool that allows you to use elementary operations (ALAMA calculator has this capability)."
  },
  {
    "objectID": "HW/projects_2.sol.html#project-4",
    "href": "HW/projects_2.sol.html#project-4",
    "title": "Projects 2 Solutions",
    "section": "",
    "text": "Project Descriptions: These projects introduce more applications of digraphs as mathematical modeling tools. You are given that the digraph \\(G\\) has vertex set \\(V=\\{1,2,3,4,5,6\\}\\) and edge set\n\\[\nE=\\{(1,2),(2,3),(3,4),(4,2),(1,4),(3,1),(3,6),(6,3),(4,5),(5,6)\\}\n\\]\nAddress the following points regarding \\(G\\).\n\n\nDraw a picture of this digraph. You may leave space in your report and draw this by hand, or if you prefer, you may use the computer drawing applications available to you on your system.\n\n\n\nExhibit the incidence matrix \\(A\\) of this digraph and find a basis for \\(\\mathcal{N}(A)\\) using its reduced row echelon form. Some of the basis elements may be algebraic but not directed loops. Use this basis to find a basis of directed loops (e.g., non-directed basis element \\(\\mathbf{c}_{1}\\) might be replaced by directed \\(\\mathbf{c}_{1}+\\mathbf{c}_{2}\\) ).\n\n\nThink of the digraph as representing an electrical circuit where an edge represents some electrical object like a resistor or capacitor. Each node represents the circuit space between these objects. and we can attach a potential value to each node, say the potentials are \\(x_{1}, \\ldots, x_{6}\\). The potential difference across an edge is the potential value of head minus tail. Kirchhoff’s second law of electrical circuits says that the sum of potential differences around a circuit loop must be zero. Assume and use the fact (p. 422) that \\(A \\mathbf{x}=\\mathbf{b}\\) implies that for all \\(\\mathbf{y} \\in \\mathcal{N}\\left(A^{T}\\right), \\mathbf{y}^{T} \\mathbf{b}=0\\) to find conditions that a vector \\(\\mathbf{b}\\) must satisfy in order for it to be a vector of potential differences for some potential distribution on the vertices."
  },
  {
    "objectID": "HW/projects_2.sol.html#project-5",
    "href": "HW/projects_2.sol.html#project-5",
    "title": "Projects 2 Solutions",
    "section": "",
    "text": "Assume that across each edge of a circuit a current flows. Thus, we can assign to each edge a “weight,” namely the current flow along the edge. This is an example of a weighted digraph. However, not just any set of current weights will do, since Kirchhoff’s first law of circuits says that the total flow of current in and out of any node should be 0 . Use this law to find a matrix condition that must be satisfied by the currents and solve it to exhibit some current flows.\nThink of the digraph as representing a directed communications network. Here loops determine which nodes have bidirectional communication since any two nodes of a loop can only communicate with each other by way of a loop. By examining only a basis of directed loops how could you determine which nodes in the network can communicate with each other?"
  },
  {
    "objectID": "HW/projects_2.sol.html#project-6",
    "href": "HW/projects_2.sol.html#project-6",
    "title": "Projects 2 Solutions",
    "section": "",
    "text": "Think of vertices of the digraph as representing airports and edges representing flight connections between airports for Gamma Airlines. Suppose further that for each connection there is a maximum number of daily flights that will be allowed by the destination airport from an origin airport and that, in the order that the edges in \\(E\\) are listed above, these limits are\n\n\\[\nM=\\{4,3,8,7,2,6,7,10,5,8\\} .\n\\]\nNow suppose that Gamma wants to maximize the flow of flights into airport 1 and out of airport 6 . Count inflows into an airport as positive and outflows as negative. Assume that the net in/outflow of Gamma flights at each airport 1 to 5 is zero, while the net inflow of such flights into airport 1 matches the net outflow from 6 .\n\nDescribe the problem of maximizing this inflow to airport 1 as a linear programming problem and express it in a standard form (block matrices are helpful.) Note that the appropriate variables are all outflows from one airport to another, i.e., along edges, together with the net inflow into airport 1.\nSolve the problem of part (a). Also solve the reverse problem: Maximize inflow into airport 6 and matching outflow from 1. Explain and justify your answers.\n\n\nWith the same limits on allowable flights into airports as in item 5, suppose that Gamma Airlines wants to determine an allocation of planes that will maximize their profits, given the following constraints: (1) Airports 1 and 6 have repair facilities for their planes, so no limit is placed on the inflow or outflow of their planes other than the airport limits. (2) Flights through airports 2-5 of Gamma planes are pass through, i.e., inflow and outflow must match. (3) Gamma has 32 planes available for this network of airports. (4) The profits per flight in thousands are, in the order that the edges in \\(E\\) are listed above,\n\n\\[\nP=\\{5,6,7,9,10,8,9,5,6,10\\}\n\\] (a) Set this problem up as a linear programming problem in standard form. Clearly identify the variables and explain how the constraints follow.\n\nSolve this problem explicitly and specify the operations taken to do so. Example 3.56 is instructive for this problem, so be aware of it. Use a technology tool that allows you to use elementary operations (ALAMA calculator has this capability)."
  },
  {
    "objectID": "HW/projects_2.html",
    "href": "HW/projects_2.html",
    "title": "Projects 2",
    "section": "",
    "text": "Project Descriptions: These projects introduce more applications of digraphs as mathematical modeling tools. You are given that the digraph \\(G\\) has vertex set \\(V=\\{1,2,3,4,5,6\\}\\) and edge set\n\\[\nE=\\{(1,2),(2,3),(3,4),(4,2),(1,4),(3,1),(3,6),(6,3),(4,5),(5,6)\\}\n\\]\nAddress the following points regarding \\(G\\).\n\n\nDraw a picture of this digraph. You may leave space in your report and draw this by hand, or if you prefer, you may use the computer drawing applications available to you on your system.\n\n\n\nExhibit the incidence matrix \\(A\\) of this digraph and find a basis for \\(\\mathcal{N}(A)\\) using its reduced row echelon form. Some of the basis elements may be algebraic but not directed loops. Use this basis to find a basis of directed loops (e.g., non-directed basis element \\(\\mathbf{c}_{1}\\) might be replaced by directed \\(\\mathbf{c}_{1}+\\mathbf{c}_{2}\\) ).\n\n\nThink of the digraph as representing an electrical circuit where an edge represents some electrical object like a resistor or capacitor. Each node represents the circuit space between these objects. and we can attach a potential value to each node, say the potentials are \\(x_{1}, \\ldots, x_{6}\\). The potential difference across an edge is the potential value of head minus tail. Kirchhoff’s second law of electrical circuits says that the sum of potential differences around a circuit loop must be zero. Assume and use the fact (p. 422) that \\(A \\mathbf{x}=\\mathbf{b}\\) implies that for all \\(\\mathbf{y} \\in \\mathcal{N}\\left(A^{T}\\right), \\mathbf{y}^{T} \\mathbf{b}=0\\) to find conditions that a vector \\(\\mathbf{b}\\) must satisfy in order for it to be a vector of potential differences for some potential distribution on the vertices.\n\n\n\n\n\nAssume that across each edge of a circuit a current flows. Thus, we can assign to each edge a “weight,” namely the current flow along the edge. This is an example of a weighted digraph. However, not just any set of current weights will do, since Kirchhoff’s first law of circuits says that the total flow of current in and out of any node should be 0 . Use this law to find a matrix condition that must be satisfied by the currents and solve it to exhibit some current flows.\nThink of the digraph as representing a directed communications network. Here loops determine which nodes have bidirectional communication since any two nodes of a loop can only communicate with each other by way of a loop. By examining only a basis of directed loops how could you determine which nodes in the network can communicate with each other?\n\n\n\n\n\nThink of vertices of the digraph as representing airports and edges representing flight connections between airports for Gamma Airlines. Suppose further that for each connection there is a maximum number of daily flights that will be allowed by the destination airport from an origin airport and that, in the order that the edges in \\(E\\) are listed above, these limits are\n\n\\[\nM=\\{4,3,8,7,2,6,7,10,5,8\\} .\n\\]\nNow suppose that Gamma wants to maximize the flow of flights into airport 1 and out of airport 6 . Count inflows into an airport as positive and outflows as negative. Assume that the net in/outflow of Gamma flights at each airport 1 to 5 is zero, while the net inflow of such flights into airport 1 matches the net outflow from 6 .\n\nDescribe the problem of maximizing this inflow to airport 1 as a linear programming problem and express it in a standard form (block matrices are helpful.) Note that the appropriate variables are all outflows from one airport to another, i.e., along edges, together with the net inflow into airport 1.\nSolve the problem of part (a). Also solve the reverse problem: Maximize inflow into airport 6 and matching outflow from 1. Explain and justify your answers.\n\n\nWith the same limits on allowable flights into airports as in item 5, suppose that Gamma Airlines wants to determine an allocation of planes that will maximize their profits, given the following constraints: (1) Airports 1 and 6 have repair facilities for their planes, so no limit is placed on the inflow or outflow of their planes other than the airport limits. (2) Flights through airports 2-5 of Gamma planes are pass through, i.e., inflow and outflow must match. (3) Gamma has 32 planes available for this network of airports. (4) The profits per flight in thousands are, in the order that the edges in \\(E\\) are listed above,\n\n\\[\nP=\\{5,6,7,9,10,8,9,5,6,10\\}\n\\] (a) Set this problem up as a linear programming problem in standard form. Clearly identify the variables and explain how the constraints follow.\n\nSolve this problem explicitly and specify the operations taken to do so. Example 3.56 is instructive for this problem, so be aware of it. Use a technology tool that allows you to use elementary operations (ALAMA calculator has this capability)."
  },
  {
    "objectID": "HW/projects_2.html#project-4",
    "href": "HW/projects_2.html#project-4",
    "title": "Projects 2",
    "section": "",
    "text": "Project Descriptions: These projects introduce more applications of digraphs as mathematical modeling tools. You are given that the digraph \\(G\\) has vertex set \\(V=\\{1,2,3,4,5,6\\}\\) and edge set\n\\[\nE=\\{(1,2),(2,3),(3,4),(4,2),(1,4),(3,1),(3,6),(6,3),(4,5),(5,6)\\}\n\\]\nAddress the following points regarding \\(G\\).\n\n\nDraw a picture of this digraph. You may leave space in your report and draw this by hand, or if you prefer, you may use the computer drawing applications available to you on your system.\n\n\n\nExhibit the incidence matrix \\(A\\) of this digraph and find a basis for \\(\\mathcal{N}(A)\\) using its reduced row echelon form. Some of the basis elements may be algebraic but not directed loops. Use this basis to find a basis of directed loops (e.g., non-directed basis element \\(\\mathbf{c}_{1}\\) might be replaced by directed \\(\\mathbf{c}_{1}+\\mathbf{c}_{2}\\) ).\n\n\nThink of the digraph as representing an electrical circuit where an edge represents some electrical object like a resistor or capacitor. Each node represents the circuit space between these objects. and we can attach a potential value to each node, say the potentials are \\(x_{1}, \\ldots, x_{6}\\). The potential difference across an edge is the potential value of head minus tail. Kirchhoff’s second law of electrical circuits says that the sum of potential differences around a circuit loop must be zero. Assume and use the fact (p. 422) that \\(A \\mathbf{x}=\\mathbf{b}\\) implies that for all \\(\\mathbf{y} \\in \\mathcal{N}\\left(A^{T}\\right), \\mathbf{y}^{T} \\mathbf{b}=0\\) to find conditions that a vector \\(\\mathbf{b}\\) must satisfy in order for it to be a vector of potential differences for some potential distribution on the vertices."
  },
  {
    "objectID": "HW/projects_2.html#project-5",
    "href": "HW/projects_2.html#project-5",
    "title": "Projects 2",
    "section": "",
    "text": "Assume that across each edge of a circuit a current flows. Thus, we can assign to each edge a “weight,” namely the current flow along the edge. This is an example of a weighted digraph. However, not just any set of current weights will do, since Kirchhoff’s first law of circuits says that the total flow of current in and out of any node should be 0 . Use this law to find a matrix condition that must be satisfied by the currents and solve it to exhibit some current flows.\nThink of the digraph as representing a directed communications network. Here loops determine which nodes have bidirectional communication since any two nodes of a loop can only communicate with each other by way of a loop. By examining only a basis of directed loops how could you determine which nodes in the network can communicate with each other?"
  },
  {
    "objectID": "HW/projects_2.html#project-6",
    "href": "HW/projects_2.html#project-6",
    "title": "Projects 2",
    "section": "",
    "text": "Think of vertices of the digraph as representing airports and edges representing flight connections between airports for Gamma Airlines. Suppose further that for each connection there is a maximum number of daily flights that will be allowed by the destination airport from an origin airport and that, in the order that the edges in \\(E\\) are listed above, these limits are\n\n\\[\nM=\\{4,3,8,7,2,6,7,10,5,8\\} .\n\\]\nNow suppose that Gamma wants to maximize the flow of flights into airport 1 and out of airport 6 . Count inflows into an airport as positive and outflows as negative. Assume that the net in/outflow of Gamma flights at each airport 1 to 5 is zero, while the net inflow of such flights into airport 1 matches the net outflow from 6 .\n\nDescribe the problem of maximizing this inflow to airport 1 as a linear programming problem and express it in a standard form (block matrices are helpful.) Note that the appropriate variables are all outflows from one airport to another, i.e., along edges, together with the net inflow into airport 1.\nSolve the problem of part (a). Also solve the reverse problem: Maximize inflow into airport 6 and matching outflow from 1. Explain and justify your answers.\n\n\nWith the same limits on allowable flights into airports as in item 5, suppose that Gamma Airlines wants to determine an allocation of planes that will maximize their profits, given the following constraints: (1) Airports 1 and 6 have repair facilities for their planes, so no limit is placed on the inflow or outflow of their planes other than the airport limits. (2) Flights through airports 2-5 of Gamma planes are pass through, i.e., inflow and outflow must match. (3) Gamma has 32 planes available for this network of airports. (4) The profits per flight in thousands are, in the order that the edges in \\(E\\) are listed above,\n\n\\[\nP=\\{5,6,7,9,10,8,9,5,6,10\\}\n\\] (a) Set this problem up as a linear programming problem in standard form. Clearly identify the variables and explain how the constraints follow.\n\nSolve this problem explicitly and specify the operations taken to do so. Example 3.56 is instructive for this problem, so be aware of it. Use a technology tool that allows you to use elementary operations (ALAMA calculator has this capability)."
  },
  {
    "objectID": "lectures/ch3lecture1b.html#linear-independence",
    "href": "lectures/ch3lecture1b.html#linear-independence",
    "title": "Ch3 Lecture 1",
    "section": "Linear Independence",
    "text": "Linear Independence\n\nThe vectors \\(\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\ldots, \\mathbf{v}_{n}\\) are linearly dependent if there exist scalars \\(c_{1}, c_{2}, \\ldots, c_{n}\\), not all zero, such that\n\\[\n\\begin{equation*}\nc_{1} \\mathbf{v}_{1}+c_{2} \\mathbf{v}_{2}+\\cdots+c_{n} \\mathbf{v}_{n}=\\mathbf{0}\n\\end{equation*}\n\\]\nOtherwise, the vectors are called linearly independent.\n\n\nThis doesn’t mean that one of the vectors is a scalar multiple of another one."
  },
  {
    "objectID": "lectures/ch3lecture1b.html#checking-for-linear-independence",
    "href": "lectures/ch3lecture1b.html#checking-for-linear-independence",
    "title": "Ch3 Lecture 1",
    "section": "Checking for Linear Independence",
    "text": "Checking for Linear Independence\nIs this set of vectors linearly independent?\n\\[\n\\left[\\begin{array}{l}1 \\\\ 1 \\\\ 0\\end{array}\\right],\\left[\\begin{array}{l}0 \\\\ 1 \\\\ 1\\end{array}\\right],\\left[\\begin{array}{r}1 \\\\ -1 \\\\ -2\\end{array}\\right]\n\\]\n\nGoal: find some set of scalars \\(c_{1}, c_{2}, c_{3}\\), not all zero, such that \\(c_{1} \\mathbf{v}_{1}+c_{2} \\mathbf{v}_{2}+c_{3} \\mathbf{v}_{3}=\\mathbf{0}\\).\n\n\nDefine: - matrix \\(A=\\left[\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\mathbf{v}_{3}\\right]\\) - vector \\(\\mathbf{c}=\\left(c_{1}, c_{2}, c_{3}\\right)\\).\n\n\nThen:\n\\[\nc_{1} \\mathbf{v}_{1}+c_{2} \\mathbf{v}_{2}+c_{3} \\mathbf{v}_{3}=\\left[\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\mathbf{v}_{3}\\right]\\left[\\begin{array}{l}\nc_{1} \\\\\nc_{2} \\\\\nc_{3}\n\\end{array}\\right]=A \\mathbf{c}\n\\]\nSo we just need to see if there is a nontrivial solution to \\(A \\mathbf{c}=\\mathbf{0}\\)."
  },
  {
    "objectID": "lectures/ch3lecture1b.html#section",
    "href": "lectures/ch3lecture1b.html#section",
    "title": "Ch3 Lecture 1",
    "section": "",
    "text": "\\[\n\\begin{equation*}\ny_{i, t+1}=y_{i, t}+d\\left(y_{i-1, t}-2 y_{i, t}+y_{i+1, t}\\right)\n\\end{equation*}\n\\]\n\\(y_{0,t+1} = 0\\) \\(y_{1,t+1}=y_{1,t}+d\\left(0-2 y_{1,t}+y_{2,t}\\right)\\) \\(y_{3,t+1}=y_{3,t}+d\\left(y_{2,t}-2 y_{3,t}+y_{4,t}\\right)\\) … \\(y_{6,t+1}=0\\)\n\nWe want to solve these for \\(y_{i,t}\\) given \\(y_{i,t+1}\\).\n\\[\n\\begin{equation*}\ny_{i, t+1}-d\\left(y_{i-1, t}-2 y_{i, t}+y_{i+1, t}\\right)=y_{i, t}\n\\end{equation*}\n\\]\n\\[\n\\approx y_{i, t+1}-d\\left(y_{i-1, t+1}-2 y_{i, t+1}+y_{i+1, t+1}\\right) ?\n\\]"
  },
  {
    "objectID": "lectures/ch3lecture1b.html#example-2",
    "href": "lectures/ch3lecture1b.html#example-2",
    "title": "Ch3 Lecture 1",
    "section": "Example 2",
    "text": "Example 2\nAre the following vectors linearly independent?\n\\(\\left[\\begin{array}{l}0 \\\\ 1 \\\\ 0\\end{array}\\right],\\left[\\begin{array}{l}1 \\\\ 1 \\\\ 0\\end{array}\\right],\\left[\\begin{array}{l}0 \\\\ 1 \\\\ 1\\end{array}\\right]\\)\n\nSet up A = \\(\\left[\\begin{array}{lll}0 & 1 & 0 \\\\ 1 & 1 & 1 \\\\ 0 & 0 & 1\\end{array}\\right]\\)\n\n\nDifferent approach: \\[\n\\operatorname{det}\\left[\\begin{array}{lll}\n0 & 1 & 0 \\\\\n1 & 1 & 1 \\\\\n0 & 0 & 1\n\\end{array}\\right]=-1 \\operatorname{det}\\left[\\begin{array}{ll}\n1 & 0 \\\\\n0 & 1\n\\end{array}\\right]=-1\n\\]\n\n\nDeterminant is non-zero, so \\(A\\) is invertible (non-singular)\n\n\nOnly solution to \\(A \\mathbf{c}=0\\) is \\(\\mathbf{c}=0\\).\n\n\nSo the vectors are linearly independent."
  },
  {
    "objectID": "lectures/ch3lecture1b.html#summary-checking-for-linear-independence",
    "href": "lectures/ch3lecture1b.html#summary-checking-for-linear-independence",
    "title": "Ch3 Lecture 1",
    "section": "Summary: Checking for Linear Independence",
    "text": "Summary: Checking for Linear Independence\n\nSet up matrix \\(A\\) with the vectors as columns.\nEither:\n\nFind the reduced row echelon form of \\(A\\).\nCalculate the determinant of \\(A\\).\nCheck if the only solution to \\(A \\mathbf{c}=0\\) is \\(\\mathbf{c}=0\\)."
  },
  {
    "objectID": "lectures/ch3lecture1b.html#basis-of-a-vector-space",
    "href": "lectures/ch3lecture1b.html#basis-of-a-vector-space",
    "title": "Ch3 Lecture 1",
    "section": "Basis of a Vector Space",
    "text": "Basis of a Vector Space\n\nA spanning set is a set of vectors that can be combined to create any vector in the vector space.\n\n\nA basis for the vector space \\(V\\) is a spanning set of vectors \\(\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\ldots, \\mathbf{v}_{n}\\) that is a linearly independent set."
  },
  {
    "objectID": "lectures/ch3lecture1b.html#standard-basis",
    "href": "lectures/ch3lecture1b.html#standard-basis",
    "title": "Ch3 Lecture 1",
    "section": "Standard Basis",
    "text": "Standard Basis\n\nThe standard basis for \\(\\mathbb{R}^{n}\\) is the set of vectors \\(\\mathbf{e}_{1}, \\mathbf{e}_{2}, \\ldots, \\mathbf{e}_{n}\\) given by the columns of the \\(n \\times n\\) identity matrix."
  },
  {
    "objectID": "lectures/ch3lecture1b.html#showing-that-the-standard-basis-is-a-basis-for-mathbbrn-or-mathbbcn",
    "href": "lectures/ch3lecture1b.html#showing-that-the-standard-basis-is-a-basis-for-mathbbrn-or-mathbbcn",
    "title": "Ch3 Lecture 1",
    "section": "Showing that the Standard Basis is a Basis for \\(\\mathbb{R}^{n}\\) (or \\(\\mathbb{C}^{n}\\))",
    "text": "Showing that the Standard Basis is a Basis for \\(\\mathbb{R}^{n}\\) (or \\(\\mathbb{C}^{n}\\))\n\nLet \\(\\mathbf{v}=\\left(c_{1}, c_{2}, \\ldots, c_{n}\\right)\\) be a vector from \\(V\\)\n\\(c_{1}, c_{2}, \\ldots, c_{n}\\) are scalars (real or complex)\n\n\n\\[\n\\begin{aligned}\n\\mathbf{v} & =\\left[\\begin{array}{c}\nc_{1} \\\\\nc_{2} \\\\\n\\vdots \\\\\nc_{n}\n\\end{array}\\right]=c_{1}\\left[\\begin{array}{c}\n1 \\\\\n0 \\\\\n\\vdots \\\\\n0\n\\end{array}\\right]+c_{2}\\left[\\begin{array}{c}\n0 \\\\\n1 \\\\\n\\vdots \\\\\n0\n\\end{array}\\right]+\\cdots+c_{n}\\left[\\begin{array}{c}\n0 \\\\\n\\vdots \\\\\n0 \\\\\n1\n\\end{array}\\right] \\\\\n& =c_{1} \\mathbf{e}_{1}+c_{2} \\mathbf{e}_{2}+\\cdots+c_{n} \\mathbf{e}_{n} .\n\\end{aligned}\n\\]\n\nTwo points here: 1. you can make any vector in V out of a linear combination of the e’s, so the e’s span V. 2. the e’s are linearly independent because if you have a linear combination of them that equals 0, then each coefficient must be 0. 3. Therefore, the e’s are a basis of V."
  },
  {
    "objectID": "lectures/ch3lecture1b.html#invertibility-basis-and-linear-independence",
    "href": "lectures/ch3lecture1b.html#invertibility-basis-and-linear-independence",
    "title": "Ch3 Lecture 1",
    "section": "Invertibility, Basis, and Linear Independence",
    "text": "Invertibility, Basis, and Linear Independence\nAn \\(n \\times n\\) real matrix \\(A\\) is invertible if and only if its columns are linearly independent\n\nin which case they form a basis of \\(\\mathbb{R}^{n}\\)"
  },
  {
    "objectID": "lectures/ch3lecture1b.html#column-and-row-spaces",
    "href": "lectures/ch3lecture1b.html#column-and-row-spaces",
    "title": "Ch3 Lecture 1",
    "section": "Column and Row Spaces",
    "text": "Column and Row Spaces\n\nThe column space of the \\(m \\times n\\) matrix \\(A\\) is the subspace \\(\\mathcal{C}(A)\\) of \\(\\mathbb{R}^{m}\\) spanned by the columns of \\(A\\).\n\n\n\nThe row space of the \\(m \\times n\\) matrix \\(A\\) is the subspace \\(\\mathcal{R}(A)\\) of \\(\\mathbb{R}^{n}\\) spanned by the transposes of the rows of \\(A\\)"
  },
  {
    "objectID": "lectures/ch3lecture1b.html#null-space",
    "href": "lectures/ch3lecture1b.html#null-space",
    "title": "Ch3 Lecture 1",
    "section": "Null Space",
    "text": "Null Space\n\nThe null space of the \\(m \\times n\\) matrix \\(A\\) is the subset \\(\\mathcal{N}(A)\\) of \\(\\mathbb{R}^{n}\\)\n\\[\n\\mathcal{N}(A)=\\left\\{\\mathbf{x} \\in \\mathbb{R}^{n} \\mid A \\mathbf{x}=\\mathbf{0}\\right\\}\n\\]\n\n\n\\(\\mathcal{N}(A)\\) is just the solution set to \\(A \\mathbf{x}=\\mathbf{0}\\)\n\n\nFor example, if \\(A\\) is invertible, \\(A \\mathbf{x}=\\mathbf{0}\\) has only the trivial solution \\(\\mathbf{x}=\\mathbf{0}\\)\n\n\nso \\(\\mathcal{N}(A)\\) is just \\(\\left\\{\\mathbf{0}\\right\\}\\).\n\n\n\\(A\\) is invertible exactly if \\(\\mathcal{N}(A)=\\{\\mathbf{0}\\}\\)"
  },
  {
    "objectID": "lectures/ch3lecture1b.html#kernel-of-an-operator",
    "href": "lectures/ch3lecture1b.html#kernel-of-an-operator",
    "title": "Ch3 Lecture 1",
    "section": "Kernel of an Operator",
    "text": "Kernel of an Operator\n\nThe kernel of the linear operator \\(T\\) : \\(V \\rightarrow W\\) is the subspace of \\(V\\) defined by\n\\[\n\\operatorname{ker}(T)=\\{\\mathbf{x} \\in V \\mid T(\\mathbf{x})=\\mathbf{0}\\}\n\\]\n\nFor matrix operators kernels are the same thing as null spaces."
  },
  {
    "objectID": "lectures/ch3lecture1b.html#example",
    "href": "lectures/ch3lecture1b.html#example",
    "title": "Ch3 Lecture 1",
    "section": "Example",
    "text": "Example\nFind the null space of the matrix\n\\[\nA=\\left[\\begin{array}{rrrr}\n1 & 1 & 1 & -1 \\\\\n0 & 1 & 2 & 1\n\\end{array}\\right]\n\\]\ncorresponding to the system of equations \\[\n\\begin{aligned}\nx_{1}+x_{2}+x_{3}-x_{4} & =0 \\\\\nx_{2}+2 x_{3}+x_{4} & =0\n\\end{aligned}\n\\]\n\n\nFind the RREF of \\(A\\):\n\n\n\n\n\n\n\nPivots are in the first and second columns, so it follows that \\(x_{3}\\) and \\(x_{4}\\) are free, \\(x_{1}\\) and \\(x_{2}\\) are bound\n\n\\[\n\\begin{aligned}\n& x_{1}=x_{3}+2 x_{4} \\\\\n& x_{2}=-2 x_{3}-x_{4} .\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lectures/ch3lecture1b.html#section-1",
    "href": "lectures/ch3lecture1b.html#section-1",
    "title": "Ch3 Lecture 1",
    "section": "",
    "text": "\\[\ny_{t+1}=(I+dM)y_{t} \\Rightarrow y_{t}=(I+dM)^{-1}y_{t+1}\n\\]\n\\[\ny_{t}=y_{t+1}- dMy_{t} \\overset{?}\\approx y_{t+1}- d My_{t+1}\n\\]\n\nConnection: we will use math to show that the two are approximately equal.\n\n\nRemember Taylor series?\nFor small \\(x\\), we can approximate \\(\\frac{1}{1+x}\\) as \\(1-x+x^2-x^3+\\ldots\\)\n\n\nIf we have a matrix \\(A\\) such that \\(X = I + A\\) is invertible, then \\(X^{-1} = I - A + A^2 - A^3 + \\ldots\\)\n\n\nSo to first order in d,\n\\[\ny_{t}=(I+dM)^{-1}y_{t+1} \\approx \\left(I-dM\\right) y_{t+1}=y_{t+1}-d M y_{t+1}\n\\]\n\n\nNow \\(y_{t+1}=y_{t} + dM y_{t}\\). Plugging this in,\n\\(y_{t}=(I+dM)^{-1}y_{t+1} \\approx y_{t+1}-d M \\left(y_{t}+d M y_{t}\\right)\\approx y_{t+1}-d M y_{t}\\)"
  },
  {
    "objectID": "lectures/ch3lecture1b.html#using-the-null-space",
    "href": "lectures/ch3lecture1b.html#using-the-null-space",
    "title": "Ch3 Lecture 1",
    "section": "Using the Null Space",
    "text": "Using the Null Space\nSuppose that a Markov chain has an stable transition matrix \\(A=\\left[\\begin{array}{ll}0.7 & 0.4 \\\\ 0.3 & 0.6\\end{array}\\right]\\), so that\n\\[\n\\mathbf{x}^{(k+1)}=A \\mathbf{x}^{(k)}\n\\]\n\nWhat is the steady-state vector \\(\\mathbf{x}\\)? (The limit as \\(k \\rightarrow \\infty\\) of \\(\\mathbf{x}^{(k)}\\))\n\n\nTake limit of both sides:\n\\[\n\\mathbf{x}=A \\mathbf{x}\n\\]\n\n\\[\n\\mathbf{0}=\\mathbf{x}-A \\mathbf{x}=I \\mathbf{x}-A \\mathbf{x}=(I-A) \\mathbf{x}\n\\]\nSo \\(\\mathbf{x}\\) is in the null space of \\(I-A\\).\n\n\n\n\\[\n\\mathbf{0}=\\mathbf{x}-A \\mathbf{x}=I \\mathbf{x}-A \\mathbf{x}=(I-A) \\mathbf{x}\n\\]\nSo \\(\\mathbf{x}\\) is in the null space of \\(I-A\\)."
  },
  {
    "objectID": "lectures/ch3lecture1b.html#section-2",
    "href": "lectures/ch3lecture1b.html#section-2",
    "title": "Ch3 Lecture 1",
    "section": "",
    "text": "Is this set of vectors linearly independent?\n\\[\n\\left[\\begin{array}{l}1 \\\\ 1 \\\\ 0\\end{array}\\right],\\left[\\begin{array}{l}0 \\\\ 1 \\\\ 1\\end{array}\\right],\\left[\\begin{array}{r}1 \\\\ -1 \\\\ -2\\end{array}\\right]\n\\]\nPut in matrix form and find the reduced row echelon form: . . .\n\\[\n\\left[\\begin{array}{rrr}\n1 & 0 & 1 \\\\\n1 & 1 & -1 \\\\\n0 & 1 & -2\n\\end{array}\\right] \\xrightarrow[E_{21}(-1)]{\\longrightarrow}\\left[\\begin{array}{rrr}\n1 & 0 & 1 \\\\\n0 & 1 & -2 \\\\\n0 & 1 & -2\n\\end{array}\\right] \\xrightarrow[E_{32}(-1)]{\\longrightarrow}\\left[\\begin{array}{rrr}\n1 & 0 & 1 \\\\\n0 & 1 & -2 \\\\\n0 & 0 & 0\n\\end{array}\\right],\n\\]\n\nSolutions to the homogeneous system are \\(\\mathbf{c}=\\left(c_{1}=-c_{3},c_{2}= 2 c_{3}, c_{3}= c_{3}\\right)=\\)\nWhole vector is \\(c_{3}(-1,2,1)\\). Take \\(c_{3}=1\\) and we have that \\[\n-1 \\mathbf{v}_{1}+2 \\mathbf{v}_{2}+1 \\mathbf{v}_{3}=0\n\\]\n\n\n\\[\n-1 \\mathbf{v}_{1}+2 \\mathbf{v}_{2}+1 \\mathbf{v}_{3}=0\n\\]\nSo the vectors are linearly dependent."
  },
  {
    "objectID": "lectures/ch3lecture1b.html#using-the-null-space-to-find-a-basis",
    "href": "lectures/ch3lecture1b.html#using-the-null-space-to-find-a-basis",
    "title": "Ch3 Lecture 1",
    "section": "Using the Null Space to find a basis",
    "text": "Using the Null Space to find a basis\nFind a basis for the column space of the matrix\n\\[\nA=\\left[\\begin{array}{rrrr}\n1 & 1 & 1 & -1 \\\\\n0 & 1 & 2 & 1\n\\end{array}\\right]\n\\]\n\nFrom before, we found the RREF and used this to find a general form of the solutions to \\(A \\mathbf{x}=\\mathbf{0}\\)\n\\(\\mathbf{x}=\\left(x_{1}, x_{2}, x_{3}, x_{4}\\right)=\\) \\(\\left(x_{3}+2 x_{4},-2 x_{3}-x_{4}, x_{3}, x_{4}\\right)\\)\n\n\nWrite \\(A=\\left[\\mathbf{a}_{1}, \\mathbf{a}_{2}, \\mathbf{a}_{3}, \\mathbf{a}_{4}\\right]\\)\n\n\n\\(\\mathbf{0}=x_{1} \\mathbf{a}_{1}+x_{2} \\mathbf{a}_{2}+x_{3} \\mathbf{a}_{3}+x_{4} \\mathbf{a}_{4}\\)\n\n\n\\(\\mathbf{0}=\\left(x_{3}+2 x_{4}\\right) \\mathbf{a}_{1}-\\left(2 x_{3}+x_{4}\\right) \\mathbf{a}_{2}+x_{3} \\mathbf{a}_{3}+x_{4} \\mathbf{a}_{4}\\)\n\n\nTake \\(x_{3}=1\\) and \\(x_{4}=0\\). Then \\(\\mathbf{0}=\\mathbf{a}_{1}-2 \\mathbf{a}_{2}+\\mathbf{a}_{3}\\)\n\n\nSo \\(\\mathbf{a}_{3}\\) is a linear combination of \\(\\mathbf{a}_{1}\\) and \\(\\mathbf{a}_{2}\\)\n\n\nSimilarly, take \\(x_{3}=0\\) and \\(x_{4}=1\\). Then \\(\\mathbf{0}=2 \\mathbf{a}_{1}-\\mathbf{a}_{2}+\\mathbf{a}_{4}\\) so \\(\\mathbf{a}_{4}\\) is a linear combination of \\(\\mathbf{a}_{1}\\) and \\(\\mathbf{a}_{2}\\)\n\n\nSo \\(\\mathbf{a}_{1}\\) and \\(\\mathbf{a}_{2}\\) are a basis for the column space of \\(A\\)\n\\(\\mathcal{C}(A)=\\operatorname{span}\\left\\{\\mathbf{a}_{1}, \\mathbf{a}_{2}\\right\\}=\\operatorname{span}\\left\\{\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right],\\left[\\begin{array}{l}1 \\\\ 1\\end{array}\\right]\\right\\}\\)"
  },
  {
    "objectID": "lectures/ch3lecture1b.html#consistency-and-column-space-back-to-linear-systems",
    "href": "lectures/ch3lecture1b.html#consistency-and-column-space-back-to-linear-systems",
    "title": "Ch3 Lecture 1",
    "section": "Consistency and Column Space: Back to Linear Systems",
    "text": "Consistency and Column Space: Back to Linear Systems\n\nThe linear system \\(A \\mathbf{x}=\\mathbf{b}\\) of \\(m\\) equations in \\(n\\) unknowns is consistent if and only if \\(\\mathbf{b} \\in \\mathcal{C}(A)\\).\n\n\n\\(A \\mathbf{x}\\) is a linear combination of the columns of \\(A\\) with the entries of \\(\\mathbf{x}\\) as scalar coefficients\nIf \\(A \\mathbf{x}=\\mathbf{b}\\) has a solution means some linear combination of columns of \\(A\\) adds up to \\(\\mathbf{b}\\)\n\\(\\mathbf{b} \\in \\mathcal{C}(A)\\)"
  },
  {
    "objectID": "lectures/ch3lecture1b.html#how-to-tell-if-a-vector-is-in-a-spanning-space",
    "href": "lectures/ch3lecture1b.html#how-to-tell-if-a-vector-is-in-a-spanning-space",
    "title": "Ch3 Lecture 1",
    "section": "How to tell if a vector is in a spanning space",
    "text": "How to tell if a vector is in a spanning space\nSuppose we have a space \\(V\\) spanned by \\(\\mathbf{v}_{1}=(1,1,3,3), \\mathbf{v}_{2}=(0,2,2,4)\\), and and \\(\\mathbf{v}_{3}=(1,0,2,1)\\)\n\nOne of these vectors is in \\(V\\). Which one?\n\\(\\mathbf{u}=(2,1,5,4)\\) and \\(\\mathbf{w}=(1,0,0,0)\\)\n\n\nDefine \\(A=\\left[\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\mathbf{v}_{3}\\right]\\)\nThen \\(\\mathbf{u} \\in \\mathcal{C}(A)\\) if and only if \\(A \\mathbf{x}=\\mathbf{u}\\) has a solution (is consistent)\n\n\nSolve both at once: \\[\n[A|\\mathbf{u}| \\mathbf{w}]=\\left[\\begin{array}{lllll}\n1 & 0 & 1 & 2 & 1 \\\\\n1 & 2 & 0 & 1 & 0 \\\\\n3 & 2 & 2 & 5 & 0 \\\\\n3 & 4 & 1 & 4 & 0\n\\end{array}\\right] \\text { with reduced row echelon form }\\left[\\begin{array}{rrrrr}\n1 & 0 & 1 & 2 & 0 \\\\\n0 & 1 & -\\frac{1}{2} & -\\frac{1}{2} & 0 \\\\\n0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 & 0\n\\end{array}\\right]\n\\]\n\nObserve that there is a pivot in the fifth column but not in the fourth column. This tells us that the system with augmented matrix \\([A \\mid \\mathbf{u}]\\) is consistent, but the system with augmented matrix \\([A \\mid \\mathbf{w}]\\) is not consistent. Therefore, \\(\\mathbf{u} \\in \\operatorname{span}\\left\\{\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\mathbf{v}_{3}\\right\\}\\), but \\(\\mathbf{w} \\notin \\operatorname{span}\\left\\{\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\mathbf{v}_{3}\\right\\}\\).\n\n\n\n\\(\\mathbf{u} \\in \\operatorname{span}\\left\\{\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\mathbf{v}_{3}\\right\\}\\),\nbut \\(\\mathbf{w} \\notin \\operatorname{span}\\left\\{\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\mathbf{v}_{3}\\right\\}\\)\n\n\n\\[\n\\mathbf{u}=\\left(2-c_{3}\\right) \\mathbf{v}_{1}+\\frac{1}{2}\\left(c_{3}-1\\right) \\mathbf{v}_{2}+c_{3} \\mathbf{v}_{3}\n\\]"
  },
  {
    "objectID": "lectures/ch3lecture1b.html#summary",
    "href": "lectures/ch3lecture1b.html#summary",
    "title": "Ch3 Lecture 1",
    "section": "Summary",
    "text": "Summary"
  },
  {
    "objectID": "lectures/ch3lecture1b.html#in-matrix-form",
    "href": "lectures/ch3lecture1b.html#in-matrix-form",
    "title": "Ch3 Lecture 1",
    "section": "In matrix form",
    "text": "In matrix form\nIn matrix form, this becomes \\(y_{t+1}=y_{t}+ My_{t}\\):\n\\[\nM=\\left[\\begin{array}{ccccccc}\n-2 & 1 & 0 & 0 & 0 & 0 & 0 \\\\\n1 & -2 & 1 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & -2 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & -2 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & -2 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 1 & -2 & 1 \\\\\n0 & 0 & 0 & 0 & 0 & 1 & -2\n\\end{array}\\right]\n\\]\n\n\\[\ny_{t}=y_{t+1}- d My_{t} \\overset{?}\\approx y_{t+1}- My_{t+1}\n\\]\nTurns out it works… but why?"
  },
  {
    "objectID": "lectures/ch3lecture1b.html#using-matrix-inversion",
    "href": "lectures/ch3lecture1b.html#using-matrix-inversion",
    "title": "Ch3 Lecture 1",
    "section": "Using matrix inversion",
    "text": "Using matrix inversion\n\n\\(y_{t+1}=y_{t}+ My_{t}\\)\nis the same as \\(y_{t+1}=Iy_{t}+ My_{t}\\)\nin other words, \\(y_{t+1}=(I+M)y_{t}\\)\n\n\nHow do we solve this for \\(y_{t}\\), given \\(y_{t+1}\\)?\n\n\n\\[\ny_{t+1}=(I+M)y_{t} \\Rightarrow y_{t}=(I+M)^{-1}y_{t+1}\n\\]\n\n\nTwo different approaches – one correct, the other one gave approximately the right answer. Why?"
  },
  {
    "objectID": "lectures/ch3lecture1b.html#the-point",
    "href": "lectures/ch3lecture1b.html#the-point",
    "title": "Ch3 Lecture 1",
    "section": "The point",
    "text": "The point\nThe problem was much easier to solve once we put it in matrix form."
  },
  {
    "objectID": "lectures/ch3lecture1b.html#section-3",
    "href": "lectures/ch3lecture1b.html#section-3",
    "title": "Ch3 Lecture 1",
    "section": "",
    "text": "\\[\n\\left[\\begin{array}{c}\nx_{1} \\\\\nx_{2} \\\\\nx_{3} \\\\\nx_{4}\n\\end{array}\\right]=\\left[\\begin{array}{c}\nx_{3}+2 x_{4} \\\\\n-2 x_{3}-x_{4} \\\\\nx_{3} \\\\\nx_{4}\n\\end{array}\\right]=x_{3}\\left[\\begin{array}{r}\n1 \\\\\n-2 \\\\\n1 \\\\\n0\n\\end{array}\\right]+x_{4}\\left[\\begin{array}{r}\n2 \\\\\n-1 \\\\\n0 \\\\\n1\n\\end{array}\\right] .\n\\]\n\nThe general solution to the homogeneous system is an arbitrary linear combination of these two vectors\n\n\n\\[\n\\mathcal{N}(A)=\\operatorname{span}\\left\\{\\left[\\begin{array}{r}\n1 \\\\\n-2 \\\\\n1 \\\\\n0\n\\end{array}\\right],\\left[\\begin{array}{r}\n2 \\\\\n-1 \\\\\n0 \\\\\n1\n\\end{array}\\right]\\right\\} \\subseteq \\mathbb{R}^{4}\n\\]\n\n\nThese are independent, so they form a basis for \\(\\mathcal{N}(A)\\)"
  },
  {
    "objectID": "lectures/ch3lecture1b.html#section-4",
    "href": "lectures/ch3lecture1b.html#section-4",
    "title": "Ch3 Lecture 1",
    "section": "",
    "text": "\\[\nI-A=\\left[\\begin{array}{ll}\n1 & 0 \\\\\n0 & 1\n\\end{array}\\right]-\\left[\\begin{array}{ll}\n0.7 & 0.4 \\\\\n0.3 & 0.6\n\\end{array}\\right]=\\left[\\begin{array}{rr}\n0.3 & -0.4 \\\\\n-0.3 & 0.4\n\\end{array}\\right] \\text {. }\n\\]\n\n\\[\n\\left[\\begin{array}{rr}\n0.3 & -0.4 \\\\\n-0.3 & 0.4\n\\end{array}\\right] \\xrightarrow{E_{21}(1)}\\left[\\begin{array}{rr}\n1 & -4 / 3 \\\\\n0 & 0\n\\end{array}\\right]\n\\]\n\nnull space of \\(I-A\\) is spanned by the single vector \\((4 / 3,1)\\). In particular, any multiple of this vector qualifies as a possible limiting vector.\nIf we want a limiting vector whose entries are nonnegative and sum to 1 (which is required for states in a Markov chain), then the only choice is the vector resulting from dividing \\((4 / 3,1)\\) by the sum of its coordinates to obtain\n\nSolution set is \\(x_{1}=4 x_{2} / 3\\)\n\n\n\\[\n(3 / 7)(4 / 3,1)=(4 / 7,3 / 7) \\approx(0.57143,0.42857)\n\\]"
  },
  {
    "objectID": "lectures/ch3lecture1b.html#form-of-general-solution-to-a-linear-system",
    "href": "lectures/ch3lecture1b.html#form-of-general-solution-to-a-linear-system",
    "title": "Ch3 Lecture 1",
    "section": "Form of general solution to a linear system",
    "text": "Form of general solution to a linear system\n\n\nSuppose the system \\(A \\mathbf{x}=\\mathbf{b}\\) has a particular solution \\(\\mathbf{x}_{*}\\).\nThen the general solution \\(\\mathbf{x}\\) to this system can be described by the equation\n\n\n\\[\n\\mathbf{x}=\\mathbf{x}_{*}+\\mathbf{z}\n\\]\nwhere \\(\\mathbf{z}\\) runs over all elements of \\(\\mathcal{N}(A)\\).\n\n\n\nSolution set is the set of all translates of elements in the null space of \\(A\\) by some fixed vector.\n\n\nWhen n is 2 or 3 this says that the solution set is either a single point, a line or a plane—not necessarily through the origin!"
  },
  {
    "objectID": "lectures/ch3lecture1b.html#example-1",
    "href": "lectures/ch3lecture1b.html#example-1",
    "title": "Ch3 Lecture 1",
    "section": "Example",
    "text": "Example\nDescribe the solution sets to the system \\[\n\\begin{array}{r}\nx+2 y=3 \\\\\nx+y+t=3 .\n\\end{array}\n\\]\n\n\n\n\n\\[\n\\begin{aligned}\n& x=3-2 t \\\\\n& y=t \\\\\n& z=t .\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lectures/ch3lecture1b.html#null-space-algorithm",
    "href": "lectures/ch3lecture1b.html#null-space-algorithm",
    "title": "Ch3 Lecture 1",
    "section": "Null Space algorithm",
    "text": "Null Space algorithm\nGiven an \\(m \\times n\\) matrix \\(A\\).\n\nCompute the reduced row echelon form \\(R\\) of \\(A\\).\nUse \\(R\\) to find the general solution to the homogeneous system \\(A \\mathbf{x}=0\\).\n\n\n\nWrite the general solution \\(\\mathbf{x}=\\left(x_{1}, x_{2}, \\ldots, x_{n}\\right)\\) to the homogeneous system in the form\n\n\\[\n\\mathbf{x}=x_{i_{1}} \\mathbf{w}_{1}+x_{i_{2}} \\mathbf{w}_{2}+\\cdots+x_{i_{n-r}} \\mathbf{w}_{n-r}\n\\]\nwhere \\(x_{i_{1}}, x_{i_{2}}, \\ldots, x_{i_{n-r}}\\) are the free variables.\n\n\n\nList the vectors \\(\\mathbf{w}_{1}, \\mathbf{w}_{2}, \\ldots, \\mathbf{w}_{n-r}\\). These form a basis of \\(\\mathcal{N}(A)\\)."
  },
  {
    "objectID": "lectures/ch3lecture1b.html#showing-that-the-standard-basis-is-a-basis-for-the-reals-or-complex-numbers",
    "href": "lectures/ch3lecture1b.html#showing-that-the-standard-basis-is-a-basis-for-the-reals-or-complex-numbers",
    "title": "Ch3 Lecture 1",
    "section": "Showing that the Standard Basis is a Basis for the reals or complex numbers",
    "text": "Showing that the Standard Basis is a Basis for the reals or complex numbers\n\nLet \\(\\mathbf{v}=\\left(c_{1}, c_{2}, \\ldots, c_{n}\\right)\\) be a vector from \\(V\\)\n\\(c_{1}, c_{2}, \\ldots, c_{n}\\) are scalars (real or complex)\n\n\n\\[\n\\begin{aligned}\n\\mathbf{v} & =\\left[\\begin{array}{c}\nc_{1} \\\\\nc_{2} \\\\\n\\vdots \\\\\nc_{n}\n\\end{array}\\right]=c_{1}\\left[\\begin{array}{c}\n1 \\\\\n0 \\\\\n\\vdots \\\\\n0\n\\end{array}\\right]+c_{2}\\left[\\begin{array}{c}\n0 \\\\\n1 \\\\\n\\vdots \\\\\n0\n\\end{array}\\right]+\\cdots+c_{n}\\left[\\begin{array}{c}\n0 \\\\\n\\vdots \\\\\n0 \\\\\n1\n\\end{array}\\right] \\\\\n& =c_{1} \\mathbf{e}_{1}+c_{2} \\mathbf{e}_{2}+\\cdots+c_{n} \\mathbf{e}_{n} .\n\\end{aligned}\n\\]\n\nTwo points here: 1. you can make any vector in V out of a linear combination of the e’s, so the e’s span V. 2. the e’s are linearly independent because if you have a linear combination of them that equals 0, then each coefficient must be 0. 3. Therefore, the e’s are a basis of V."
  },
  {
    "objectID": "lectures/ch3lecture1.html#section",
    "href": "lectures/ch3lecture1.html#section",
    "title": "Ch3 Lecture 1",
    "section": "",
    "text": "\\[\n\\begin{equation*}\ny_{i, t+1}=y_{i, t}+d\\left(y_{i-1, t}-2 y_{i, t}+y_{i+1, t}\\right)\n\\end{equation*}\n\\]\n\\(y_{0,t+1} = 0\\) \\(y_{1,t+1}=y_{1,t}+d\\left(0-2 y_{1,t}+y_{2,t}\\right)\\) \\(y_{3,t+1}=y_{3,t}+d\\left(y_{2,t}-2 y_{3,t}+y_{4,t}\\right)\\) … \\(y_{6,t+1}=0\\)\n\nWe want to solve these for \\(y_{i,t}\\) given \\(y_{i,t+1}\\).\n\\[\n\\begin{equation*}\ny_{i, t+1}-d\\left(y_{i-1, t}-2 y_{i, t}+y_{i+1, t}\\right)=y_{i, t}\n\\end{equation*}\n\\]\n\\[\n\\approx y_{i, t+1}-d\\left(y_{i-1, t+1}-2 y_{i, t+1}+y_{i+1, t+1}\\right) ?\n\\]"
  },
  {
    "objectID": "lectures/ch3lecture1.html#in-matrix-form",
    "href": "lectures/ch3lecture1.html#in-matrix-form",
    "title": "Ch3 Lecture 1",
    "section": "In matrix form",
    "text": "In matrix form\nIn matrix form, this becomes \\(y_{t+1}=y_{t}+ My_{t}\\):\n\\[\nM=\\left[\\begin{array}{ccccccc}\n-2 & 1 & 0 & 0 & 0 & 0 & 0 \\\\\n1 & -2 & 1 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & -2 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & -2 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & -2 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 1 & -2 & 1 \\\\\n0 & 0 & 0 & 0 & 0 & 1 & -2\n\\end{array}\\right]\n\\]\n\n\\[\ny_{t}=y_{t+1}- d My_{t} \\overset{?}\\approx y_{t+1}- My_{t+1}\n\\]\nTurns out it works… but why?"
  },
  {
    "objectID": "lectures/ch3lecture1.html#using-matrix-inversion",
    "href": "lectures/ch3lecture1.html#using-matrix-inversion",
    "title": "Ch3 Lecture 1",
    "section": "Using matrix inversion",
    "text": "Using matrix inversion\n\n\\(y_{t+1}=y_{t}+ My_{t}\\)\nis the same as \\(y_{t+1}=Iy_{t}+ My_{t}\\)\nin other words, \\(y_{t+1}=(I+M)y_{t}\\)\n\n\nHow do we solve this for \\(y_{t}\\), given \\(y_{t+1}\\)?\n\n\n\\[\ny_{t+1}=(I+M)y_{t} \\Rightarrow y_{t}=(I+M)^{-1}y_{t+1}\n\\]\n\n\nTwo different approaches – one correct, the other one gave approximately the right answer. Why?"
  },
  {
    "objectID": "lectures/ch3lecture1.html#section-1",
    "href": "lectures/ch3lecture1.html#section-1",
    "title": "Ch3 Lecture 1",
    "section": "",
    "text": "\\[\ny_{t+1}=(I+dM)y_{t} \\Rightarrow y_{t}=(I+dM)^{-1}y_{t+1}\n\\]\n\\[\ny_{t}=y_{t+1}- dMy_{t} \\overset{?}\\approx y_{t+1}- d My_{t+1}\n\\]\n\nConnection: we will use math to show that the two are approximately equal.\n\n\nRemember Taylor series?\nFor small \\(x\\), we can approximate \\(\\frac{1}{1+x}\\) as \\(1-x+x^2-x^3+\\ldots\\)\n\n\nIf we have a matrix \\(A\\) such that \\(X = I + A\\) is invertible, then \\(X^{-1} = I - A + A^2 - A^3 + \\ldots\\)\n\n\nSo to first order in d,\n\\[\ny_{t}=(I+dM)^{-1}y_{t+1} \\approx \\left(I-dM\\right) y_{t+1}=y_{t+1}-d M y_{t+1}\n\\]\n\n\nNow \\(y_{t+1}=y_{t} + dM y_{t}\\). Plugging this in,\n\\(y_{t}=(I+dM)^{-1}y_{t+1} \\approx y_{t+1}-d M \\left(y_{t}+d M y_{t}\\right)\\approx y_{t+1}-d M y_{t}\\)"
  },
  {
    "objectID": "lectures/ch3lecture1.html#the-point",
    "href": "lectures/ch3lecture1.html#the-point",
    "title": "Ch3 Lecture 1",
    "section": "The point",
    "text": "The point\nThe problem was much easier to solve once we put it in matrix form."
  },
  {
    "objectID": "lectures/ch3lecture1.html#linear-independence",
    "href": "lectures/ch3lecture1.html#linear-independence",
    "title": "Ch3 Lecture 1",
    "section": "Linear Independence",
    "text": "Linear Independence\n\nThe vectors \\(\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\ldots, \\mathbf{v}_{n}\\) are linearly dependent if there exist scalars \\(c_{1}, c_{2}, \\ldots, c_{n}\\), not all zero, such that\n\\[\n\\begin{equation*}\nc_{1} \\mathbf{v}_{1}+c_{2} \\mathbf{v}_{2}+\\cdots+c_{n} \\mathbf{v}_{n}=\\mathbf{0}\n\\end{equation*}\n\\]\nOtherwise, the vectors are called linearly independent.\n\n\nThis doesn’t mean that one of the vectors is a scalar multiple of another one."
  },
  {
    "objectID": "lectures/ch3lecture1.html#checking-for-linear-independence",
    "href": "lectures/ch3lecture1.html#checking-for-linear-independence",
    "title": "Ch3 Lecture 1",
    "section": "Checking for Linear Independence",
    "text": "Checking for Linear Independence\nIs this set of vectors linearly independent?\n\\[\n\\left[\\begin{array}{l}1 \\\\ 1 \\\\ 0\\end{array}\\right],\\left[\\begin{array}{l}0 \\\\ 1 \\\\ 1\\end{array}\\right],\\left[\\begin{array}{r}1 \\\\ -1 \\\\ -2\\end{array}\\right]\n\\]\n\nGoal: find some set of scalars \\(c_{1}, c_{2}, c_{3}\\), not all zero, such that \\(c_{1} \\mathbf{v}_{1}+c_{2} \\mathbf{v}_{2}+c_{3} \\mathbf{v}_{3}=\\mathbf{0}\\).\n\n\nDefine: - matrix \\(A=\\left[\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\mathbf{v}_{3}\\right]\\) - vector \\(\\mathbf{c}=\\left(c_{1}, c_{2}, c_{3}\\right)\\).\n\n\nThen:\n\\[\nc_{1} \\mathbf{v}_{1}+c_{2} \\mathbf{v}_{2}+c_{3} \\mathbf{v}_{3}=\\left[\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\mathbf{v}_{3}\\right]\\left[\\begin{array}{l}\nc_{1} \\\\\nc_{2} \\\\\nc_{3}\n\\end{array}\\right]=A \\mathbf{c}\n\\]\nSo we just need to see if there is a nontrivial solution to \\(A \\mathbf{c}=\\mathbf{0}\\)."
  },
  {
    "objectID": "lectures/ch3lecture1.html#section-2",
    "href": "lectures/ch3lecture1.html#section-2",
    "title": "Ch3 Lecture 1",
    "section": "",
    "text": "Is this set of vectors linearly independent?\n\\[\n\\left[\\begin{array}{l}1 \\\\ 1 \\\\ 0\\end{array}\\right],\\left[\\begin{array}{l}0 \\\\ 1 \\\\ 1\\end{array}\\right],\\left[\\begin{array}{r}1 \\\\ -1 \\\\ -2\\end{array}\\right]\n\\]\nPut in matrix form and find the reduced row echelon form: . . .\n\\[\n\\left[\\begin{array}{rrr}\n1 & 0 & 1 \\\\\n1 & 1 & -1 \\\\\n0 & 1 & -2\n\\end{array}\\right] \\xrightarrow[E_{21}(-1)]{\\longrightarrow}\\left[\\begin{array}{rrr}\n1 & 0 & 1 \\\\\n0 & 1 & -2 \\\\\n0 & 1 & -2\n\\end{array}\\right] \\xrightarrow[E_{32}(-1)]{\\longrightarrow}\\left[\\begin{array}{rrr}\n1 & 0 & 1 \\\\\n0 & 1 & -2 \\\\\n0 & 0 & 0\n\\end{array}\\right],\n\\]\n\nSolutions to the homogeneous system are \\(\\mathbf{c}=\\left(c_{1}=-c_{3},c_{2}= 2 c_{3}, c_{3}= c_{3}\\right)=\\)\nWhole vector is \\(c_{3}(-1,2,1)\\). Take \\(c_{3}=1\\) and we have that \\[\n-1 \\mathbf{v}_{1}+2 \\mathbf{v}_{2}+1 \\mathbf{v}_{3}=0\n\\]\n\n\n\\[\n-1 \\mathbf{v}_{1}+2 \\mathbf{v}_{2}+1 \\mathbf{v}_{3}=0\n\\]\nSo the vectors are linearly dependent."
  },
  {
    "objectID": "lectures/ch3lecture1.html#example-2",
    "href": "lectures/ch3lecture1.html#example-2",
    "title": "Ch3 Lecture 1",
    "section": "Example 2",
    "text": "Example 2\nAre the following vectors linearly independent?\n\\(\\left[\\begin{array}{l}0 \\\\ 1 \\\\ 0\\end{array}\\right],\\left[\\begin{array}{l}1 \\\\ 1 \\\\ 0\\end{array}\\right],\\left[\\begin{array}{l}0 \\\\ 1 \\\\ 1\\end{array}\\right]\\)\n\nSet up A = \\(\\left[\\begin{array}{lll}0 & 1 & 0 \\\\ 1 & 1 & 1 \\\\ 0 & 0 & 1\\end{array}\\right]\\)\n\n\nDifferent approach: \\[\n\\operatorname{det}\\left[\\begin{array}{lll}\n0 & 1 & 0 \\\\\n1 & 1 & 1 \\\\\n0 & 0 & 1\n\\end{array}\\right]=-1 \\operatorname{det}\\left[\\begin{array}{ll}\n1 & 0 \\\\\n0 & 1\n\\end{array}\\right]=-1\n\\]\n\n\nDeterminant is non-zero, so \\(A\\) is invertible (non-singular)\n\n\nOnly solution to \\(A \\mathbf{c}=0\\) is \\(\\mathbf{c}=0\\).\n\n\nSo the vectors are linearly independent."
  },
  {
    "objectID": "lectures/ch3lecture1.html#summary",
    "href": "lectures/ch3lecture1.html#summary",
    "title": "Ch3 Lecture 1",
    "section": "Summary",
    "text": "Summary"
  },
  {
    "objectID": "lectures/ch3lecture1.html#basis-of-a-vector-space",
    "href": "lectures/ch3lecture1.html#basis-of-a-vector-space",
    "title": "Ch3 Lecture 1",
    "section": "Basis of a Vector Space",
    "text": "Basis of a Vector Space\n\nA spanning set is a set of vectors that can be combined to create any vector in the vector space.\n\n\nA basis for the vector space \\(V\\) is a spanning set of vectors \\(\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\ldots, \\mathbf{v}_{n}\\) that is a linearly independent set."
  },
  {
    "objectID": "lectures/ch3lecture1.html#standard-basis",
    "href": "lectures/ch3lecture1.html#standard-basis",
    "title": "Ch3 Lecture 1",
    "section": "Standard Basis",
    "text": "Standard Basis\n\nThe standard basis for \\(\\mathbb{R}^{n}\\) is the set of vectors \\(\\mathbf{e}_{1}, \\mathbf{e}_{2}, \\ldots, \\mathbf{e}_{n}\\) given by the columns of the \\(n \\times n\\) identity matrix."
  },
  {
    "objectID": "lectures/ch3lecture1.html#showing-that-the-standard-basis-is-a-basis-for-the-reals-or-complex-numbers",
    "href": "lectures/ch3lecture1.html#showing-that-the-standard-basis-is-a-basis-for-the-reals-or-complex-numbers",
    "title": "Ch3 Lecture 1",
    "section": "Showing that the Standard Basis is a Basis for the reals or complex numbers",
    "text": "Showing that the Standard Basis is a Basis for the reals or complex numbers\n\nLet \\(\\mathbf{v}=\\left(c_{1}, c_{2}, \\ldots, c_{n}\\right)\\) be a vector from \\(V\\)\n\\(c_{1}, c_{2}, \\ldots, c_{n}\\) are scalars (real or complex)\n\n\n\\[\n\\begin{aligned}\n\\mathbf{v} & =\\left[\\begin{array}{c}\nc_{1} \\\\\nc_{2} \\\\\n\\vdots \\\\\nc_{n}\n\\end{array}\\right]=c_{1}\\left[\\begin{array}{c}\n1 \\\\\n0 \\\\\n\\vdots \\\\\n0\n\\end{array}\\right]+c_{2}\\left[\\begin{array}{c}\n0 \\\\\n1 \\\\\n\\vdots \\\\\n0\n\\end{array}\\right]+\\cdots+c_{n}\\left[\\begin{array}{c}\n0 \\\\\n\\vdots \\\\\n0 \\\\\n1\n\\end{array}\\right] \\\\\n& =c_{1} \\mathbf{e}_{1}+c_{2} \\mathbf{e}_{2}+\\cdots+c_{n} \\mathbf{e}_{n} .\n\\end{aligned}\n\\]\n\nTwo points here: 1. you can make any vector in V out of a linear combination of the e’s, so the e’s span V. 2. the e’s are linearly independent because if you have a linear combination of them that equals 0, then each coefficient must be 0. 3. Therefore, the e’s are a basis of V."
  },
  {
    "objectID": "lectures/ch3lecture1.html#invertibility-basis-and-linear-independence",
    "href": "lectures/ch3lecture1.html#invertibility-basis-and-linear-independence",
    "title": "Ch3 Lecture 1",
    "section": "Invertibility, Basis, and Linear Independence",
    "text": "Invertibility, Basis, and Linear Independence\nAn \\(n \\times n\\) real matrix \\(A\\) is invertible if and only if its columns are linearly independent\n\nin which case they form a basis of \\(\\mathbb{R}^{n}\\)"
  },
  {
    "objectID": "lectures/ch3lecture1.html#column-and-row-spaces",
    "href": "lectures/ch3lecture1.html#column-and-row-spaces",
    "title": "Ch3 Lecture 1",
    "section": "Column and Row Spaces",
    "text": "Column and Row Spaces\n\nThe column space of the \\(m \\times n\\) matrix \\(A\\) is the subspace \\(\\mathcal{C}(A)\\) of \\(\\mathbb{R}^{m}\\) spanned by the columns of \\(A\\).\n\n\n\nThe row space of the \\(m \\times n\\) matrix \\(A\\) is the subspace \\(\\mathcal{R}(A)\\) of \\(\\mathbb{R}^{n}\\) spanned by the transposes of the rows of \\(A\\)"
  },
  {
    "objectID": "lectures/ch3lecture1.html#null-space",
    "href": "lectures/ch3lecture1.html#null-space",
    "title": "Ch3 Lecture 1",
    "section": "Null Space",
    "text": "Null Space\n\nThe null space of the \\(m \\times n\\) matrix \\(A\\) is the subset \\(\\mathcal{N}(A)\\) of \\(\\mathbb{R}^{n}\\)\n\\[\n\\mathcal{N}(A)=\\left\\{\\mathbf{x} \\in \\mathbb{R}^{n} \\mid A \\mathbf{x}=\\mathbf{0}\\right\\}\n\\]\n\n\n\\(\\mathcal{N}(A)\\) is just the solution set to \\(A \\mathbf{x}=\\mathbf{0}\\)\n\n\nFor example, if \\(A\\) is invertible, \\(A \\mathbf{x}=\\mathbf{0}\\) has only the trivial solution \\(\\mathbf{x}=\\mathbf{0}\\)\n\n\nso \\(\\mathcal{N}(A)\\) is just \\(\\left\\{\\mathbf{0}\\right\\}\\).\n\n\n\\(A\\) is invertible exactly if \\(\\mathcal{N}(A)=\\{\\mathbf{0}\\}\\)"
  },
  {
    "objectID": "lectures/ch3lecture1.html#kernel-of-an-operator",
    "href": "lectures/ch3lecture1.html#kernel-of-an-operator",
    "title": "Ch3 Lecture 1",
    "section": "Kernel of an Operator",
    "text": "Kernel of an Operator\n\nThe kernel of the linear operator \\(T\\) : \\(V \\rightarrow W\\) is the subspace of \\(V\\) defined by\n\\[\n\\operatorname{ker}(T)=\\{\\mathbf{x} \\in V \\mid T(\\mathbf{x})=\\mathbf{0}\\}\n\\]\n\nFor matrix operators kernels are the same thing as null spaces."
  },
  {
    "objectID": "lectures/ch3lecture1.html#example",
    "href": "lectures/ch3lecture1.html#example",
    "title": "Ch3 Lecture 1",
    "section": "Example",
    "text": "Example\nFind the null space of the matrix\n\\[\nA=\\left[\\begin{array}{rrrr}\n1 & 1 & 1 & -1 \\\\\n0 & 1 & 2 & 1\n\\end{array}\\right]\n\\]\ncorresponding to the system of equations \\[\n\\begin{aligned}\nx_{1}+x_{2}+x_{3}-x_{4} & =0 \\\\\nx_{2}+2 x_{3}+x_{4} & =0\n\\end{aligned}\n\\]\n\n\nFind the RREF of \\(A\\):\n\n\n\n\n\n\n\nPivots are in the first and second columns, so it follows that \\(x_{3}\\) and \\(x_{4}\\) are free, \\(x_{1}\\) and \\(x_{2}\\) are bound\n\n\\[\n\\begin{aligned}\n& x_{1}=x_{3}+2 x_{4} \\\\\n& x_{2}=-2 x_{3}-x_{4} .\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lectures/ch3lecture1.html#section-3",
    "href": "lectures/ch3lecture1.html#section-3",
    "title": "Ch3 Lecture 1",
    "section": "",
    "text": "\\[\n\\left[\\begin{array}{c}\nx_{1} \\\\\nx_{2} \\\\\nx_{3} \\\\\nx_{4}\n\\end{array}\\right]=\\left[\\begin{array}{c}\nx_{3}+2 x_{4} \\\\\n-2 x_{3}-x_{4} \\\\\nx_{3} \\\\\nx_{4}\n\\end{array}\\right]=x_{3}\\left[\\begin{array}{r}\n1 \\\\\n-2 \\\\\n1 \\\\\n0\n\\end{array}\\right]+x_{4}\\left[\\begin{array}{r}\n2 \\\\\n-1 \\\\\n0 \\\\\n1\n\\end{array}\\right] .\n\\]\n\nThe general solution to the homogeneous system is an arbitrary linear combination of these two vectors\n\n\n\\[\n\\mathcal{N}(A)=\\operatorname{span}\\left\\{\\left[\\begin{array}{r}\n1 \\\\\n-2 \\\\\n1 \\\\\n0\n\\end{array}\\right],\\left[\\begin{array}{r}\n2 \\\\\n-1 \\\\\n0 \\\\\n1\n\\end{array}\\right]\\right\\} \\subseteq \\mathbb{R}^{4}\n\\]\n\n\nThese are independent, so they form a basis for \\(\\mathcal{N}(A)\\)"
  },
  {
    "objectID": "lectures/ch3lecture1.html#using-the-null-space",
    "href": "lectures/ch3lecture1.html#using-the-null-space",
    "title": "Ch3 Lecture 1",
    "section": "Using the Null Space",
    "text": "Using the Null Space\nSuppose that a Markov chain has an stable transition matrix \\(A=\\left[\\begin{array}{ll}0.7 & 0.4 \\\\ 0.3 & 0.6\\end{array}\\right]\\), so that\n\\[\n\\mathbf{x}^{(k+1)}=A \\mathbf{x}^{(k)}\n\\]\n\nWhat is the steady-state vector \\(\\mathbf{x}\\)? (The limit as \\(k \\rightarrow \\infty\\) of \\(\\mathbf{x}^{(k)}\\))\n\n\nTake limit of both sides:\n\\[\n\\mathbf{x}=A \\mathbf{x}\n\\]\n\n\\[\n\\mathbf{0}=\\mathbf{x}-A \\mathbf{x}=I \\mathbf{x}-A \\mathbf{x}=(I-A) \\mathbf{x}\n\\]\nSo \\(\\mathbf{x}\\) is in the null space of \\(I-A\\).\n\n\n\n\\[\n\\mathbf{0}=\\mathbf{x}-A \\mathbf{x}=I \\mathbf{x}-A \\mathbf{x}=(I-A) \\mathbf{x}\n\\]\nSo \\(\\mathbf{x}\\) is in the null space of \\(I-A\\)."
  },
  {
    "objectID": "lectures/ch3lecture1.html#section-4",
    "href": "lectures/ch3lecture1.html#section-4",
    "title": "Ch3 Lecture 1",
    "section": "",
    "text": "\\[\nI-A=\\left[\\begin{array}{ll}\n1 & 0 \\\\\n0 & 1\n\\end{array}\\right]-\\left[\\begin{array}{ll}\n0.7 & 0.4 \\\\\n0.3 & 0.6\n\\end{array}\\right]=\\left[\\begin{array}{rr}\n0.3 & -0.4 \\\\\n-0.3 & 0.4\n\\end{array}\\right] \\text {. }\n\\]\n\n\\[\n\\left[\\begin{array}{rr}\n0.3 & -0.4 \\\\\n-0.3 & 0.4\n\\end{array}\\right] \\xrightarrow{E_{21}(1)}\\left[\\begin{array}{rr}\n1 & -4 / 3 \\\\\n0 & 0\n\\end{array}\\right]\n\\]\n\nnull space of \\(I-A\\) is spanned by the single vector \\((4 / 3,1)\\). In particular, any multiple of this vector qualifies as a possible limiting vector.\nIf we want a limiting vector whose entries are nonnegative and sum to 1 (which is required for states in a Markov chain), then the only choice is the vector resulting from dividing \\((4 / 3,1)\\) by the sum of its coordinates to obtain\n\nSolution set is \\(x_{1}=4 x_{2} / 3\\)\n\n\n\\[\n(3 / 7)(4 / 3,1)=(4 / 7,3 / 7) \\approx(0.57143,0.42857)\n\\]"
  },
  {
    "objectID": "lectures/ch3lecture1.html#using-the-null-space-to-find-a-basis",
    "href": "lectures/ch3lecture1.html#using-the-null-space-to-find-a-basis",
    "title": "Ch3 Lecture 1",
    "section": "Using the Null Space to find a basis",
    "text": "Using the Null Space to find a basis\nFind a basis for the column space of the matrix\n\\[\nA=\\left[\\begin{array}{rrrr}\n1 & 1 & 1 & -1 \\\\\n0 & 1 & 2 & 1\n\\end{array}\\right]\n\\]\n\nFrom before, we found the RREF and used this to find a general form of the solutions to \\(A \\mathbf{x}=\\mathbf{0}\\)\n\\(\\mathbf{x}=\\left(x_{1}, x_{2}, x_{3}, x_{4}\\right)=\\) \\(\\left(x_{3}+2 x_{4},-2 x_{3}-x_{4}, x_{3}, x_{4}\\right)\\)\n\n\nWrite \\(A=\\left[\\mathbf{a}_{1}, \\mathbf{a}_{2}, \\mathbf{a}_{3}, \\mathbf{a}_{4}\\right]\\)\n\n\n\\(\\mathbf{0}=x_{1} \\mathbf{a}_{1}+x_{2} \\mathbf{a}_{2}+x_{3} \\mathbf{a}_{3}+x_{4} \\mathbf{a}_{4}\\)\n\n\n\\(\\mathbf{0}=\\left(x_{3}+2 x_{4}\\right) \\mathbf{a}_{1}-\\left(2 x_{3}+x_{4}\\right) \\mathbf{a}_{2}+x_{3} \\mathbf{a}_{3}+x_{4} \\mathbf{a}_{4}\\)\n\n\nTake \\(x_{3}=1\\) and \\(x_{4}=0\\). Then \\(\\mathbf{0}=\\mathbf{a}_{1}-2 \\mathbf{a}_{2}+\\mathbf{a}_{3}\\)\n\n\nSo \\(\\mathbf{a}_{3}\\) is a linear combination of \\(\\mathbf{a}_{1}\\) and \\(\\mathbf{a}_{2}\\)\n\n\nSimilarly, take \\(x_{3}=0\\) and \\(x_{4}=1\\). Then \\(\\mathbf{0}=2 \\mathbf{a}_{1}-\\mathbf{a}_{2}+\\mathbf{a}_{4}\\) so \\(\\mathbf{a}_{4}\\) is a linear combination of \\(\\mathbf{a}_{1}\\) and \\(\\mathbf{a}_{2}\\)\n\n\nSo \\(\\mathbf{a}_{1}\\) and \\(\\mathbf{a}_{2}\\) are a basis for the column space of \\(A\\)\n\\(\\mathcal{C}(A)=\\operatorname{span}\\left\\{\\mathbf{a}_{1}, \\mathbf{a}_{2}\\right\\}=\\operatorname{span}\\left\\{\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right],\\left[\\begin{array}{l}1 \\\\ 1\\end{array}\\right]\\right\\}\\)"
  },
  {
    "objectID": "lectures/ch3lecture1.html#consistency-and-column-space-back-to-linear-systems",
    "href": "lectures/ch3lecture1.html#consistency-and-column-space-back-to-linear-systems",
    "title": "Ch3 Lecture 1",
    "section": "Consistency and Column Space: Back to Linear Systems",
    "text": "Consistency and Column Space: Back to Linear Systems\n\nThe linear system \\(A \\mathbf{x}=\\mathbf{b}\\) of \\(m\\) equations in \\(n\\) unknowns is consistent if and only if \\(\\mathbf{b} \\in \\mathcal{C}(A)\\).\n\n\n\\(A \\mathbf{x}\\) is a linear combination of the columns of \\(A\\) with the entries of \\(\\mathbf{x}\\) as scalar coefficients\nIf \\(A \\mathbf{x}=\\mathbf{b}\\) has a solution means some linear combination of columns of \\(A\\) adds up to \\(\\mathbf{b}\\)\n\\(\\mathbf{b} \\in \\mathcal{C}(A)\\)"
  },
  {
    "objectID": "lectures/ch3lecture1.html#how-to-tell-if-a-vector-is-in-a-spanning-space",
    "href": "lectures/ch3lecture1.html#how-to-tell-if-a-vector-is-in-a-spanning-space",
    "title": "Ch3 Lecture 1",
    "section": "How to tell if a vector is in a spanning space",
    "text": "How to tell if a vector is in a spanning space\nSuppose we have a space \\(V\\) spanned by \\(\\mathbf{v}_{1}=(1,1,3,3), \\mathbf{v}_{2}=(0,2,2,4)\\), and and \\(\\mathbf{v}_{3}=(1,0,2,1)\\)\n\nOne of these vectors is in \\(V\\). Which one?\n\\(\\mathbf{u}=(2,1,5,4)\\) and \\(\\mathbf{w}=(1,0,0,0)\\)\n\n\nDefine \\(A=\\left[\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\mathbf{v}_{3}\\right]\\)\nThen \\(\\mathbf{u} \\in \\mathcal{C}(A)\\) if and only if \\(A \\mathbf{x}=\\mathbf{u}\\) has a solution (is consistent)\n\n\nSolve both at once: \\[\n[A|\\mathbf{u}| \\mathbf{w}]=\\left[\\begin{array}{lllll}\n1 & 0 & 1 & 2 & 1 \\\\\n1 & 2 & 0 & 1 & 0 \\\\\n3 & 2 & 2 & 5 & 0 \\\\\n3 & 4 & 1 & 4 & 0\n\\end{array}\\right] \\text { with reduced row echelon form }\\left[\\begin{array}{rrrrr}\n1 & 0 & 1 & 2 & 0 \\\\\n0 & 1 & -\\frac{1}{2} & -\\frac{1}{2} & 0 \\\\\n0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 & 0\n\\end{array}\\right]\n\\]\n\nObserve that there is a pivot in the fifth column but not in the fourth column. This tells us that the system with augmented matrix \\([A \\mid \\mathbf{u}]\\) is consistent, but the system with augmented matrix \\([A \\mid \\mathbf{w}]\\) is not consistent. Therefore, \\(\\mathbf{u} \\in \\operatorname{span}\\left\\{\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\mathbf{v}_{3}\\right\\}\\), but \\(\\mathbf{w} \\notin \\operatorname{span}\\left\\{\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\mathbf{v}_{3}\\right\\}\\).\n\n\n\n\\(\\mathbf{u} \\in \\operatorname{span}\\left\\{\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\mathbf{v}_{3}\\right\\}\\),\nbut \\(\\mathbf{w} \\notin \\operatorname{span}\\left\\{\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\mathbf{v}_{3}\\right\\}\\)\n\n\n\\[\n\\mathbf{u}=\\left(2-c_{3}\\right) \\mathbf{v}_{1}+\\frac{1}{2}\\left(c_{3}-1\\right) \\mathbf{v}_{2}+c_{3} \\mathbf{v}_{3}\n\\]"
  },
  {
    "objectID": "lectures/ch3lecture1.html#form-of-general-solution-to-a-linear-system",
    "href": "lectures/ch3lecture1.html#form-of-general-solution-to-a-linear-system",
    "title": "Ch3 Lecture 1",
    "section": "Form of general solution to a linear system",
    "text": "Form of general solution to a linear system\n\n\nSuppose the system \\(A \\mathbf{x}=\\mathbf{b}\\) has a particular solution \\(\\mathbf{x}_{*}\\).\nThen the general solution \\(\\mathbf{x}\\) to this system can be described by the equation\n\n\n\\[\n\\mathbf{x}=\\mathbf{x}_{*}+\\mathbf{z}\n\\]\nwhere \\(\\mathbf{z}\\) runs over all elements of \\(\\mathcal{N}(A)\\).\n\n\n\nSolution set is the set of all translates of elements in the null space of \\(A\\) by some fixed vector.\n\n\nWhen n is 2 or 3 this says that the solution set is either a single point, a line or a plane—not necessarily through the origin!"
  },
  {
    "objectID": "lectures/ch3lecture1.html#example-1",
    "href": "lectures/ch3lecture1.html#example-1",
    "title": "Ch3 Lecture 1",
    "section": "Example",
    "text": "Example\nDescribe the solution sets to the system \\[\n\\begin{array}{r}\nx+2 y=3 \\\\\nx+y+t=3 .\n\\end{array}\n\\]\n\n\n\n\n\\[\n\\begin{aligned}\n& x=3-2 t \\\\\n& y=t \\\\\n& z=t .\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lectures/ch3lecture1.html#null-space-algorithm",
    "href": "lectures/ch3lecture1.html#null-space-algorithm",
    "title": "Ch3 Lecture 1",
    "section": "Null Space algorithm",
    "text": "Null Space algorithm\nGiven an \\(m \\times n\\) matrix \\(A\\).\n\nCompute the reduced row echelon form \\(R\\) of \\(A\\).\nUse \\(R\\) to find the general solution to the homogeneous system \\(A \\mathbf{x}=0\\).\n\n\n\nWrite the general solution \\(\\mathbf{x}=\\left(x_{1}, x_{2}, \\ldots, x_{n}\\right)\\) to the homogeneous system in the form\n\n\\[\n\\mathbf{x}=x_{i_{1}} \\mathbf{w}_{1}+x_{i_{2}} \\mathbf{w}_{2}+\\cdots+x_{i_{n-r}} \\mathbf{w}_{n-r}\n\\]\nwhere \\(x_{i_{1}}, x_{i_{2}}, \\ldots, x_{i_{n-r}}\\) are the free variables.\n\n\n\nList the vectors \\(\\mathbf{w}_{1}, \\mathbf{w}_{2}, \\ldots, \\mathbf{w}_{n-r}\\). These form a basis of \\(\\mathcal{N}(A)\\)."
  },
  {
    "objectID": "lectures/test.html#a-slide",
    "href": "lectures/test.html#a-slide",
    "title": "a test",
    "section": "A slide",
    "text": "A slide\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/ch4lecture1.html#linear-systems-revisted",
    "href": "lectures/ch4lecture1.html#linear-systems-revisted",
    "title": "Stat 24320",
    "section": "Linear systems revisted",
    "text": "Linear systems revisted"
  },
  {
    "objectID": "lectures/ch2_lecture5.html#block-multiplication-1",
    "href": "lectures/ch2_lecture5.html#block-multiplication-1",
    "title": "Ch2 Lecture 5",
    "section": "Block Multiplication",
    "text": "Block Multiplication\n\nSuppose we need to multiply these matrices…\n\\[\n\\left[\\begin{array}{llll}\n1 & 2 & 0 & 0 \\\\\n3 & 4 & 0 & 0 \\\\\n0 & 0 & 1 & 0\n\\end{array}\\right]\\left[\\begin{array}{llll}\n0 & 0 & 2 & 1 \\\\\n0 & 0 & 1 & 1 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right] .\n\\]\npause\n\nIt’s a pain, right?\nDraw out the blocks on the left and right matrices.\n\\[\nA=\\left[\\begin{array}{ll}\n1 & 2 \\\\\n3 & 4\n\\end{array}\\right], \\quad B=\\left[\\begin{array}{ll}\n1 & 0\n\\end{array}\\right], \\quad C=\\left[\\begin{array}{ll}\n2 & 1 \\\\\n-1 & 1\n\\end{array}\\right], \\quad I_{2}=\\left[\\begin{array}{ll}\n1 & 0 \\\\\n0 & 1\n\\end{array}\\right]\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture5.html#column-vectors-as-blocks",
    "href": "lectures/ch2_lecture5.html#column-vectors-as-blocks",
    "title": "Ch2 Lecture 5",
    "section": "Column Vectors as Blocks",
    "text": "Column Vectors as Blocks\n\\[\nA \\mathbf{x}=\\left[\\mathbf{a}_{1}, \\mathbf{a}_{2}, \\mathbf{a}_{3}\\right]\\left[\\begin{array}{c}\nx_{1} \\\\\nx_{2} \\\\\nx_{3}\n\\end{array}\\right]=\\mathbf{a}_{1} x_{1}+\\mathbf{a}_{2} x_{2}+\\mathbf{a}_{3} x_{3}\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture5.html#transpose-and-conjugate-transpose-1",
    "href": "lectures/ch2_lecture5.html#transpose-and-conjugate-transpose-1",
    "title": "Ch2 Lecture 5",
    "section": "Transpose and Conjugate Transpose",
    "text": "Transpose and Conjugate Transpose\n\n\n\nLet \\(A=\\left[a_{i j}\\right]\\) be an \\(m \\times n\\) matrix with (possibly) complex entries.\nThe transpose of \\(A\\) is the \\(n \\times m\\) matrix \\(A^{T}\\) obtained by interchanging the rows and columns of \\(A\\)\nThe conjugate of \\(A\\) is the matrix \\(\\bar{A}=\\left[\\overline{a_{i j}}\\right]\\)\nFinally, the conjugate (Hermitian) transpose of \\(A\\) is the matrix \\(A^{*}=\\bar{A}^{T}\\)."
  },
  {
    "objectID": "lectures/ch2_lecture5.html#example-1",
    "href": "lectures/ch2_lecture5.html#example-1",
    "title": "Ch2 Lecture 5",
    "section": "Example 1",
    "text": "Example 1\nFind the transpose and conjugate transpose of:\n\\[\n\\left[\\begin{array}{lll}1 & 0 & 2 \\\\ 0 & 1 & 1\\end{array}\\right]\n\\]\n\n\\[\n\\left[\\begin{array}{lll}\n1 & 0 & 2 \\\\\n0 & 1 & 1\n\\end{array}\\right]^{*}=\\left[\\begin{array}{lll}\n1 & 0 & 2 \\\\\n0 & 1 & 1\n\\end{array}\\right]^{T}=\\left[\\begin{array}{ll}\n1 & 0 \\\\\n0 & 1 \\\\\n2 & 1\n\\end{array}\\right]\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture5.html#example-2",
    "href": "lectures/ch2_lecture5.html#example-2",
    "title": "Ch2 Lecture 5",
    "section": "Example 2",
    "text": "Example 2\nFind the transpose and conjugate transpose of:\n\\[\n\\left[\\begin{array}{rr}1 & 1+\\mathrm{i} \\\\ 0 & 2 \\mathrm{i}\\end{array}\\right]\n\\]\n\n\\[\n\\left[\\begin{array}{rr}\n1 & 1+\\mathrm{i} \\\\\n0 & 2 \\mathrm{i}\n\\end{array}\\right]^{*}=\\left[\\begin{array}{rr}\n1 & 0 \\\\\n1-\\mathrm{i} & -2 \\mathrm{i}\n\\end{array}\\right], \\quad\\left[\\begin{array}{rr}\n1 & 1+\\mathrm{i} \\\\\n0 & 2 \\mathrm{i}\n\\end{array}\\right]^{T}=\\left[\\begin{array}{rr}\n1 & 0 \\\\\n1+\\mathrm{i} & 2 \\mathrm{i}\n\\end{array}\\right]\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture5.html#laws-of-matrix-transpose",
    "href": "lectures/ch2_lecture5.html#laws-of-matrix-transpose",
    "title": "Ch2 Lecture 5",
    "section": "Laws of Matrix Transpose",
    "text": "Laws of Matrix Transpose\nLet \\(A\\) and \\(B\\) be matrices of the appropriate sizes so that the following operations make sense, and \\(c\\) a scalar.\n\n\\((A+B)^{T}=A^{T}+B^{T}\\)\n\\((A B)^{T}=B^{T} A^{T}\\)\n\\((c A)^{T}=c A^{T}\\)\n\\(\\left(A^{T}\\right)^{T}=A\\)"
  },
  {
    "objectID": "lectures/ch2_lecture5.html#symmetric-and-hermetian-matrices",
    "href": "lectures/ch2_lecture5.html#symmetric-and-hermetian-matrices",
    "title": "Ch2 Lecture 5",
    "section": "Symmetric and Hermetian Matrices",
    "text": "Symmetric and Hermetian Matrices\n\nThe matrix \\(A\\) is said to be:\n\nsymmetric if \\(A^{T}=A\\)\nHermitian if \\(A^{*}=A\\)\n\n\n\n\nIs this matrix symmetric? Hermetian?\n\\(\\left[\\begin{array}{rr}1 & 1+\\mathrm{i} \\\\ 1-\\mathrm{i} & 2\\end{array}\\right]\\)\n\nIt’s Hermetian, but not symmetric.\n\\[\n\\left[\\begin{array}{rr}\n1 & 1+\\mathrm{i} \\\\\n1-\\mathrm{i} & 2\n\\end{array}\\right]^{*}=\\left[\\begin{array}{rr}\n1 & \\overline{1+\\mathrm{i}} \\\\\n\\overline{1-\\mathrm{i}} & 2\n\\end{array}\\right]^{T}=\\left[\\begin{array}{rr}\n1 & 1+\\mathrm{i} \\\\\n1-\\mathrm{i} & 2\n\\end{array}\\right]\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture5.html#inner-product",
    "href": "lectures/ch2_lecture5.html#inner-product",
    "title": "Ch2 Lecture 5",
    "section": "Inner product",
    "text": "Inner product\n\n\nLet \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) be column vectors of the same size, say \\(n \\times 1\\).\nThen the inner product of \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) is the scalar quantity \\(\\mathbf{u}^{T} \\mathbf{v}\\)\n\n\n\n\nFind the inner product of \\[\n\\mathbf{u}=\\left[\\begin{array}{r}\n2 \\\\\n-1 \\\\\n1\n\\end{array}\\right] \\text { and } \\mathbf{v}=\\left[\\begin{array}{l}\n3 \\\\\n4 \\\\\n1\n\\end{array}\\right]\n\\]\n\n\\[\n\\mathbf{u}^{T} \\mathbf{v}=[2,-1,1]\\left[\\begin{array}{l}\n3 \\\\\n4 \\\\\n1\n\\end{array}\\right]=2 \\cdot 3+(-1) 4+1 \\cdot 1=3\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture5.html#outer-product",
    "href": "lectures/ch2_lecture5.html#outer-product",
    "title": "Ch2 Lecture 5",
    "section": "Outer product",
    "text": "Outer product\n\n\nThe outer product of \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) is the \\(n \\times n\\) matrix \\(\\mathbf{u v}^{T}\\).\n\n\n\n\nFind the outer product of\n\\[\n\\mathbf{u}=\\left[\\begin{array}{r}\n2 \\\\\n-1 \\\\\n1\n\\end{array}\\right] \\text { and } \\mathbf{v}=\\left[\\begin{array}{l}\n3 \\\\\n4 \\\\\n1\n\\end{array}\\right]\n\\]\n\n\\[\n\\mathbf{u v}^{T}=\\left[\\begin{array}{r}\n2 \\\\\n-1 \\\\\n1\n\\end{array}\\right][3,4,1]=\\left[\\begin{array}{rrr}\n2 \\cdot 3 & 2 \\cdot 4 & 2 \\cdot 1 \\\\\n-1 \\cdot 3 & -1 \\cdot 4 & -1 \\cdot 1 \\\\\n1 \\cdot 3 & 1 \\cdot 4 & 1 \\cdot 1\n\\end{array}\\right]=\\left[\\begin{array}{rrr}\n6 & 8 & 2 \\\\\n-3 & -4 & -1 \\\\\n3 & 4 & 1\n\\end{array}\\right]\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture5.html#section",
    "href": "lectures/ch2_lecture5.html#section",
    "title": "Ch2 Lecture 5",
    "section": "",
    "text": "The determinant of a square \\(n \\times n\\) matrix \\(A=\\left[a_{i j}\\right]\\), \\(\\operatorname{det} A\\), is defined recursively:\nIf \\(n=1\\) then \\(\\operatorname{det} A=a_{11}\\);\n\notherwise,\n\nsuppose we have determinents for all square matrices of size less than \\(n\\)\nDefine \\(M_{i j}(A)\\) as the determinant of the \\((n-1) \\times(n-1)\\) matrix obtained from \\(A\\) by deleting the \\(i\\) th row and \\(j\\) th column of \\(A\\)\n\n\n\nthen\n\\[\n\\begin{aligned}\n\\operatorname{det} A & =\\sum_{k=1}^{n} a_{k 1}(-1)^{k+1} M_{k 1}(A) \\\\\n& =a_{11} M_{11}(A)-a_{21} M_{21}(A)+\\cdots+(-1)^{n+1} a_{n 1} M_{n 1}(A)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture5.html#laws-of-determinants",
    "href": "lectures/ch2_lecture5.html#laws-of-determinants",
    "title": "Ch2 Lecture 5",
    "section": "Laws of Determinants",
    "text": "Laws of Determinants\nDeterminant of an upper-triangular matrix: \\[\n\\begin{aligned}\n\\operatorname{det} A & =\\left|\\begin{array}{cccc}\na_{11} & a_{12} & \\cdots & a_{1 n} \\\\\n0 & a_{22} & \\cdots & a_{2 n} \\\\\n\\vdots & \\vdots & & \\vdots \\\\\n0 & 0 & \\cdots & a_{n n}\n\\end{array}\\right|=a_{11}\\left|\\begin{array}{cccc}\na_{22} & a_{23} & \\cdots & a_{2 n} \\\\\n0 & a_{33} & \\cdots & a_{3 n} \\\\\n\\vdots & \\vdots & & \\vdots \\\\\n0 & 0 & \\cdots & a_{n n}\n\\end{array}\\right| \\\\\n& =\\cdots=a_{11} \\cdot a_{22} \\cdots a_{n n} .\n\\end{aligned}\n\\]\n\nD1: If \\(A\\) is an upper triangular matrix, then the determinant of \\(A\\) is the product of all the diagonal elements of \\(A\\)."
  },
  {
    "objectID": "lectures/ch2_lecture5.html#more-laws-of-determinants",
    "href": "lectures/ch2_lecture5.html#more-laws-of-determinants",
    "title": "Ch2 Lecture 5",
    "section": "More Laws of Determinants",
    "text": "More Laws of Determinants\n\nD2: If \\(B\\) is obtained from \\(A\\) by multiplying one row of \\(A\\) by the scalar \\(c\\), then \\(\\operatorname{det} B=c \\cdot \\operatorname{det} A\\).\nD3: If \\(B\\) is obtained from \\(A\\) by interchanging two rows of \\(A\\), then \\(\\operatorname{det} B=\\) \\(-\\operatorname{det} A\\).\nD4: If \\(B\\) is obtained from \\(A\\) by adding a multiple of one row of \\(A\\) to another row of \\(A\\), then \\(\\operatorname{det} B=\\operatorname{det} A\\).\n\nNote: this means that the determinant of a matrix is unchanged by elementary row operations."
  },
  {
    "objectID": "lectures/ch2_lecture5.html#determinant-laws-in-terms-of-elementary-matrices",
    "href": "lectures/ch2_lecture5.html#determinant-laws-in-terms-of-elementary-matrices",
    "title": "Ch2 Lecture 5",
    "section": "Determinant Laws in terms of Elementary Matrices",
    "text": "Determinant Laws in terms of Elementary Matrices\n\nD2: \\(\\operatorname{det}\\left(E_{i}(c) A\\right)=c \\cdot \\operatorname{det} A\\) (remember that for \\(E_{i}(c)\\) to be an elementary matrix, \\(c \\neq 0\\) ).\nD3: \\(\\operatorname{det}\\left(E_{i j} A\\right)=-\\operatorname{det} A\\).\nD4: \\(\\operatorname{det}\\left(E_{i j}(s) A\\right)=\\operatorname{det} A\\)."
  },
  {
    "objectID": "lectures/ch2_lecture5.html#determinant-of-row-echelon-form",
    "href": "lectures/ch2_lecture5.html#determinant-of-row-echelon-form",
    "title": "Ch2 Lecture 5",
    "section": "Determinant of Row Echelon Form",
    "text": "Determinant of Row Echelon Form\nLet R be the reduced row echelon form of A, obtained through multiplication by elementary matrices:\n\\[\nR=E_{1} E_{2} \\cdots E_{k} A .\n\\]\n\nDeterminant of both sides:\n\\[\n\\operatorname{det} R=\\operatorname{det}\\left(E_{1} E_{2} \\cdots E_{k} A\\right)= \\pm(\\text { nonzero constant }) \\cdot \\operatorname{det} A \\text {. }\n\\]\n\n\nTherefore, \\(\\operatorname{det} A=0\\) precisely when \\(\\operatorname{det} R=0\\).\n\n\n\n\\(R\\) is upper triangular, so \\(\\operatorname{det} R\\) is the product of the diagonal entries of \\(R\\).\nIf \\(\\operatorname{rank} A&lt;n\\), then there will be zeros in some of the diagonal entries, so \\(\\operatorname{det} R=0\\).\nIf \\(\\operatorname{rank} A=n\\), the diagonal entries are all 1, so \\(\\operatorname{det} R=1\\).\n\nA square matrix with rank \\(n\\) is invertible\n\n\n\n\nTherefore,\nD5: The matrix \\(A\\) is invertible if and only if \\(\\operatorname{det} A \\neq 0\\)."
  },
  {
    "objectID": "lectures/ch2_lecture5.html#two-more-determinant-laws",
    "href": "lectures/ch2_lecture5.html#two-more-determinant-laws",
    "title": "Ch2 Lecture 5",
    "section": "Two more Determinant Laws",
    "text": "Two more Determinant Laws\nD6: Given matrices \\(A, B\\) of the same size,\n\\[\n\\operatorname{det} A B=\\operatorname{det} A \\operatorname{det} B \\text {. }\n\\]\n\n(but beware, \\(\\operatorname{det} A+\\operatorname{det} B \\neq \\operatorname{det}(A+B)\\))\nD7: For all square matrices \\(A\\), \\(\\operatorname{det} A^{T}=\\operatorname{det} A\\)"
  },
  {
    "objectID": "lectures/ch2_lecture5.html#easier-way-to-calculate-determinant",
    "href": "lectures/ch2_lecture5.html#easier-way-to-calculate-determinant",
    "title": "Ch2 Lecture 5",
    "section": "Easier way to calculate determinant",
    "text": "Easier way to calculate determinant\nJust use the det function in Python! (That was copilot’s autocomplete…)\n\nUse elementary row operations to get the matrix into upper triangular form, then multiply the diagonal entries."
  },
  {
    "objectID": "lectures/ch2_lecture5.html#an-inverse-formula",
    "href": "lectures/ch2_lecture5.html#an-inverse-formula",
    "title": "Ch2 Lecture 5",
    "section": "An Inverse Formula",
    "text": "An Inverse Formula\n\nLet \\(A=\\left[a_{i j}\\right]\\) be an \\(n \\times n\\) matrix. We have already seen that we can expand the determinant of \\(A\\) down any column of \\(A\\) (see the discussion following Example 2.59). These expansions lead to cofactor formulas for each column number \\(j\\) :\n\\[\n\\operatorname{det} A=\\sum_{k=1}^{n} a_{k j} A_{k j}=\\sum_{k=1}^{n} A_{k j} a_{k j}\n\\]\nThis formula resembles a matrix multiplication formula. Consider the slightly altered sum\n\\[\n\\sum_{k=1}^{n} A_{k i} a_{k j}=A_{1 i} a_{1 j}+A_{2 i} a_{2 j}+\\cdots+A_{n i} a_{n j}\n\\]\nThis is exactly what we would get if we replaced the \\(i\\) th column of the matrix \\(A\\) by its \\(j\\) th column and then computed the determinant of the resulting matrix by expansion down the \\(i\\) th column.\nBut such a matrix has two equal columns and therefore has a zero determinant. So this sum must be 0 if \\(i \\neq j\\). We can combine these two sums by means of the Kronecker delta.\n\\[\n\\begin{equation*}\n\\sum_{k=1}^{n} A_{k i} a_{k j}=\\delta_{i j} \\operatorname{det} A \\tag{2.9}\n\\end{equation*}\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture5.html#adjoint-minor-cofactor-matrices",
    "href": "lectures/ch2_lecture5.html#adjoint-minor-cofactor-matrices",
    "title": "Ch2 Lecture 5",
    "section": "Adjoint, Minor, Cofactor Matrices",
    "text": "Adjoint, Minor, Cofactor Matrices\n\n\n\\(M_{i j}(A)\\), the \\((i,j)\\) th minor of \\(A\\), is the determinant of the \\((n-1) \\times (n-1)\\) matrix obtained by deleting the \\(i\\)th row and \\(j\\)th column of \\(A\\).\n\\(A_{i j}=(-1)^{i+j} M_{i j}(A)\\) is the \\((i,j)\\)t h cofactor of \\(A\\).\nMatrix of minors \\(M(A)\\) is the matrix whose \\((i,j)\\) th entry is the minor \\(M_{i j}(A)\\).\nCofactor matrix \\(A_{\\text {cof }}\\) is the matrix whose \\((i,j)\\) th entry is the cofactor \\(A_{i j}\\).\nAdjoint of \\(A\\) is the transpose of the cofactor matrix, \\(A^{*}=A_{\\text {cof }}^{T}\\)."
  },
  {
    "objectID": "lectures/ch2_lecture5.html#example",
    "href": "lectures/ch2_lecture5.html#example",
    "title": "Ch2 Lecture 5",
    "section": "Example",
    "text": "Example\n\\(A=\\left[\\begin{array}{rrr}1 & 2 & 0 \\\\ 0 & 0 & -1 \\\\ 0 & 2 & 1\\end{array}\\right]\\)\n\nMatrix of minors:\n\n\n\nCofactor matrix and adjoint:\nOverlay with the checkerboard \\(\\left[\\begin{array}{l}+-+ \\\\ -+- \\\\ +-+\\end{array}\\right]\\). Take the transpose, to get\n\\[\n\\operatorname{adj} A=\\left[\\begin{array}{rrr}\n2 & -2 & -2 \\\\\n0 & 1 & 1 \\\\\n0 & -2 & 0\n\\end{array}\\right]\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture5.html#back-to-the-inverse-formula",
    "href": "lectures/ch2_lecture5.html#back-to-the-inverse-formula",
    "title": "Ch2 Lecture 5",
    "section": "Back to the Inverse Formula",
    "text": "Back to the Inverse Formula\n\\[\n\\begin{equation*}\n\\sum_{k=1}^{n} A_{k i} a_{k j}=\\delta_{i j} \\operatorname{det} A\n\\end{equation*}\n\\]\n\nWe recognize the left-hand side as the dot product of the \\(i\\) th row of the adjoint of \\(A\\) with the \\(j\\) th column of \\(A\\).\nSo then we have \\(\\left(A^{*} A\\right)_{i j}=\\delta_{i j} \\operatorname{det} A\\).\nIn matrix form, this is \\(A^{*} A=\\operatorname{det} A I_{n}\\).\nWe can divide both sides by \\(\\operatorname{det} A\\) (if the matrix is invertible, \\(\\operatorname{det} A \\neq 0\\)) to get \\(\\frac{A^{*} A}{\\operatorname{det} A}=I_{n}\\).\nThis gives us a formula for the inverse of \\(A\\):\n\\[\nA^{-1}=\\frac{1}{\\operatorname{det} A} A^{*}\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture5.html#checking-with-our-example",
    "href": "lectures/ch2_lecture5.html#checking-with-our-example",
    "title": "Ch2 Lecture 5",
    "section": "Checking, with our example",
    "text": "Checking, with our example\nRemeber, we had \\(A=\\left[\\begin{array}{rrr}1 & 2 & 0 \\\\ 0 & 0 & -1 \\\\ 0 & 2 & 1\\end{array}\\right]\\)\n\nWe had computed the adjoint as \\(A^{*}=\\left[\\begin{array}{rrr}2 & -2 & -2 \\\\ 0 & 1 & 1 \\\\ 0 & -2 & 0\\end{array}\\right]\\)\n\n\nCheck:\n\\[\nA \\operatorname{adj} A=\\left[\\begin{array}{rrr}\n1 & 2 & 0 \\\\\n0 & 0 & -1 \\\\\n0 & 2 & 1\n\\end{array}\\right]\\left[\\begin{array}{rrr}\n2 & -2 & -2 \\\\\n0 & 1 & 1 \\\\\n0 & -2 & 0\n\\end{array}\\right]=\\left[\\begin{array}{lll}\n2 & 0 & 0 \\\\\n0 & 2 & 0 \\\\\n0 & 0 & 2\n\\end{array}\\right]=(\\operatorname{det} A) I_{3} .\n\\]\n\n\nSo, \\(A^{-1}=\\frac{1}{\\operatorname{det} A} A^{*}\\)."
  },
  {
    "objectID": "lectures/ch2_lecture5.html#cramers-rule",
    "href": "lectures/ch2_lecture5.html#cramers-rule",
    "title": "Ch2 Lecture 5",
    "section": "Cramer’s Rule",
    "text": "Cramer’s Rule\nExplicit formula for solving linear systems with a nonsingular coefficient matrix.\n\nSolve \\(A \\mathbf{x}=\\mathbf{b}\\)\n\n\nMultiply both sides by \\(A^{-1}\\): \\(\\mathbf{x}=A^{-1} \\mathbf{b}\\)\n\n\nUse the formula for \\(A^{-1}\\): \\(\\mathbf{x}=\\frac{1}{\\operatorname{det} A} A^{*} \\mathbf{b}\\)\n\n\nThe \\(i\\) th component of \\(\\mathbf{x}\\) is\n\\[\nx_{i}=\\frac{1}{\\operatorname{det} A} \\sum_{j=1}^{n} A_{j i} b_{j}\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture5.html#interpretation-of-cramers-rule",
    "href": "lectures/ch2_lecture5.html#interpretation-of-cramers-rule",
    "title": "Ch2 Lecture 5",
    "section": "Interpretation of Cramer’s Rule",
    "text": "Interpretation of Cramer’s Rule\n\\[\nx_{i}=\\frac{1}{\\operatorname{det} A} \\sum_{j=1}^{n} A_{j i} b_{j}\n\\]\n\nThe summation term is exactly what we would obtain if we started with the determinant of the matrix \\(B_{i}\\) obtained from \\(A\\) by replacing the \\(i\\) th column of \\(A\\) by \\(\\mathbf{b}\\) and then expanding the determinant down the \\(i\\) th column. Therefore, we have arrived at the following rule:\n\n** pause **\n\n\n\nLet \\(A\\) be an invertible \\(n \\times n\\) matrix and \\(\\mathbf{b}\\) an \\(n \\times 1\\) column vector.\nDenote by \\(B_{i}\\) the matrix obtained from \\(A\\) by replacing the \\(i\\) th column of \\(A\\) by \\(\\mathbf{b}\\).\nThen the linear system \\(A \\mathbf{x}=\\mathbf{b}\\) has unique solution \\(\\mathbf{x}=\\left(x_{1}, x_{2}, \\ldots, x_{n}\\right)\\), where\n\\[\nx_{i}=\\frac{\\operatorname{det} B_{i}}{\\operatorname{det} A}, \\quad i=1,2, \\ldots, n\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture5.html#summary-of-laws-of-determinants",
    "href": "lectures/ch2_lecture5.html#summary-of-laws-of-determinants",
    "title": "Ch2 Lecture 5",
    "section": "Summary of Laws of Determinants",
    "text": "Summary of Laws of Determinants\nLet \\(A, B\\) be \\(n \\times n\\) matrices.\n\nD1: If \\(A\\) is upper triangular, \\(\\operatorname{det} A\\) is the product of all the diagonal elements of \\(A\\).\nD2: \\(\\operatorname{det}\\left(E_{i}(c) A\\right)=c \\cdot \\operatorname{det} A\\).\nD3: \\(\\operatorname{det}\\left(E_{i j} A\\right)=-\\operatorname{det} A\\).\nD4: \\(\\operatorname{det}\\left(E_{i j}(s) A\\right)=\\operatorname{det} A\\).\nD5: The matrix \\(A\\) is invertible if and only if \\(\\operatorname{det} A \\neq 0\\).\nD6: \\(\\operatorname{det} A B=\\operatorname{det} A \\operatorname{det} B\\).\nD7: \\(\\operatorname{det} A^{T}=\\operatorname{det} A\\).\nD8: \\(A \\operatorname{adj} A=(\\operatorname{adj} A) A=(\\operatorname{det} A) I\\).\nD9: If \\(\\operatorname{det} A \\neq 0\\), then \\(A^{-1}=\\frac{1}{\\operatorname{det} A} \\operatorname{adj} A\\)."
  },
  {
    "objectID": "lectures/ch2_lecture5.html#quadratic-forms",
    "href": "lectures/ch2_lecture5.html#quadratic-forms",
    "title": "Ch2 Lecture 5",
    "section": "Quadratic Forms",
    "text": "Quadratic Forms\nA quadratic form is a homogeneous polynomial of degree 2 in \\(n\\) variables. For example,\n\n\\[\nQ(x, y, z)=x^{2}+2 y^{2}+z^{2}+2 x y+y z+3 x z .\n\\]\n\n\nWe can express this in matrix form!\n\\[\n\\begin{aligned}\nx(x+2 y+3 z)+y(2 y+z)+z^{2} & =\\left[\\begin{array}{lll}\nx & y & z\n\\end{array}\\right]\\left[\\begin{array}{c}\nx+2 y+3 z \\\\\n2 y+z \\\\\nz\n\\end{array}\\right]\n\\end{aligned}\n\\]\n\n\n\\[\n\\begin{aligned}\n=\\left[\\begin{array}{lll}\nx & y & z\n\\end{array}\\right]\\left[\\begin{array}{lll}\n1 & 2 & 3 \\\\\n0 & 2 & 1 \\\\\n0 & 0 & 1\n\\end{array}\\right]\\left[\\begin{array}{c}\nx \\\\\ny \\\\\nz\n\\end{array}\\right]=\\mathbf{x}^{T} A \\mathbf{x},\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture3.html#difference-equations-1",
    "href": "lectures/ch2_lecture3.html#difference-equations-1",
    "title": "Ch2 Lecture 3",
    "section": "Difference Equations",
    "text": "Difference Equations"
  },
  {
    "objectID": "lectures/ch2_lecture3.html#digital-filters",
    "href": "lectures/ch2_lecture3.html#digital-filters",
    "title": "Ch2 Lecture 3",
    "section": "Digital Filters",
    "text": "Digital Filters\n\nAnother use for difference equations:\n\\[\n\\begin{equation*}\ny_{k}=a_{0} x_{k}+a_{1} x_{k-1}+\\cdots+a_{m} x_{k-m}, \\quad k=m, m+1, m+2, \\ldots,\n\\end{equation*}\n\\]\n\nBefore, in the case of temperature in the metal rod, we were trying to find values of \\(x\\) that would satisfy the equation. Here, different context: imagine the \\(x\\)’s are known inputs, perhaps discrete samples of a continuous variable, and \\(y\\) are outputs that we want to calculate, such as a filter.\n\n\n\\(x\\) is a continuous variable – perhaps sound in time domain, or parts of an image in space.\n\\(y\\) is some filtered version of \\(x\\)."
  },
  {
    "objectID": "lectures/ch2_lecture3.html#example-cleaning-up-a-noisy-signal",
    "href": "lectures/ch2_lecture3.html#example-cleaning-up-a-noisy-signal",
    "title": "Ch2 Lecture 3",
    "section": "Example: Cleaning up a noisy signal",
    "text": "Example: Cleaning up a noisy signal\nTrue signal: \\(f(t)=\\cos (\\pi t),-1 \\leq t \\leq 1\\)\nSignal plus noise: \\(g(t)=\\cos (\\pi t)+\\) \\(\\frac{1}{5} \\sin (24 \\pi t)+\\frac{1}{4} \\cos (30 \\pi t)\\)\n\n\n\n\n\n\n\n\n\n\nSuppose the exact signal we want is the line in black, and we’d like to measure it. But all we have is the noisy set of measurements in red.\nNotice the low-frequency part of the measurement is the signal, and the high-frequency part is the noise.\n\nHow can we use this fact to build a good filter?"
  },
  {
    "objectID": "lectures/ch2_lecture3.html#filtered-data",
    "href": "lectures/ch2_lecture3.html#filtered-data",
    "title": "Ch2 Lecture 3",
    "section": "Filtered data",
    "text": "Filtered data\nIdea: we can sample nearby points and take a weighted average of them.\n\\[\ny_{k}=\\frac{1}{4} x_{k+1}+\\frac{1}{2} x_{k}+\\frac{1}{4} x_{k-1}, \\quad k=1,2, \\ldots, 63\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nThis captured the low-frequency part of the signal, and filtered out the high-frequency noise. That’s just what we wanted! This is called a low-pass filter."
  },
  {
    "objectID": "lectures/ch2_lecture3.html#section",
    "href": "lectures/ch2_lecture3.html#section",
    "title": "Ch2 Lecture 3",
    "section": "",
    "text": "We can zoom in on a few points to see the effect of the filter.\n\n\n\n\n\n\n\n\n\n\nWhat if we subtract the values at \\(x_{k+1}\\) and \\(x_{k-1}\\) instead of adding them?"
  },
  {
    "objectID": "lectures/ch2_lecture3.html#now-you-try",
    "href": "lectures/ch2_lecture3.html#now-you-try",
    "title": "Ch2 Lecture 3",
    "section": "Now you try",
    "text": "Now you try\nSee if you can make a figure which will just find the noisyness…\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/ch2_lecture3.html#a-different-filter",
    "href": "lectures/ch2_lecture3.html#a-different-filter",
    "title": "Ch2 Lecture 3",
    "section": "A different filter",
    "text": "A different filter\nWhat if we subtract the values at \\(x_{k+1}\\) and \\(x_{k-1}\\) instead of adding them?\n\\[\ny_{k}=-\\frac{1}{4} x_{k+1}+\\frac{1}{2} x_{k}-\\frac{1}{4} x_{k-1}, \\quad k=1,2, \\ldots, 63\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nThis is a high-pass filter. It captures the high-frequency noise, but filters out the low-frequency signal."
  },
  {
    "objectID": "lectures/ch2_lecture3.html#definition-of-inverse-matrix",
    "href": "lectures/ch2_lecture3.html#definition-of-inverse-matrix",
    "title": "Ch2 Lecture 3",
    "section": "Definition of inverse matrix",
    "text": "Definition of inverse matrix\nLet \\(A\\) be a square matrix.\nInverse for \\(A\\) is a square matrix \\(B\\) of the same size as \\(A\\)\n\nsuch that \\(A B=I=B A\\).\n\nIf such a \\(B\\) exists, then the matrix \\(A\\) is said to be invertible.\nAlso called “singular” (non-invertible), or “nonsingular” (invertible)"
  },
  {
    "objectID": "lectures/ch2_lecture3.html#conditions-for-invertibility",
    "href": "lectures/ch2_lecture3.html#conditions-for-invertibility",
    "title": "Ch2 Lecture 3",
    "section": "Conditions for invertibility",
    "text": "Conditions for invertibility\nConditions for Invertibility The following are equivalent conditions on the square \\(n \\times n\\) matrix \\(A\\) :\n\nThe matrix \\(A\\) is invertible.\nThere is a square matrix \\(B\\) such that \\(B A=I\\).\nThe linear system \\(A \\mathbf{x}=\\mathbf{b}\\) has a unique solution for every right-hand-side vector \\(\\mathbf{b}\\).\nThe linear system \\(A \\mathbf{x}=\\mathbf{b}\\) has a unique solution for some right-hand-side vector \\(\\mathbf{b}\\).\nThe linear system \\(A \\mathbf{x}=0\\) has only the trivial solution.\n\\(\\operatorname{rank} A=n\\).\nThe reduced row echelon form of \\(A\\) is \\(I_{n}\\).\nThe matrix \\(A\\) is a product of elementary matrices.\nThere is a square matrix \\(B\\) such that \\(A B=I\\).\n\n\nWe can prove this by starting with 1, which implies 2, etc, and then 9 implies 1…"
  },
  {
    "objectID": "lectures/ch2_lecture3.html#elementary-matrices",
    "href": "lectures/ch2_lecture3.html#elementary-matrices",
    "title": "Ch2 Lecture 3",
    "section": "Elementary matrices",
    "text": "Elementary matrices\n\nEach of the operations in row reduction can be represented by a matrix \\(E\\).\n\n\nRemember: - \\(E_{i j}\\) : The elementary operation of switching the ith and jth rows of the matrix. - \\(E_{i}(c)\\) : The elementary operation of multiplying the ith row by the nonzero constant \\(c\\). - \\(E_{i j}(d)\\) : The elementary operation of adding \\(d\\) times the jth row to the ith row.\n\n\nFind an elementary matrix of size \\(n\\) is by performing the corresponding elementary row operation on the identity matrix \\(I_{n}\\).\n\n\nExample: Find the elementary matrix for \\(E_{13}(-4)\\)\n\n\nAdd -4 times the 3rd row of \\(I_{3}\\) to its first row…\n\n\n\\[\nE_{13}(-4)=\\left[\\begin{array}{rrr}\n1 & 0 & -4 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{array}\\right]\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture3.html#section-1",
    "href": "lectures/ch2_lecture3.html#section-1",
    "title": "Ch2 Lecture 3",
    "section": "",
    "text": "Recall an example from the first week:\nSolve the simple system\n\\[\n\\begin{gather*}\n2 x-y=1 \\\\\n4 x+4 y=20 . \\tag{1.5}\n\\end{gather*}\n\\]\n\n\\[\n\\begin{aligned}\n& {\\left[\\begin{array}{rrr}\n2 & -1 & 1 \\\\\n4 & 4 & 20\n\\end{array}\\right] \\overrightarrow{E_{12}}\\left[\\begin{array}{rrr}\n4 & 4 & 20 \\\\\n2 & -1 & 1\n\\end{array}\\right] \\overrightarrow{E_{1}(1 / 4)}\\left[\\begin{array}{rrr}\n1 & 1 & 5 \\\\\n2 & -1 & 1\n\\end{array}\\right]} \\\\\n& \\overrightarrow{E_{21}(-2)}\\left[\\begin{array}{rrr}\n1 & 1 & 5 \\\\\n0 & -3 & -9\n\\end{array}\\right] \\overrightarrow{E_{2}(-1 / 3)}\\left[\\begin{array}{lll}\n1 & 1 & 5 \\\\\n0 & 1 & 3\n\\end{array}\\right] \\overrightarrow{E_{12}(-1)}\\left[\\begin{array}{lll}\n1 & 0 & 2 \\\\\n0 & 1 & 3\n\\end{array}\\right] .\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture3.html#section-2",
    "href": "lectures/ch2_lecture3.html#section-2",
    "title": "Ch2 Lecture 3",
    "section": "",
    "text": "Rewrite this using matrix multiplication: \\[\n\\left[\\begin{array}{lll}\n1 & 0 & 2 \\\\\n0 & 1 & 3\n\\end{array}\\right]=E_{12}(-1) E_{2}(-1 / 3) E_{21}(-2) E_{1}(1 / 4) E_{12}\\left[\\begin{array}{rrr}\n2 & -1 & 1 \\\\\n4 & 4 & 20\n\\end{array}\\right] \\text {. }\n\\]\n\nWe are rewriting the row operations as a product of matrices. Notice too that the reduced row echelon form is the identity matrix."
  },
  {
    "objectID": "lectures/ch2_lecture3.html#section-3",
    "href": "lectures/ch2_lecture3.html#section-3",
    "title": "Ch2 Lecture 3",
    "section": "",
    "text": "Remove the last columns in each matrix above. (They were the “augmented” part of the original problem.) All the operations still work:\n\\[\n\\left[\\begin{array}{ll}\n1 & 0 \\\\\n0 & 1\n\\end{array}\\right]=E_{12}(-1) E_{2}(-1 / 3) E_{21}(-2) E_{1}(1 / 4) E_{12}\\left[\\begin{array}{rr}\n2 & -1 \\\\\n4 & 4\n\\end{array}\\right] \\text {. }\n\\]\n\nAha! We now have a formula for the inverse of the original matrix:\n\\[\nA^{-1}=E_{12}(-1) E_{2}(-1 / 3) E_{21}(-2) E_{1}(1 / 4) E_{12}\n\\]\n\nHow can we generalize this? How can we keep track of what we are doing?"
  },
  {
    "objectID": "lectures/ch2_lecture3.html#superaugmented-matrix",
    "href": "lectures/ch2_lecture3.html#superaugmented-matrix",
    "title": "Ch2 Lecture 3",
    "section": "Superaugmented matrix",
    "text": "Superaugmented matrix\n\nForm the superaugmented matrix \\([A \\mid I]\\).\nIf we perform the elementary operation \\(E\\) on the superaugmented matrix, we get the matrix \\(E\\) in the augmented part:\n\n\\[\nE[A \\mid I]=[E A \\mid E I]=[E A \\mid E]\n\\]\n\nThis can help us keep track of our operations as we do row reduction\nThe augmented part is just the product of the elementary matrices that we have used so far.\nNow continue applying elementary row operations until the part of the matrix originally occupied by \\(A\\) is reduced to the reduced row echelon form of \\(A\\). . . . \\[\n[A \\mid I] \\overrightarrow{E_{1}, E_{2}, \\ldots, E_{k}}[I \\mid B]\n\\]\n\\(B=E_{k} E_{k-1} \\cdots E_{1}\\) is the product of the various elementary matrices we used."
  },
  {
    "objectID": "lectures/ch2_lecture3.html#inverse-algorithm",
    "href": "lectures/ch2_lecture3.html#inverse-algorithm",
    "title": "Ch2 Lecture 3",
    "section": "Inverse Algorithm",
    "text": "Inverse Algorithm\nGiven an \\(n \\times n\\) matrix \\(A\\), to compute \\(A^{-1}\\) :\n\nForm the superaugmented matrix \\(\\widetilde{A}=\\left[A \\mid I_{n}\\right]\\).\nReduce the first \\(n\\) columns of \\(\\tilde{A}\\) to reduced row echelon form by performing elementary operations on the matrix \\(\\widetilde{A}\\) resulting in the matrix \\([R \\mid B]\\).\nIf \\(R=I_{n}\\) then set \\(A^{-1}=B\\); otherwise, \\(A\\) is singular and \\(A^{-1}\\) does not exist."
  },
  {
    "objectID": "lectures/ch2_lecture3.html#x2-matrices",
    "href": "lectures/ch2_lecture3.html#x2-matrices",
    "title": "Ch2 Lecture 3",
    "section": "2x2 matrices",
    "text": "2x2 matrices\nSuppose we have the 2x2 matrix\n\\[\nA=\\left[\\begin{array}{ll}\na & b \\\\\nc & d\n\\end{array}\\right]\n\\]\n\nDo row reduction on the superaugmented matrix:\n\\[\n\\left[\\begin{array}{ll|ll}\na & b & 1 & 0 \\\\\nc & d & 0 & 1\n\\end{array}\\right]\n\\]\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/ch2_lecture3.html#section-4",
    "href": "lectures/ch2_lecture3.html#section-4",
    "title": "Ch2 Lecture 3",
    "section": "",
    "text": "\\[\nA^{-1}=\\frac{1}{D}\\left[\\begin{array}{rr}\nd & -b \\\\\n-c & a\n\\end{array}\\right]\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture3.html#back-to-pagerank",
    "href": "lectures/ch2_lecture3.html#back-to-pagerank",
    "title": "Ch2 Lecture 3",
    "section": "Back to PageRank",
    "text": "Back to PageRank\n\nDo page ranking as in the first week:\n\nfor page \\(j\\) let \\(n_{j}\\) be its total number of outgoing links on that page.\nThen the score for vertex \\(i\\) is the sum of the scores of all vertices \\(j\\) that link to \\(i\\), divided by the total number of outgoing links on page \\(j\\). . . . \\[\n\\begin{equation*}\nx_{i}=\\sum_{x_{j} \\in L_{i}} \\frac{x_{j}}{n_{j}} . \\tag{1.4}\n\\end{equation*}\n\\]\n\n\n\\[\n\\begin{aligned}\n& x_{1}=\\frac{x_{3}}{3} \\\\\n& x_{2}=\\frac{x_{1}}{2}+\\frac{x_{3}}{3} \\\\\n& x_{3}=\\frac{x_{1}}{2}+\\frac{x_{2}}{1} \\\\\n& x_{4}=\\frac{x_{3}}{3} \\\\\n& x_{5}=\\frac{x_{6}}{1} \\\\\n& x_{6}=\\frac{x_{5}}{1}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture3.html#pagerank-as-a-matrix-equation",
    "href": "lectures/ch2_lecture3.html#pagerank-as-a-matrix-equation",
    "title": "Ch2 Lecture 3",
    "section": "PageRank as a matrix equation",
    "text": "PageRank as a matrix equation\nDefine the matrix \\(Q\\) and vector \\(\\mathbf{x}\\) by\n\\[\nQ=\\left[\\begin{array}{llllll}\n0 & 0 & \\frac{1}{3} & 0 & 0 & 0 \\\\\n\\frac{1}{2} & 0 & \\frac{1}{3} & 0 & 0 & 0 \\\\\n\\frac{1}{2} & 1 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & \\frac{1}{3} & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 & 1 & 0\n\\end{array}\\right], \\mathbf{x}=\\left(x_{1}, x_{2}, x_{3}, x_{4}, x_{5}\\right)\n\\]\n\nThen the equation \\(x=Q x\\) is equivalent to the system of equations above.\n\n\n\\(\\mathbf{x}\\) is a stationary vector for the transition matrix \\(Q\\)."
  },
  {
    "objectID": "lectures/ch2_lecture3.html#connection-between-adjacency-matrix-and-transition-matrix",
    "href": "lectures/ch2_lecture3.html#connection-between-adjacency-matrix-and-transition-matrix",
    "title": "Ch2 Lecture 3",
    "section": "Connection between adjacency matrix and transition matrix",
    "text": "Connection between adjacency matrix and transition matrix\n\n\\(A\\): adjacency matrix of a graph or digraph\n\\(D\\): be a diagonal matrix whose \\(i\\) th entry is either:\n\nthe inverse of the sum of all entries in the \\(i\\) th row of \\(\\mathrm{A}\\), or\nzero if if this sum is zero.\n\nThen \\(Q=A^{T} D\\) is the transition matrix for the page ranking of this graph.\n\n. . . Check:\n\\[\nA=\\left[\\begin{array}{llllll}\n0 & 1 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & 0 \\\\\n1 & 1 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 & 1 & 0\n\\end{array}\\right]\n\\]\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/ch2_lecture3.html#pagerank-as-a-markov-chain",
    "href": "lectures/ch2_lecture3.html#pagerank-as-a-markov-chain",
    "title": "Ch2 Lecture 3",
    "section": "PageRank as a Markov chain",
    "text": "PageRank as a Markov chain\nThink of web surfing as a random process\n\neach page is a state\n\nprobabilities of moving from one state to another given by a stochastic transition matrix P.\n\nequal probability of moving to any of the outgoing links of a page\n\n\n\\[\nP\\stackrel{?}{=}\\left[\\begin{array}{llllll}\n0 & 0 & \\frac{1}{3} & 0 & 0 & 0 \\\\\n\\frac{1}{2} & 0 & \\frac{1}{3} & 0 & 0 & 0 \\\\\n\\frac{1}{2} & 1 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & \\frac{1}{3} & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 & 1 & 0\n\\end{array}\\right]\n\\]\nPause: Is \\(P\\) a stochastic matrix?\n\nIs P a stochastic matrix? What’s the problem? (One of the columns doesn’t sum to 1.)"
  },
  {
    "objectID": "lectures/ch2_lecture3.html#correction-vector",
    "href": "lectures/ch2_lecture3.html#correction-vector",
    "title": "Ch2 Lecture 3",
    "section": "Correction vector",
    "text": "Correction vector\nWe can introduce a correction vector, equivalent to adding links from the dangling node to every other node, or to all connecting nodes (via some path).\n\n\\[\nP=\\left[\\begin{array}{llllll}\n0 & 0 & \\frac{1}{3} & \\frac{1}{3} & 0 & 0 \\\\\n\\frac{1}{2} & 0 & \\frac{1}{3} & \\frac{1}{3} & 0 & 0 \\\\\n\\frac{1}{2} & 1 & 0 & \\frac{1}{3} & 0 & 0 \\\\\n0 & 0 & \\frac{1}{3} & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 & 1 & 0\n\\end{array}\\right]\n\\]\n\n\nThis is now a stochastic matrix “surfing matrix”\n\n\nFind the new transition matrix \\(Q\\)…\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/ch2_lecture3.html#teleportation",
    "href": "lectures/ch2_lecture3.html#teleportation",
    "title": "Ch2 Lecture 3",
    "section": "Teleportation",
    "text": "Teleportation\n\nThere may not be a single stationary vector for the surfing matrix \\(P\\).\n\nCan you think of two different vectors that can never move towards each other? That is, can you think of ways of getting “stuck”?\n\n\nSolution: Jump around. Every so often, instead of following a link, jump to a random page.\n\n\nHow can we modify the surfing matrix to include this?\nIntroduce a teleportation matrix \\(E\\), which is a transition matrix that allows the surfer to jump to any page with equal probability.\nPause: What must \\(E\\) be?\n\n\n\nLet \\(\\mathbf{v}\\) be the teleportation vector with entries 1/n -Let \\(\\mathbf{e}=\\) \\((1,1, \\ldots, 1)\\) be the vector of all ones.\nThen \\(\\mathbf{v e}^{T}\\) is a stochastic matrix whose columns are all equal to \\(\\mathbf{v}\\). This is \\(E\\)."
  },
  {
    "objectID": "lectures/ch2_lecture3.html#pagerank-matrix",
    "href": "lectures/ch2_lecture3.html#pagerank-matrix",
    "title": "Ch2 Lecture 3",
    "section": "PageRank Matrix",
    "text": "PageRank Matrix\nLet:\n\n\\(P\\) be a stochastic matrix,\n\\(\\mathbf{v}\\) a distribution vector of compatible size\n\\(\\alpha\\) a teleportation parameter with \\(0&lt;\\alpha&lt;1\\).\nThen \\(\\alpha P+(1-\\alpha) \\mathbf{v e}^{T}\\) and\n. . . our goal is to find stationary vectors \\[\n\\begin{equation*}\n\\left(\\alpha P+(1-\\alpha) \\mathbf{v e}^{T}\\right) \\mathbf{x}=\\mathbf{x} \\tag{2.4}\n\\end{equation*}\n\\]\n\n\nRearranging,\n\\[\n\\begin{equation*}\n(I-\\alpha P) \\mathbf{x}=(1-\\alpha) \\mathbf{v} \\tag{2.5}\n\\end{equation*}\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture3.html#now-solve-pagerank-for-our-example",
    "href": "lectures/ch2_lecture3.html#now-solve-pagerank-for-our-example",
    "title": "Ch2 Lecture 3",
    "section": "Now solve pagerank for our example",
    "text": "Now solve pagerank for our example\nUse M.solve(b)…\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/ch2_lecture1.html#diffusion",
    "href": "lectures/ch2_lecture1.html#diffusion",
    "title": "Ch2 Lecture 1",
    "section": "Diffusion",
    "text": "Diffusion\n\nLast week, we talked about the diffusion equation. Here is the exact form:\n\\[\\frac{\\partial a(x,t)}{\\partial t} = D_{a}\\frac{\\partial^{2} a(x,t)}{\\partial x^{2}}\\]\n\nWalk through why we have a first derivative for time, and a second derivative for space. It’s because if the concentration is increasing steadily in space at a given position, the same amount will enter (from the left) and leave (from the right). We only end up with a changing concentration if the rate of entry and exit are different.\n\n\nWe approximated this using the finite-difference method:\nTime derivative (which we had set to zero):\n\\[\n\\frac{\\partial a(x,t)}{\\partial t} \\approx \\frac{1}{dt}(a_{x,t+1} - a_{x,t})\n\\]\nSpacial part of the derivative (which is usually know as the Laplacian):\n\\[\n\\frac{\\partial^{2} a(x,t)}{\\partial x^{2}} \\approx \\frac{1}{dx^{2}}(a_{x+1,t} + a_{x-1,t} - 2a_{x,t})\n\\]\n\n\nThis gives us the finite-difference equation:\n\\[\na_{x,t+1} = a_{x,t} + dt\\left(  \\frac{D_{a}}{dx^{2}}(a_{x+1,t} + a_{x-1,t} - 2a_{x,t})  \\right)\n\\]\n\n\nAdapted from this blog post."
  },
  {
    "objectID": "lectures/ch2_lecture1.html#boundary-conditions",
    "href": "lectures/ch2_lecture1.html#boundary-conditions",
    "title": "Ch2 Lecture 1",
    "section": "Boundary Conditions",
    "text": "Boundary Conditions\n\nLast week, with our metal rod, we had boundary conditions where the temperatures at the ends of the rod were fixed at 0 degrees.\nWe needed to say something about the boundaries to solve the system of equations.\nAnother option is periodic boundary conditions.\nThese are like imagining the rod is a loop, so the temperature at the end of the rod is the same as the temperature at the beginning of the rod."
  },
  {
    "objectID": "lectures/ch2_lecture1.html#periodic-boundary-conditions-np.roll",
    "href": "lectures/ch2_lecture1.html#periodic-boundary-conditions-np.roll",
    "title": "Ch2 Lecture 1",
    "section": "Periodic Boundary Conditions: np.roll",
    "text": "Periodic Boundary Conditions: np.roll\n\nWant an easy way to compute \\(a_{x+1}\\) for all \\(x\\)’s\nSuppose we have n points, and we want to compute \\(a_{x+1}\\) for all \\(x\\)=n.\nProblem: \\(a_{x+1}\\) would be off the end of the array!\nJust replace \\(a_{n+1}\\) with \\(a_{1}\\)\nCan do this easily with np.roll. Lets us compute \\(a_{x+1}\\) for all \\(x\\)’s.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/ch2_lecture1.html#laplacian-with-periodic-boundary-conditions-in-python",
    "href": "lectures/ch2_lecture1.html#laplacian-with-periodic-boundary-conditions-in-python",
    "title": "Ch2 Lecture 1",
    "section": "Laplacian with Periodic Boundary Conditions in Python",
    "text": "Laplacian with Periodic Boundary Conditions in Python\n\nimport numpy as np\ndef laplacian1D(a, dx):\n    return (\n        - 2 * a\n        + np.roll(a,1,axis=0)\n        + np.roll(a,-1,axis=0)\n    ) / (dx ** 2)\n\ndef laplacian2D(a, dx):\n    return (\n        - 4 * a\n        + np.roll(a,1,axis=0)\n        + np.roll(a,-1,axis=0)\n        + np.roll(a,+1,axis=1)\n        + np.roll(a,-1,axis=1)\n    ) / (dx ** 2)\n\n\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "lectures/ch2_lecture1.html#solving-and-animating",
    "href": "lectures/ch2_lecture1.html#solving-and-animating",
    "title": "Ch2 Lecture 1",
    "section": "Solving and animating",
    "text": "Solving and animating\n\nclass OneDimensionalDiffusionEquation(BaseStateSystem):\n    def __init__(self, D):\n        self.D = D\n        self.width = 1000\n        self.dx = 10 / self.width\n        self.dt = 0.9 * (self.dx ** 2) / (2 * D)\n        self.steps = int(0.1 / self.dt)\n\n    def initialise(self):\n        self.t = 0\n        self.X = np.linspace(-5,5,self.width)\n        self.a = np.exp(-self.X**2)\n\n    def update(self):\n        for _ in range(self.steps):\n            self.t += self.dt\n            self._update()\n\n    def _update(self):\n        La = laplacian1D(self.a, self.dx)\n        delta_a = self.dt * (self.D * La)\n        self.a += delta_a\n\n    def draw(self, ax):\n        ax.clear()\n        ax.plot(self.X,self.a, color=\"r\")\n        ax.set_ylim(0,1)\n        ax.set_xlim(-5,5)\n        ax.set_title(\"t = {:.2f}\".format(self.t))\n\none_d_diffusion = OneDimensionalDiffusionEquation(D=1)\n\none_d_diffusion.plot_time_evolution(\"diffusion.gif\")\n\n\n\n\ndiffusion.gif"
  },
  {
    "objectID": "lectures/ch2_lecture1.html#reaction-terms",
    "href": "lectures/ch2_lecture1.html#reaction-terms",
    "title": "Ch2 Lecture 1",
    "section": "Reaction Terms",
    "text": "Reaction Terms\n\nWe will have a system with two chemical components, \\(a\\) and \\(b\\).\nSuppose \\(a\\) activates genes which produce pigmentation. \\(b\\) inhibits \\(a\\)\nWill start with some random small initial concentrations of \\(a\\) and \\(b\\).\nEach will diffuse according to the diffusion equation.\nThey will also react with each other, changing the concentration of each.\n\n\nFor the reaction equations, will use the FitzHugh–Nagumo equation\n\\(R_a(a, b) = a - a^{3} - b + \\alpha\\)\n\\(R_{b}(a, b) = \\beta (a - b)\\)\nWhere \\(\\alpha\\) and \\(\\beta\\) are constants."
  },
  {
    "objectID": "lectures/ch2_lecture1.html#reaction-equation-evolution-at-one-point-in-space",
    "href": "lectures/ch2_lecture1.html#reaction-equation-evolution-at-one-point-in-space",
    "title": "Ch2 Lecture 1",
    "section": "Reaction Equation evolution at one point in space",
    "text": "Reaction Equation evolution at one point in space\n\nclass ReactionEquation(BaseStateSystem):\n    def __init__(self, Ra, Rb):\n        self.Ra = Ra\n        self.Rb = Rb\n        self.dt = 0.01\n        self.steps = int(0.1 / self.dt)\n\n    def initialise(self):\n        self.t = 0\n        self.a = 0.1\n        self.b = 0.7\n        self.Ya = []\n        self.Yb = []\n        self.X = []\n\n    def update(self):\n        for _ in range(self.steps):\n            self.t += self.dt\n            self._update()\n\n    def _update(self):\n        delta_a = self.dt * self.Ra(self.a,self.b)\n        delta_b = self.dt * self.Rb(self.a,self.b)\n\n        self.a += delta_a\n        self.b += delta_b\n\n    def draw(self, ax):\n        ax.clear()\n\n        self.X.append(self.t)\n        self.Ya.append(self.a)\n        self.Yb.append(self.b)\n\n        ax.plot(self.X,self.Ya, color=\"r\", label=\"A\")\n        ax.plot(self.X,self.Yb, color=\"b\", label=\"B\")\n        ax.legend()\n\n        ax.set_ylim(0,1)\n        ax.set_xlim(0,5)\n        ax.set_xlabel(\"t\")\n        ax.set_ylabel(\"Concentrations\")\n\nalpha, beta =  0.2, 5\n\ndef Ra(a,b): return a - a ** 3 - b + alpha\ndef Rb(a,b): return (a - b) * beta\n\none_d_reaction = ReactionEquation(Ra, Rb)\none_d_reaction.plot_time_evolution(\"reaction.gif\", n_steps=50)"
  },
  {
    "objectID": "lectures/ch2_lecture1.html#section",
    "href": "lectures/ch2_lecture1.html#section",
    "title": "Ch2 Lecture 1",
    "section": "",
    "text": "reaction.gif"
  },
  {
    "objectID": "lectures/ch2_lecture1.html#full-model",
    "href": "lectures/ch2_lecture1.html#full-model",
    "title": "Ch2 Lecture 1",
    "section": "Full Model",
    "text": "Full Model\nWe now have two parts: - a diffusion term that “spreads” out concentration - a reaction part the equalises the two concentrations. . . .\nWhat happens when we put the two together? Do we get something stable?"
  },
  {
    "objectID": "lectures/ch2_lecture1.html#section-1",
    "href": "lectures/ch2_lecture1.html#section-1",
    "title": "Ch2 Lecture 1",
    "section": "",
    "text": "def random_initialiser(shape):\n    return(\n        np.random.normal(loc=0, scale=0.05, size=shape),\n        np.random.normal(loc=0, scale=0.05, size=shape)\n    )\n\nclass OneDimensionalRDEquations(BaseStateSystem):\n    def __init__(self, Da, Db, Ra, Rb,\n                 initialiser=random_initialiser,\n                 width=1000, dx=1,\n                 dt=0.1, steps=1):\n\n        self.Da = Da\n        self.Db = Db\n        self.Ra = Ra\n        self.Rb = Rb\n\n        self.initialiser = initialiser\n        self.width = width\n        self.dx = dx\n        self.dt = dt\n        self.steps = steps\n\n    def initialise(self):\n        self.t = 0\n        self.a, self.b = self.initialiser(self.width)\n\n    def update(self):\n        for _ in range(self.steps):\n            self.t += self.dt\n            self._update()\n\n    def _update(self):\n\n        # unpack so we don't have to keep writing \"self\"\n        a,b,Da,Db,Ra,Rb,dt,dx = (\n            self.a, self.b,\n            self.Da, self.Db,\n            self.Ra, self.Rb,\n            self.dt, self.dx\n        )\n\n        La = laplacian1D(a, dx)\n        Lb = laplacian1D(b, dx)\n\n        delta_a = dt * (Da * La + Ra(a,b))\n        delta_b = dt * (Db * Lb + Rb(a,b))\n\n        self.a += delta_a\n        self.b += delta_b\n\n    def draw(self, ax):\n        ax.clear()\n        ax.plot(self.a, color=\"r\", label=\"A\")\n        ax.plot(self.b, color=\"b\", label=\"B\")\n        ax.legend()\n        ax.set_ylim(-1,1)\n        ax.set_title(\"t = {:.2f}\".format(self.t))\n\nDa, Db, alpha, beta = 1, 100, -0.005, 10\n\ndef Ra(a,b): return a - a ** 3 - b + alpha\ndef Rb(a,b): return (a - b) * beta\n\nwidth = 100\ndx = 1\ndt = 0.001\n\nOneDimensionalRDEquations(\n    Da, Db, Ra, Rb,\n    width=width, dx=dx, dt=dt,\n    steps=100\n).plot_time_evolution(\"1dRD.gif\", n_steps=150)"
  },
  {
    "objectID": "lectures/ch2_lecture1.html#section-2",
    "href": "lectures/ch2_lecture1.html#section-2",
    "title": "Ch2 Lecture 1",
    "section": "",
    "text": "1dRD.gif"
  },
  {
    "objectID": "lectures/ch2_lecture1.html#in-two-dimensions",
    "href": "lectures/ch2_lecture1.html#in-two-dimensions",
    "title": "Ch2 Lecture 1",
    "section": "In two dimensions",
    "text": "In two dimensions\n\n\n\n2dRD.png"
  },
  {
    "objectID": "lectures/ch2_lecture1.html#matrix-addition-and-subtraction",
    "href": "lectures/ch2_lecture1.html#matrix-addition-and-subtraction",
    "title": "Ch2 Lecture 1",
    "section": "Matrix Addition and Subtraction",
    "text": "Matrix Addition and Subtraction\nReview matrix addition and subtraction on your own!"
  },
  {
    "objectID": "lectures/ch2_lecture1.html#matrix-multiplication",
    "href": "lectures/ch2_lecture1.html#matrix-multiplication",
    "title": "Ch2 Lecture 1",
    "section": "Matrix Multiplication",
    "text": "Matrix Multiplication\nFor motivation:\n\\[\n2 x-3 y+4 z=5 \\text {. }\n\\]\n\nCan write as a “product” of the coefficient matrix \\([2,-3,4]\\) and and the column matrix of unknowns \\(\\left[\\begin{array}{l}x \\\\ y \\\\ z\\end{array}\\right]\\).\n\n\nThus, product is:\n\\[\n[2,-3,4]\\left[\\begin{array}{l}\nx \\\\\ny \\\\\nz\n\\end{array}\\right]=[2 x-3 y+4 z]\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture1.html#definition-of-matrix-product",
    "href": "lectures/ch2_lecture1.html#definition-of-matrix-product",
    "title": "Ch2 Lecture 1",
    "section": "Definition of matrix product",
    "text": "Definition of matrix product\n\\(A=\\left[a_{i j}\\right]\\): \\(m \\times p\\) matrix\n\\(B=\\left[b_{i j}\\right]\\): \\(p \\times n\\) matrix.\nProduct of \\(A\\) and \\(B\\) – \\(A B\\)\n\nis \\(m \\times n\\) matrix whose\n\\((i, j)\\) th entry is the entry of the product of the \\(i\\) th row of \\(A\\) and the \\(j\\) th column of \\(B\\);\n\n\nMore specifically, the \\((i, j)\\) th entry of \\(A B\\) is\n\\[\na_{i 1} b_{1 j}+a_{i 2} b_{2 j}+\\cdots+a_{i p} b_{p j} .\n\\]\nReminder: Matrix Multiplication is not Commutative\n\n\\(A B \\neq B A\\) in general."
  },
  {
    "objectID": "lectures/ch2_lecture1.html#linear-systems-as-a-matrix-product",
    "href": "lectures/ch2_lecture1.html#linear-systems-as-a-matrix-product",
    "title": "Ch2 Lecture 1",
    "section": "Linear Systems as a Matrix Product",
    "text": "Linear Systems as a Matrix Product\nWe can express a linear system of equations as a matrix product:\n\\[\n\\begin{aligned}\nx_{1}+x_{2}+x_{3} & =4 \\\\\n2 x_{1}+2 x_{2}+5 x_{3} & =11 \\\\\n4 x_{1}+6 x_{2}+8 x_{3} & =24\n\\end{aligned}\n\\]\n\n\\[\n\\mathbf{x}=\\left[\\begin{array}{l}\nx_{1} \\\\\nx_{2} \\\\\nx_{3}\n\\end{array}\\right], \\quad \\mathbf{b}=\\left[\\begin{array}{r}\n4 \\\\\n11 \\\\\n24\n\\end{array}\\right], \\quad \\text { and } A=\\left[\\begin{array}{lll}\n1 & 1 & 1 \\\\\n2 & 2 & 5 \\\\\n4 & 6 & 8\n\\end{array}\\right]\n\\]\n\nOf course, \\(A\\) is just the coefficient matrix of the system and \\(b\\) is the righthand-side vector, which we have seen several times before. But now these take on a new significance. Notice that if we take the first row of \\(A\\) and multiply it by \\(\\mathbf{x}\\) we get the left-hand side of the first equation of our system. Likewise for the second and third rows. Therefore, we may write in the language of matrices that\n\n\n\n\\[\nA \\mathbf{x}=\\left[\\begin{array}{lll}\n1 & 1 & 1 \\\\\n2 & 2 & 5 \\\\\n4 & 6 & 8\n\\end{array}\\right]\\left[\\begin{array}{l}\nx_{1} \\\\\nx_{2} \\\\\nx_{3}\n\\end{array}\\right]=\\left[\\begin{array}{r}\n4 \\\\\n11 \\\\\n24\n\\end{array}\\right]=\\mathbf{b}\n\\]\n\nWe can multiply this out and get that \\(x_1\\) multiplies each of the elements in the first column… (show this)"
  },
  {
    "objectID": "lectures/ch2_lecture1.html#matrix-multiplication-as-a-linear-combination-of-column-vectors",
    "href": "lectures/ch2_lecture1.html#matrix-multiplication-as-a-linear-combination-of-column-vectors",
    "title": "Ch2 Lecture 1",
    "section": "Matrix Multiplication as a Linear Combination of Column Vectors",
    "text": "Matrix Multiplication as a Linear Combination of Column Vectors\nAnother way of writing this system:\n\\[\nx_{1}\\left[\\begin{array}{l}\n1 \\\\\n2 \\\\\n4\n\\end{array}\\right]+x_{2}\\left[\\begin{array}{l}\n1 \\\\\n2 \\\\\n6\n\\end{array}\\right]+x_{3}\\left[\\begin{array}{l}\n1 \\\\\n5 \\\\\n8\n\\end{array}\\right]=\\left[\\begin{array}{r}\n4 \\\\\n11 \\\\\n24\n\\end{array}\\right] .\n\\]\n\nName the columns of A as \\(\\mathbf{a_1}, \\mathbf{a_2}, \\mathbf{a_3}\\), then we can write the matrix as \\(A=\\left[\\mathbf{a}_{1}, \\mathbf{a}_{2}, \\mathbf{a}_{3}\\right]\\)\n\n\nLet \\(\\mathbf{x}=\\left(x_{1}, x_{2}, x_{3}\\right)\\). Then\n\\[\nA \\mathbf{x}=x_{1} \\mathbf{a}_{1}+x_{2} \\mathbf{a}_{2}+x_{3} \\mathbf{a}_{3} .\n\\]\nThis is a very important way of thinking about matrix multiplication\n\nWhat happens if we are trying to get something which can’t be made up of any combination of the columns of \\(A\\)?\nWhat happens if two or more of the columns of \\(A\\) are the same?\nWhat if they are multiples of one another?\nWhat if one column is a linear combination of several others?"
  },
  {
    "objectID": "lectures/ch2_lecture1.html#try-it-yourself",
    "href": "lectures/ch2_lecture1.html#try-it-yourself",
    "title": "Ch2 Lecture 1",
    "section": "Try it yourself",
    "text": "Try it yourself\nTry to find the solution for the following system, by trying different values of \\(x_i\\) to use in a sum of the columns of \\(A\\).\n\\[ A \\mathbf{x} = \\left[\\begin{array}{lll}   1 & 1 & 1 \\\\   2 & 2 & 5 \\\\   4 & 6 & 8 \\end{array}\\right]\\left[\\begin{array}{l} x_1 \\\\ x_2 \\\\ x_3 \\end{array}\\right]=x_{1} \\mathbf{a}_{1}+x_{2} \\mathbf{a}_{2}+x_{3} \\mathbf{a}_{3} =\\left[\\begin{array}{l} 6 \\\\ 21 \\\\ 38 \\end{array}\\right]\n\\]\n\nThe answer is 2,1,3\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nNow try changing the right-hand side to a different vector. Can you still find a solution? (You may need to use non-integer values for the \\(x\\)’s.)"
  },
  {
    "objectID": "lectures/ch2_lecture1.html#section-3",
    "href": "lectures/ch2_lecture1.html#section-3",
    "title": "Ch2 Lecture 1",
    "section": "",
    "text": "This is a slightly changed system.\n\\[ A \\mathbf{x} = \\left[\\begin{array}{lll}   1 & 1 & 2 \\\\   1 & 2 & 2 \\\\   4 & 6 & 8 \\end{array}\\right]\\left[\\begin{array}{l} x_1 \\\\ x_2 \\\\ x_3 \\end{array}\\right]=x_{1} \\mathbf{a}_{1}+x_{2} \\mathbf{a}_{2}+x_{3} \\mathbf{a}_{3} =\\left[\\begin{array}{l} 11 \\\\ 12 \\\\ 46 \\end{array}\\right]\n\\]\n\nThe answer is 2, 1, 4\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nAre you able to find more than one solution? Can you find a right-hand-side that doesn’t have a solution?"
  },
  {
    "objectID": "lectures/ch2_lecture1.html#example-benzoic-acid",
    "href": "lectures/ch2_lecture1.html#example-benzoic-acid",
    "title": "Ch2 Lecture 1",
    "section": "Example: Benzoic acid",
    "text": "Example: Benzoic acid\nBenzoic acid (chemical formula \\(\\mathrm{C}_{7} \\mathrm{H}_{6} \\mathrm{O}_{2}\\) ) oxidizes to carbon dioxide and water.\n\\[\n\\mathrm{C}_{7} \\mathrm{H}_{6} \\mathrm{O}_{2}+\\mathrm{O}_{2} \\rightarrow \\mathrm{CO}_{2}+\\mathrm{H}_{2} \\mathrm{O} .\n\\]\nBalance this equation. (Make the number of atoms of each element match on the two sides of the equation.)\n\nDefine \\((c, o, h)\\) as the number of atoms of carbon, oxygen, and hydrogen atoms in the equation.\n\n\nNext let \\(x_1\\), \\(x_2\\), \\(x_3\\), and \\(x_4\\) be the number of molecules of benzoic acid, oxygen, carbon dioxide, and water, respectively.\n\n\nThen we have the equation\n\\[\nx_{1}\\left[\\begin{array}{l}\n7 \\\\\n2 \\\\\n6\n\\end{array}\\right]+x_{2}\\left[\\begin{array}{l}\n0 \\\\\n2 \\\\\n0\n\\end{array}\\right]=x_{3}\\left[\\begin{array}{l}\n1 \\\\\n2 \\\\\n0\n\\end{array}\\right]+x_{4}\\left[\\begin{array}{l}\n0 \\\\\n1 \\\\\n2\n\\end{array}\\right] .\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture1.html#section-4",
    "href": "lectures/ch2_lecture1.html#section-4",
    "title": "Ch2 Lecture 1",
    "section": "",
    "text": "Rearrange:\n\\[\nx_{1}\\left[\\begin{array}{l}\n7 \\\\\n2 \\\\\n6\n\\end{array}\\right]+x_{2}\\left[\\begin{array}{l}\n0 \\\\\n2 \\\\\n0\n\\end{array}\\right]=x_{3}\\left[\\begin{array}{l}\n1 \\\\\n2 \\\\\n0\n\\end{array}\\right]+x_{4}\\left[\\begin{array}{l}\n0 \\\\\n1 \\\\\n2\n\\end{array}\\right] .\n\\]\nbecomes\n\\[\nA \\mathbf{x}=\\left[\\begin{array}{cccc}\n7 & 0 & -1 & 0 \\\\\n2 & 2 & -2 & -1 \\\\\n6 & 0 & 0 & -2\n\\end{array}\\right]\\left[\\begin{array}{l}\nx_{1} \\\\\nx_{2} \\\\\nx_{3} \\\\\nx_{4}\n\\end{array}\\right]=\\left[\\begin{array}{l}\n0 \\\\\n0 \\\\\n0\n\\end{array}\\right]\n\\]\n\nThis is just like a matrix like we were solving in class last time."
  },
  {
    "objectID": "lectures/ch2_lecture1.html#section-5",
    "href": "lectures/ch2_lecture1.html#section-5",
    "title": "Ch2 Lecture 1",
    "section": "",
    "text": "We solve with row reduction:\n\\[\n\\begin{aligned}\n& {\\left[\\begin{array}{cccc}\n7 & 0 & -1 & 0 \\\\\n2 & 2 & -2 & -1 \\\\\n6 & 0 & 0 & -2\n\\end{array}\\right] \\xrightarrow[E_{21}\\left(-\\frac{2}{7}\\right)]{E_{31}\\left(-\\frac{6}{7}\\right)}\\left[\\begin{array}{cccc}\n7 & 0 & -1 & 0 \\\\\n0 & 2 & -\\frac{12}{7} & -1 \\\\\n0 & 0 & \\frac{6}{7} & -2\n\\end{array}\\right] \\begin{array}{c}\nE_{1}\\left(\\frac{1}{7}\\right) \\\\\nE_{2}\\left(\\frac{1}{2}\\right) \\\\\nE_{3}\\left(\\frac{7}{6}\\right)\n\\end{array} \\left[\\begin{array}{cccc}\n1 & 0 & -\\frac{1}{7} & 0 \\\\\n0 & 1 & -\\frac{6}{7} & -\\frac{1}{2} \\\\\n0 & 0 & 1 & -\\frac{7}{3}\n\\end{array}\\right]} \\\\\n& \\begin{array}{l}\n\\overrightarrow{E_{23}\\left(\\frac{6}{7}\\right)} \\\\\nE_{13}\\left(\\frac{1}{7}\\right)\n\\end{array}\\left[\\begin{array}{llll}\n1 & 0 & 0 & -\\frac{1}{3} \\\\\n0 & 1 & 0 & -\\frac{5}{2} \\\\\n0 & 0 & 1 & -\\frac{7}{3}\n\\end{array}\\right]\n\\end{aligned}\n\\]\n\n\\(x_{4}\\) is free, others are bound. Now pick smallest \\(x_4\\) where others are all positive integers…\n\n\n\\[\n2 \\mathrm{C}_{7} \\mathrm{H}_{6} \\mathrm{O}_{2}+15 \\mathrm{O}_{2} \\rightarrow 14 \\mathrm{CO}_{2}+6 \\mathrm{H}_{2} \\mathrm{O}\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture1.html#scaling",
    "href": "lectures/ch2_lecture1.html#scaling",
    "title": "Ch2 Lecture 1",
    "section": "Scaling",
    "text": "Scaling\nGoal: Make a matrix that will take a vector of coordinates \\(\\mathbf{x}\\) and scale each coordinate \\(x_i\\) by a factor of \\(z_i\\).\n\\[ A x = \\left[\\begin{array}{ll} a1 & a2 \\\\ a3 & a4 \\end{array}\\right] \\left[\\begin{array}{l} x_1 \\\\ x_2 \\end{array}\\right] = \\left[\\begin{array}{l} z_1 \\times x_1 \\\\ z_2\\times  x_2 \\end{array}\\right] \\]\n\n\\[ A = \\left[\\begin{array}{ll} z_1 & 0 \\\\ 0 & z_2 \\end{array}\\right] \\]"
  },
  {
    "objectID": "lectures/ch2_lecture1.html#shearing",
    "href": "lectures/ch2_lecture1.html#shearing",
    "title": "Ch2 Lecture 1",
    "section": "Shearing",
    "text": "Shearing\nShearing: adding a constant shear factor times one coordinate to another coordinate of the point.\nGoal: make a matrix which will transform each coordinate \\(x_i\\) into \\(x_i + \\sum_{j \\ne i} s_{j} \\times x_j\\).\n\\[ A x = \\left[\\begin{array}{ll} a1 & a2 \\\\ a3 & a4 \\end{array}\\right] \\left[\\begin{array}{l} x_1 \\\\ x_2 \\end{array}\\right] = \\left[\\begin{array}{l} x_1 + s_2 x_2 \\\\ x_2 + s_1 x_1 \\end{array}\\right] \\]\n\n\\[ A = \\left[\\begin{array}{ll} 1 & s_2 \\\\ s_1 & 1 \\end{array}\\right] \\]"
  },
  {
    "objectID": "lectures/ch2_lecture1.html#example",
    "href": "lectures/ch2_lecture1.html#example",
    "title": "Ch2 Lecture 1",
    "section": "Example",
    "text": "Example\n\nLet the scaling operator \\(S\\) on points in two dimensions have scale factors of \\(\\frac{3}{2}\\) in the \\(x\\)-direction and \\(\\frac{1}{2}\\) in the \\(y\\)-direction.\nLet the shearing operator \\(H\\) on these points have a shear factor of \\(\\frac{1}{2}\\) by the \\(y\\)-coordinate on the \\(x\\)-coordinate.\nExpress these operators as matrix operators and graph their action on four unit squares situated diagonally from the origin."
  },
  {
    "objectID": "lectures/ch2_lecture1.html#solution",
    "href": "lectures/ch2_lecture1.html#solution",
    "title": "Ch2 Lecture 1",
    "section": "Solution",
    "text": "Solution\n\nScaling operator \\(S\\):\n\n\n\\[\nS((x, y))=\\left[\\begin{array}{c}\n\\frac{3}{2} x \\\\\n\\frac{1}{2} y\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n\\frac{3}{2} & 0 \\\\\n0 & \\frac{1}{2}\n\\end{array}\\right]\\left[\\begin{array}{l}\nx \\\\\ny\n\\end{array}\\right]=T_{A}((x, y))\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture1.html#verify",
    "href": "lectures/ch2_lecture1.html#verify",
    "title": "Ch2 Lecture 1",
    "section": "Verify:",
    "text": "Verify:\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/ch2_lecture1.html#section-6",
    "href": "lectures/ch2_lecture1.html#section-6",
    "title": "Ch2 Lecture 1",
    "section": "",
    "text": "Shearing operator \\(H\\):\n\n\\[\nH((x, y))=\\left[\\begin{array}{c}\nx+\\frac{1}{2} y \\\\\ny\n\\end{array}\\right]=\\left[\\begin{array}{ll}\n1 & \\frac{1}{2} \\\\\n0 & 1\n\\end{array}\\right]\\left[\\begin{array}{l}\nx \\\\\ny\n\\end{array}\\right]=T_{B}((x, y))\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture1.html#verify-1",
    "href": "lectures/ch2_lecture1.html#verify-1",
    "title": "Ch2 Lecture 1",
    "section": "Verify",
    "text": "Verify\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/ch2_lecture1.html#concatenation-of-operators-s-and-h",
    "href": "lectures/ch2_lecture1.html#concatenation-of-operators-s-and-h",
    "title": "Ch2 Lecture 1",
    "section": "Concatenation of operators \\(S\\) and \\(H\\)",
    "text": "Concatenation of operators \\(S\\) and \\(H\\)\n\nThe concatenation \\(S \\circ H\\) of the scaling operator \\(S\\) and shearing operator \\(H\\) is the action of scaling followed by shearing.\nFunction composition corresponds to matrix multiplication\n\n\n\\[\n\\begin{aligned}\nS \\circ H((x, y)) & =T_{A} \\circ T_{B}((x, y))=T_{A B}((x, y)) \\\\\n& =\\left[\\begin{array}{cc}\n\\frac{3}{2} & 0 \\\\\n0 & \\frac{1}{2}\n\\end{array}\\right]\\left[\\begin{array}{ll}\n1 & \\frac{1}{2} \\\\\n0 & 1\n\\end{array}\\right]\\left[\\begin{array}{l}\nx \\\\\ny\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n\\frac{3}{2} & \\frac{3}{4} \\\\\n0 & \\frac{1}{2}\n\\end{array}\\right]\\left[\\begin{array}{l}\nx \\\\\ny\n\\end{array}\\right]=T_{C}((x, y)),\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture1.html#verify-2",
    "href": "lectures/ch2_lecture1.html#verify-2",
    "title": "Ch2 Lecture 1",
    "section": "Verify",
    "text": "Verify\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/ch2_lecture1.html#rotation",
    "href": "lectures/ch2_lecture1.html#rotation",
    "title": "Ch2 Lecture 1",
    "section": "Rotation",
    "text": "Rotation\nGoal: rotate a point in two dimensions counterclockwise by an angle \\(\\phi\\). Suppose the point is initally at an angle \\(\\theta\\) from the \\(x\\)-axis."
  },
  {
    "objectID": "lectures/ch2_lecture1.html#section-7",
    "href": "lectures/ch2_lecture1.html#section-7",
    "title": "Ch2 Lecture 1",
    "section": "",
    "text": "\\[\n\\left[\\begin{array}{l}\nx \\\\\ny\n\\end{array}\\right]\n=\\left[\\begin{array}{l}\nr \\cos \\theta \\\\\nr \\sin \\theta\n\\end{array}\\right]\n\\]\nWe can use trigonometry to find the values of x and y after rotation.\n\\[\n\\left[\\begin{array}{l}\nx^{\\prime} \\\\\ny^{\\prime}\n\\end{array}\\right] =\\left[\\begin{array}{l}\nr \\cos (\\theta+\\phi) \\\\\nr \\sin (\\theta+\\phi)\n\\end{array}\\right]=\\left[\\begin{array}{l}\nr \\cos \\theta \\cos \\phi-r \\sin \\theta \\sin \\phi \\\\\nr \\sin \\theta \\cos \\phi+r \\cos \\theta \\sin \\phi\n\\end{array}\\right]\n\\]\n\nUsing the double-angle rule,\n\\[\n=\\left[\\begin{array}{rr}\n\\cos \\theta &-\\sin \\theta \\\\\n\\sin \\theta & \\cos \\theta\n\\end{array}\\right]\\left[\\begin{array}{l}\nr \\cos \\phi \\\\\nr \\sin \\phi\n\\end{array}\\right]=\\left[\\begin{array}{rr}\n\\cos \\theta&-\\sin \\theta \\\\\n\\sin \\theta & \\cos \\theta\n\\end{array}\\right]\\left[\\begin{array}{l}\nx \\\\\ny\n\\end{array}\\right]\n\\]\n\n\nSo we define the rotation matrix \\(R(\\theta)\\) by\n\\[\nR(\\theta)=\\left[\\begin{array}{rr}\n\\cos \\theta & -\\sin \\theta \\\\\n\\sin \\theta & \\cos \\theta\n\\end{array}\\right]\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture1.html#now-you-try",
    "href": "lectures/ch2_lecture1.html#now-you-try",
    "title": "Ch2 Lecture 1",
    "section": "Now you try",
    "text": "Now you try\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n \n\n\n \n\n\n\n\n \n\n\n\ndiffusion.gif\nreaction.gif\n1dRD.gif\n2dRD.png"
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#what-are-linear-systems",
    "href": "lectures/ch1_lecture1b.html#what-are-linear-systems",
    "title": "Intro to linear systems",
    "section": "What are linear systems?",
    "text": "What are linear systems?\n\nSet of one or more linear equations involving the same variables.\nEach equation can be be put in form \\(a_1 x_1 + a_2 x_2 + \\dots + a_n x_n+ b = 0\\)\n\n\n\\[\n\\begin{cases}\na_{11} x_1 + a_{12} x_2 +\\dots + a_{1n} x_n = b_1 \\\\\na_{21} x_1 + a_{22} x_2  + \\dots + a_{2n} x_n = b_2 \\\\\n\\vdots\\\\\na_{m1} x_1 + a_{m2} x_2 + \\dots + a_{mn} x_n = b_m,\n\\end{cases}\n\\]"
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#example-railroad-cars",
    "href": "lectures/ch1_lecture1b.html#example-railroad-cars",
    "title": "Intro to linear systems",
    "section": "Example: railroad cars",
    "text": "Example: railroad cars\nA chemical manufacturer wants to lease a fleet of 24 railroad tank cars with a combined carrying capacity of 520,000 gallons.\n\nTank cars with three different carrying capacities are available:\n\n8,000 gallons\n16,000 gallons\n24,000 gallons.\n\nHow many of each type of tank car should be leased?\n\n\nCan write this as \\(x \\times 8+y \\times 16 + z \\times 24=520\\) and also \\(x+y+z=24\\). We won’t have a single solution… Two equations, three unknowns."
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#example-traffic-flow.",
    "href": "lectures/ch1_lecture1b.html#example-traffic-flow.",
    "title": "Intro to linear systems",
    "section": "Example: traffic flow.",
    "text": "Example: traffic flow.\n\n\n\n\nNumbers = # of vehicles/hr that enter and leave on that street.\n\\(x_{1}, x_{2}, x_{3}\\), and \\(x_{4}\\): flow of traffic between the four intersections.\n\n\n\nNumber of vehicles entering each intersection should always equal the number leaving. E.g.:\n\n1,500 vehicles enter the intersection of 5th Street and Washington Avenue each hour\n\\(x_{1}+x_{4}\\) vehicles leave this intersection\n\\(\\rightarrow\\) \\(x_{1}+x_{4}=1,500\\)\n\nFind the traffic flow at each of the other three intersections.\nWhat is the # of vehicles that travel from Washington Avenue to Lincoln Avenue on 5th Street?\n\n\n\n\n\\(x_1+x_2=1200\\), and \\(x_2+x_3=1000\\), and \\(x_3+x_4=1300\\)."
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#example-us-population",
    "href": "lectures/ch1_lecture1b.html#example-us-population",
    "title": "Intro to linear systems",
    "section": "Example: US population",
    "text": "Example: US population\n\nThe U.S. population was approximately 75 million in 1900, 150 million in 1950, and 275 million in 2000.\nFind a quadratic equation whose graph passes through the points \\((0,75)\\), \\((50,150)\\), \\((100,275)\\)\n\n\nCall the years \\((0, 50, 100)\\) \\(t_i\\). Call the populations \\((75,150,275)\\) \\(y_i\\).\nThen we have\n\\[\n\\begin{cases}\nc_1 t_1 + c_2 t_1^2 = y_1 \\\\\nc_1 t_2 + c_2 t_2^2 = y_2 \\\\\nc_1 t_3 + c_2 t_3^2 = y_3\n\\end{cases}\n\\]\n\n\nMake the substitutions \\(c_i \\rightarrow x_i\\) and \\(t_i \\rightarrow a_{1i}\\) and \\(t^2_i \\rightarrow a_{2i}\\), then this becomes\n\\[\n\\begin{cases}\nx_1 a_{11} + x_2 a_{21} =a_{11} x_1  + a_{21} x_2  = y_1 \\\\\nx_1 a_{12} + x_2 a_{22} =a_{12} x_1  +a_{22} x_2 = y_2 \\\\\nx_1 a_{13} + x_2 a_{23} = a_{13} x_1  + a_{23} x_2  = y_3\n\\end{cases}\n\\]\nWe see that we now have a standard system of linear equations."
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#goals-for-solving-algorithms",
    "href": "lectures/ch1_lecture1b.html#goals-for-solving-algorithms",
    "title": "Intro to linear systems",
    "section": "Goals for solving algorithms",
    "text": "Goals for solving algorithms\nAn algorithm should be:\n\nfeasible\naccurate\n\nstable\n\nefficient\n\nreusable computations"
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#example-very-simple-linear-system",
    "href": "lectures/ch1_lecture1b.html#example-very-simple-linear-system",
    "title": "Intro to linear systems",
    "section": "Example: very simple linear system",
    "text": "Example: very simple linear system\n\\[\n\\begin{gather*}\n2 x-y=1 \\\\\n4x+4y=20 . \\tag{1.5}\n\\end{gather*}\n\\]\n\nWe can solve this by hand: first we might add four times the first row to the second, so that we get (4+8)x + 0y = 24, so that we realize x = \\(2\\), and then we have 4-y=1 so y = 3.\n\n\nMake an augmented matrix to represent the problem:\n\\[\n\\begin{aligned}\n& x \\quad y=r . h . s . \\\\\n& {\\left[\\begin{array}{rrr}\n2 & -1 & 1 \\\\\n4 & 4 & 20\n\\end{array}\\right]}\n\\end{aligned}\n\\]\n\n\nManipulate the augmented matrix to solve the problem…"
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#elementary-matrix-operations",
    "href": "lectures/ch1_lecture1b.html#elementary-matrix-operations",
    "title": "Intro to linear systems",
    "section": "Elementary Matrix Operations",
    "text": "Elementary Matrix Operations\nWhen we do these on augmented matrices, the solutions are unchanged…\n\n\\(E_{i j}\\) : Swap: Switch the \\(i\\)th and \\(j\\) th rows of the matrix.\n\\(E_{i}(c)\\) : Scale: Multiply the \\(i\\)th row by the nonzero constant \\(c\\).\n\\(E_{i j}(d)\\) : Add: Add \\(d\\) times the \\(j\\)th row to the \\(i\\)th row."
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#reduced-row-echelon-form",
    "href": "lectures/ch1_lecture1b.html#reduced-row-echelon-form",
    "title": "Intro to linear systems",
    "section": "Reduced Row Echelon Form",
    "text": "Reduced Row Echelon Form\nEvery matrix can be reduced by a sequence of elementary row operations to one and only one reduced row echelon form:\n\nNonzero rows of \\(R\\) precede the zero rows.\nColumn numbers of the leading entries of the nonzero rows, say rows \\(1,2, \\ldots, r\\), form an increasing sequence of numbers \\(c_{1}&lt;c_{2}&lt;\\cdots&lt;c_{r}\\).\nEach leading entry is a 1.\nEach leading entry has only zeros above it."
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#use-elementary-ops-to-get-reduced-row-echelon-form",
    "href": "lectures/ch1_lecture1b.html#use-elementary-ops-to-get-reduced-row-echelon-form",
    "title": "Intro to linear systems",
    "section": "Use elementary ops to get reduced row echelon form",
    "text": "Use elementary ops to get reduced row echelon form"
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#g-j-elimination-for-our-toy-system",
    "href": "lectures/ch1_lecture1b.html#g-j-elimination-for-our-toy-system",
    "title": "Intro to linear systems",
    "section": "G-J Elimination for our toy system",
    "text": "G-J Elimination for our toy system\n\\[\n\\left[\\begin{array}{rrr}\n4 & 4 & 20 \\\\\n2 & -1 & 1\n\\end{array}\\right] \\overrightarrow{E_{1}(\\frac{1}{4}}\\left[\\begin{array}{lll}\n1 & 1 & 5 \\\\\n2 & -1 & 1\n\\end{array}\\right] \\overrightarrow{E_{12}(-2)}\\left[\\begin{array}{lll}\n1 & 1 & 5 \\\\\n0 & -3 & -9\n\\end{array}\\right]\n\\]\nand then…\n\n\\[\n\\left[\\begin{array}{rrr}\n1 & 1 & 5 \\\\\n0 & -3 & -9\n\\end{array}\\right] \\overrightarrow{E_{2}(-1 / 3)}\\left[\\begin{array}{lll}\n1 & 1 & 5 \\\\\n0 & 1 & 3\n\\end{array}\\right] \\overrightarrow{E_{12}(-1)}\\left[\\begin{array}{lll}\n1 & 0 & 2 \\\\\n0 & 1 & 3\n\\end{array}\\right] .\n\\]\n\n\nWriting this back as a linear system, we have:\n\\[\n\\begin{gather*}\n2 x-y=1 \\\\\n4 x+4 y=20\n\\end{gather*}  \\Longrightarrow {\\begin{aligned}\n& 1 \\cdot x+0 \\cdot y=2 \\\\\n& 0 \\cdot x+1 \\cdot y=3\n\\end{aligned}}  \\Longrightarrow {\\begin{aligned}x &= 2 \\\\ y &= 3 \\end{aligned}}\n\\]"
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#now-you-try-birds-in-a-tree",
    "href": "lectures/ch1_lecture1b.html#now-you-try-birds-in-a-tree",
    "title": "Intro to linear systems",
    "section": "Now you try: birds in a tree",
    "text": "Now you try: birds in a tree\nThere are 2 trees in a garden (tree “A” and “B”) and in both trees are some birds.\nThe birds of tree A say to the birds of tree B that if one of you comes to our tree, then our population will be double yours.\nThen the birds of tree B tell the birds of tree A that if one of you comes here, then our population will be equal to yours.\nHow many birds in each tree?\n(Solve by making an augmented matrix and doing G-J elimination.)\n\nThe answer is 7 birds in tree A and 5 birds in tree B.\n\n\n\nhttps://www.mathsisfun.com/puzzles/birds-in-trees.html"
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#example-mining",
    "href": "lectures/ch1_lecture1b.html#example-mining",
    "title": "Intro to linear systems",
    "section": "Example: Mining",
    "text": "Example: Mining\n\nA mine produces silica, iron, and gold\nNeeds Money (in $$), operating time (in hours), and labor (in person-hours).\n\n1 pound of silica needs: $.0055, . 0011 hours of operating time, and .0093 hours of labor.\n1 pound of iron needs: $.095, . 01 operating hours, and .025 labor hours.\n1 pound of gold needs: $ 960, 112 operating hours, and 560 labor hours.\n\n\nMeyer Ch 1.5"
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#section",
    "href": "lectures/ch1_lecture1b.html#section",
    "title": "Intro to linear systems",
    "section": "",
    "text": "Suppose that during 600 hours of operation, exactly $ 5000 and 3000 labor-hours are used.\nHow much silica (\\(x\\)), iron (\\(y\\)), and gold (\\(z\\)) were produced?\n\nSet up the linear system whose solution will yield the values for \\(x, y\\), and \\(z\\).\n\n\n\\[\n\\begin{aligned}\n.0055 x + .095 y + 960 z &=  5000 \\quad \\text{(dollars)}\\\\\n.0011 x + .01 y + 112 z &= 600 \\quad \\text{(operating hours)}\\\\\n.0093 x + .025 y + 560 z &= 3000\\quad \\text{(labor hours)}\n\\end{aligned}\n\\]\n\n\nMake the augmented matrix:\n\\[\n\\left[\\begin{array}{@{}ccc|c@{}}\n.0055 & .095 & 960 & 5000 \\\\\n.0011 & .01  & 112 & 600 \\\\\n.0093 & .025 & 560 & 3000\n\\end{array}\\right]\n\\]\n\nDraw the line on the augmented matrix."
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#section-1",
    "href": "lectures/ch1_lecture1b.html#section-1",
    "title": "Intro to linear systems",
    "section": "",
    "text": "We can solve this using Gauss-Jordan elimination."
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#getting-into-row-echelon-form-rounding-after-3-digits",
    "href": "lectures/ch1_lecture1b.html#getting-into-row-echelon-form-rounding-after-3-digits",
    "title": "Intro to linear systems",
    "section": "Getting into row echelon form, rounding after 3 digits",
    "text": "Getting into row echelon form, rounding after 3 digits\n\\[\\left[\\begin{matrix}0.0055 & 0.095 & 960 & 5000\\\\0.0011 & 0.01 & 112 & 600\\\\0.0093 & 0.025 & 560 & 3000\\end{matrix}\\right] \\overrightarrow{E_{12}(\\frac{-1}{5})} \\left[\\begin{matrix}0.0055 & 0.095 & 9.6 \\cdot 10^{2} & 5.0 \\cdot 10^{3}\\\\0 & -0.009 & -80.0 & -4.0 \\cdot 10^{2}\\\\0.0093 & 0.025 & 5.6 \\cdot 10^{2} & 3.0 \\cdot 10^{3}\\end{matrix}\\right]\\]\n\\[\\overrightarrow{E_{13}(\\frac{93}{55})} \\left[\\begin{matrix}0.0055 & 0.095 & 9.6 \\cdot 10^{2} & 5.0 \\cdot 10^{3}\\\\0 & -0.009 & -80.0 & -4.0 \\cdot 10^{2}\\\\0 & -0.14 & -1.1 \\cdot 10^{3} & -5.4 \\cdot 10^{3}\\end{matrix}\\right]\\]\n\\[\\overrightarrow{E_{23}(\\frac{-14}{.9})} \\left[\\begin{matrix}0.0055 & 0.095 & 9.6 \\cdot 10^{2} & 5.0 \\cdot 10^{3}\\\\0 & -0.009 & -80.0 & -4.0 \\cdot 10^{2}\\\\0 & 0 & 1.4 \\cdot 10^{2} & 5.7 \\cdot 10^{2}\\end{matrix}\\right]\\]"
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#section-2",
    "href": "lectures/ch1_lecture1b.html#section-2",
    "title": "Intro to linear systems",
    "section": "",
    "text": "This has solutions \\(\\left[\\begin{matrix}5.7 \\cdot 10^{4}\\\\8.9 \\cdot 10^{3}\\\\4.0\\end{matrix}\\right]\\).\n\nHow do these compare to the exact solutions? These are\n\n\n\\(\\displaystyle \\left[\\begin{matrix}56753.6889897841\\\\8626.56072644719\\\\4.02951191827469\\end{matrix}\\right]\\)"
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#doing-it-again-rounding-after-15-digits",
    "href": "lectures/ch1_lecture1b.html#doing-it-again-rounding-after-15-digits",
    "title": "Intro to linear systems",
    "section": "Doing it again, rounding after 15 digits",
    "text": "Doing it again, rounding after 15 digits\n\\(\\left[\\begin{matrix}0.0055 & 0.095 & 960 & 5000\\\\0.0011 & 0.01 & 112 & 600\\\\0.0093 & 0.025 & 560 & 3000\\end{matrix}\\right] \\rightarrow \\left[\\begin{matrix}0.0055 & 0.095 & 960.0 & 5000.0\\\\0 & -0.009 & -80.0 & -400.0\\\\0.0093 & 0.025 & 560.0 & 3000.0\\end{matrix}\\right]\\)\n\\(\\rightarrow \\left[\\begin{matrix}0.0055 & 0.095 & 960.0 & 5000.0\\\\0 & -0.009 & -80.0 & -400.0\\\\0 & -0.135636363636364 & -1063.27272727273 & -5454.54545454545\\end{matrix}\\right] \\rightarrow \\left[\\begin{matrix}0.0055 & 0.095 & 960.0 & 5000.0\\\\0 & -0.009 & -80.0 & -400.0\\\\0 & 0 & 142.383838383838 & 573.737373737372\\end{matrix}\\right]\\)\nThis has solutions \\(\\left[\\begin{matrix}56753.6889897845\\\\8626.56072644727\\\\4.02951191827468\\end{matrix}\\right]\\).\nReminder, the exact solution was:\n\n\n\\(\\displaystyle \\left[\\begin{matrix}56753.6889897841\\\\8626.56072644719\\\\4.02951191827469\\end{matrix}\\right]\\)"
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#reminder-of-goals",
    "href": "lectures/ch1_lecture1b.html#reminder-of-goals",
    "title": "Intro to linear systems",
    "section": "Reminder of goals",
    "text": "Reminder of goals\nAn algorithm should be:\n\n\nfeasible\naccurate\n\nstable\n\nefficient\n\nreusable computations"
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#roundoff-errors-with-g-j-elimination",
    "href": "lectures/ch1_lecture1b.html#roundoff-errors-with-g-j-elimination",
    "title": "Intro to linear systems",
    "section": "Roundoff errors with G-J elimination",
    "text": "Roundoff errors with G-J elimination\nTry this on your calculator. What do you get?\n\\[\n\\left(\\left(\\frac{2}{3}+100\\right)-100\\right)-\\frac{2}{3}\\stackrel{?}{=}0\n\\]\n\nNow try this…\n\\[\n\\left(\\left(\\frac{2}{3}+100\\right)-100\\right)-\\frac{2}{3}\\stackrel{?}{=}0\n\\]\n\nOn my calculator, I get 3.33e-32. That’s a small number, but not zero!\nWhat is happening is that 2/3 = 0.666666…., and your calculator represents this as a very long string but eventually puts in a 7.\nThen when it adds the 100, it has to drop a few of those digits in its representation. They are then gone! When we subtract the 100, we don’t get them back again."
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#section-3",
    "href": "lectures/ch1_lecture1b.html#section-3",
    "title": "Intro to linear systems",
    "section": "",
    "text": "Let \\(\\epsilon\\) be a number so small that our calculator yields \\(1+\\epsilon=1\\).\nWith this calculator, \\(1+1 / \\epsilon=(\\epsilon+1) / \\epsilon=1 / \\epsilon\\)\nWant to solve the linear system\n\\[\n  \\begin{align*}\n  \\epsilon x_{1}+x_{2} & =1\\\\\n  x_{1}-x_{2} & =0 .\n  \\end{align*}\n\\]"
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#roundoff-errors-with-g-j-elimination-1",
    "href": "lectures/ch1_lecture1b.html#roundoff-errors-with-g-j-elimination-1",
    "title": "Intro to linear systems",
    "section": "Roundoff errors with G-J elimination",
    "text": "Roundoff errors with G-J elimination\nWith our calculator,\n\nIn the second step, we are replacing \\(-1+\\frac{1}{\\epsilon}\\) with just \\(\\frac{1}{\\epsilon}\\). It’s a case where we are adding together numbers of very different scale.\n\n\\[\n    \\left[\\begin{array}{rrr}\n    \\epsilon & 1 & 1 \\\\\n    1 & -1 & 0\n    \\end{array}\\right] \\overrightarrow{E_{21}\\left(-\\frac{1}{\\epsilon}\\right)}\\left[\\begin{array}{ccc}\n    \\epsilon & 1 & 1 \\\\\n    0 & \\frac{1}{\\epsilon} & -\\frac{1}{\\epsilon}\n    \\end{array}\\right] \\overrightarrow{E_{2}(\\epsilon)}\\left[\\begin{array}{ccc}\n    \\epsilon & 1 & 1 \\\\\n    0 & 1 & 1\n    \\end{array}\\right] \\\\ \\overrightarrow{E_{12}(-1)}\\left[\\begin{array}{ccc}\n    \\epsilon & 0 & 0 \\\\\n    0 & 1 & 1\n    \\end{array}\\right]  \\overrightarrow{E_{1}\\left(\\frac{1}{\\epsilon}\\right)}\\left[\\begin{array}{lll}\n    1 & 0 & 0 \\\\\n    0 & 1 & 1\n    \\end{array}\\right] .\n\\]\n\nCalculated solution: \\(x_{1}=0, x_{2}=1\\)\nCorrect answer should be\n\\[ x_{1}=x_{2}=\\frac{1}{1+\\epsilon}=0.999999099999990 \\ldots\\]"
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#sensitivity-to-small-changes",
    "href": "lectures/ch1_lecture1b.html#sensitivity-to-small-changes",
    "title": "Intro to linear systems",
    "section": "Sensitivity to small changes",
    "text": "Sensitivity to small changes\nProblem arose because we took a computational step where we added two numbers of very different scale, essentially losing the smaller number.\nLed to a big changes in output!\nThere is no general cure for these difficulties…\nWant to be aware of them, know when we are doing computations that might be susceptible."
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#partial-pivoting",
    "href": "lectures/ch1_lecture1b.html#partial-pivoting",
    "title": "Intro to linear systems",
    "section": "Partial pivoting",
    "text": "Partial pivoting\nWe can improve performance of G-J by introducing a new step into the algorithm:\n\nFind the entry in the left column with the largest absolute value. This entry is called the pivot. Perform row interchange (if necessary), so that the pivot is in the first row.\nUse a row operation to get a 1 as the entry in the first row and first column.\nUse row operations to make all other entries as zeros in column one.\nInterchange rows if necessary to obtain a nonzero number with the largest absolute value in the second row, second column. Use a row operation to make this entry 1. Use row operations to make all other entries as zeros in column two.\nRepeat step 4 for row 3, column 3. Continue moving along the main diagonal until you reach the last row, or until the number is zero.\n\n\nFull pivoting is where we also move the columns around to get the largest element to the front.\n” A square matrix 𝐴 has an 𝐿𝑈 factorization (without pivoting) if, and only if, no zero is encountered in the pivot position when computing an 𝐿𝑈 factorization of 𝐴. However, when does computations using floating point numbers a pivot that is nearly zero can lead to dramatic rounding errors. The simple workaround is to always permute the rows of the matrix such that the largest nonzero entry in a column is chosen as the pivot entry. This ensures that a nearly zero is never chosen. Complete pivoting goes even further by using row and column permutations to select the largest entry in the entire matrix as the pivot entry.” from here\nIt’s very rare to find yourself in a situation where you need complete pivoting, and sometimes it can be quite computationally expensive."
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#using-partial-pivoting-in-our-example",
    "href": "lectures/ch1_lecture1b.html#using-partial-pivoting-in-our-example",
    "title": "Intro to linear systems",
    "section": "Using partial pivoting in our example",
    "text": "Using partial pivoting in our example\n\nwork this one out by hand"
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#ill-conditioned-linear-systems-1",
    "href": "lectures/ch1_lecture1b.html#ill-conditioned-linear-systems-1",
    "title": "Intro to linear systems",
    "section": "Ill-conditioned linear systems",
    "text": "Ill-conditioned linear systems\n\nSometimes it’s not the procedure that introduces the susceptibility to small changes, but the actual problem itself. Then, small differences in the inputs lead to big differences in the exact solutions.\n\n\nA system of linear equations is said to be ill-conditioned when some small perturbation in the system (in the \\(b\\)s) can produce relatively large changes in the exact solution (in the \\(x\\)’s). Otherwise, the system is said to be well-conditioned."
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#ill-conditioned-linear-systems-2",
    "href": "lectures/ch1_lecture1b.html#ill-conditioned-linear-systems-2",
    "title": "Intro to linear systems",
    "section": "Ill-conditioned linear systems",
    "text": "Ill-conditioned linear systems\n\n\n\nConsider \\[\n\\begin{aligned}\n& .835 x+.667 y=.168, \\\\\n& .333 x+.266 y=.067,\n\\end{aligned}\n\\]\nExact solution:\n\\[\nx=1 \\quad \\text { and } \\quad y=-1 .\n\\]\n\n\nBut if we change just one digit…\n\\[\n\\begin{aligned}\n& .835 x+.667 y=.168, \\\\\n& .333 x+.266 y=.06\\color{red}{6},\n\\end{aligned}\n\\]\nNow exact solution:\n\\[\n\\hat{x}=-666 \\quad \\text { and } \\quad \\hat{y}=834\n\\]\n\n\n\n\nWhat’s going on here? One thing to notice is that the lines corresponding to solutions for each equation have very similar slopes: r .667/.835 vs r .266/.33.\n\n\nMeyer Ch 1.6"
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#section-4",
    "href": "lectures/ch1_lecture1b.html#section-4",
    "title": "Intro to linear systems",
    "section": "",
    "text": "What if \\(b_1\\) and \\(b_2\\) are the results of an experiment, need to be read off a dial? Suppose:\n\ndial can be read to tolerance of \\(\\pm .001\\),\nvalues for \\(b_{1}\\) and \\(b_{2}\\) are read as .168 and .067, respectively. Then the exact solution is\n\n\n\\[\n\\begin{equation*}\n(x, y)=(1,-1)\n\\end{equation*}\n\\]\n\n\nBut due to uncertainty, we have\n\n\n\\[\n\\begin{equation*}\n.167 \\leq b_{1} \\leq .169 \\quad \\text { and } \\quad .066 \\leq b_{2} \\leq .068\n\\end{equation*}\n\\]"
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#section-5",
    "href": "lectures/ch1_lecture1b.html#section-5",
    "title": "Intro to linear systems",
    "section": "",
    "text": "Possible readings\n\n\n\\(b_1\\)\n\\(b_2\\)\n\\(x\\)\n\\(y\\)\n\n\n\n\n.168\n.067\n1\n-1\n\n\n.167\n.068\n934\n-1169\n\n\n.169\n.066\n-932\n1169"
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#geometrical-interpretation",
    "href": "lectures/ch1_lecture1b.html#geometrical-interpretation",
    "title": "Intro to linear systems",
    "section": "Geometrical interpretation",
    "text": "Geometrical interpretation\nIf two straight lines are almost parallel and if one of the lines is moved only slightly, then the point of intersection is drastically altered.\n\nThe point of intersection is the solution of the associated \\(2 \\times 2\\) linear system, so this is also drastically altered."
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#section-6",
    "href": "lectures/ch1_lecture1b.html#section-6",
    "title": "Intro to linear systems",
    "section": "",
    "text": "Often in real life, coefficients are empirically obtained\nWill be off from “true” values by small amounts\nFor ill-conditioned systems, this means that solutions can be very far off from true solutions\nWe’ll cover techniques for quantifying conditioning, later in quarter\nFor now, can just try making small changes to some coefficients. Big changes in result? Ill-conditioned system!"
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#polynomial-interpolation",
    "href": "lectures/ch1_lecture1b.html#polynomial-interpolation",
    "title": "Intro to linear systems",
    "section": "Polynomial interpolation",
    "text": "Polynomial interpolation\n\nSuppose we’d like to find a polynomial that can interpolate a given function.\nTake points 𝑥0,…𝑥𝑁 and values 𝑦0,…𝑦𝑁\nSimple way: finding the coefficients $ c_1, c_n $ where\n\\[\nP(x) = \\sum_{i=0}^N c_i x^i\n\\]\n\nThis is a system of equations:\n\\[\n\\begin{array}\n    cy_0 = c_0 + c_1 x_0 + \\ldots c_N x_0^N\\\\\n    \\,\\ldots\\\\\n    \\,y_N = c_0 + c_1 x_N + \\ldots c_N x_N^N\n\\end{array}\n\\]\n\n\nhttps://colab.research.google.com/github/quantecon/lecture-julia.notebooks/blob/main/tools_and_techniques/iterative_methods_sparsity.ipynb#scrollTo=9aa1791b"
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#polynomial-interpolation-1",
    "href": "lectures/ch1_lecture1b.html#polynomial-interpolation-1",
    "title": "Intro to linear systems",
    "section": "Polynomial interpolation",
    "text": "Polynomial interpolation\nOr, stacking, \\(c = \\begin{bmatrix} c_0 & \\ldots & c_N\\end{bmatrix}\\), \\(y = \\begin{bmatrix} y_0 & \\ldots & y_N\\end{bmatrix}\\) , and\n\\[\nA = \\begin{bmatrix} 1 & x_0 & x_0^2 & \\ldots &x_0^N\\\\\n                    \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n                    1 & x_N & x_N^2 & \\ldots & x_N^N\n    \\end{bmatrix}\n\\]\nLet’s try solving this for a simple function, \\(y=exp(x)\\)."
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#polynomial-interpolation-2",
    "href": "lectures/ch1_lecture1b.html#polynomial-interpolation-2",
    "title": "Intro to linear systems",
    "section": "Polynomial interpolation",
    "text": "Polynomial interpolation\n\\[\nx*y\n\\]\nWe’ll start with picking 4 interpolating points: \\(\\vec x = \\mathtt{\\text{[ 1.          4.66666667  8.33333333 12.        ]}}\\). Then we have our matrix A:\n\\[A = \\begin{bmatrix} 1 & x_0 & x_0^2 & \\ldots &x_0^N\\\\\n                    \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n                    1 & x_N & x_N^2 & \\ldots & x_N^N\n    \\end{bmatrix} = \\left[\\begin{matrix}1.0 & 1.0 & 1.0 & 1.0\\\\1.0 & 4.66666666666667 & 21.7777777777778 & 101.62962962963\\\\1.0 & 8.33333333333333 & 69.4444444444444 & 578.703703703703\\\\1.0 & 12.0 & 144.0 & 1728.0\\end{matrix}\\right] \\]\nOur \\(y\\) values are \\(\\vec y = \\mathtt{\\text{[     2.71828183    106.3426754    4160.26200538 162754.791419  ]}}\\). We could solve this using Gauss-Jordan elimination, or here the solving algorithm built into Numpy."
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#results-for-n4-and-n10-points",
    "href": "lectures/ch1_lecture1b.html#results-for-n4-and-n10-points",
    "title": "Intro to linear systems",
    "section": "Results for n=4 and n=10 points",
    "text": "Results for n=4 and n=10 points"
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#max-error-for-different-values-of-n",
    "href": "lectures/ch1_lecture1b.html#max-error-for-different-values-of-n",
    "title": "Intro to linear systems",
    "section": "Max error for different values of n",
    "text": "Max error for different values of n"
  },
  {
    "objectID": "lectures/ch1_lecture1b.html#the-problem-linear-dependency",
    "href": "lectures/ch1_lecture1b.html#the-problem-linear-dependency",
    "title": "Intro to linear systems",
    "section": "The problem: linear dependency",
    "text": "The problem: linear dependency\n\n\n\n\n\n\n\n\n\n\nAs n gets higher, \\(x^n\\) gets closer to \\(x^{n+1}\\) (at least in the range \\((0,1)\\)). This means that a solution of \\(a_n x^n + a_{n+1} x^{n+1}\\) will be very similar to e.g. \\(a_{n+1} x^n + a_{n} x^{n+1}\\)"
  },
  {
    "objectID": "lectures/ch1_lecture2.html#a-resource-for-review",
    "href": "lectures/ch1_lecture2.html#a-resource-for-review",
    "title": "More Systems of Linear Equations",
    "section": "A resource for review",
    "text": "A resource for review\nThe glossary from the end of Strand: glossary.pdf"
  },
  {
    "objectID": "lectures/ch1_lecture2.html#systems-with-non-unique-solutions",
    "href": "lectures/ch1_lecture2.html#systems-with-non-unique-solutions",
    "title": "More Systems of Linear Equations",
    "section": "Systems with non-unique solutions",
    "text": "Systems with non-unique solutions\nSolve for the variables \\(x, y\\), and \\(z\\) in the system\n\\[\n\\begin{aligned}\nz & =2 \\\\\nx+y+z & =2 \\\\\n2 x+2 y+4 z & =8\n\\end{aligned}\n\\]\n\nMake an augmented matrix: Example 1.20. Solve for the variables \\(x, y\\), and \\(z\\) in the system\n\\[\n\\left[\\begin{array}{llll}\n0 & 0 & 1 & 2 \\\\\n1 & 1 & 1 & 2 \\\\\n2 & 2 & 4 & 8\n\\end{array}\\right]\n\\]\n\nNote that the first column corresponds to x, the second to y, and the third to z.\n\n\n\n\\[\n\\left[\\begin{array}{llll}\n0 & 0 & 1 & 2 \\\\\n1 & 1 & 1 & 2 \\\\\n2 & 2 & 4 & 8\n\\end{array}\\right] \\stackrel{E_{12}}{\\longrightarrow}\\left[\\begin{array}{llll}\n(1 & 1 & 1 & 2 \\\\\n0 & 0 & 1 & 2 \\\\\n2 & 2 & 4 & 8\n\\end{array}\\right] \\xrightarrow[E_{31}(-2)]{\\longrightarrow}\\left[\\begin{array}{rlll}\n1 & 1 & 1 & 2 \\\\\n0 & 0 & 2 & 4 \\\\\n0 & 0 & 1 & 2\n\\end{array}\\right]\n\\]\n\nNow we are stuck! The first row we are going to use to solve for x. Neither the second or third row tell us anything about y…"
  },
  {
    "objectID": "lectures/ch1_lecture2.html#section",
    "href": "lectures/ch1_lecture2.html#section",
    "title": "More Systems of Linear Equations",
    "section": "",
    "text": "We keep on going… \\[\n\\begin{aligned}\n& {\\left[\\begin{array}{rrrr}\n(1) & 1 & 1 & 2 \\\\\n0 & 0 & 2 & 4 \\\\\n0 & 0 & 1 & 2\n\\end{array}\\right] \\xrightarrow[E_{2}(1 / 2)]{\\longrightarrow}\\left[\\begin{array}{rrrr}\n1 & 1 & 1 & 2 \\\\\n0 & 0 & 1 & 2 \\\\\n0 & 0 & 1 & 2\n\\end{array}\\right]} \\\\\n& \\overrightarrow{E_{32}(-1)}\\left[\\begin{array}{rrrr}\n(1) & 1 & 1 & 2 \\\\\n0 & 0 & 1 & 2 \\\\\n0 & 0 & 0 & 0\n\\end{array}\\right] \\xrightarrow[E_{12}(-1)]{\\longrightarrow}\\left[\\begin{array}{rrrr}\n(1) & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 2 \\\\\n0 & 0 & 0 & 0\n\\end{array}\\right] .\n\\end{aligned}\n\\]\nThere’s still no information on \\(y\\).\n\\[\n\\begin{aligned}\n& x=-y \\\\\n& z=2 \\\\\n& y \\text { is free. }\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lectures/ch1_lecture2.html#free-versus-bound-variables",
    "href": "lectures/ch1_lecture2.html#free-versus-bound-variables",
    "title": "More Systems of Linear Equations",
    "section": "Free versus bound variables",
    "text": "Free versus bound variables\nSuppose we have another augmented matrix, which after GJ elimination has become:\n\\[\n\\left[\\begin{array}{rrrrr}\nx & y & z & w & \\mathrm{rhs} \\\\\n1 & 2 & 0 & -1 & 2 \\\\\n0 & 0 & 1 & 3 & 0 \\\\\n0 & 0 & 0 & 0 & 0\n\\end{array}\\right] .\n\\]\n\nSolutions are: \\[\n\\begin{aligned}\nx+2 y-w & =2 \\\\\nz+3 w & =0 .\n\\end{aligned}\n\\]\nHow can we tell from looking at the matrix which variables are free vs bound?\nColumns which contain pivots correspond to bound variables. The others correspond to free variables.\n(Columns with pivots are called “basic” columns.)"
  },
  {
    "objectID": "lectures/ch1_lecture2.html#zero-solutions",
    "href": "lectures/ch1_lecture2.html#zero-solutions",
    "title": "More Systems of Linear Equations",
    "section": "Zero solutions",
    "text": "Zero solutions\nSolve this system… \\[\n\\begin{array}{r}\nx+y=1 \\\\\n2 x+y=2 \\\\\n3 x+2 y=5 .\n\\end{array}\n\\]\n\n\\[\n\\left[\\begin{array}{lll}\n1 & 1 & 1 \\\\\n2 & 1 & 2 \\\\\n3 & 2 & 5\n\\end{array}\\right] \\xrightarrow[E_{21}(-2)]{E_{31}(-3)}\\left[\\begin{array}{lrl}\n1 & 1 & 1 \\\\\n0 & -1 & 0 \\\\\n0 & -1 & 2\n\\end{array}\\right] \\xrightarrow[E_{32}(-1)]{\\longrightarrow}\\left[\\begin{array}{rrr}\n1 & 1 & 1 \\\\\n0 & -1 & 0 \\\\\n0 & 0 & 2\n\\end{array}\\right]\n\\]\n\nWe didn’t even finish the G-J elimination, but there’s clearly a problem. We can’t have 0 = 2. Maybe not a surprise, since we have three equations with only two unknowns!\n\n\n\nSystem is inconsistent."
  },
  {
    "objectID": "lectures/ch1_lecture2.html#consistency",
    "href": "lectures/ch1_lecture2.html#consistency",
    "title": "More Systems of Linear Equations",
    "section": "Consistency",
    "text": "Consistency\nFor an augmented matrix in RREF, if the only only nonzero entry in a row appears on the right-hand side,\n\\[\\text { Row } i \\longrightarrow\\left(\n\\begin{array}{cccccc|c}\n* & * & * & * & * & * & * \\\\\n0 & 0 & 0 & * & * & * & * \\\\\n0 & 0 & 0 & 0 & * & * & * \\\\\n0 & 0 & 0 & 0 & 0 & 0 & \\alpha \\\\\n\\bullet & \\bullet & \\bullet & \\bullet & \\bullet & \\bullet & \\bullet \\\\\n\\bullet & \\bullet & \\bullet & \\bullet & \\bullet & \\bullet & \\bullet\n\\end{array}\n\\right) \\longleftarrow \\alpha \\neq 0\n\\]\nthe \\(i^{\\text {th }}\\) equation of the associated system is\n\\[\n0 x_{1}+0 x_{2}+\\cdots+0 x_{n}=\\alpha .\n\\]\nIf \\(\\alpha\\ne 0\\), this has no solution!\nThis is an inconsistent system.\n\nNote that there is not inconsistency if there is a row of all zeros."
  },
  {
    "objectID": "lectures/ch1_lecture2.html#what-reduced-row-echelon-form-tells-us",
    "href": "lectures/ch1_lecture2.html#what-reduced-row-echelon-form-tells-us",
    "title": "More Systems of Linear Equations",
    "section": "What Reduced Row Echelon Form tells us",
    "text": "What Reduced Row Echelon Form tells us\nSuppose a matrix \\(A_{m \\times n}\\) has a reduced row echelon form \\(E_{m \\times n}\\): \\[ E_A=\\left(\\begin{array}{cccccccc}1 & * & 0 & 0 & * & * & 0 & * \\\\ 0 & 0 & 1 & 0 & * & * & 0 & * \\\\ 0 & 0 & 0 & 1 & * & * & 0 & * \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 1 & * \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\end{array}\\right)\\]\n\nWhat can we learn from the RREF of a matrix? What does it tell us? It can tell us which variables are free vs bound, and therefore how many variables are bound. (This is the rank of the matrix!)"
  },
  {
    "objectID": "lectures/ch1_lecture2.html#which-variables-are-free-and-bound",
    "href": "lectures/ch1_lecture2.html#which-variables-are-free-and-bound",
    "title": "More Systems of Linear Equations",
    "section": "Which variables are free and bound",
    "text": "Which variables are free and bound\nSuppose a matrix \\(A_{m \\times n}\\) has a reduced row echelon form \\(E_{m \\times n}\\): \\[ E_A=\\left(\\begin{array}{cccccccc}1 & * & 0 & 0 & * & * & 0 & * \\\\ 0 & 0 & 1 & 0 & * & * & 0 & * \\\\ 0 & 0 & 0 & 1 & * & * & 0 & * \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 1 & * \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\end{array}\\right)\\]\n“Basic columns” correspond to the columns in \\(A\\) which contain the pivotal positions.\nEach non-basic column can be formed as a sum of the basic columns to the left.\nBasic columns are the same in \\(E\\) and \\(A\\)!"
  },
  {
    "objectID": "lectures/ch1_lecture2.html#rank",
    "href": "lectures/ch1_lecture2.html#rank",
    "title": "More Systems of Linear Equations",
    "section": "Rank",
    "text": "Rank\nSuppose a matrix \\(A_{m \\times n}\\) has a reduced row echelon form \\(E_{m \\times n}\\): \\[ E_A=\\left(\\begin{array}{cccccccc}1 & * & 0 & 0 & * & * & 0 & * \\\\ 0 & 0 & 1 & 0 & * & * & 0 & * \\\\ 0 & 0 & 0 & 1 & * & * & 0 & * \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 1 & * \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\end{array}\\right)\\]\nrank(\\(A\\)) = # of pivots = # of nonzero rows in \\(E\\) = # bound variables in the system"
  },
  {
    "objectID": "lectures/ch1_lecture2.html#nullity",
    "href": "lectures/ch1_lecture2.html#nullity",
    "title": "More Systems of Linear Equations",
    "section": "Nullity",
    "text": "Nullity\nSuppose a matrix \\(A_{m \\times n}\\) has a reduced row echelon form \\(E_{m \\times n}\\): \\[ E_A=\\left(\\begin{array}{cccccccc}1 & * & 0 & 0 & * & * & 0 & * \\\\ 0 & 0 & 1 & 0 & * & * & 0 & * \\\\ 0 & 0 & 0 & 1 & * & * & 0 & * \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 1 & * \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\end{array}\\right)\\]\nnullity(\\(A\\)) = # of non-pivot columns = # free variables in the system"
  },
  {
    "objectID": "lectures/ch1_lecture2.html#consistency-1",
    "href": "lectures/ch1_lecture2.html#consistency-1",
    "title": "More Systems of Linear Equations",
    "section": "Consistency",
    "text": "Consistency\nEquivalent ways of saying \\(A\\) is consistent:\n\nIn row reducing \\([\\mathbf{A} \\mid \\mathbf{b}]\\), a row of the following form never appears:\n\n. . . \\[\n\\left(\\begin{array}{llll|l}\n0 & 0 & \\cdots & 0 & \\alpha \\tag{2.3.2}\n\\end{array}\\right) \\text {, where } \\alpha \\neq 0 \\text {. }\n\\]\n\n\\(\\operatorname{rank}[\\mathbf{A} \\mid \\mathbf{b}]=\\operatorname{rank}(\\mathbf{A})\\), in which case either:\n\n\\(\\operatorname{rank}[\\mathbf{A}=n\\), and system has a unique solution or\n\\(\\operatorname{rank}[\\mathbf{A}&lt;n\\), and system has infinite solutions\n\n\\(\\mathbf{b}\\) is a nonbasic column in \\([\\mathbf{A} \\mid \\mathbf{b}]\\).\n\\(\\mathbf{b}\\) is a combination of the basic columns in \\(\\mathbf{A}\\).\n\n\n\nMeyers p. 51"
  },
  {
    "objectID": "lectures/ch1_lecture2.html#homogeneous-systems",
    "href": "lectures/ch1_lecture2.html#homogeneous-systems",
    "title": "More Systems of Linear Equations",
    "section": "Homogeneous Systems",
    "text": "Homogeneous Systems\n\nThe general linear system with \\(m \\times n\\) coefficient matrix \\(A\\) and right-hand-side vector \\(\\mathbf{b}\\) is homogeneous if the entries of \\(\\mathbf{b}\\) are all zero.\nOtherwise, the system is inhomogeneous.\nHomogeneous systems are always consistent!\nCan solve by setting all variables to zero. This is the trivial solution\nIf rank of the matrix = n, there is only the trivial solution"
  },
  {
    "objectID": "lectures/ch1_lecture2.html#pagerank",
    "href": "lectures/ch1_lecture2.html#pagerank",
    "title": "More Systems of Linear Equations",
    "section": "Pagerank",
    "text": "Pagerank\nGoal: rank pages in terms of “importance”. Suppose we have four pages:\n\n\n\n\nHow can we rank these pages? Which one is most important?\n\n\n\n\nFirst try: count number of backlinks. But that can be artifically inflated (just link to a page many times), and it doesn’t care about importance of the linking page.\nSecond try: For page \\(i\\) let \\(x_{i}\\) be its score and \\(L_{i}\\) be the set of all indices of pages linking to it. Then \\[\n\\begin{equation*}\nx_{i}=\\sum_{x_{j} \\in L_{i}} x_{j}\n\\end{equation*}\n\\]\nBut it’s still easy to game this system, by setting up a page and making a ton of outgoing links to pages we want to promote.\nThird try: For page \\(j\\) let \\(n_{j}\\) be its total number of outgoing links on that page. Then score is\n\\[\n\\begin{equation*}\nx_{i}=\\sum_{x_{j} \\in L_{i}} \\frac{x_{j}}{n_{j}}\n\\end{equation*}\n\\]"
  },
  {
    "objectID": "lectures/ch1_lecture2.html#working-through-pagerank-try-3",
    "href": "lectures/ch1_lecture2.html#working-through-pagerank-try-3",
    "title": "More Systems of Linear Equations",
    "section": "Working through PageRank: try 3",
    "text": "Working through PageRank: try 3\n\n\\[\n\\begin{equation*}\nx_{i}=\\sum_{x_{j} \\in L_{i}} \\frac{x_{j}}{n_{j}}\n\\end{equation*}\n\\]\n\n\n\nWrite down the system of equations this gives us. Then make it in an augmented matrix.\n\n\n\n\n\\[\n\\begin{aligned}\n& x_{1}=\\frac{x_{2}}{1}+\\frac{x_{3}}{3} \\\\\n& x_{2}=\\frac{x_{1}}{2}+\\frac{x_{3}}{3} \\\\\n& x_{3}=\\frac{x_{1}}{2}+\\frac{x_{4}}{1} \\\\\n& x_{4}=\\frac{x_{3}}{3} .\n\\end{aligned}\n\\]\nAugmented matrix:\n\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & -1 & -1 & 0 & 0\\\\-1 & 1 & -1 & 0 & 0\\\\-1 & 0 & 1 & -1 & 0\\\\0 & 0 & -1 & 1 & 0\\end{matrix}\\right]\\)"
  },
  {
    "objectID": "lectures/ch1_lecture2.html#jupyterlab",
    "href": "lectures/ch1_lecture2.html#jupyterlab",
    "title": "More Systems of Linear Equations",
    "section": "JupyterLab",
    "text": "JupyterLab\nhttps://tinyurl.com/stat24320notebook1"
  },
  {
    "objectID": "lectures/ch1_lecture2.html#now-what-about-the-second-try",
    "href": "lectures/ch1_lecture2.html#now-what-about-the-second-try",
    "title": "More Systems of Linear Equations",
    "section": "Now what about the second try?",
    "text": "Now what about the second try?\nRemember this was our ‘second try’: \\[\n\\begin{equation*}\nx_{i}=\\sum_{x_{j} \\in L_{i}} x_{j}\n\\end{equation*}\n\\]\nThis was the system of equations. \\[\n\\begin{aligned}\n& x_{1}=x_{2}+x_{3} \\\\\n& x_{2}=x_{1}+x_{3} \\\\\n& x_{3}=x_{1}+x_{4} \\\\\n& x_{4}=x_{3} .\n\\end{aligned}\n\\]\nWhat happens if we try to solve this? Try it in Python.\n\nWhat happens is that we get only the trivial solution. This makes sense because it is a homogeneous system with full rank."
  },
  {
    "objectID": "lectures/ch1_lecture2.html#discretizing-continuous-functions",
    "href": "lectures/ch1_lecture2.html#discretizing-continuous-functions",
    "title": "More Systems of Linear Equations",
    "section": "Discretizing continuous functions",
    "text": "Discretizing continuous functions\n\nRod with fixed temperatures \\(y_{left}\\) and \\(y_{right}\\) at each end, internal heat source given as \\(f(x), 0 \\leq x \\leq 1\\)\nWhat’s the temperature at each point along the length?"
  },
  {
    "objectID": "lectures/ch1_lecture2.html#section-1",
    "href": "lectures/ch1_lecture2.html#section-1",
    "title": "More Systems of Linear Equations",
    "section": "",
    "text": "Discrete approximation to temperature function \\((n=5)\\)\n\n\n\n\n\nequally spaced points, called nodes, \\(x_{0}=0, x_{1}, x_{2}, \\ldots, x_{n+1}=1\\), distance \\(h\\) apart.\nheat on the \\(i\\) th segment \\(\\approx\\) temperature at its left node.\ntemperature is \\(y(x)\\), \\(y_{i}=y\\left(x_{i}\\right)\\).\n\n\n\n\nEquations to balance the flow of heat from each node to its neighbors:\n\\[\n\\begin{equation*}\n-y_{i-1}+2 y_{i}-y_{i+1}=\\frac{h^{2}}{K} f\\left(x_{i}\\right)\n\\end{equation*}\n\\]"
  },
  {
    "objectID": "lectures/ch1_lecture2.html#section-2",
    "href": "lectures/ch1_lecture2.html#section-2",
    "title": "More Systems of Linear Equations",
    "section": "",
    "text": "Build the matrix representing the system of equations:\n\nN=6\nM = sym.zeros(N)\nfor i in range(N):\n  if (i&gt;0):\n      M[i-1,i]=-1\n  M[i,i]=2\n  if (i&lt;N-1):\n      M[i+1,i]=-1\nM\n\n\\(\\displaystyle \\left[\\begin{matrix}2 & -1 & 0 & 0 & 0 & 0\\\\-1 & 2 & -1 & 0 & 0 & 0\\\\0 & -1 & 2 & -1 & 0 & 0\\\\0 & 0 & -1 & 2 & -1 & 0\\\\0 & 0 & 0 & -1 & 2 & -1\\\\0 & 0 & 0 & 0 & -1 & 2\\end{matrix}\\right]\\)"
  },
  {
    "objectID": "lectures/ch1_lecture2.html#section-3",
    "href": "lectures/ch1_lecture2.html#section-3",
    "title": "More Systems of Linear Equations",
    "section": "",
    "text": "\\[\n\\begin{equation*}\n-y_{i-1}+2 y_{i}-y_{i+1}=\\frac{h^{2}}{K} f\\left(x_{i}\\right)\n\\end{equation*}\n\\]\nDecide on a function for f. Let’s say \\(\\frac{h^{2}}{K} f = x_i^2\\).\nSay that x is running from 0 to 1. Then we can have\n\nimport numpy as np\nx=np.arange(0,1,1/N)\nf=x**2\nx=sym.Matrix(x)\nf=sym.Matrix(f)"
  },
  {
    "objectID": "lectures/ch1_lecture2.html#section-4",
    "href": "lectures/ch1_lecture2.html#section-4",
    "title": "More Systems of Linear Equations",
    "section": "",
    "text": "sol=M.gauss_jordan_solve(f)\nsol[0]\n\n\\(\\displaystyle \\left[\\begin{matrix}0.416666666666667\\\\0.833333333333333\\\\1.22222222222222\\\\1.5\\\\1.52777777777778\\\\1.11111111111111\\end{matrix}\\right]\\)"
  },
  {
    "objectID": "lectures/ch1_lecture2.html#now-you",
    "href": "lectures/ch1_lecture2.html#now-you",
    "title": "More Systems of Linear Equations",
    "section": "Now you",
    "text": "Now you\n\nImplement this yourself in Python\nMake a plot of the output (you’ll want to use PyPlot)\nIncrease N and make more plots\nThis was assuming the temperature at the ends was 0. How can we change this?"
  },
  {
    "objectID": "lectures/ch1_lecture2.html#turing-pattern-formation",
    "href": "lectures/ch1_lecture2.html#turing-pattern-formation",
    "title": "More Systems of Linear Equations",
    "section": "Turing pattern formation",
    "text": "Turing pattern formation\nhttps://colab.research.google.com/github/ijmbarr/turing-patterns/blob/master/turing-patterns.ipynb#scrollTo=GsEPBsz33E14\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n \n\n\n \n\n\n\n\n \n\n\n\nDiscrete approximation to temperature function \\((n=5)\\)"
  },
  {
    "objectID": "lectures/ch2_lecture2.html#definitions",
    "href": "lectures/ch2_lecture2.html#definitions",
    "title": "Ch2 Lecture 2",
    "section": "Definitions",
    "text": "Definitions\nDefinition: A discrete linear dynamical system is a sequence of vectors \\(\\mathbf{x}^{(k)}, k=0,1, \\ldots\\), called states, which is defined by an initial vector \\(\\mathbf{x}^{(0)}\\) and by the rule\n\\[\n\\mathbf{x}^{(k+1)}=A \\mathbf{x}^{(k)}+\\mathbf{b}_{k}, \\quad k=0,1, \\ldots\n\\]\n\n\n\\(A\\) is a fixed square matrix, called the transition matrix of the system\nvectors \\(\\mathbf{b}_{k}, k=0,1, \\ldots\\) are called the input vectors of the system.\nIf we don’t specify input vectors, assume that \\(\\mathbf{b}_{k}=\\mathbf{0}\\) for all \\(k\\). Then call the system a homogeneous dynamical system."
  },
  {
    "objectID": "lectures/ch2_lecture2.html#stability",
    "href": "lectures/ch2_lecture2.html#stability",
    "title": "Ch2 Lecture 2",
    "section": "Stability",
    "text": "Stability\nAn important question: is the system stable?\nDoes \\(\\mathbf{x}^{(k)}\\) tend towards a constant state \\(\\mathrm{x}\\)?\n\nIf system is homogeneous, then if a stable state is the initial state, it will equal all subsequent states.\nDefinition: A vector \\(\\mathbf{x}\\) satisfying \\(\\mathbf{x}=A \\mathbf{x}\\), for a square matrix \\(A\\), is called a stationary vector for \\(A\\).\n\n\nIf \\(A\\) is the transition matrix for a homogeneous discrete dynamical system, we also call such a vector a stationary state."
  },
  {
    "objectID": "lectures/ch2_lecture2.html#example",
    "href": "lectures/ch2_lecture2.html#example",
    "title": "Ch2 Lecture 2",
    "section": "Example",
    "text": "Example\n\nTwo toothpaste companies compete for customers in a fixed market\nEach customer uses either Brand A or Brand B.\nBuying habits. In each quarter:\n\n\\(30 \\%\\) of A users will switch to B, while the rest stay with A.\n\\(40 \\%\\) of B users will switch to A , while the the rest stay with B.\n\nThis is an example of a Markov chain model."
  },
  {
    "objectID": "lectures/ch2_lecture2.html#section",
    "href": "lectures/ch2_lecture2.html#section",
    "title": "Ch2 Lecture 2",
    "section": "",
    "text": "Let \\(a_1\\) be the fraction of customers using Brand A at the end of the first quarter, and \\(b_1\\) be the fraction using Brand B.\nThen we have the following system of equations: \\[\n\\begin{aligned}\na_{1} & =0.7 a_{0}+0.4 b_{0} \\\\\nb_{1} & =0.3 a_{0}+0.6 b_{0}\n\\end{aligned}\n\\]\n\nMore generally,\n\\[\n\\begin{aligned}\na_{k+1} & =0.7 a_{k}+0.4 b_{k} \\\\\nb_{k+1} & =0.3 a_{k}+0.6 b_{k} .\n\\end{aligned}\n\\]\n\n\nIn matrix form,\n\\[\n\\mathbf{x}^{(k)}=\\left[\\begin{array}{c}\na_{k} \\\\\nb_{k}\n\\end{array}\\right] \\text { and } A=\\left[\\begin{array}{ll}\n0.7 & 0.4 \\\\\n0.3 & 0.6\n\\end{array}\\right]\n\\]\nwith\n\\[\n\\mathbf{x}^{(k+1)}=A \\mathbf{x}^{(k)}\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture2.html#section-1",
    "href": "lectures/ch2_lecture2.html#section-1",
    "title": "Ch2 Lecture 2",
    "section": "",
    "text": "We can continue into future quarters by multiplying by \\(A\\) again: \\[\n\\mathbf{x}^{(2)}=A \\mathbf{x}^{(1)}=A \\cdot\\left(A \\mathbf{x}^{(0)}\\right)=A^{2} \\mathbf{x}^{(0)}\n\\]\n\nIn general,\n\\[\n\\mathbf{x}^{(k)}=A \\mathbf{x}^{(k-1)}=A^{2} \\mathbf{x}^{(k-2)}=\\cdots=A^{k} \\mathbf{x}^{(0)} .\n\\]\n\n\nThis is true of any homogeneous linear dynamical system!\nFor any positive integer \\(k\\) and discrete dynamical system with transition matrix \\(A\\) and initial state \\(\\mathbf{x}^{(0)}\\), the \\(k\\)-th state is given by\n\\[\n\\mathbf{x}^{(k)}=A^{k} \\mathbf{x}^{(0)}\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture2.html#distribution-vector-and-stochastic-matrix",
    "href": "lectures/ch2_lecture2.html#distribution-vector-and-stochastic-matrix",
    "title": "Ch2 Lecture 2",
    "section": "Distribution vector and stochastic matrix",
    "text": "Distribution vector and stochastic matrix\n\n\\(\\mathbf{x}^{(k)}\\) of the toothpaste example are column vectors with nonnegative coordinates that sum to 1.\nSuch vectors are called distribution vectors.\nAlso, each of the columns the matrix \\(A\\) is a distribution vector.\nA square matrix \\(A\\) whose columns are distribution vectors is called a stochastic matrix."
  },
  {
    "objectID": "lectures/ch2_lecture2.html#markov-chain-definition",
    "href": "lectures/ch2_lecture2.html#markov-chain-definition",
    "title": "Ch2 Lecture 2",
    "section": "Markov Chain definition",
    "text": "Markov Chain definition\nA Markov chain is a discrete dynamical system whose initial state \\(\\mathbf{x}^{(0)}\\) is a distribution vector and whose transition matrix \\(A\\) is stochastic, i.e., each column of \\(A\\) is a distribution vector.\n\nThink this through. - Suppose our current state is \\(\\mathbf{e}_{j}\\) - The system has selected \\(j\\) th event exclusively.\n- The next state is \\(\\mathbf{p}_{j}=P \\mathbf{e}_{j}\\), that is, the \\(j\\) th column of \\(P\\). - This implies that the entry \\(p_{i j}\\) is the probability that the \\(i\\) th event will occur, given that the \\(j\\) th event has just occurred. - Since events are mutually exclusive and some subsequent event must occur, the sum of these probabilities is 1 ."
  },
  {
    "objectID": "lectures/ch2_lecture2.html#checking-that-our-toothpase-example-is-a-markov-chain",
    "href": "lectures/ch2_lecture2.html#checking-that-our-toothpase-example-is-a-markov-chain",
    "title": "Ch2 Lecture 2",
    "section": "Checking that our toothpase example is a Markov chain",
    "text": "Checking that our toothpase example is a Markov chain\n\\[\n\\mathbf{x}^{(k)}=\\left[\\begin{array}{c}\na_{k} \\\\\nb_{k}\n\\end{array}\\right] \\text { and } A=\\left[\\begin{array}{ll}\n0.7 & 0.4 \\\\\n0.3 & 0.6\n\\end{array}\\right]\n\\]\n\n\nThe toothpaste example is a Markov chain:\n\nThe columns of \\(A\\) are distribution vectors.\nThe columns of \\(A\\) sum to 1.\nThe events in the example are mutually exclusive."
  },
  {
    "objectID": "lectures/ch2_lecture2.html#stochastic-matrix-inequality",
    "href": "lectures/ch2_lecture2.html#stochastic-matrix-inequality",
    "title": "Ch2 Lecture 2",
    "section": "Stochastic Matrix Inequality",
    "text": "Stochastic Matrix Inequality\n\nWe can define the 1-norm of a vector \\(\\mathbf{x}\\) as \\(\\|\\mathbf{x}\\|_{1}=\\sum_{i=1}^{n}\\left|x_{i}\\right|\\).\n\nIf all the elements of a vector are nonnegative, then \\(\\|\\mathbf{x}\\|_{1}\\) is the sum of the elements.\n\nCan show: For any stochastic matrix \\(P\\) and compatible vector \\(\\mathbf{x},\\|P \\mathbf{x}\\|_{1} \\leq\\|\\mathbf{x}\\|_{1}\\), with equality if the coordinates of \\(\\mathbf{x}\\) are all nonnegative.\n→ if a state in a Markov chain is a distribution vector (nonnegative entries and sums to 1), then the sum of the coordinates of the next state will also sum to one\n→ all subsequent states in a Markov chain are themselves Markov Chain State distribution vectors."
  },
  {
    "objectID": "lectures/ch2_lecture2.html#moving-into-the-future",
    "href": "lectures/ch2_lecture2.html#moving-into-the-future",
    "title": "Ch2 Lecture 2",
    "section": "Moving into the future",
    "text": "Moving into the future\nSuppose that initially Brand A has all the customers (i.e., Brand B is just entering the market). What are the market shares 2 quarters later? 20 quarters?\n\n\nInitial state vector is \\(\\mathbf{x}^{(0)}=(1,0)\\).\nNow do the arithmetic to find \\(\\mathbf{x}^{(2)}\\) : . . . \\[\n\\begin{aligned}\n{\\left[\\begin{array}{l}\na_{2} \\\\\nb_{2}\n\\end{array}\\right] } & =\\mathbf{x}^{(2)}=A^{2} \\mathbf{x}^{(0)}=\\left[\\begin{array}{ll}\n0.7 & 0.4 \\\\\n0.3 & 0.6\n\\end{array}\\right]\\left(\\left[\\begin{array}{ll}\n0.7 & 0.4 \\\\\n0.3 & 0.6\n\\end{array}\\right]\\left[\\begin{array}{l}\n1 \\\\\n0\n\\end{array}\\right]\\right) \\\\\n& =\\left[\\begin{array}{ll}\n0.7 & 0.4 \\\\\n0.3 & 0.6\n\\end{array}\\right]\\left[\\begin{array}{l}\n0.7 \\\\\n0.3\n\\end{array}\\right]=\\left[\\begin{array}{l}\n.61 \\\\\n.39\n\\end{array}\\right] .\n\\end{aligned}\n\\]\n\n\nBrand A will have \\(61 \\%\\) of the market and Brand B will have \\(39 \\%\\) of the market in the second quarter."
  },
  {
    "objectID": "lectures/ch2_lecture2.html#toothpaste-after-many-quarters",
    "href": "lectures/ch2_lecture2.html#toothpaste-after-many-quarters",
    "title": "Ch2 Lecture 2",
    "section": "Toothpaste after many quarters",
    "text": "Toothpaste after many quarters\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNow:\n\nfind the state after 2 quarters.\nfind the state after 20 quarters. Use matrix exponentiation: A**n\nGet specific numbers if toothpaste A has 100% of the market at the beginning. (Use sym.subs())\nNow what if toothpaste B has 100% at the beginning?\n\n\n\nOur calculation is reusable! Could go back after and then change the inital state.\nNotice that we also get the same result eventually, no matter where we started"
  },
  {
    "objectID": "lectures/ch2_lecture2.html#example-structured-population-model",
    "href": "lectures/ch2_lecture2.html#example-structured-population-model",
    "title": "Ch2 Lecture 2",
    "section": "Example: Structured Population Model",
    "text": "Example: Structured Population Model\n\n\n\ninsect life stages\n\n\n\nEvery week, 20% of the eggs die, and 60% move to the larva stage.\nAlso, 10% of larvae die, 60% become adults\n20% of adults die. Each adult produces 0.25 eggs.\nInitially we have 10k adults, 8k larvae, 6k eggs. How does the population evolve over time?\n\n\n\n\n\n\nSet up \\(\\mathbf{x}^{(k)}=\\left(a_{k}, b_{k}, c_{k}\\right)\\) \\(\\mathbf{x}^{(0)}=(10,8,6)\\) at week 0 Note that after first week, we have 20% of the initial eggs (20% died, 60% became larvae…) Transition matrix is: \\[\nA=\\left[\\begin{array}{ccc}\n0.2 & 0 & 0.25 \\\\\n0.6 & 0.3 & 0 \\\\\n0 & 0.6 & 0.8\n\\end{array}\\right]\n\\]\nWhat do we think will happen over time? How can we tell?"
  },
  {
    "objectID": "lectures/ch2_lecture2.html#section-2",
    "href": "lectures/ch2_lecture2.html#section-2",
    "title": "Ch2 Lecture 2",
    "section": "",
    "text": "class InsectEvolution(BaseStateSystem):\n    def __init__(self,transition_matrix=np.array([[.2,0,.25],[.6,.3,0],[0,.6,.8]]),max_y=10):\n        self.steps = 10;\n        self.t=0\n        self.transition_matrix = transition_matrix\n        self.max_y=max_y\n\n    def initialise(self):\n        self.x = np.array([6,8,10])\n        self.Ya = []\n        self.Yb = []\n        self.Yc = []\n        self.X = []\n\n    def update(self):\n        for _ in range(self.steps):\n            self.t += 1\n            self._update()\n\n    def _update(self):\n        self.x = self.transition_matrix.dot(self.x)\n\n    def draw(self, ax):\n        ax.clear()\n        self.X.append(self.t)\n        self.Ya.append(self.x[0])\n        self.Yb.append(self.x[1])\n        self.Yc.append([self.x[2]])\n        ax.plot(self.X,self.Ya, color=\"r\", label=\"Eggs\")\n        ax.plot(self.X,self.Yb, color=\"b\", label=\"Larvae\")\n        ax.plot(self.X,self.Yc, color=\"g\", label=\"Adults\")\n        ax.legend()\n\n        ax.set_ylim(0,self.max_y)\n        ax.set_xlim(0,200)\n        ax.set_title(\"t = {:.2f}\".format(self.t))"
  },
  {
    "objectID": "lectures/ch2_lecture2.html#section-3",
    "href": "lectures/ch2_lecture2.html#section-3",
    "title": "Ch2 Lecture 2",
    "section": "",
    "text": "t1=np.array([[.2,0,.25],[.6,.3,0],[0,.6,.8]])\ndying_insects = InsectEvolution(t1)\ndying_insects.plot_time_evolution(\"insects.gif\")\n\n\n\n\ndying insects\n\n\n\n\nt2=np.array([[.4,0,.45],[.5,.1,0],[0,.6,.8]])\nliving_insects = InsectEvolution(t2,200)\nliving_insects.plot_time_evolution(\"insects2.gif\")\n\n\n\n\nliving insects"
  },
  {
    "objectID": "lectures/ch2_lecture2.html#graph",
    "href": "lectures/ch2_lecture2.html#graph",
    "title": "Ch2 Lecture 2",
    "section": "Graph",
    "text": "Graph\n\nA graph is a set \\(V\\) of vertices (or nodes), together with a set or list \\(E\\) of unordered pairs with coordinates in \\(V\\), called edges.\n\nWhat are some things which can be represented with graphs?"
  },
  {
    "objectID": "lectures/ch2_lecture2.html#digraph",
    "href": "lectures/ch2_lecture2.html#digraph",
    "title": "Ch2 Lecture 2",
    "section": "Digraph",
    "text": "Digraph\n\nA directed graph (or “digraph”) has directed edges.\n\nWhat are some things which can be represented with digraphs?"
  },
  {
    "objectID": "lectures/ch2_lecture2.html#walks",
    "href": "lectures/ch2_lecture2.html#walks",
    "title": "Ch2 Lecture 2",
    "section": "Walks",
    "text": "Walks\n\nA walk is a sequence of edges \\(\\left\\{v_{0}, v_{1}\\right\\},\\left\\{v_{1}, v_{2}\\right\\}, \\ldots,\\left\\{v_{m-1}, v_{m}\\right\\}\\) that goes from vertex \\(v_{0}\\) to vertex \\(v_{m}\\).\nThe length of the walk is \\(m\\).\n\nA directed walk is a sequence of directed edges.\n\nWhat is an example of what a walk tells us? Work through from one of the examples of graph or digraph that came up.\nOne example: how many people are in your extended network. “Six degrees of separation”…"
  },
  {
    "objectID": "lectures/ch2_lecture2.html#winning-and-losing",
    "href": "lectures/ch2_lecture2.html#winning-and-losing",
    "title": "Ch2 Lecture 2",
    "section": "Winning and losing",
    "text": "Winning and losing\n\nSuppose this represents wins and losses by different teams. How can we rank the teams?\n\nIdea: a team which beats another team is good. Even better if the team they beat had beaten another team…\nIdea: count the number of walks of length 1 or 2 originating from each vertex.\nHave them do this…\nPower of 1 is 5, vertex 2 is 4, vertex 3 is 7, vertex 4 is 4, vertex 5 is 1, and the power of vertex 6 is 0."
  },
  {
    "objectID": "lectures/ch2_lecture2.html#power",
    "href": "lectures/ch2_lecture2.html#power",
    "title": "Ch2 Lecture 2",
    "section": "Power",
    "text": "Power\nVertex power: the number of walks of length 1 or 2 originating from a vertex.\n\nMakes most sense for graphs which don’t have any self-loops and at most one edge between nodes. These are called dominance directed graphs\n\n\nHow can we compute the number of walks for a given graph?"
  },
  {
    "objectID": "lectures/ch2_lecture2.html#adjacency-matrix",
    "href": "lectures/ch2_lecture2.html#adjacency-matrix",
    "title": "Ch2 Lecture 2",
    "section": "Adjacency matrix",
    "text": "Adjacency matrix\nAdjacency matrix: A square matrix whose \\((i, j)\\) th entry is the number of edges going from vertex \\(i\\) to vertex \\(j\\)\n\nEdges in non-directed graphs appear twice (at (i,j) and at (j,i))\nEdges in digraphs appear only once\n\n\nWhat is the adjacency matrix for this graph?\n\n\n\n\\[\nB=\\left[\\begin{array}{llllll}\n0 & 1 & 0 & 1 & 0 & 1 \\\\\n1 & 0 & 1 & 0 & 0 & 1 \\\\\n0 & 1 & 0 & 0 & 1 & 0 \\\\\n1 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 1 & 0 & 0 & 1 \\\\\n1 & 1 & 0 & 1 & 1 & 0\n\\end{array}\\right]\n\\]\n\nWe can reconstruct the graph entirely from this matrix! It must have all the information encapsulated in it."
  },
  {
    "objectID": "lectures/ch2_lecture2.html#section-4",
    "href": "lectures/ch2_lecture2.html#section-4",
    "title": "Ch2 Lecture 2",
    "section": "",
    "text": "What is the adjacency matrix for this graph?\n\n\n\\[\nA=\\left[\\begin{array}{llllll}\n0 & 1 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 1 & 0 & 1 \\\\\n0 & 1 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 & 0 & 0\n\\end{array}\\right]\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture2.html#computing-power-from-an-adjacencty-matrix",
    "href": "lectures/ch2_lecture2.html#computing-power-from-an-adjacencty-matrix",
    "title": "Ch2 Lecture 2",
    "section": "Computing power from an adjacencty matrix",
    "text": "Computing power from an adjacencty matrix\n\\[\nA=\\left[\\begin{array}{llllll}\n0 & 1 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 1 & 0 & 1 \\\\\n0 & 1 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 & 0 & 0\n\\end{array}\\right]\n\\]\nHow can we count the walks of length 1 emanating from vertex \\(i\\)?\n\nAnswer: Add up the elements of the \\(i\\) th row of \\(A\\)."
  },
  {
    "objectID": "lectures/ch2_lecture2.html#section-5",
    "href": "lectures/ch2_lecture2.html#section-5",
    "title": "Ch2 Lecture 2",
    "section": "",
    "text": "\\[\nA=\\left[\\begin{array}{llllll}\n0 & 1 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 1 & 0 & 1 \\\\\n0 & 1 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 & 0 & 0\n\\end{array}\\right]\n\\]\nWhat about walks of length 2?\n\nStart by finding number of walks of length 2 from vertex i to vertex j.\n\n\n\\[\na_{i 1} a_{1 j}+a_{i 2} a_{2 j}+\\cdots+a_{i n} a_{n j} .\n\\]\n\n\nThis is just the \\((i, j)\\) th entry of the matrix \\(A^{2}\\).\n\nWhen is there an edge from \\(i\\) to \\(k\\) and then from \\(k\\) to \\(j\\)?\nIf the adjacency matrix has 1’s in both \\(a_{i,k}\\) and \\(a_{k,j}\\).\nCan represent this by the product…\nSo number of paths is the sum over k’s…\n\\[\na_{i 1} a_{1 j}+a_{i 2} a_{2 j}+\\cdots+a_{i n} a_{n j} .\n\\]\nThis is just the \\((i, j)\\) th entry of the matrix \\(A^{2}\\)."
  },
  {
    "objectID": "lectures/ch2_lecture2.html#adjacency-matrix-and-power",
    "href": "lectures/ch2_lecture2.html#adjacency-matrix-and-power",
    "title": "Ch2 Lecture 2",
    "section": "Adjacency matrix and power",
    "text": "Adjacency matrix and power\nResult: The \\((i, j)\\) th entry of \\(A^{r}\\) gives the number of (directed) walks of length \\(r\\) starting at vertex \\(i\\) and ending at vertex \\(j\\).\n\nTherefore: power of the \\(i\\) th vertex is the sum of all entries in the \\(i\\) th row of the matrix \\(A+A^{2}\\)."
  },
  {
    "objectID": "lectures/ch2_lecture2.html#vertex-power-in-our-teams-example",
    "href": "lectures/ch2_lecture2.html#vertex-power-in-our-teams-example",
    "title": "Ch2 Lecture 2",
    "section": "Vertex power in our teams example",
    "text": "Vertex power in our teams example\n\\[\nA+A^{2}=\\left[\\begin{array}{llllll}\n0 & 1 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 1 & 0 & 1 \\\\\n0 & 1 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 & 0 & 0\n\\end{array}\\right]+\\left[\\begin{array}{llllll}\n0 & 1 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 1 & 0 & 1 \\\\\n0 & 1 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 & 0 & 0\n\\end{array}\\right]\n\\left[\\begin{array}{llllll}\n0 & 1 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 1 & 0 & 1 \\\\\n0 & 1 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 & 0 & 0\n\\end{array}\\right]\n\\]\n\\[=\\left[\\begin{array}{llllll}\n0 & 2 & 1 & 1 & 1 & 0 \\\\\n1 & 0 & 1 & 1 & 0 & 1 \\\\\n1 & 2 & 0 & 2 & 1 & 1 \\\\\n0 & 1 & 1 & 0 & 1 & 1 \\\\\n0 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 & 0 & 0\n\\end{array}\\right] .\n\\]\n\nThe sum of each row gives the vertex power.\n\nHow can you use multiplication to find the sum of each row?\nCan multiply by a column of ones…"
  },
  {
    "objectID": "lectures/ch2_lecture2.html#now-you-try",
    "href": "lectures/ch2_lecture2.html#now-you-try",
    "title": "Ch2 Lecture 2",
    "section": "Now you try",
    "text": "Now you try\nWorking together,\n\npick a topic (sports? elections?).\nLook up data and make a graph or digraph.\nCompute the adjacency matrix.\nFind the vertex powers.\n\nDo your results make sense in context?\nDoes it make sense?\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n \n\n\n \n\n\n\n\n \n\n\n\ninsect life stages\ndying insects\nliving insects"
  },
  {
    "objectID": "lectures/ch2_lecture4.html#idea-of-mcmc",
    "href": "lectures/ch2_lecture4.html#idea-of-mcmc",
    "title": "Ch2 Lecture 4",
    "section": "Idea of MCMC",
    "text": "Idea of MCMC\n\nSuppose there’s something we want to calculate – say the average web revenue for pages every day.\nStart out simple: suppose we have a revenue amount for each page, per user. How do we calculate the average revenue per page?\nWe can figure out how many users visit each page, and then multiply that by the revenue per user for that page.\nWe can do this by calculating the stationary distribution of the Markov chain that describes the transitions between pages.\nOr, we can do it by sampling.\nWork through the notebook to see how this works in practice.\nOK, now make it more complicated.\nWe have a formula for a given webpage that tells us how much revenue it generates given the user’s age, their gender, their location, etc.\nHow can we find the average revenue per page now?\nWe need to know the distribution of users and their characteristics. We can’t just use the stationary distribution of the Markov chain anymore.\nWe could try to calculate the distribution of users, but that’s hard.\nInstead, we can use a Markov Chain Monte Carlo method to sample from the distribution of users. We can then use these samples to estimate the average revenue per page.\nTo do Gibbs sampling, we will sample from the distribution of each variable given the other variables. This is easier than sampling from the joint distribution of all the variables.\nSo for instance, we might suppose that a user is male, 25 years old, and on the webpage for the Chicago Cubs. We can specify a distribution of locations for the user’s location given this information. Sample from that distribution.\nThen, we can say ok, we’ve got a user who is 25 years old, on the webpage for the Cubs, and in Chicago. What’s the probability that the user is male?\nGo through all the variables in turn, sampling from the distribution of each given the others, including for the webpages."
  },
  {
    "objectID": "lectures/ch2_lecture4.html#intro-to-the-idea-of-a-restricted-boltzmann-machine",
    "href": "lectures/ch2_lecture4.html#intro-to-the-idea-of-a-restricted-boltzmann-machine",
    "title": "Ch2 Lecture 4",
    "section": "Intro to the idea of a Restricted Boltzmann Machine",
    "text": "Intro to the idea of a Restricted Boltzmann Machine\n\nYou’d like to learn about what sort of objects exist in the world. You have a bunch of data, but you don’t know what the objects are. You’d like to learn about the objects and the features that describe them.\nOne way to do this is to start trying to assign labels to the objects you are seeing. You can start with random labels, and then try to improve them.\nYour goal can be that if you imagine objects given the labels, you’ll end up with a distribution that looks like the data you have.\n\n\n\nRBM schematic\n\n\nYour dreams should look like the real world.\nOK, but how do you measure how well your dreams match the real world?\nYou could calculate the probability of imagining each possible image… but that’s intractable.\nInstead, you can do a Markov Chain Monte Carlo method called Gibbs sampling. The basic idea of MCMC is that you can sample from a distribution by starting at some point and then moving around in a way that the distribution of your samples will eventually match the distribution you’re interested in.\nFor Gibbs sampling, you start with a random sample, and then you update each variable in turn, given the other variables. This may be much easier to compute than needing to know the full transition matrix for every possible pair of states. (We are in high dimensions here, because every node in our network is a variable.)\nIn the case of the RBM, you update each hidden node given the visible nodes, and then you update each visible node given the hidden nodes. You keep doing this for a while, and then you have a sample from the distribution you’re interested in.\nNow you compare this sample to your data, and you adjust the weights in your network to make the sample look more like the data."
  },
  {
    "objectID": "lectures/ch2_lecture4.html#math-of-the-rbm",
    "href": "lectures/ch2_lecture4.html#math-of-the-rbm",
    "title": "Ch2 Lecture 4",
    "section": "Math of the RBM",
    "text": "Math of the RBM\n\n\n\nStates are determined by an energy function \\(E(\\mathbf{v}, \\mathbf{h})\\). \\[\nE(\\mathbf{v}, \\mathbf{h})=-\\sum_{i} a_{i} v_{i}-\\sum_{j} b_{j} h_{j}-\\sum_{i j} v_{i} W_{i j} h_{j}\n\\]\n\n\nThen the probability distribution is given by the Boltzmann distribution:\n\\(P_{\\mathrm{rbm}}(\\mathbf{v}, \\mathbf{h})=\\frac{1}{Z} e^{-E(\\mathbf{v}, \\mathbf{h})}\\) where \\(Z=\\sum_{\\mathbf{v}, \\mathbf{h}} e^{-E(\\mathbf{v}, \\mathbf{h})}\\)\n\n\nFrom https://ml-lectures.org/docs/unsupervised_learning/ml_unsupervised-1.html"
  },
  {
    "objectID": "lectures/ch2_lecture4.html#section",
    "href": "lectures/ch2_lecture4.html#section",
    "title": "Ch2 Lecture 4",
    "section": "",
    "text": "The probability of a visible vector \\(\\mathbf{v}\\) is given by marginalizing over the hidden variables:\n\\[\n\\begin{equation*}\nP_{\\mathrm{rbm}}(\\mathbf{v})=\\sum_{\\mathbf{h}} P_{\\mathrm{rbm}}(\\mathbf{v}, \\mathbf{h})=\\frac{1}{Z} \\sum_{h} e^{-E(\\mathbf{v}, \\mathbf{h})}\n\\end{equation*}\n\\]\nConveniently, this gives each visible unit an independent probability of activation:\n\\[\nP_{\\mathrm{rbm}}\\left(v_{i}=1 | \\mathbf{h}\\right)=\\sigma\\left(a_{i}+\\sum_{j} W_{i j} h_{j}\\right), \\quad i=1, \\ldots, n_{\\mathrm{v}}\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture4.html#section-1",
    "href": "lectures/ch2_lecture4.html#section-1",
    "title": "Ch2 Lecture 4",
    "section": "",
    "text": "The same is true for hidden units, given the visible units:\n\\[\n\\begin{equation*}\nP_{\\mathrm{rbm}}\\left(h_{j}=1 | \\mathbf{v}\\right)=\\sigma\\left(b_{j}+\\sum_{i} v_{i} W_{i j}\\right) \\quad j=1, \\ldots, n_{\\mathrm{h}}\n\\end{equation*}\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture4.html#training",
    "href": "lectures/ch2_lecture4.html#training",
    "title": "Ch2 Lecture 4",
    "section": "Training",
    "text": "Training\nConsider a set of binary input data \\(\\mathbf{x}_{k}, k=1, \\ldots, M\\), drawn from a probability distribution \\(P_{\\text {data }}(\\mathbf{x})\\).\nGoal: tune the parameters \\(\\{\\mathbf{a}, \\mathbf{b}, W\\}\\) such that after training \\(P_{\\mathrm{rbm}}(\\mathbf{x}) \\approx P_{\\mathrm{data}}(\\mathbf{x})\\).\n\nTo do this, we need to be able to estimate \\(P_{\\mathrm{rbm}}\\)!\nUnfortunately, this is often intractable, because it requires calculating the partition function \\(Z\\)."
  },
  {
    "objectID": "lectures/ch2_lecture4.html#details-of-the-training",
    "href": "lectures/ch2_lecture4.html#details-of-the-training",
    "title": "Ch2 Lecture 4",
    "section": "Details of the training",
    "text": "Details of the training\nWe want to maximize the log-likelihood of the data under the model: \\[\nL(\\mathbf{a}, \\mathbf{b}, W)=-\\sum_{k=1}^{M} \\log P_{\\mathrm{rbm}}\\left(\\mathbf{x}_{k}\\right)\n\\]\n\nTake derivatives of this with respect to the parameters, and use gradient descent:\n\\[\n\\begin{equation*}\n\\frac{\\partial L(\\mathbf{a}, \\mathbf{b}, W)}{\\partial W_{i j}}=-\\sum_{k=1}^{M} \\frac{\\partial \\log P_{\\mathrm{rbm}}\\left(\\mathbf{x}_{k}\\right)}{\\partial W_{i j}}\n\\end{equation*}\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture4.html#section-2",
    "href": "lectures/ch2_lecture4.html#section-2",
    "title": "Ch2 Lecture 4",
    "section": "",
    "text": "The derivative has two terms: \\[\n\\begin{equation*}\n\\frac{\\partial \\log P_{\\mathrm{rbm}}(\\mathbf{x})}{\\partial W_{i j}}=x_{i} P_{\\mathrm{rbm}}\\left(h_{j}=1 |\\mathbf{x}\\right)-\\sum_{\\mathbf{v}} v_{i} P_{\\mathrm{rbm}}\\left(h_{j}=1 | \\mathbf{v}\\right) P_{\\mathrm{rbm}}(\\mathbf{v})\n\\end{equation*}\n\\]\n\nUse this to update the weights:\n\\[\nW_{i j} \\rightarrow W_{i j}-\\eta \\frac{\\partial L(a, b, W)}{\\partial W_{i j}}\n\\]\nProblem: the second term in the derivative is intractable! It has \\(2^{n_{\\mathrm{v}}}\\) terms:\n\\[\n\\sum_{\\mathbf{v}} v_{i} P_{\\mathrm{rbm}}\\left(h_{j}=1 | \\mathbf{v}\\right) P_{\\mathrm{rbm}}(\\mathbf{v})\n\\]\nInstead, we will use Gibbs sampling to estimate \\(P_{\\mathrm{rbm}}(\\mathbf{v})\\)."
  },
  {
    "objectID": "lectures/ch2_lecture4.html#gibbs-sampling-to-the-rescue",
    "href": "lectures/ch2_lecture4.html#gibbs-sampling-to-the-rescue",
    "title": "Ch2 Lecture 4",
    "section": "Gibbs sampling to the rescue",
    "text": "Gibbs sampling to the rescue\n\nInput: Any visible vector \\(\\mathbf{v}(0)\\)\nOutput: Visible vector \\(\\mathbf{v}(r)\\)\nfor: \\(n=1 \\backslash\\) dots \\(r\\)\n\\(\\operatorname{sample} \\mathbf{h}(n)\\) from \\(P_{\\mathrm{rbm}}(\\mathbf{h} \\mathbf{v}=\\mathbf{v}(n-1))\\)\nsample \\(\\mathbf{v}(n)\\) from \\(P_{\\mathrm{rbm}}(\\mathbf{v} \\mathbf{h}=\\mathbf{h}(n))\\) end"
  },
  {
    "objectID": "lectures/ch2_lecture4.html#using-an-rbm",
    "href": "lectures/ch2_lecture4.html#using-an-rbm",
    "title": "Ch2 Lecture 4",
    "section": "Using an RBM",
    "text": "Using an RBM"
  },
  {
    "objectID": "lectures/ch2_lecture4.html#section-3",
    "href": "lectures/ch2_lecture4.html#section-3",
    "title": "Ch2 Lecture 4",
    "section": "",
    "text": "Suppose we want to solve a nonsingular linear system \\(A x=b\\) repeatedly, with different choices of \\(b\\).\n\n\nE.g. Heat flow problem, where the right-hand side is determined by the heat source term \\(f(x)\\).\n\n\n\\[\n\\begin{equation*}\n-y_{i-1}+2 y_{i}-y_{i+1}=\\frac{h^{2}}{K} f\\left(x_{i}\\right), i=1,2, \\ldots, n\n\\end{equation*}\n\\]\n\n\nPerhaps you want to experiment with different functions for the heat source term.\n\n\nWhat do we do? Each time, we create the augmented matrix \\(\\widetilde{A}=[A \\mid b]\\), then get it into reduced row echelon form.\n\n\nEach time change \\(b\\), we have to redo all the work of Gaussian or Gauss-Jordan elimination !\n\n\nEspecially frustrating because the main part of our work is the same: putting the part of \\(\\widetilde{A}\\) corresponding to the coefficient matrix \\(A\\) into reduced row echelon form."
  },
  {
    "objectID": "lectures/ch2_lecture4.html#lu-factorization-saving-that-work",
    "href": "lectures/ch2_lecture4.html#lu-factorization-saving-that-work",
    "title": "Ch2 Lecture 4",
    "section": "LU Factorization: Saving that work",
    "text": "LU Factorization: Saving that work\nGoal: Find a way to record our work on \\(A\\), so that solving a new system involves very little additional work.\n\nLU Factorization: Let \\(A\\) be an \\(n \\times n\\) matrix. An LU factorization of \\(A\\) is a pair of \\(n \\times n\\) matrices \\(L, U\\) such that\n\n\\(L\\) is lower triangular.\n\\(U\\) is upper triangular.\n\\(A=L U\\).\n\n\n\nWhy is this so wonderful? Triangular systems \\(A \\mathbf{x}=\\mathbf{b}\\) are easy to solve.\nRemember: If \\(A\\) is upper triangular, we can solve for the last variable, then the next-to-last variable, etc."
  },
  {
    "objectID": "lectures/ch2_lecture4.html#solving-an-upper-triangular-system",
    "href": "lectures/ch2_lecture4.html#solving-an-upper-triangular-system",
    "title": "Ch2 Lecture 4",
    "section": "Solving an upper triangular system",
    "text": "Solving an upper triangular system\nLet’s say we have the following system:\n\\(A x = b\\) where A is the upper-triangular matrix \\(A = \\begin{bmatrix} 2 & 1 & 0 \\\\ 0 & 1 & -1 \\\\ 0 & 0 & -1 \\end{bmatrix}\\), and we want to solve for \\(b = \\begin{bmatrix} 1 \\\\ 1 \\\\ -2 \\end{bmatrix}\\).\n\nWe form the augmented matrix \\(\\widetilde{A} = [A | b] = \\begin{bmatrix} 2 & 1 & 0 & | & 1 \\\\ 0 & 1 & -1 & | & 1 \\\\ 0 & 0 & -1 & | & -2 \\end{bmatrix}\\).\n\n\nBack substitution:\n\nLast equation: \\(-x_3 = -2\\), so \\(x_3 = 2\\).\nSubstitute this value into the second equation, \\(x_2 - x_3 = 1\\), so \\(x_2 = 3\\).\nFinally, we substitute \\(x_2\\) and \\(x_3\\) into the first equation, \\(2x_1 + x_2 = 1\\), so \\(x_1 = -1\\)."
  },
  {
    "objectID": "lectures/ch2_lecture4.html#solving-a-lower-triangular-system",
    "href": "lectures/ch2_lecture4.html#solving-a-lower-triangular-system",
    "title": "Ch2 Lecture 4",
    "section": "Solving a lower triangular system",
    "text": "Solving a lower triangular system\nIf \\(A\\) is lower triangular, we can solve for the first variable, then the second variable, etc.\nLet’s say we have the following system:\n\\(A y = b\\) where A is the lower-triangular matrix \\(A = \\begin{bmatrix} 1 & 0 & 0 \\\\ -1 & 1 & 0 \\\\ 1 & 2 & 1 \\end{bmatrix}\\), and we want to solve for \\(b = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix}\\).\n\nWe form the augmented matrix \\(\\widetilde{A} = [A | b] = \\begin{bmatrix} 1 & 0 & 0 & | & 1 \\\\ -1 & 1 & 0 & | & 0 \\\\ 1 & 2 & 1 & | & 1 \\end{bmatrix}\\).\n\n\nForward substitution:\n\nFirst equation: \\(y_1 = 1\\).\nSubstitute this value into the second equation, \\(-y_1 + y_2 = 0\\), so \\(y_2 = 1\\).\nFinally, we substitute \\(y_1\\) and \\(y_2\\) into the third equation, \\(y_1 + 2y_2 + y_3 = 1\\), so \\(y_3 = -2\\).\n\n\n\nThis was just as easy as solving the upper triangular system!"
  },
  {
    "objectID": "lectures/ch2_lecture4.html#solving-a-x-b-with-lu-factorization",
    "href": "lectures/ch2_lecture4.html#solving-a-x-b-with-lu-factorization",
    "title": "Ch2 Lecture 4",
    "section": "Solving \\(A x = b\\) with LU factorization",
    "text": "Solving \\(A x = b\\) with LU factorization\nNow suppose we want to solve \\(A x = b\\) and we know that \\(A = L U\\). The original system becomes \\(L U x = b\\).\nIntroduce an intermediate variable \\(y = U x\\). Our system is now \\(L y = b\\). Now perform these steps:\n\nForward solve: Solve lower triangular system \\(L y = b\\) for the variable \\(y\\).\nBack solve: Solve upper triangular system \\(U x = y\\) for the variable \\(x\\).\nThis does it!\n\n\nOnce we have the matrices \\(L, U\\), the right-hand sides only come when solving the two triangular systems. Easy!"
  },
  {
    "objectID": "lectures/ch2_lecture4.html#example",
    "href": "lectures/ch2_lecture4.html#example",
    "title": "Ch2 Lecture 4",
    "section": "Example",
    "text": "Example\nYou are given that\n\\[\nA=\\left[\\begin{array}{rrr}\n2 & 1 & 0 \\\\\n-2 & 0 & -1 \\\\\n2 & 3 & -3\n\\end{array}\\right]=\\left[\\begin{array}{rrr}\n1 & 0 & 0 \\\\\n-1 & 1 & 0 \\\\\n1 & 2 & 1\n\\end{array}\\right]\\left[\\begin{array}{rrr}\n2 & 1 & 0 \\\\\n0 & 1 & -1 \\\\\n0 & 0 & -1\n\\end{array}\\right] .\n\\]\nSolve this system for \\(\\mathbf{b} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix}\\).\n\nSay that y = Ux.\n\n\n\n\n\nForward solve: \\[\n\\left[\\begin{array}{rrr}\n1 & 0 & 0 \\\\\n-1 & 1 & 0 \\\\\n1 & 2 & 1\n\\end{array}\\right]\\left[\\begin{array}{l}\ny_{1} \\\\\ny_{2} \\\\\ny_{3}\n\\end{array}\\right]=\\left[\\begin{array}{l}\n1 \\\\\n0 \\\\\n1\n\\end{array}\\right]\n\\]\n\\(y_{1}=1\\), then \\(y_{2}=0+1 y_{1}=1\\), then \\(y_{3}=1-1 y_{1}-2 y_{2}=-2\\).\n\n\n\nBack solve:\n\\[\n\\left[\\begin{array}{rrr}\n2 & 1 & 0 \\\\\n0 & 1 & -1 \\\\\n0 & 0 & -1\n\\end{array}\\right]\\left[\\begin{array}{l}\nx_{1} \\\\\nx_{2} \\\\\nx_{3}\n\\end{array}\\right]=\\left[\\begin{array}{r}\n1 \\\\\n1 \\\\\n-2\n\\end{array}\\right]\n\\]\n\\(x_{3}=-2 /(-1)=2\\), then \\(x_{2}=1+x_{3}=3\\), then \\(x_{1}=\\left(1-1 x_{2}\\right) / 2=-1\\)."
  },
  {
    "objectID": "lectures/ch2_lecture4.html#when-we-can-do-lu-factorization",
    "href": "lectures/ch2_lecture4.html#when-we-can-do-lu-factorization",
    "title": "Ch2 Lecture 4",
    "section": "When we can do LU factorization",
    "text": "When we can do LU factorization\n\nNot all square matrices have LU factorizations! This one doesn’t: \\(\\left[\\begin{array}{ll}0 & 1 \\\\ 1 & 0\\end{array}\\right]\\)\nIf Gaussian elimination can be performed on the matrix \\(A\\) without row exchanges, then the factorization exists\n\n(it’s really a by-product of Gaussian elimination.)\n\nIf row exchanges are needed, there is still a factorization that will work, but it’s a bit more complicated."
  },
  {
    "objectID": "lectures/ch2_lecture4.html#intuition-behind-lu-factorization",
    "href": "lectures/ch2_lecture4.html#intuition-behind-lu-factorization",
    "title": "Ch2 Lecture 4",
    "section": "Intuition behind LU factorization",
    "text": "Intuition behind LU factorization\n\nWhen we do Gaussian elimination, we are multiplying \\(A\\) by a series of elementary matrices to get it into upper triangular form.\nCall the product of all those elementary matrices \\(\\tilde L\\).\nSo our original system \\(A x = b\\) becomes \\(\\tilde L A x = \\tilde L b\\).\nWe recognize that \\(\\tilde L A\\) will be an upper triangular matrix, because that’s what we get when we do Gaussian elimination. This is our \\(U\\).\nBut what about the other part? We can multiply both sides by the inverse of \\(\\tilde L\\) to get\n\\[\n\\begin{aligned}\n\\tilde{L}^{-1} \\tilde{L} A x &= \\tilde{L}^{-1} \\tilde L b \\\\\n\\tilde{L}^{-1} U x &= b\n\\end{aligned}\n\\]\nNow the challenge is simply to find the inverse of \\(\\tilde L\\).\nRemember that \\(\\tilde L\\) is the product of the elementary matrices that we used when doing Gaussian elimination – call them \\(E_1\\), \\(E_2\\), etc.\nSo \\(\\tilde L = E_n E_{n-1} \\ldots E_1\\).\nThe inverse of a product of matrices is the product of the inverses in reverse order: $(E_n E_{n-1} E_1)^{-1} = E_1^{-1} E_2^{-1} E_n^{-1} $.\nFortunately, the inverse of an elementary matrix is easy to find: it’s just the same matrix with the opposite sign on the entry that was used to eliminate. So if we have the matrix which will add twice the first row to the second row, its inverse will be the matrix which will subtract twice the first row from the second row.\n\\[\nE = \\begin{bmatrix} 1 & 0 & 0 \\\\ 2 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\quad E^{-1} = \\begin{bmatrix} 1 & 0 & 0 \\\\ -2 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\n\\]\nAs it happens, when we follow the steps of Gaussian elimination (with no row swaps), the product of all these inverse matrices is just a lower triangular matrix, with each entry equal to the negative of the multiplier we used in doing the associated elimination step.\nThis is our \\(L\\)!\nSo all we need to do is keep track of these multipliers as we do Gaussian elimination, and we can use them to find the inverse of \\(\\tilde L\\). If we are doing things by hand, we can even write them into the lower part of our matrix as we go, since that part will be zeroed out anyways."
  },
  {
    "objectID": "lectures/ch2_lecture4.html#example-1",
    "href": "lectures/ch2_lecture4.html#example-1",
    "title": "Ch2 Lecture 4",
    "section": "Example",
    "text": "Example\nHere we do Gaussian elimination on the matrix \\(A = \\begin{bmatrix} 2 & 1 & 0 \\\\ -2 & 0 & -1 \\\\ 2 & 3 & -3 \\end{bmatrix}\\):\n\\(\\left[\\begin{smallmatrix}2 & 1 & 0\\\\-2 & 0 & -1\\\\2 & 3 & -3\\end{smallmatrix}\\right]\\) \\(\\xrightarrow[E_{31}(-1)]{E_{21}(1)}\\) \\(\\left[\\begin{smallmatrix}2 & 1 & 0\\\\0 & 1 & -1\\\\0 & 2 & -3\\end{smallmatrix}\\right]\\) \\(\\xrightarrow[E_{32}(-2)]{\\longrightarrow}\\) \\(\\left[\\begin{smallmatrix}2 & 1 & 0\\\\0 & 1 & -1\\\\0 & 0 & -1\\end{smallmatrix}\\right]\\)\n\n\n\nLet’s put those elementary row operations into matrix form. There were three of them:\n\n\\(E_{21}(1)\\) : \\(\\begin{bmatrix} 1 & 0 & 0 \\\\ 1 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\)\n\\(E_{31}(-1)\\): \\(\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ -1 & 0 & 1 \\end{bmatrix}\\)\n\\(E_{32}(-2)\\): \\(\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & -2 & 1 \\end{bmatrix}\\)\n\n\n\n\nThe inverses of these matrices are\n\n\\(\\begin{bmatrix} 1 & 0 & 0 \\\\ -1 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\), \\(\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 1 \\end{bmatrix}\\), and \\(\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 2 & 1 \\end{bmatrix}\\)."
  },
  {
    "objectID": "lectures/ch2_lecture4.html#section-4",
    "href": "lectures/ch2_lecture4.html#section-4",
    "title": "Ch2 Lecture 4",
    "section": "",
    "text": "The product of all these matrices is\n\\[\n\\left[\\begin{matrix}1 & 0 & 0\\\\0 & 1 & 0\\\\-1 & 0 & 1\\end{matrix}\\right]\\left[\\begin{matrix}1 & 0 & 0\\\\1 & 1 & 0\\\\0 & 0 & 1\\end{matrix}\\right]\\left[\\begin{matrix}1 & 0 & 0\\\\0 & 1 & 0\\\\0 & -2 & 1\\end{matrix}\\right]=\\left[\\begin{matrix}1 & 0 & 0\\\\1 & 1 & 0\\\\-1 & -2 & 1\\end{matrix}\\right]\n\\]\nThis is a lower triangular matrix, and it is the inverse of the matrix we used to do Gaussian elimination.\nWe can also see that the entries below the diagonal are the negatives of the multipliers we used in the elimination steps."
  },
  {
    "objectID": "lectures/ch2_lecture4.html#steps-to-lu-factorization",
    "href": "lectures/ch2_lecture4.html#steps-to-lu-factorization",
    "title": "Ch2 Lecture 4",
    "section": "Steps to LU factorization",
    "text": "Steps to LU factorization\nLet \\(\\left[a_{i j}^{(k)}\\right]\\) be the matrix obtained from \\(A\\) after using the \\(k\\) th pivot to clear out entries below it.\n\n(The original matrix is \\(A=\\left[a_{i j}^{(0)}\\right]\\))\n\n\nAll the row operations we will use include ratios \\(\\left(-a_{i j} / a_{j j}\\right)\\).\n\n\nThe row-adding elementary operations are of the form\n\\(E_{i j}\\left(-a_{i j}^{(k)} / a_{j j}^{(k)}\\right)\\)\n\n\nWe can give these ratios a name: multipliers.\n\\(m_{i j}=-a_{i j}^{(k)} / a_{j j}^{(k)}\\), where \\(i&gt;j\\)\n\n\n\nIf Gaussian elimination is used without row exchanges on the nonsingular matrix \\(A\\), resulting in the upper triangular matrix \\(U\\), and if \\(L\\) is the unit lower triangular matrix whose entries below the diagonal are the negatives of the multipliers \\(m_{i j}\\), then \\(A=L U\\)."
  },
  {
    "objectID": "lectures/ch2_lecture4.html#storing-the-multipliers-as-we-go",
    "href": "lectures/ch2_lecture4.html#storing-the-multipliers-as-we-go",
    "title": "Ch2 Lecture 4",
    "section": "Storing the multipliers as we go",
    "text": "Storing the multipliers as we go\nFor efficiency, we can just “store” the multipliers in the lower triangular part of the matrix on the left as we go along, since that will be zero anyways.\n\\[\n\\left[\\begin{array}{rrr}\n(2) & 1 & 0 \\\\\n-2 & 0 & -1 \\\\\n2 & 3 & -3\n\\end{array}\\right] \\xrightarrow[E_{31}(-1)]{E_{21}(1)}\\left[\\begin{array}{rrr}\n2 & 1 & 0 \\\\\n-1 & (1) & -1 \\\\\n1 & 2 & -3\n\\end{array}\\right] \\xrightarrow[E_{32}(-2)]{\\longrightarrow}\\left[\\begin{array}{rrr}\n2 & 1 & 0 \\\\\n-1 & 1 & -1 \\\\\n1 & 2 & -1\n\\end{array}\\right] .\n\\]\n\n\ncircle the multipliers as we go along\n\nNow we read off the results from the final matrix:\n\\[\nL=\\left[\\begin{array}{rrr}\n1 & 0 & 0 \\\\\n1 & 1 & 0 \\\\\n-1 & 2 & 1\n\\end{array}\\right] \\text { and } U=\\left[\\begin{array}{rrr}\n2 & 1 & 0 \\\\\n0 & 1 & -1 \\\\\n0 & 0 & -1\n\\end{array}\\right]\n\\]"
  },
  {
    "objectID": "lectures/ch2_lecture4.html#superaugmented-matrix",
    "href": "lectures/ch2_lecture4.html#superaugmented-matrix",
    "title": "Ch2 Lecture 4",
    "section": "Superaugmented matrix",
    "text": "Superaugmented matrix\nCould we just keep track by using the superaugmented matrix, like we did last lecture? What would that look like?\npause\n\n\\(\\left[\\begin{smallmatrix}2 & 1 & 0 & 1 & 0 & 0\\\\-2 & 0 & -1 & 0 & 1 & 0\\\\2 & 3 & -3 & 0 & 0 & 1\\end{smallmatrix}\\right]\\) \\(\\xrightarrow[E_{31}(-1)]{E_{21}(1)}\\) \\(\\left[\\begin{smallmatrix}2 & 1 & 0 & 1 & 0 & 0\\\\0 & 1 & -1 & 1 & 1 & 0\\\\0 & 2 & -3 & -1 & 0 & 1\\end{smallmatrix}\\right]\\) \\(\\xrightarrow[E_{32}(-2)]{\\longrightarrow}\\) \\(\\left[\\begin{smallmatrix}2 & 1 & 0 & 1 & 0 & 0\\\\0 & 1 & -1 & 1 & 1 & 0\\\\0 & 0 & -1 & -3 & -2 & 1\\end{smallmatrix}\\right]\\)\n\n\nOur superaugmented matrix does become an upper triangular matrix on the left and a lower triangular matrix on the right.\nUnfortunately, the lower triangular matrix on the right is \\(\\tilde{L}^{-1}\\), not \\(\\tilde{L}\\).\nSo we can’t just read off \\(L\\) and \\(U\\) from the superaugmented matrix."
  },
  {
    "objectID": "lectures/ch2_lecture4.html#plu-factorization",
    "href": "lectures/ch2_lecture4.html#plu-factorization",
    "title": "Ch2 Lecture 4",
    "section": "PLU factorization",
    "text": "PLU factorization\nWhat if we need row exchanges?\n\nWe could start off by doing all the row-exchanging elementary operations that we need, and store the product of these row-exchanging matrices as a matrix \\(P\\).\nThis product is called a permutation matrix\nApplying the correct permuatation matrix \\(P\\) to \\(A\\), we get a matrix for which Gaussian elimination will succeed without further row exchanges.\n\n\nNow we have a theorem that applies to all nonsingular matrices:\n\nIf \\(A\\) is a nonsingular matrix, then there exists a permutation matrix \\(P\\), upper triangular matrix \\(U\\), and unit lower triangular matrix \\(L\\) such that \\(P A=L U\\).\n\n\n\nSo, if you’ve got a nonsingular matrix \\(A\\), you can always find a permutation matrix \\(P\\), an upper triangular matrix \\(U\\), and a unit lower triangular matrix \\(L\\) that satisfy \\(P A=L U\\). Pretty neat, huh?\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n \n\n\n \n\n\n\n\n \n\n\n\nRBM schematic"
  }
]