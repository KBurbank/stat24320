---
jupyter: python3
---

```{python}
import networkx as nx
import numpy as np
import matplotlib.pyplot as plt
```

**Note: Project 1 was a joint effor by me (Hanyan Cai) AND Bobby Buyalos. We worked extensively on the discussions of the theories behind Project 1: Prologue and its three parts. Project 2 and 3 was completed independently by me with discussions with Bobby**

# Modeling with Directed Graphs

We are given that the digraph $G$ has vertex set $V = \{1, 2,3, 4,5,6\}$ and an edge set $E$. Let us first implement this information with networkx.

```{python}
#| colab: {base_uri: 'https://localhost:8080/', height: 534}
# Coding in edge and node information for the digraph G
graph = nx.DiGraph()
graph.add_nodes_from([1,2,3,4,5,6])
edges = [(1,2), (2,3), (3,4), (4,2), (1,4), (3,1),(3,6), (6,3), (4,5), (5,6)]
graph.add_edges_from(edges)

# Plotting G
nx.draw_circular(graph, with_labels=True)
plt.show()

# Getting the incidence matrix
print("The incidence matrix: ")
incidence = nx.incidence_matrix(graph, oriented=True).todense(
)
```

Great, we have the matrix $\begin{bmatrix}
 -1.00 & -1.00 &  0.00 &  0.00 &  1.00 &  0.00 &  0.00 &  0.00 &  0.00 &  0.00\\
  1.00 &  0.00 & -1.00 &  0.00 &  0.00 &  0.00 &  1.00 &  0.00 &  0.00 &  0.00\\
  0.00 &  0.00 &  1.00 & -1.00 & -1.00 & -1.00 &  0.00 &  0.00 &  0.00 &  1.00\\
  0.00 &  1.00 &  0.00 &  1.00 &  0.00 &  0.00 & -1.00 & -1.00 &  0.00 &  0.00\\
  0.00 &  0.00 &  0.00 &  0.00 &  0.00 &  0.00 &  0.00 &  1.00 & -1.00 &  0.00\\
  0.00 &  0.00 &  0.00 &  0.00 &  0.00 &  1.00 &  0.00 &  0.00 &  1.00 & -1.00
\end{bmatrix}$ as our incidence matrix.

Once we put this into reduced echelon form and solve it as a homogenous system of equations, we get:
$\begin{align*}
x_1 &= x_4 + x_5 - x_7 - x_9 \\
x_2 &= -x_4 + x_7 + x_9 \\
x_3 &= x_4 + x_5 - x_9 \\
x_6 &= -x_9 + x_{10} \\
x_8 &= x_9
\end{align*}$.

The basis of the nullspace is then $x_4\begin{bmatrix}
1 \\ -1 \\ 1 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\0 \\0
\end{bmatrix}  + x_5\begin{bmatrix}
1 \\ 0 \\ 1 \\ 0 \\ 1 \\ 0 \\ 0 \\0\\0\\0\end{bmatrix} + x_7\begin{bmatrix}
-1 \\ 1\\0\\0\\0\\0\\1\\0\\0\\0 \end{bmatrix} + x_9\begin{bmatrix} -1\\1\\-1\\0\\0\\-1\\0\\1\\1\\0\end{bmatrix} + x_{10}\begin{bmatrix} 0\\0\\0\\0\\0\\1\\0\\0\\0\\1\end{bmatrix}$. These vectors form the basis of directed loops.

This means that any vector within the span of these column vectors will be in the kernel. So when applied to the incidence matrix, the resulting vector will be 0. However, we have to careful because in our graph language, having -1 components in the basis of the nullspace means trouble. For example, suppose $x_4 = 1$ and the rest of the coefficients are 0. Then, even though the incidence matrix times the resulting linear combination vector is 0, the meaning of the operation is that we have a subtraction of an edge, which may interpretted as a reversal of the edge direction. Therefore, we can restrict ourselves to only the elements in this nullspace such that all components are positive.

## Part 1
For nodes $1, \ldots, 6$, let $x_1, \ldots, x_6$ represent the potentials for these nodes. Then, for an edge between node $i$ and node $j,$ we have that $x_j - x_i$ represents the potential of that edge.

Let's do this for each edge in our graph:

\begin{align*}
    &x_2 - x_1 \\
    &x_3 - x_2 \\
    &x_4 - x_3\\
    &x_2 - x_4\\
    &x_4 - x_1\\
    &x_1 - x_3\\
    &x_6 - x_3\\
    &x_3 - x_6 \\
    &x_5 - x_4 \\
    &x_6 - x_5
\end{align*}

We can then put this in a matrix $A$, and have that


A = \begin{bmatrix}
-1 & 1 & 0 & 0 & 0 & 0 \\
0 & -1 & 1 & 0 & 0 & 0 \\
0 & 0 & -1 & 1 & 0 & 0 \\
0 & 1 & 0 & -1 & 0 & 0 \\
-1 & 0 & 0 & 1 & 0 & 0 \\
1 & 0 & -1 & 0 & 0 & 0 \\
0 & 0 & -1 & 0 & 0 & 1 \\
0 & 0 & 1 & 0 & 0 & -1 \\
0 & 0 & 0 & -1 & 1 & 0 \\
0 & 0 & 0 & 0 & -1 & 1 \\
\end{bmatrix}
x = \begin{bmatrix}
x_1 \\
x_2 \\
x_3 \\
x_4 \\
x_5 \\
x_6 \\
\end{bmatrix}
b = \begin{bmatrix}
x_2 - x_1 \\
x_3 - x_2 \\
x_4 - x_3 \\
x_2 - x_4 \\
x_4 - x_1 \\
x_1 - x_3 \\
x_6 - x_3 \\
x_3 - x_6 \\
x_5 - x_4 \\
x_6 - x_5 \\
\end{bmatrix}

Call this system $Ax = b$.

Now, here is the crucial observation: $A^T$ is the incidence matrix of our graph! The transpose is


\[
\begin{bmatrix}
-1 & 0 & 0 & 0 & -1 & 1 & 0 & 0 & 0 & 0 \\
1 & -1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 1 & -1 & 0 & 0 & -1 & -1 & 1 & 0 & 0 \\
0 & 0 & 1 & -1 & 1 & 0 & 0 & 0 & -1 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & -1 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\
\end{bmatrix}
\]

Here, every column represents an edge, and every row represents a node!

Let $b = b_1, \ldots, b_{10}$. Our condition for b is that it is consistent with $Ax = b,$ with our $A, x, b$ defined above. If this is the case, then we know that there exists a $y\in \mathcal{N}(A^T)$ such that $y\cdot b = 0$.

Since $A^T$ is the incidence matrix of our graph, then we have that $y$ is a loop of our graph! And, the $i-th$ edge of our graph is represented by the $i-th$ column of $A^T$ and the $i-th$ row of $A$.

Observe that $y\cdot b = y_1b_1 + \ldots + y_{10}b_{10} = y_1(x_2 - x_1) + \ldots + y_{10}(x_6 - x_5) = 0$. But, we can think of this as $y_i$ as the coefficient applied to the $i-th$ edge, $b_i$. So, we know we have a loop (since $y$ is a loop) and the sums of our potentials on the loop y are zero (since $y \cdot b = 0)$, thus $b$ is a vector of potential differences!

::: comments
Very nicely put!
:::

## Part 2
How should we characterize Kirchoff's Loop Rule for Currents constraint? Let us first find a way to characterize the summation of each nodes. Recall our incidence matrix: $\begin{bmatrix}
 -1.00 & -1.00 &  0.00 &  0.00 &  1.00 &  0.00 &  0.00 &  0.00 &  0.00 &  0.00\\
  1.00 &  0.00 & -1.00 &  0.00 &  0.00 &  0.00 &  1.00 &  0.00 &  0.00 &  0.00\\
  0.00 &  0.00 &  1.00 & -1.00 & -1.00 & -1.00 &  0.00 &  0.00 &  0.00 &  1.00\\
  0.00 &  1.00 &  0.00 &  1.00 &  0.00 &  0.00 & -1.00 & -1.00 &  0.00 &  0.00\\
  0.00 &  0.00 &  0.00 &  0.00 &  0.00 &  0.00 &  0.00 &  1.00 & -1.00 &  0.00\\
  0.00 &  0.00 &  0.00 &  0.00 &  0.00 &  1.00 &  0.00 &  0.00 &  1.00 & -1.00
\end{bmatrix}$.

A given row corresponds to a given node. This node could be a resistor or some set junction of the circuit. For this row representing the node, $-1$ represents an outgoing wire connection and $1$ represents an ingoing wire connection. For example, consider row 1 which corresponds to node 1. We can see that there are two outgoing wire connections from node 1 to node 2 and node 4, while we have one incoming wire connection to node 1 from node 3.

```{python}
from pip._internal import main as pipmain
pipmain(['install', 'google.colab'])
```

Now, consider the vector $d = \begin{bmatrix} d_1 \\ \vdots \\ d_i \\ \vdots \\ d_n\end{bmatrix}$, where $d_i = $ weight of edge $i$, $n = $ number of edges.

The matrix product of row 1 with $d$ gives us the sum of all weighted incoming and outgoing edges. So in our example, we consider: $\begin{bmatrix} -1.00 & -1.00 &  0.00 &  0.00 &  1.00 &  0.00 &  0.00 &  0.00 &  0.00 &  0.00 \end{bmatrix} \begin{bmatrix} d_1 \\ \vdots \\ d_i \\ \vdots \\ d_n\end{bmatrix} = -d_1 -d_2 + d_4$. Since we are viewing the weights for the edges as the current that corresponds to the edge (wire), this product gives us the total change in current through node 1! Kirchoff's Loop Rule for Current states that the total flow of current through any node has to be 0, which is equivalent to the condition that the incidence matrix $A^T$ times $d$: $A^Td = 0$! Therefore, any weight vector $d$ constructed from the definition above must be in the nullspace of $A^T$. Wow! Therefore, let us recall that $N(A^T) = $ Span$\left\{\begin{bmatrix}
1 \\ -1 \\ 1 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\0 \\0
\end{bmatrix}, \begin{bmatrix}
1 \\ 0 \\ 1 \\ 0 \\ 1 \\ 0 \\ 0 \\0\\0\\0\end{bmatrix}, \begin{bmatrix}
-1 \\ 1\\0\\0\\0\\0\\1\\0\\0\\0 \end{bmatrix}, \begin{bmatrix} -1\\1\\-1\\0\\0\\-1\\0\\1\\1\\0\end{bmatrix}, \begin{bmatrix} 0\\0\\0\\0\\0\\1\\0\\0\\0\\1\end{bmatrix}\right\}$.

So the weight vector must be an element in this span in order for the Kirchoff's Law to be satisfied. For example, using this requirement, any basis vector in the above span and their scalar products are valid weights. The vector $\begin{bmatrix}
1 \\ -1 \\ 1 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\0 \\0
\end{bmatrix} + \begin{bmatrix}
1 \\ 0 \\ 1 \\ 0 \\ 1 \\ 0 \\ 0 \\0\\0\\0\end{bmatrix} + \begin{bmatrix}
-1 \\ 1\\0\\0\\0\\0\\1\\0\\0\\0 \end{bmatrix} + \begin{bmatrix} -1\\1\\-1\\0\\0\\-1\\0\\1\\1\\0\end{bmatrix} + \begin{bmatrix} 0\\0\\0\\0\\0\\1\\0\\0\\0\\1\end{bmatrix}$

## Part 3
We need to first clarify what we mean when we talk about a basis of directed loops. Since the problem does not expand in detail about the definition, we will find a clever way to characterize this basis without breaking the rules. Suppose we have a digraph and we have $n$ directed loops that form a basis. Let us denote this basis by $\{b_1, \dots, b_n\}$, where each $b_i$ will be the directed loop vector that is linearly independent from $b_j$, where $i,j \in \{1, \dots, n\}$.

Now, we aren't given any more information about this, so we think it is fair to have some leeway in expanding on the definition of any $b_i$. Since $b_i$ is a directed loop, we claim that it is fair for us to know what edges this directed loop contains. For any $i \in \{1, \dots, n\}$, $b_i = \{ e_{i_1}, \dots, e_{i_k}\}$ for some finite index sequence $i_1, \dots, i_k$, where $e_{i_l}$ is some edge vector for the graph (Remember, an arbitrary edge vector for the graph is simply an arbitrary column vector in the incidence matrix of the graph). This is a fair expansion to the assumption that was given because we require no information other than the actual edges that compose the directed loop. It is also clear that the linear independence of the directed loops within the basis is preserved by our expansion of definition.

Now, let node $a$ and node $b$ of the graph be given. We want to know whether these two nodes can communicate with each other, i.e, whether these two nodes are in a loop.

Consider the set $D = \{e \text{ a column vector of the graph incidence matrix :} e \in  b_i = \{ e_{i_1}, \dots, e_{i_k}\} \text{for some } i \in \{1, \dots, n\} \}$. The $Span(D)$ is defined because these column vectors have to be linearly independent by zero-nonzero observations.

Define $e^* $ as the edge vector from node $a$ to node $b$. For example, if we have a graph with node $a$, $b$, $c$ as row $1, 2, 3$ of the incidence matrix respectively, $e^* = \begin{bmatrix} -1 \\ 1 \\ 0  \end{bmatrix}$.

**Theorem: If $e^* \in Span(D)$ if and only if node $a$ and node $b$ is in a loop of the graph.**

A formal proof will not be given. A sketch is as follows:

The edge vector $e^*$ denotes a path from node $a$ to node $b$. If $e^* \in Span(D)$, then the path from node $a$ to node $b$ exists by a combination of paths that lie in arbitrary number of loops in the graph. The path from node $b$ to node $a$ also has to exists as we simply multiply the linear combination of the paths in $D$ that make up $e^*$ by $-1$, which gives us the edge vector from node $b$ to node $a$. Therefore, we have found a loop that connects node $a$ and node $b$.

Now, suppose node $a$ and node $b$ is in a loop of the graph. Then there exists a linear combination of directed loop basis elements that equals the loop that makes up node $a$ and node $b$. It quickly follows that there is a linear combination of elements in $D$ that equals both the path from node $a$ to node $b$ and the path from node $b$ to node $a$.

Therefore, we have found a way to determine whether two nodes can communicate with each other. That is, they can talk with each other as long as a path between the two nodes is in the column space of $D$. To determine this, we simply build $D$ matrix and solve for the system $Dx = b$, where $b$ denotes the incidence column vector/edge vector that denotes a path between the two nodes. If there is a solution, then the two nodes can talk with each other. If not, then they cannot.

Let us test this theorem with a real example.

```{python}
#| colab: {base_uri: 'https://localhost:8080/', height: 516}
V = [1, 2, 3, 4, 5, 6, 7]
E = [(1, 2), (2, 3), (3,1), (1,4), (4,5), (5,1), (6,7)]

#construct graph
G = nx.DiGraph()
G.add_nodes_from(V)
G.add_edges_from(E)

# Draw the graph
nx.draw_circular(G, with_labels=True)
plt.show()
```

Now, we can create a basis of directed loops for this graph in the following way:

$$
\left\{
\left\{
\begin{pmatrix}
-1 \\
1 \\
0 \\
0 \\
0 \\
0 \\
0
\end{pmatrix},
\begin{pmatrix}
0 \\
-1 \\
1 \\
0 \\
0 \\
0 \\
0
\end{pmatrix},
\begin{pmatrix}
1 \\
0 \\
-1 \\
0 \\
0 \\
0 \\
0
\end{pmatrix}
\right\},
\left\{
\begin{pmatrix}
-1 \\
0 \\
0 \\
1 \\
0 \\
0 \\
0
\end{pmatrix},
\begin{pmatrix}
0 \\
0 \\
0 \\
-1 \\
1 \\
0 \\
0
\end{pmatrix},
\begin{pmatrix}
1 \\
0 \\
0 \\
0 \\
-1 \\
0 \\
0  
\end{pmatrix}
\right\}
\right\}
$$



This will produce the vector enclosed in curly braces.

Now, we want to see if this works! Lets first test if our method will work. Lets see if it can detect if node 5 and 2 are connected.

So, our first step is to consider the column space of all of the vectors within our sets in our basis of directed loops. We can visualize this with a matrix:

$$
\begin{pmatrix}
-1 & 0 & 1 & -1 & 0 & 1 \\
1 & -1 & 0 & 0 & 0 & 0 \\
0 & 1 & -1 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & -1 & 0 \\
0 & 0 & 0 & 0 & 1 & -1 \\
0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 \\
\end{pmatrix}
$$

Lets put this as a numpy array. We want to consider the vector from 2 to 5, which looks like $$\begin{pmatrix}
0 \\
-1 \\
0 \\
0 \\
1 \\
0 \\
0  
\end{pmatrix} $$.

We want to test if this is in the column space of our matrix! We can check this by solving Ax = b.

To do this, we will use a least squares solver and show that this produces a geniune solution to Ax = b, so we know that b is in the column space of A.

```{python}
#| colab: {base_uri: 'https://localhost:8080/'}

A = np.array([
    [-1, 0, 1, -1, 0, 1],
    [1, -1, 0, 0, 0, 0],
    [0, 1, -1, 0, 0, 0],
    [0, 0, 0, 1, -1, 0],
    [0, 0, 0, 0, 1, -1],
    [0, 0, 0, 0, 0, 0],
    [0, 0, 0, 0, 0, 0]
])

b= np.array([
    [0],
    [-1],
    [0],
    [0],
    [1],
    [0],
    [0]
])

x, residuals, _, _ = np.linalg.lstsq(A, b, rcond=None)


print("Does Ax = b:", np.allclose(A@x, b))





```

Now lets make sure that is doesn't work when it should not! Lets show that it will not detect a connection between node 7 and 1. Lets see if the following vector is in the column space of the matrix from above. $$\begin{pmatrix}
1 \\
0 \\
0 \\
0 \\
0 \\
0 \\
-1  
\end{pmatrix} $$

To do this, we will use a least squares solver and show that this will produce a non-genuine solution, i.e. Ax = b is not consistent:

```{python}
#| colab: {base_uri: 'https://localhost:8080/'}

A = np.array([
    [-1, 0, 1, -1, 0, 1],
    [1, -1, 0, 0, 0, 0],
    [0, 1, -1, 0, 0, 0],
    [0, 0, 0, 1, -1, 0],
    [0, 0, 0, 0, 1, -1],
    [0, 0, 0, 0, 0, 0],
    [0, 0, 0, 0, 0, 0]
])

b = np.array([
    [1],
    [0],
    [0],
    [0],
    [1],
    [0],
    [-1]
])

x, residuals, _, _ = np.linalg.lstsq(A, b, rcond=None)


print("Does Ax = b:", np.allclose(A@x, b))



```

Therefore, we see that our method is working! In this way, give a basis of directed loops, our theorem gives us an algorithms that we can use to know if any two nodes are connected in a loop!

A question for future exploration may be: how can one efficiently obtain such a basis, especially for graphs with a large number of nodes (can

# Image Compression and Text Detection

See Quarto Document

# Least Squares

We will use the sports NFL data from [GitHub](https://github.com/devstopfix/nfl_results/blob/master/nfl%201978.csv).

```{python}
#| colab: {base_uri: 'https://localhost:8080/', height: 365}
import pandas as pd
#from google.colab import drive

#drive.mount('/content/drive')
#df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/nfl 1978.csv')
df = pd.read_csv('nfl 2014.csv')
data = df
```

Let us cut this data in half. We will get an estimate with this data and compare our estimated potential to the other half later.

```{python}
df = df.head(133)
```

Now let us put everything into a graph. We have very good motivation for this. Let us discuss this in detail. Our goal for this project is to find a way to estimate the score spreads of unknown games from existing data regarding the score spreads of the teams that play each other. To do this, we will consider a linear system of equations and fit this system of equations by the normal equations method.

Now, why do we want to use the graph formalism? The system of equations we will define will be very close to the incidence matrix of a graph. How? Let us check out a simplified example first. Suppose we have three teams A, B, C. Using the same formalism as the problem, the table will be given as follows, the first column and row will be A, second will be B, and third will be C:
$\begin{bmatrix} & 2 & 3 \\ 3 &  & 1 \\ 8 & 2 &   \end{bmatrix}$. For example, from this matrix we know that team A played against team B and team A had a score of 2, team B had a score of 3. (This table representation is not important for us here, because we will analyze the data straight from the imported csv) Then, the absolute value of the score difference will be 1. One linear equation we have is then going to be $x(A) - x(B) = 1$, where the X will be the three dimension vector $\begin{bmatrix} x(A) \\ x(B) \\ x(C) \end{bmatrix}$ and 1 is the absolute value of the score difference. Hopefully you can already see the resemblance! From this one system of equation we can get the matrix $\begin{bmatrix}  1 & -1 & 0 \end{bmatrix}\begin{bmatrix}  x(A) \\ x(B) \\ x(C) \end{bmatrix} = 1$. If we take the transpose of $\begin{bmatrix}  1 & -1 & 0 \end{bmatrix}$, it looks like a column of the incidence matrix of a graph! Note that I intentionally made the equation $x(A) - x(B) = 1$, which represents the edge B goes to A, representing that B wins over A. Then we can put this into a graph, the incidence matrix of this graph will have that transpose as a column vector! Therefore, the graph formalism will provide us with an easy way to construct our systeam of linear equations.

Ok. First let us define a dictionary to include all of the teams. We will introduce some code to delete extra games that two teams play. This is because if we don't, we will get different data for the same paramters (same teams). If that happens, for our incidence matrix we will get more than one row with -1 and 1 in the same columns, leading to two linearly dependent row vectors! Then, the transpose of the incidence matrix will not have full column rank, which results in the normal equation matrix $A^TA$, where $A$ is the tranpose to the incidence matrix, becoming uninvertible.

```{python}
# Get team names
teamNames = pd.concat([df["home_team"], df["visiting_team"]]).reset_index(drop=True)
uniqueTeamNames = teamNames.unique()
teamIndex = {team: index for index, team in enumerate(uniqueTeamNames)}

numberOfTeams = len(teamIndex)

# Construct the vertices for the graph. The vertice will correspond to the sports
# team in the teamIndex dictionary.
teamGraph = nx.DiGraph()
teamGraph.add_nodes_from(range(numberOfTeams))

# Cut the graph into the columns we need
teamTable = df[["home_team", "home_score", "visitors_score", "visiting_team"]]

# Iterate and look at the wins and score differences
edges = []
repeatChecker = [["null","null"]]
weights = []

for index, row in teamTable.iterrows():
  ticker = 0
  # Checking for repeating games. We will skip if two teams already played a game.
  for teamPair in repeatChecker:
    if (row["home_team"] == teamPair[0]) & (row["visiting_team"] == teamPair[1]):
      ticker = 1
      break
    elif (row["home_team"] == teamPair[1]) & (row["visiting_team"] == teamPair[0]):
      ticker = 1
      break

  if ticker == 1:
    continue

  scoreDiff = row["home_score"] - row["visitors_score"]
  weight = abs(scoreDiff)
  weights.append(weight)

  if scoreDiff >= 0:
    # The home team won or tied against the visiting team. We count that as wins.
    # the edge will be from the home team to the losing team
    edge = (teamIndex[row["home_team"]], teamIndex[row["visiting_team"]], weight)
    edges.append(edge)

  elif scoreDiff < 0:
    # The home team lost against the visiting team.
    edge = (teamIndex[row["visiting_team"]], teamIndex[row["home_team"]], weight)
    edges.append(edge)

  # Appending the pair of teams to the repeatChecker
  repeatChecker.append([row["visiting_team"], row["home_team"]])

teamGraph.add_weighted_edges_from(edges)
nx.draw_circular(teamGraph, with_labels=True)
plt.show()

print(len(edges))

print("The incidence matrix: ")
incidence = nx.incidence_matrix(teamGraph, oriented=True).todense(
)
print(incidence)
```

We have placed every team into distinct indices. Now, we should construct the normal equations. The b is going to be the weights vector. The A will have to be the transpose of the incidence matrix.

```{python}
A = np.transpose(incidence)
print(A)

b = np.array(weights)

ATA = incidence @ A

if np.linalg.det(ATA) == 0:
  print("A transpose times A is noninvertible!")
else:
  print("A tranpose times A is invertible!")
```

Great, we have determined that A transpose times A is invertible. This result is not surprising considering we only consider one game per pair of teams. This allows us to solve the normal equations very easily. We will simply find the inverse to ATA. The inverse ATA applied to b will give us the best estimate for the x potential vector.

$A^T A x = A^T b\\ \Rightarrow x = (A^TA)^{-1} A^Tx$

```{python}
inverseATA = np.linalg.inv(ATA)
print(np.shape(A))
print(np.shape(incidence))
print(np.shape(inverseATA))
print(np.shape(b))

xEstimate = inverseATA @ incidence @ b
```

The xEstimate will then give us the best linear estimate for the potential value for every team. Let us put the results in a nicer format.

```{python}
def get_key_from_value(dictionary, value):
    for key, val in dictionary.items():
        if val == value:
            return key
    return "Key not found"


for index in range(np.size(xEstimate)):
  print(f"Potential for the {get_key_from_value(teamIndex, index)}: {np.round(xEstimate[index], decimals=2)}")

```

Nice, now we have a general estimate for the potentials of each team. Let us see how well this potential estimate does against our actual data. We need to first reconstruct our graph by adding the new data.

```{python}
teamTable = data[["home_team", "home_score", "visitors_score", "visiting_team"]]
for index, row in teamTable.iterrows():
  ticker = 0
  # Adding a condition to skip the previous rows
  if index < 133:
    continue
  # Checking for repeating games. We will skip if two teams already played a game.
  for teamPair in repeatChecker:
    if (row["home_team"] == teamPair[0]) & (row["visiting_team"] == teamPair[1]):
      ticker = 1
      break
    elif (row["home_team"] == teamPair[1]) & (row["visiting_team"] == teamPair[0]):
      ticker = 1
      break

  if ticker == 1:
    continue

  scoreDiff = row["home_score"] - row["visitors_score"]
  weight = abs(scoreDiff)
  weights.append(weight)

  if scoreDiff >= 0:
    # The home team won or tied against the visiting team. We count that as wins.
    # the edge will be from the home team to the losing team
    edge = (teamIndex[row["home_team"]], teamIndex[row["visiting_team"]], weight)
    edges.append(edge)

  elif scoreDiff < 0:
    # The home team lost against the visiting team.
    edge = (teamIndex[row["visiting_team"]], teamIndex[row["home_team"]], weight)
    edges.append(edge)

  # Appending the pair of teams to the repeatChecker
  repeatChecker.append([row["visiting_team"], row["home_team"]])

teamGraph.add_weighted_edges_from(edges)
nx.draw_circular(teamGraph, with_labels=True)
plt.show()

print(len(edges))

print("The incidence matrix: ")
incidence = nx.incidence_matrix(teamGraph, oriented=True).todense(
)
print(incidence)
```

Now let us get the estimated score spreads. We will multiply ATA by our xEstimate.

```{python}
A = np.transpose(incidence)
print(A)

b = np.array(weights)

estimatedScoreSpreads = A @ xEstimate

print(np.shape(estimatedScoreSpreads))

print("Percent difference between the estimate and the actual score spread for the game between:")
deviationDistribution = []

for index in range(np.size(estimatedScoreSpreads)):
  difference = np.abs(np.abs(estimatedScoreSpreads[index]) - np.abs(b[index])) / np.abs(b[index])
  print(f"{get_key_from_value(teamIndex, edges[index][0])} and {get_key_from_value(teamIndex, edges[index][1])}: {np.round(difference, decimals=2)}")
  deviationDistribution.append(difference)
```

```{python}
import matplotlib.pyplot as plt
dd=np.array(deviationDistribution)
dd=dd[np.isfinite(dd)]
# Create plots
plt.figure(figsize=(10, 5))
# Plot histogram of the data
plt.hist(dd, bins=50, density=False, alpha=0.6, color='g', label='Data Histogram')

# Add title and labels
plt.title('Histogram of all percent differences for 173 games')
plt.xlabel('Absolute percent Difference between estimated score spread and actual score spread')
plt.ylabel('Number of occurences')

# Add legend
plt.legend()

# Show the plot
plt.show()

```

I define the absolute percent difference for each game as $\frac{|\text{Estimated Score Spread}| - |\text{Actual Score Spread}|}{|\text{Actual Score Spread}|}$. We used 117 games to construct our normal equations fit and tested it with 173 games. From the histogram plot, we can see that our normal equation estimate works surprisingly well in estimating the score spreads of these games, with a small percentage of the games deviating more from the percent difference estimate. Now, let us try a different metric because this percent difference equation gives us a lot of leeway. If the estimate is -1 and the actual is 1, then we will get 0 for the percent difference, which is not right! So let us try again, but this time define the percent difference equation as $\frac{\text{Estimated Score Spread} - \text{Actual Score Spread}}{\text{Actual Score Spread}}$.

```{python}
deviationDistribution = []

print("Percent difference between the estimate and the actual score spread for the game between:")
for index in range(np.size(estimatedScoreSpreads)):
  difference = ((estimatedScoreSpreads[index]) - b[index]) / b[index]
  print(f"{get_key_from_value(teamIndex, edges[index][0])} and {get_key_from_value(teamIndex, edges[index][1])}: {np.round(difference, decimals=2)}")
  deviationDistribution.append(difference)

# Create plots
plt.figure(figsize=(10, 5))
# Plot histogram of the data
dd=np.array(deviationDistribution)
dd=dd[np.isfinite(dd)]
plt.hist(dd, bins=50, density=False, alpha=0.6, color='g', label='Data Histogram')

# Add title and labels
plt.title('Histogram of all percent differences for 173 games')
plt.xlabel('Percent Difference between estimated score spread and actual score spread')
plt.ylabel('Number of occurences')

# Add legend
plt.legend()

# Show the plot
plt.show()
```

We can see a more reasonable distribution. But it is still cool to see that our seems to not deviate much from the actual score difference, as the majority of the score percent differences lay between 0 and 2 percent. Also, how does the histogram look? Let us try a guassian fit! It will probably not be a perfect fit.

```{python}
from scipy.stats import norm

data = deviationDistribution

mean = np.mean(data) - 0.4
std = 1/2.95 * np.std(data)

# Generate points on the x axis:
x = np.linspace(min(data), max(data), 1000)
# Calculate the PDF of the normal distribution
pdf = norm.pdf(x, mean, std)

# Create plots
plt.figure(figsize=(10, 5))
# Plot histogram of the data
dd=np.array(data)
dd=dd[np.isfinite(dd)]
plt.hist(dd, bins=100, density=True, alpha=0.6, color='g', label='Data Histogram')
# Plot the Gaussian PDF
plt.plot(x, pdf, 'k', linewidth=2, label='Gaussian PDF')

# Add title and labels
plt.title(f'Histogram and Gaussian Fit of the Percent Difference Distribution \n mean = {np.round(mean, decimals=2)}, standard deviation = {np.round(std, decimals=2)}')
plt.xlabel('Percent Difference Between Actual and Estimate')
plt.ylabel('Normalized Probability')

# Add legend
plt.legend()

# Show the plot
plt.show()
```

I artifically decreased the calculated standard deviation to make the fit correspond more to the data. We can see that this Guassian fit further supports the efficacy of the model. Most of the game prediction compared to the actual score spreads seems to have a percent change of -1 percent, which is quite small. However, we cannot say that this model is perfect as there are many limitations to our normal equations approach. First, we ground our fit in the assumption that the potentials that we define for each team accurately reflect the properties of each team. Specifically, we assume that knowing the potential of each team will give us enough information to determine all of their possible wins and losses. We also assume that the potentials for each team are independent of one another. These two assumptions are very likely to be false. External factors like how well each team member play in different games, luck, weather, and the conditions of each team member may very well make this "potential", if it even exists, a very complicated function with many variables. However, there is still merit to this technique. It makes sense for powerful teams to have higher potentials, less skilled teams to have lower potentials. Therefore, the score spreads should in theory be similar to the difference of the potentials. This normal equations technique provides a very simple, easy to understand way of getting a quick prediction as opposed to machine learning and other techniques which may require more time or more data.  

